<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Machine Learning——支持向量机</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/SupportVectorMachine%20/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/SupportVectorMachine%20/</url>
    
    <content type="html"><![CDATA[<h1 id="Support-Vector-Machine-支持向量机"><a href="#Support-Vector-Machine-支持向量机" class="headerlink" title="Support Vector Machine 支持向量机"></a>Support Vector Machine 支持向量机</h1><p>支持向量机（SVM）的核心关键词： 间隔、对偶、核函数</p><p>SVM在分类问题中有着重要地位，其主要思想是最大化两类之间的间隔。按照数据集的特点：</p><ol><li>线性可分问题，如之前的感知机算法处理的问题</li><li>线性可分，只有一点点错误点，如感知机算法发展出来的 Pocket 算法处理的问题</li><li>非线性问题，完全不可分，如在感知机问题发展出来的多层感知机和深度学习</li></ol><p>这三种情况对于 SVM 分别有下面三种处理手段：</p><ol><li>Hard-margin SVM</li><li>Soft-margin SVM</li><li>Kernel SVM</li></ol><p>SVM 的求S解中，大量用到了 Lagrange 乘子法，首先对这种方法进行介绍。</p><h2 id="约束优化问题"><a href="#约束优化问题" class="headerlink" title="约束优化问题"></a>约束优化问题</h2><p>一般地，约束优化问题（原问题）可以写成：<br>$$<br>\begin{align}</p><p>&amp;\min_{x\in\mathbb{R^p}}f(x)\<br>&amp;s.t.\ m_i(x)\le0,i&#x3D;1,2,\cdots,M\<br>&amp;\ \ \ \ \ \ \ \ n_j(x)&#x3D;0,j&#x3D;1,2,\cdots,N</p><p>\end{align}<br>$$<br>定义 Lagrange 函数：<br>$$<br>L(x,\lambda,\eta)&#x3D;f(x)+\sum\limits_{i&#x3D;1}^M\lambda_im_i(x)+\sum\limits_{i&#x3D;1}^N\eta_in_i(x)<br>$$<br>那么原问题可以等价于无约束形式：<br>$$<br>\min_{x\in\mathbb{R}^p}\max_{\lambda,\eta}L(x,\lambda,\eta)\ s.t.\ \lambda_i\ge0<br>$$</p><ul><li>如果$x$违反了约束$m_i(x)$，则：</li></ul><p>$$<br>m_i(x)&gt;0 \implies \max_{\lambda} L&#x3D; \infty<br>$$</p><ul><li>如果$x$符合约束$m_i(x)$，则</li></ul><p>$$<br>m_i(x) \leq 0 \implies \exists \max_\lambda L<br>$$</p><p>这是由于，当满足原问题的不等式约束的时候，$\lambda_i&#x3D;0$ 才能取得最大值，直接等价于原问题，如果不满足原问题的不等式约束，那么最大值就为 $+\infin$，由于需要取最小值，于是不会取到这个情况。这里的前后变量$x$都是属于$\mathbb{R}^p$但是原式的$x$代表的是可取整个样本空间，后一个变量$x$则是代表符合条件的好的样本。</p><p>这个问题的对偶形式：<br>$$<br>\max_{\lambda,\eta}\min_{x\in\mathbb{R}^p}L(x,\lambda,\eta)\ s.t.\ \lambda_i\ge0<br>$$<br>对偶问题是关于 $ \lambda, \eta$ 的最大化问题。</p><p>由于：<br>$$<br>\max_{\lambda_i,\eta_j}\min_{x}L(x,\lambda_i,\eta_j)\le\min_{x}\max_{\lambda_i,\eta_j}L(x,\lambda_i,\eta_j)<br>$$</p><blockquote><p>  证明：显然有 $\min\limits_{x}L\le L\le\max\limits_{\lambda,\eta}L$，于是显然有 $\max\limits_{\lambda,\eta}\min\limits_{x}L\le L$，且 $\min\limits_{x}\max\limits_{\lambda,\eta}L\ge L$。</p></blockquote><p>对偶问题的解小于原问题，有两种情况：</p><ol><li>强对偶：可以取等于号</li><li>弱对偶：不可以取等于号</li></ol><p>$$<br>G:{(m_1(x),f(x)) \ | \  x \in D }\<br>p^* &#x3D; inf { f(x) \ | \  (m_1(x),f(x)) \in G }\<br>d^* &#x3D; \max_{\lambda} \min_{x} L(x,\lambda) &#x3D; \max_\lambda g(\lambda)<br>$$<br>对于一个凸优化问题，有如下定理：</p><blockquote><p>  如果凸优化问题满足某些条件如 Slater 条件，那么它和其对偶问题满足强对偶关系。记问题的定义域为：$\mathcal{D}&#x3D;domf(x)\cap dom m_i(x)\cap domn_j(x)$。于是 Slater 条件为：<br>  $$<br>  \exist\hat{x}\in Relint , \mathcal{D}\ s.t.\ \forall i&#x3D;1,2,\cdots,M,m_i(x)\lt0<br>  $$<br>  其中 Relint 表示相对内部（不包含边界的内部）。</p></blockquote><ol><li>对于大多数凸优化问题，Slater 条件成立。</li><li>松弛 Slater 条件，如果 M 个不等式约束中，有 K 个函数为仿射函数，那么只要其余的函数满足 Slater 条件，该凸二次规划问题就是个强对偶问题。</li></ol><p>上面介绍了原问题和对偶问题的对偶关系，但是实际还需要对参数进行求解，求解方法使用 KKT 条件进行:（原始可行性、原始最优化、对偶可行性、互补松弛条件）</p><blockquote><p>  KKT 条件和强对偶关系是等价关系。KKT 条件对最优解的条件为：</p><ol><li><p>可行域：<br>$$<br>\begin{align}<br>m_i(x^*)\le0\<br>n_j(x^*)&#x3D;0\<br>\lambda^*\ge0<br>\end{align}<br>$$</p></li><li><p>互补松弛 $\lambda^<em>m_i(x^</em>)&#x3D;0,\forall m_i$，对偶问题的最佳值为 $d^*$，原问题为 $p^*$<br>$$<br>\begin{align}<br>d^*&amp;&#x3D;\max_{\lambda,\eta}g(\lambda,\eta)&#x3D;g(\lambda^*,\eta^*)\nonumber\<br>&amp;&#x3D;\min_{x}L(x,\lambda^*,\eta^*)\nonumber\<br>&amp;\le L(x^*,\lambda^*,\eta^*)\nonumber\<br>&amp;&#x3D;f(x^*)+\sum\limits_{i&#x3D;1}^M\lambda^<em>m_i(x^</em>)\nonumber\<br>&amp;\le f(x^*)&#x3D;p^*<br>\end{align}<br>$$<br>为了满足相等，两个不等式必须成立，于是，对于第一个不等于号，需要有梯度为0条件，对于第二个不等于号需要满足互补松弛条件。</p></li><li><p>对偶可行，梯度为0： $\frac{\partial L(x,\lambda^*,\eta^*)}{\partial x}|_{x&#x3D;x^*}&#x3D;0$</p></li></ol></blockquote><h2 id="Hard-margin-SVM"><a href="#Hard-margin-SVM" class="headerlink" title="Hard-margin SVM"></a>Hard-margin SVM</h2><p>支撑向量机也是一种硬分类模型，在之前的感知机模型中，我们在线性模型的基础上叠加了符号函数，在几何直观上，可以看到，如果两类分的很开的话，那么其实会存在无穷多条线可以将两类分开。在 SVM 中，我们引入最大化间隔这个概念，间隔指的是数据和直线的距离的最小值，因此最大化这个值反映了我们的模型倾向。解决了perception存在多解的问题。</p><p>分割的超平面可以写为：<br>$$<br>0&#x3D;w^Tx+b<br>$$<br>那么最大化间隔（约束为分类任务的要求）：<br>$$<br>\mathop{argmax}<em>{w,b}[\min_i\frac{|w^Tx_i+b|}{||w||}]\quad s.t.\ y_i(w^Tx_i+b)&gt;0\\Longrightarrow\mathop{argmax}</em>{w,b}[\min_i\frac{y_i(w^Tx_i+b)}{||w||}] \quad s.t.\ y_i(w^Tx_i+b)&gt;0<br>$$<br>对于这个约束 $y_i(w^Tx_i+b)&gt;0$，不妨固定 $\min y_i(w^Tx_i+b)&#x3D;1&gt;0$，这是由于分开两类的超平面的系数经过比例放缩不会改变这个平面，这也相当于给超平面的系数作出了约束。化简后的式子可以表示为：<br>$$<br>\mathop{argmin}<em>{w,b}\frac{1}{2}w^Tw \quad s.t.\ \min_iy_i(w^Tx_i+b)&#x3D;1\\Rightarrow\mathop{argmin}</em>{w,b}\frac{1}{2}w^Tw \quad s.t.\ y_i(w^Tx_i+b)\ge1,i&#x3D;1,2,\cdots,N<br>$$<br>这就是一个包含 $N$ 个约束的凸优化问题，有很多求解这种问题的软件。</p><blockquote><p>凸集、凸条件、凸函数的性质：</p><ul><li><p>凸集：一个点集，如果其中任意两个点$x_1,x_2$的线段，均包含在集合内，这样的点集成为凸集。</p></li><li><p>凸条件：</p><ol><li><p>一阶：$f(x)$定义在凸集上，且具有连续的一阶导数的函数，则$f(x)$满足$\quad \forall x_1,x_2,f(x_1)&gt;f(x_2)+(x_2-x_1)^T \nabla f(x_1)$</p></li><li><p>二阶：f(x)$定义在凸集上，且具有连续的二阶导数的函数。</p></li></ol><p>   ​充要条件：Hessen矩阵在$\mathbb{R}$上处处半正定。<br>   $$<br>   A&#x3D;\begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1x_2 } \<br>   \frac{\partial^2 f}{\partial x_2x_1 } &amp;\frac{\partial^2 f}{\partial x_2^2 }\end{pmatrix}  ,\forall x,x^TAx \geq 0 \iff A是半正定矩阵<br>   $$</p></li><li><p>凸函数的性质：</p><ol><li>若给定一点$x_0$，集合$R &#x3D; {x| f(x)&lt;f(x_0)}$为凸集</li><li>可行域$R &#x3D; {x| g_i(x) \leq 0,i&#x3D; 1,2, \cdots,}$为凸集</li><li>凸规划的任何局部最优解就是全局最优解</li></ol></li></ul></blockquote><p>但是，如果样本数量或维度非常高，直接求解困难甚至不可解，于是需要对这个问题进一步处理。引入Lagrange 函数：<br>$$<br>L(w,b,\lambda)&#x3D;\frac{1}{2}w^Tw+\sum\limits_{i&#x3D;1}^N\lambda_i(1-y_i(w^Tx_i+b))<br>$$<br>我们有原问题就等价于：<br>$$<br>\mathop{argmin}<em>{w,b}\max</em>{\lambda}L(w,b,\lambda_i)\ s.t.\ \lambda_i\ge0<br>$$<br>我们交换最小和最大值的符号得到对偶问题：<br>$$<br>\max_{\lambda_i}\min_{w,b}L(w,b,\lambda_i)\ s.t.\ \lambda_i\ge0<br>$$<br>由于不等式约束是仿射函数（仿射函数是一阶多项式），对偶问题和原问题等价：</p><ul><li><p>$b$：$\frac{\partial}{\partial b}L&#x3D;0\Rightarrow\sum\limits_{i&#x3D;1}^N\lambda_iy_i&#x3D;0$</p></li><li><p>$w$：首先将 $b$ 代入：<br>$$<br>\begin{align}L(w,b,\lambda_i)&amp;&#x3D;\frac{1}{2}w^Tw+\sum\limits_{i&#x3D;1}^N\lambda_i(1-y_iw^Tx_i-y_ib)\ &amp;&#x3D;\frac{1}{2}w^Tw+\sum\limits_{i&#x3D;1}^N\lambda_i-\sum\limits_{i&#x3D;1}^N\lambda_iy_iw^Tx_i\end{align}<br>$$<br>所以：<br>$$<br>\frac{\partial}{\partial w}L&#x3D;0\Rightarrow w&#x3D;\sum\limits_{i&#x3D;1}^N\lambda_iy_ix_i<br>$$</p></li><li><p>将上面两个参数代入：<br>$$<br>L(w,b,\lambda_i)&#x3D;-\frac{1}{2}\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i&#x3D;1}^N\lambda_i<br>$$</p></li></ul><p>因此，对偶问题就是：<br>$$<br>\max_{\lambda}-\frac{1}{2}\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i&#x3D;1}^N\lambda_i,\quad s.t.\ \lambda_i\ge0 \<br>\implies \min_{\lambda}\frac{1}{2}\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i&#x3D;1}^N\lambda_i,\quad s.t.\ \lambda_i\ge0 \<br>$$<br>从 KKT条件 (Kaursh-Kuhn-Tucker Conditions)得到超平面的参数：</p><blockquote><p>  原问题和对偶问题满足强对偶关系的充要条件为其满足 KKT 条件：<br>  $$<br>  \begin{align}<br>  &amp;\frac{\partial L}{\partial w}&#x3D;0,\frac{\partial L}{\partial b}&#x3D;0<br>  \&amp;\lambda_k(1-y_k(w^Tx_k+b))&#x3D;0(slackness\ complementary)\<br>  &amp;\lambda_i\ge0\<br>  &amp;1-y_i(w^Tx_i+b)\le0<br>  \end{align}<br>  $$</p></blockquote><p>根据这个条件就得到了对应的最佳参数：<br>$$<br>\hat{w}&#x3D;\sum\limits_{i&#x3D;1}^N\lambda_iy_ix_i\<br>\hat{b}&#x3D;y_k-w^Tx_k&#x3D;y_k-\sum\limits_{i&#x3D;1}^N\lambda_iy_ix_i^Tx_k,\exist k,1-y_k(w^Tx_k+b)&#x3D;0<br>$$<br>于是这个超平面的参数 $w$ 就是数据点的线性组合，最终的参数值就是部分满足 $y_i(w^Tx_i+b)&#x3D;1$向量的线性组合（互补松弛条件给出），这些向量也叫支撑向量。由于SVM解的稀疏性：大部分训练样本不保留，最终模型仅与支持向量有关。</p><p>![屏幕截图 2021-10-25 162156](E:\PHOTO\learning\屏幕截图 2021-10-25 162156.png)</p><h2 id="Soft-margin-SVM"><a href="#Soft-margin-SVM" class="headerlink" title="Soft-margin SVM"></a>Soft-margin SVM</h2><p>Hard-margin 的 SVM 只对可分数据可解，如果不可分的情况，我们的基本想法是在损失函数中加入错误分类的可能性。错误分类的个数可以写成：<br>$$<br>error&#x3D;\sum\limits_{i&#x3D;1}^N\mathbb{I}{y_i(w^Tx_i+b)\lt1}<br>$$<br>这个指示函数不连续，可以将其改写为：<br>$$<br>if \quad 1-y_i(w^Tx_i+b) &gt;1   \implies error &#x3D;0<br>\ if \quad 1-y_i(w^Tx_i+b) &lt;1   \implies error &#x3D;1-y_i(w^Tx_i+b)\<br>Conclusion:error&#x3D;\sum\limits_{i&#x3D;1}^N\max{0,1-y_i(w^Tx_i+b)}<br>$$<br>求和符号中的式子又叫做 Hinge Function。</p><p>将这个错误加入 Hard-margin SVM 中，于是：<br>$$<br>\mathop{argmin}<em>{w,b}\frac{1}{2}w^Tw+C\sum\limits</em>{i&#x3D;1}^N\max{0,1-y_i(w^Tx_i+b)}\ s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,i&#x3D;1,2,\cdots,N<br>$$<br>这个式子中，常数 $C$ 可以看作允许的错误水平，同时上式为了进一步消除 $\max$ 符号，对数据集中的每一个观测，我们可以认为其大部分满足约束，但是其中部分违反约束，因此这部分约束变成 $y_i(w^Tx+b)\ge1-\xi_i$，其中 $\xi_i&#x3D;1-y_i(w^Tx_i+b)$，进一步的化简：<br>$$<br>\mathop{argmin}<em>{w,b}\frac{1}{2}w^Tw+C\sum\limits</em>{i&#x3D;1}^N\xi_i \ s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i&#x3D;1,2,\cdots,N<br>$$</p><p>![屏幕截图 2021-10-25 163525](E:\PHOTO\learning\屏幕截图 2021-10-25 163525.png)</p><h2 id="Kernel-Method"><a href="#Kernel-Method" class="headerlink" title="Kernel Method"></a>Kernel Method</h2><p>核方法可以应用在很多问题上，在分类问题中，对于严格不可分问题，我们引入一个特征转换函数将原来的不可分的数据集变为可分的数据集，然后再来应用已有的模型。往往将低维空间的数据集变为高维空间的数据集后，数据会变得可分（数据变得更为稀疏）：</p><blockquote><p>  Cover Theorem：高维空间比低维空间更易线性可分。</p></blockquote><p>应用在 SVM 中时，观察上面的 SVM 对偶问题：<br>$$<br>\max_{\lambda}-\frac{1}{2}\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i&#x3D;1}^N\lambda_i,\ s.t.\ \lambda_i\ge0<br>$$<br>在求解的时候需要求得内积，于是不可分数据在通过特征变换后，需要求得变换后的内积。我们常常很难求得变换函数的内积。于是直接引入内积的变换函数：<br>$$<br>\forall x,x’\in\mathcal{X},\exist\phi\in\mathcal{H}:x\rightarrow z\ s.t.\ k(x,x’)&#x3D;\phi(x)^T\phi(x)<br>$$<br>称 $k(x,x’)$ 为一个正定核函数，其中$\mathcal{H}$ 是 Hilbert 空间（完备的、可能是无线维的被赋予内积的线性空间）。</p><ul><li><p>完备的： 极限值是封闭的<br>$$<br>{k_n} \to \lim_{n \to \infty}{k_n} &#x3D; K \in \mathcal{H}<br>$$</p></li><li><p>内积：$&lt;a,b&gt; &#x3D; |a||b|\cos\theta$  线性、正定性、对称性</p></li><li><p>线性空间：向量空间&lt;加法和数乘&gt; 是个封闭空间</p></li></ul><blockquote><p>  $k(x,x’)&#x3D;\exp(-\frac{(x-x’)^2}{2\sigma^2})$ 是一个核函数。</p><p>  证明：<br>  $$<br>  \begin{align}<br>  \exp(-\frac{(x-x’)^2}{2\sigma^2})&amp;&#x3D;\exp(-\frac{x^2}{2\sigma^2})\exp(\frac{xx’}{\sigma^2})\exp(-\frac{x’^2}{2\sigma^2})\nonumber\<br>  &amp;&#x3D;\exp(-\frac{x^2}{2\sigma^2})\sum\limits_{n&#x3D;0}^{+\infin}\frac{x^nx’^n}{\sigma^{2n}n!}\exp(-\frac{x’^2}{2\sigma^2})\nonumber\<br>  &amp;&#x3D;\exp(-\frac{x^2}{2\sigma^2})\varphi(x)\varphi(x’)\exp(-\frac{x’^2}{2\sigma^2})\nonumber\<br>  &amp;&#x3D;\phi(x)\phi(x’)<br>  \end{align}<br>  $$</p></blockquote><p>正定核函数有下面的等价定义：</p><blockquote><p>  如果核函数满足：</p><ol><li>对称性</li><li>正定性</li></ol><p>  那么这个核函数时正定核函数。</p><p>  证明：</p><ol><li>对称性 $\Leftrightarrow$ $k(x,z)&#x3D;k(z,x)$，显然满足内积的定义</li><li>正定性 $\Leftrightarrow$ $\forall N,x_1,x_2,\cdots,x_N\in\mathcal{X}$，对应的 Gram Matrix $K&#x3D;[k(x_i,x_j)]$ 是半正定的。</li></ol><p>  要证：$k(x,z)&#x3D;\phi(x)^T\phi(z)\Leftrightarrow K$ 半正定+对称性。</p><ol><li><p>$\Rightarrow$：首先，对称性是显然的，对于正定性：<br>$$<br>K&#x3D;\begin{pmatrix}k(x_1,x_2)&amp;\cdots&amp;k(x_1,x_N)\\vdots&amp;\vdots&amp;\vdots\k(x_N,x_1)&amp;\cdots&amp;k(x_N,x_N)\end{pmatrix}<br>$$<br>任意取 $\alpha\in\mathbb{R}^N$，即需要证明 $\alpha^TK\alpha\ge0$：<br>$$<br>\begin{align}\alpha^TK\alpha&amp;&#x3D;\sum\limits_{i,j}\alpha_i\alpha_jK_{ij}&#x3D;\sum\limits_{i,j}\alpha_i\phi^T(x_i)\phi(x_j)\alpha_j\ &amp;&#x3D;\sum\limits_{i}\alpha_i\phi^T(x_i)\sum\limits_{j}\alpha_j\phi(x_j) \end{align}<br>$$<br>这个式子就是内积的形式，Hilbert 空间满足线性性，于是正定性的证。</p></li><li><p>$\Leftarrow$：对于 $K$ 进行分解，对于对称矩阵 $K&#x3D;V\Lambda V^T$，那么令 $\phi(x_i)&#x3D;\sqrt{\lambda_i}V_i$，其中 $V_i$是特征向量，于是就构造了 $k(x,z)&#x3D;\sqrt{\lambda_i\lambda_j}V_i^TV_j$</p></li></ol></blockquote><p>常见的核函数：</p><ul><li>linear kernel:$k(x_i,x_j)&#x3D;x_i^T x_j$ </li><li>polynomial kernel:$k(x_i,x_j)&#x3D;(x_i^T x_j)^d   \ \ d\geq1$为多项式的次数 </li><li>Gauss kernel:$k(x_i,x_j)&#x3D;\exp(-\frac{||(x_i-x_j)||^2}{2\sigma^2}) \ \  \sigma &gt;0$ 为高斯核的宽度</li><li>Laplace kernel:$k(x_i,x_j)&#x3D;\exp(-\frac{||(x_i-x_j)||}{\sigma^2}) \ \  \sigma &gt;0$</li><li>Sigmoid kernel:$k(x_i,x_j)&#x3D;tanh(\beta x_i^T x_j + \theta)\ \  \beta &gt;0 \ \ \theta&lt;0 \ \ tanh$是双曲正切函数</li></ul><h2 id="SMO-Sequential-Minimal-Optimization"><a href="#SMO-Sequential-Minimal-Optimization" class="headerlink" title="SMO: Sequential Minimal Optimization"></a>SMO: Sequential Minimal Optimization</h2><p>这是一种SVM学习的快速算法，其特点是不断地将原二次规划问题分解成只有两个变量的二次规划子问题。并对子问题进行解析求解，直到所有的变量都能满足KKT条件为止。</p><p>基本思路：</p><ol><li><p>选取一对需要更新的变量$\alpha_i$和$\alpha_j$</p></li><li><p>固定$\alpha_i$和$\alpha_j$以外的参数，求解对偶问题更新$\alpha_i$和$\alpha_j$</p></li></ol><p>   仅考虑$\alpha_i$和$\alpha_j$时，对偶问题的约束为：<br>   $$<br>   \alpha_iy_i+\alpha_jy_j&#x3D;-\sum_{k\neq i,j}\alpha_ky_k,\alpha_i \geq0,\alpha_j\geq 0<br>   $$</p><p>SMO算法要解决如下凸二次规划问题的对偶问题：<br>$$<br>\min_{\alpha} \quad \frac{1}{2}\sum^N_{i&#x3D;1}\sum^N_{j&#x3D;1}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum^N_{i&#x3D;1}\alpha_i \<br>s.t. \sum^N_{i&#x3D;1}\alpha_iy_i&#x3D;0 \quad 0\leq a_i \leq C, \ i&#x3D;1,2,\cdots,N<br>$$<br>则不失一般性，假设选取的两个变量$\alpha_1,\alpha_2$,其他变量$\alpha_i(i&#x3D;3,4,\cdots,\N)$是固定的。于是SMO的最优化问题的子问题可以写成：<br>$$<br>\min_{\alpha_1,\alpha_2} \quad W(\alpha_1,\alpha_2)&#x3D;\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+y_1y_2K_{12}\alpha_1\alpha_2-\(\alpha_1+\alpha_2)+y_1\alpha_1\sum^N_{i&#x3D;3}y_i\alpha_iK_{i1}+y_2\alpha_2\sum^N_{i&#x3D;3}y_i\alpha_iK_{i2}\<br>s.t. \quad \alpha_1y_1+\alpha_2y_2&#x3D;-\sum^N_{i&#x3D;3}y_i\alpha_{i}&#x3D;\varsigma \quad 0\leq \alpha_i \leq C,  i&#x3D;1,2<br>$$<br>对于该问题的<strong>变量选取方法</strong>：</p><p>第一个变量选取：外层循环。外层循环在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第一个变量。</p><p>第二个变量选取：内层循环。假设在外层循环中已经找到第一个变量$\alpha_1$,现在需要在内层循环中找到第二个变量$\alpha_2$。第二个变量选择的标准是希望能使$\alpha_2$有足够大的变化。</p><p>则对于该问题的<strong>求解方法</strong>：<br>$$<br>g(x)&#x3D;\sum^N_{i&#x3D;1}\alpha_iy_iK(x_i,x)+b\<br>E_i&#x3D;g(x_i)-y_i&#x3D;(\sum^N_{i&#x3D;1}\alpha_iy_iK(x_i,x)+b)-y_i \quad i&#x3D;1,2<br>$$</p><ul><li>$E_i$为函数$g(x)$对输入$x_i$的预测值与真实输出$y_i$之差。</li></ul><p>记$\alpha_2$的最优解$\alpha_2^{new,nuc}$，则最优化问题沿着约束方向未经剪辑时的解是<br>$$<br>\alpha_2^{new,nuc} &#x3D; \alpha_2^{old}+\frac{y_2(E_1-E_2)}{\eta}\<br>\eta&#x3D;K_{11}+K_{22}-2K_{12}&#x3D;||\phi(x_1)-\phi(x_2)||^2<br>$$</p><ul><li>$\phi（x）$是输入空间到特征空格的映射。</li></ul><p>经过剪辑后$\alpha_2$的解是<br>$$<br>\alpha_2^{new} &#x3D; \begin{cases} H,&amp;\alpha_2^{new,nuc} &gt;H \<br>    \alpha_2^{new,nuc}, &amp;L \leq \alpha_2^{new,nuc}\leq H \<br>    L,&amp;\alpha_2^{new,nuc}&lt;L<br> \end{cases}<br>$$</p><p>由$\alpha_2^{new}$求得$\alpha_1^{new}$是<br>$$<br>\alpha_1^{new}&#x3D;a_1^{old}+y_1y_2(\alpha_2^{old}-\alpha_2^{new})<br>$$</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h2><p>分类问题在很长一段时间都依赖 SVM，对于严格可分的数据集，Hard-margin SVM 选定一个超平面，保证所有数据到这个超平面的距离最大，对这个平面施加约束，固定 $y_i(w^Tx_i+b)&#x3D;1$，得到了一个凸优化问题并且所有的约束条件都是仿射函数，于是满足 Slater 条件，将这个问题变换成为对偶的问题，可以得到等价的解，并求出约束参数：<br>$$<br>\max_{\lambda}-\frac{1}{2}\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i&#x3D;1}^N\lambda_i,\ s.t.\ \lambda_i\ge0<br>$$<br>对需要的超平面参数的求解采用强对偶问题的 KKT 条件进行。<br>$$<br>\begin{align}<br>&amp;\frac{\partial L}{\partial w}&#x3D;0,\frac{\partial L}{\partial b}&#x3D;0<br>\&amp;\lambda_k(1-y_k(w^Tx_k+b))&#x3D;0(slackness\ complementary)\<br>&amp;\lambda_i\ge0\<br>&amp;1-y_i(w^Tx_i+b)\le0<br>\end{align}<br>$$<br>解就是：<br>$$<br>\hat{w}&#x3D;\sum\limits_{i&#x3D;1}^N\lambda_iy_ix_i\<br>\hat{b}&#x3D;y_k-w^Tx_k&#x3D;y_k-\sum\limits_{i&#x3D;1}^N\lambda_iy_ix_i^Tx_k,\ \exist k,1-y_k(w^Tx_k+b)&#x3D;0<br>$$<br>当允许一点错误的时候，可以在 Hard-margin SVM 中加入错误项。用 Hinge Function 表示错误项的大小，得到：<br>$$<br>\mathop{argmin}<em>{w,b}\frac{1}{2}w^Tw+C\sum\limits</em>{i&#x3D;1}^N\xi_i\ s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i&#x3D;1,2,\cdots,N<br>$$<br>对于完全不可分的问题，我们采用特征转换的方式，在 SVM 中，我们引入正定核函数来直接对内积进行变换，只要这个变换满足对称性和正定性，那么就可以用做核函数。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——机器学习引入</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/Preface/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/Preface/</url>
    
    <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a><strong>Preface</strong></h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a><strong>定义</strong></h2><p>Machine learning is a filed that bridge compation and statistics with ties to information ,signal process,algorithm control theroy and optimization theory.</p><p>Machine Learning &#x3D; Matrix + Optimization + Algorithm + Statistics</p><p>Tom Mitchell提出了一个更形式化的定义：假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过经验E在T中任务上得到了提升，则我们就说关于T和P，程序对E“学习”了。</p><table><thead><tr><th>发展</th><th></th><th></th></tr></thead><tbody><tr><td>1950s-1970s  逻辑推理时期</td><td></td><td></td></tr><tr><td>1970s-1990s  专家系统时期</td><td></td><td></td></tr><tr><td>1990s-至今     机器学习时期</td><td></td><td></td></tr></tbody></table><h2 id="基本分类："><a href="#基本分类：" class="headerlink" title="基本分类："></a>基本分类：</h2><ul><li>监督学习 supervised learning：有标记、有反馈、预测结果</li><li>无监督学习 Unsupervised learning : 无标记、无反馈、内在结构</li><li>强化学习 Reinforcement learning： 决策过程、奖励机制、学习一系列动作</li><li>半监督学习 Self-supervised learning： 利用未标注的数据中信息，辅助标记数据，进行监督学习，以较低成本达到较好的学习效果</li><li>主动学习 Active learning：机器不断主公给出实例让人标注，然后利用标注数据学习预测模型的机器学习问题。</li></ul><h2 id="任务："><a href="#任务：" class="headerlink" title="任务："></a>任务：</h2><p><strong>Regression、Classification、Clustering</strong></p><h2 id="模型偏好："><a href="#模型偏好：" class="headerlink" title="模型偏好："></a>模型偏好：</h2><p>Occam‘s Razor：若多个假设与观察一致，则选择最简单的那个</p><p>No Free Lunch Theorem：两个算法$\xi_a,\xi_b$没有绝对的好坏</p><h2 id="统计学习三要素：模型-策略-算法"><a href="#统计学习三要素：模型-策略-算法" class="headerlink" title="统计学习三要素：模型+策略+算法"></a>统计学习三要素：模型+策略+算法</h2><p><strong>模型</strong>：模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数)。</p><p>假设空间用$\mathcal{F}$表示。假设空间可以定义为决策函数的集合：$\mathcal{F} &#x3D; { f\   | Y&#x3D;f,(X)}$</p><p>其中，$\mathit{X}，\mathit{Y}$均是定义在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$上的变量。这时$\mathcal{F}$通常由一个参数向量决定的函数族：$\mathcal{F}&#x3D;{\mathit{f}|\mathit{Y}&#x3D;\mathit{f_\theta(\mathit{X}),\theta\in\mathbb{R^n}}}$</p><p><strong>策略</strong>: 损失函数和风险函数</p><p>经验风险最小化：empirical risk minimization，ERM:<br>$$<br>\min_{f \in \mathcal{F}} \frac{1}{N},\sum_{i&#x3D;1}^{N}L(y_i,f(x_i))<br>$$<br>结构风险最小化：structural risk minimization，SRM：<br>$$<br>\min_{f \in \mathcal{F}} \frac{1}{N},\sum_{i&#x3D;1}^{N}L(y_i,f(x_i))+\lambda\mathit{J}(f)<br>$$<br><strong>算法：</strong>其实就是指学习模型的具体算法。</p><ul><li>泛化误差：机器学习模型在样本集合$\mathit{D}\ $上的整体误差。（也称为期望风险）</li></ul><p>$$<br>\mathit{R}<em>{exp}(f)&#x3D; \mathit{E}</em>{p(\mathit{D})}[\boldsymbol{L}(y,f(x))]<br>$$</p><ul><li>经验误差：机器学习模型在训练样本$\mathit{G},$上的整体误差。（也称为经验风险）</li></ul><p>$$<br>\mathit{R}<em>{emp}(f)&#x3D; \frac{1}{N}\sum^n</em>{k&#x3D;1}[\boldsymbol{L}(y_k,f(x_k))]&#x3D;\frac{1}{n}\sum^n_{i&#x3D;1}\boldsymbol{L}(y_i,\hat{y_i})<br>$$</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——感知机</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/Perceptron/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/Perceptron/</url>
    
    <content type="html"><![CDATA[<h1 id="Perceptron-Linear-Algorithm-感知机"><a href="#Perceptron-Linear-Algorithm-感知机" class="headerlink" title="Perceptron Linear Algorithm 感知机"></a>Perceptron Linear Algorithm 感知机</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>假设输入空间（特征空间）是$\mathcal{X} \subset \mathbb{R}^n$，输出空间是$\mathcal{Y}&#x3D;{-1,+1}$。输入$x\in \mathcal{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in \mathcal{Y}$表示类别。由输入空间到输出空间的如下函数：<br>$$<br>f(x)&#x3D;sign(w \cdot x+b)<br>$$<br>成为感知机。其中，$w,b$成为感知机模型的参数,$w \in \mathbb{R}^n$叫做权值（weight）或权值向量（weight vector），$b \in \mathbb{R}$叫做偏置（bias），$w \cdot x$表示两者的内积。$sign$是符号函数，即<br>$$<br>sign(a)&#x3D;\left {\begin{matrix}+1,a\ge0\-1,a\lt0\end{matrix} \right.<br>$$</p><p>感知机是一种线性分类器，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数模型 ${f,|,f(x)&#x3D;w \cdot x +b }$</p><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><ol><li><p>统计误分类点：<br>$$<br>l(w)&#x3D;\sum_{i&#x3D;1}^N \ \mathbb{I}(y_i \cdot (w^Tx +b ) &lt; 0)<br>$$<br>但是指示函数不连续，在计算过程中不太友好</p></li><li><p>统计误分类点到分来超平面的距离：<br>$$<br>\begin{align}l(w) &amp;&#x3D;|w^Tx +b |\ &amp;&#x3D;\sum_{i&#x3D;1}^N - ,y_i \cdot (w^Tx +b )\end{align}<br>$$</p></li></ol><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>为了找到更好的$w,b$。使用随机梯度下降SGD, 但是容易受特征缩放、学习率因素影响陷入局部最小值。<br>$$<br>w_{n+1} \leftarrow w_n +\eta \frac{\partial l}{\partial w}\<br>b_{n+1} \leftarrow b_n +\eta \frac{\partial l}{\partial b}<br>$$<br>对该算法的超参数：学习率$\eta$，误差上限$\varepsilon$，学习次数$epoch$</p><h2 id="收敛性："><a href="#收敛性：" class="headerlink" title="收敛性："></a>收敛性：</h2><p>设训练数据集$T&#x3D;{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$是线性可分的，其中$x_i\in \mathcal{X}&#x3D;\mathbb{R}^n,\ y_i\in\mathcal{Y}&#x3D;{+1,-1},\ i&#x3D;1,2,\cdots,N$则:</p><ul><li><p>存在满足条件 $||\hat{w}<em>{opt}||&#x3D;1$ 的超平面 $\hat{w}</em>{opt} \cdot \hat{x}&#x3D;w_{opt}\cdot x +b_{opt}&#x3D;0$ 将训练数据集完全正确分开；且存在 $\gamma&gt;0$ ,对所有 $i&#x3D;1,2,\cdots,N$ :</p><p>  $$<br>  y_i (\hat{w}<em>{opt} \cdot \hat{x})&#x3D;y_i(w</em>{opt}\cdot x +b_{opt})\geq \gamma<br>  $$</p></li><li><p>令$R&#x3D;max_{i \leq i \leq N} ||\hat{x}_i||$,则感知机算法在训练数据集上的误分类次数$k$满足不等式:</p></li></ul><p>$$<br>k \leq (\frac{R}{\gamma})^2<br>$$</p><h2 id="Disadvantage"><a href="#Disadvantage" class="headerlink" title="Disadvantage"></a><strong>Disadvantage</strong></h2><ol><li>存在多解</li><li>依赖初值，也依赖选择顺序</li><li>当数据集线性不可分就会产生迭代震荡。</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——线性回归</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/LinearRegression/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/LinearRegression/</url>
    
    <content type="html"><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ul><li><p>回归定义： 通过带标签样本训练构造适当模型并通过该模型算出新样本的预测值</p></li><li><p>线性回归： 基于线性模型的回归学习任务通常称之为线性回归，相应的线性模型称为线性回归模型</p></li><li><p>对于任意给定的样本$X&#x3D; (x_1, x_2, … , x_𝑚)^𝑇,Y&#x3D; (y_1, y_2, … , y_𝑚)^𝑇$， 线性回归的初始模型表示为：<br>$$<br>𝑓(X) &#x3D; 𝑤_1𝑥_1 + 𝑤_2𝑥_2 + ··· +𝑤_𝑚𝑥_𝑚<br>$$<br>其中$W &#x3D; (𝑤_1, 𝑤_2, … , 𝑤_𝑚)^𝑇$为参数向量</p></li></ul><h2 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h2><p>给定训练样本$x_1, x_2, … , x_𝑚$，使用最小二乘法，即基于均方误差最小化进行模型求解：<br>$$<br>𝒘^∗&#x3D; \arg min_{w} \sum^n_{i&#x3D;1}(𝑓(𝒙<em>𝑖) - 𝑦_𝑖)^2 &#x3D; \arg min</em>{w}  \sum^n_{i&#x3D;1}(𝒘^𝑇𝒙<em>𝑖 - 𝑦_𝑖)^2<br>$$<br>则：<br>$$<br>𝐽(𝒘) &#x3D; \sum^n</em>{i&#x3D;1}(𝒘^𝑇𝒙_𝑖 - 𝑦_𝑖)^2 &#x3D; \vert \vert X𝒘 - 𝒚 \vert \vert ^2<br>$$<br>令$𝐽(𝒘)$对参数向量$𝒘$各分量的偏导数为0，即：<br>$$<br>\frac{\partial J}{\partial w}&#x3D; X^𝑇(𝐲 - X𝒘 )&#x3D; 0 \<br>\longrightarrow 𝒘 &#x3D; (X^𝑇X)^{-1}X^𝑇𝐲&#x3D;X^+Y<br>$$<br>这个式子中 $(X^TX)^{-1}X^T$ 又被称为伪逆。对于行满秩或者列满秩的 $X$，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法，对 $X$ 求奇异值分解，得到<br>$$<br>X&#x3D;U\Sigma V^T \<br>X^+&#x3D;V\Sigma^{-1}U^T<br>$$<br>在几何上的解释可以这样说：</p><ul><li>最小二乘法相当于模型（这里就是直线）和试验值的距离的平方求和。<br><strong>分散到每个样本</strong></li><li>假设我们的试验样本重构成一个 $p$ 维空间（满秩的情况）：$X&#x3D;Span(x_1,\cdots,x_N)$，而模型可以写成 $f(w)&#x3D;X \beta$，也就是 $x_1,\cdots,x_N$ 的某种组合，而最小二乘法就是说希望 $Y$ 和这个模型距离越小越好，于是它们的差应该与这个重构的空间垂直：</li></ul><p>$$<br>X^T\cdot(Y-X\beta)&#x3D;0\longrightarrow\beta&#x3D;(X^TX)^{-1}X^TY<br>$$</p><p>​<strong>分散到每个重构的维度</strong>。这里的0并非是实数，而是0向量。</p><h2 id="多重共线现象与过拟合"><a href="#多重共线现象与过拟合" class="headerlink" title="多重共线现象与过拟合"></a>多重共线现象与过拟合</h2><ul><li><p>多元线性回归模型： 其重要假定之一不同样本之间的属性标记值之间不存在线性关系。即$𝐗^𝑇𝐗$是可逆矩阵</p></li><li><p>多重共线现象： 当矩阵$𝐗$的行向量之间存在一定的线性相关性时，就会使得矩阵$𝐗^𝑇𝐗$不可逆,此时就无法解出w的解析解。</p></li><li><p>在实际应用时，如果样本容量不远远大于样本的特征维度($n&lt;&lt;d$)，很可能造成过拟合，对这种情况，我们有下面三个解决方式：</p><ol><li>加数据</li><li>特征选择（降低特征维度）如 PCA 算法。</li><li>正则化</li></ol></li></ul><h2 id="线性回归的贝叶斯方法解释"><a href="#线性回归的贝叶斯方法解释" class="headerlink" title="线性回归的贝叶斯方法解释"></a>线性回归的贝叶斯方法解释</h2><h3 id="噪声为高斯分布的-MLE"><a href="#噪声为高斯分布的-MLE" class="headerlink" title="噪声为高斯分布的 MLE"></a>噪声为高斯分布的 MLE</h3><p>对于一维的情况，记 $y&#x3D;w^Tx+\epsilon,\epsilon\sim\mathcal{N}(0,\sigma^2)$，那么 $y\sim\mathcal{N}(w^Tx,\sigma^2)$。代入极大似然估计中：<br>$$<br>\begin{align}<br>L(w)&#x3D;\log p(Y|X,w)&amp;&#x3D;\log\prod\limits_{i&#x3D;1}^Np(y_i|x_i,w)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^N\log(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\<br>\mathop{argmax}\limits_wL(w)&amp;&#x3D;\mathop{argmin}\limits_w\sum\limits_{i&#x3D;1}^N(y_i-w^Tx_i)^2<br>\end{align}<br>$$<br>这个表达式和最小二乘估计得到的结果一样。<br>$$<br>LSE  \iff  MLE \ (noise \ is \ Gaussian \ Distribution)<br>$$</p><h3 id="权重先验也为高斯分布的-MAP"><a href="#权重先验也为高斯分布的-MAP" class="headerlink" title="权重先验也为高斯分布的 MAP"></a>权重先验也为高斯分布的 MAP</h3><p>取先验分布 $w\sim\mathcal{N}(0,\sigma_0^2)$。于是： <br>$$<br>\begin{align}<br>\hat{w}&#x3D;\mathop{argmax}\limits_wp(w|Y)&amp;&#x3D;\mathop{argmax}\limits_wp(Y|w)p(w)\nonumber\<br>&amp;&#x3D;\mathop{argmax}\limits_w\log p(Y|w)p(w)\nonumber\<br>&amp;&#x3D;\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\nonumber\<br>&amp;&#x3D;\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw]<br>\end{align}<br>$$<br>这里省略了 $X$，$p(Y)$和 $w$ 没有关系，同时也利用了上面高斯分布的 MLE的结果。</p><p>我们将会看到，超参数 $\sigma_0$的存在和下面会介绍的 Ridge 正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 $L_1$正则类似的结果。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a><a href="./Regularization.md">正则化</a></h2><p>正则化一般是在损失函数（如上面介绍的最小二乘损失）上加入正则化项（表示模型的复杂度对模型的惩罚），下面我们介绍一般情况下的两种正则化框架。<br>$$<br>\begin{align}<br>L_1&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\<br>L_2&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0<br>\end{align}<br>$$<br>下面对最小二乘误差分别分析这两者的区别。</p><h3 id="Lasso-L-1-范数"><a href="#Lasso-L-1-范数" class="headerlink" title="Lasso : $ L_1 $范数"></a>Lasso : $ L_1 $范数</h3><p> $L_1$正则化可以引起稀疏解。<br>从最小化损失的角度看，由于  $L_1$项求导在0附近的左右导数都不是0，因此更容易取到0解。<br>从另一个方面看， $L_1$正则化相当于：<br>$$<br>\mathop{argmin}\limits_wL(w)\<br>s.t. ||w||_1\lt C<br>$$<br>我们已经看到平方误差损失函数在 $w$ 空间是一个椭球，因此上式求解就是椭球和 $||w||_1&#x3D;C$的切点，因此更容易相切在坐标轴上。</p><h3 id="Ridge-L-2-范数"><a href="#Ridge-L-2-范数" class="headerlink" title="Ridge : $L_2$范数"></a>Ridge : $L_2$范数</h3><p>也称岭回归 为了解决多重共线现象带来的问题，对线性回归参数的求解方法进行改进 。同时这种方法又称权值衰减。</p><p><strong>基本思想：</strong> 在线性回归模型损失函数上增加一个针对$𝒘$的范数惩罚函数，通过对目标函数做正则化处理，将参数向量 $𝒘 $中所有参数的取值压缩到一个相对较小的范围，即要求$𝒘$中所有参数的取值不能过大</p><p>岭回归的<strong>损失函数</strong>：<br>$$<br>𝐽(𝒘) &#x3D; \vert\vert X𝒘 - 𝒚\vert \vert ^2 + \lambda\vert \vert 𝒘 \vert \vert ^2<br>$$<br>其中， $\lambda \ge 0$称为正则化参数。当$\lambda $的取值较大时，惩罚项$\lambda 𝒘^𝑇𝒘$就会对损失函数的最小化产生一定的干扰，优化算法就会对回归模型参数$𝒘$赋予较小的取值以消除这种干扰 。令$𝐽(𝒘)$对参数$𝒘$的偏导数为0，得：<br>$$<br>𝒘 &#x3D; (𝐗^𝑇𝐗 + \lambda 𝐈)^{-1}𝐗^𝑇𝐲<br>$$<br>其中$𝐈$为𝑚阶单位矩阵。可以看到，这个正则化参数和前面的 MAP 结果不谋而合。利用2范数进行正则化不仅可以是模型选择 $w$ 较小的参数，同时也避免 $ X^TX$不可逆的问题。即使$𝐗^𝑇𝐗$本身不是可逆矩阵，加上$\lambda 𝐈$也可使得$𝐗^𝑇𝐗 + \lambda𝐈$组成为可逆矩阵，这也解决了多重共线问题<br>$$<br>Regularized \ LSE  \iff  MAP \ (noise \ is \ Gaussian \ Distribution)<br>$$</p><p>正则化参数的影响  </p><p><img src="/%5Csrc%5Cimage-20220114102556206.png" alt="image-20220114102556206"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>线性回归模型是最简单的模型，但是麻雀虽小，五脏俱全，在这里，我们利用最小二乘误差得到了闭式解。同时也发现，在噪声为高斯分布的时候，MLE 的解等价于最小二乘误差，而增加了正则项后，最小二乘误差加上 L2 正则项等价于高斯噪声先验下的 MAP解，加上 L1 正则项后，等价于 Laplace 噪声先验。</p><p>传统的机器学习方法或多或少都有线性回归模型的影子：</p><ol><li>线性模型往往不能很好地拟合数据，因此有三种方案克服这一劣势：<ol><li>对特征的维数进行变换，例如多项式回归模型就是在线性特征的基础上加入高次项。</li><li>在线性方程后面加入一个非线性变换，即引入一个非线性的激活函数，典型的有线性分类模型如感知机。</li><li>对于一致的线性系数，我们进行多次变换，这样同一个特征不仅仅被单个系数影响，例如多层感知机（深度前馈网络）。</li></ol></li><li>线性回归在整个样本空间都是线性的，我们修改这个限制，在不同区域引入不同的线性或非线性，例如线性样条回归和决策树模型。</li><li>线性回归中使用了所有的样本，但是对数据预先进行加工学习的效果可能更好（所谓的维数灾难，高维度数据更难学习），例如 PCA 算法和流形学习。</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——线性高斯模型</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/LinearGaussianModel/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/LinearGaussianModel/</url>
    
    <content type="html"><![CDATA[<h1 id="LinearGaussianModel"><a href="#LinearGaussianModel" class="headerlink" title="LinearGaussianModel"></a>LinearGaussianModel</h1><p>高斯作为机器学习中的常客也是无法避免的,而线性模型作为比较简单的模型，两者结合出的线性高斯模型，在今后的机器学习中大量涉及到这方面的知识。例如在各种滤波中,高斯滤波，卡曼滤波，粒子滤波。</p><h3 id="一维情况-MLE-Maximum-Likelihood-Estimation"><a href="#一维情况-MLE-Maximum-Likelihood-Estimation" class="headerlink" title="一维情况 MLE: Maximum Likelihood Estimation"></a>一维情况 MLE: Maximum Likelihood Estimation</h3><p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p><p>$$<br>\theta&#x3D;(\mu,\Sigma)&#x3D;(\mu,\sigma^{2}),x_i \stackrel{i.i.d}{\longrightarrow} p(x|\theta),\<br>\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{&#x3D;}\limits _{i.i.d}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)<br>$$<br>一般地，高斯分布的概率密度函数 PDF（Probability Density Function）写为：</p><p>$$<br>p(x|\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{p&#x2F;2}|\Sigma|^{1&#x2F;2} }e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$<br>带入 MLE 中我们考虑一维的情况</p><p>$$<br>\log p(X|\theta)&#x3D;\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)&#x3D;\sum\limits <em>{i&#x3D;1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(\frac{-(x</em>{i}-\mu)^{2} }{2\sigma^{2} })<br>$$<br>首先对 $\mu$ 的极值可以得到 ：<br>$$<br>\mu_{MLE}&#x3D;\mathop{argmax}\limits <em>{\mu}\log p(X|\theta)&#x3D;\mathop{argmax}\limits <em>{\mu}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu)^{2}<br>$$<br> 于是：<br>$$<br>\frac{\partial}{\partial\mu}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu)^{2}&#x3D;0\longrightarrow\mu</em>{MLE}&#x3D;\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}<br>$$<br>其次对 $\theta$ 中的另一个参数 $\sigma$ ，有：<br>$$<br>\begin{align}<br>\sigma</em>{MLE}&#x3D;\mathop{argmax}\limits <em>{\sigma}\log p(X|\theta)&amp;&#x3D;\mathop{argmax}\limits <em>{\sigma}\sum\limits <em>{i&#x3D;1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2} }(x</em>{i}-\mu)^{2}]\nonumber\<br>&amp;&#x3D;\mathop{argmin}\limits <em>{\sigma}\sum\limits <em>{i&#x3D;1}^{N}[\log\sigma+\frac{1}{2\sigma^{2} }(x</em>{i}-\mu)^{2}]<br>\end{align}<br>$$<br>于是：<br>$$<br>\frac{\partial}{\partial\sigma}\sum\limits <em>{i&#x3D;1}^{N}[\log\sigma+\frac{1}{2\sigma^{2} }(x</em>{i}-\mu)^{2}]&#x3D;0\longrightarrow\sigma</em>{MLE}^{2}&#x3D;\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu)^{2}<br>$$<br>值得注意的是，上面的推导中，首先对 $\mu$ 求 MLE， 然后利用这个结果求 $\sigma</em>{MLE}$ ，因此可以预期的是对数据集$\mathcal{D}$求期望时 $\mathbb{E}</em>{\mathcal{D} }[\mu_{MLE}]$ 是<strong>无偏差</strong>的：<br>$$<br>\mathbb{E}<em>{\mathcal{D} }[\mu</em>{MLE}]&#x3D;\mathbb{E}<em>{\mathcal{D} }[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}]&#x3D;\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}\mathbb{E}</em>{\mathcal{D} }[x</em>{i}]&#x3D;\mu<br>$$<br>但是当对 $\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\sigma_{MLE}$ 是 <strong>有偏</strong>的：</p><p>$$<br>\begin{align}<br>\mathbb{E}<em>{\mathcal{D} }[\sigma</em>{MLE}^{2}]&amp;&#x3D;\mathbb{E}<em>{\mathcal{D} }[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu</em>{MLE})^{2}]&#x3D;\mathbb{E}<em>{\mathcal{D} }[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}^{2}-2x</em>{i}\mu_{MLE}+\mu_{MLE}^{2})\nonumber<br>\&amp;&#x3D;\mathbb{E}<em>{\mathcal{D} }[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}^{2}-\mu</em>{MLE}^{2}]&#x3D;\mathbb{E}<em>{\mathcal{D} }[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}^{2}-\mu^{2}+\mu^{2}-\mu</em>{MLE}^{2}]\nonumber\<br>&amp;&#x3D; \mathbb{E}<em>{\mathcal{D} }[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}^{2}-\mu^{2}]-\mathbb{E}</em>{\mathcal{D} }[\mu_{MLE}^{2}-\mu^{2}]&#x3D;\sigma^{2}-(\mathbb{E}<em>{\mathcal{D} }[\mu</em>{MLE}^{2}]-\mu^{2})\nonumber\&amp;&#x3D;\sigma^{2}-(\mathbb{E}<em>{\mathcal{D} }[\mu</em>{MLE}^{2}]-\mathbb{E}<em>{\mathcal{D} }^{2}[\mu</em>{MLE}])&#x3D;\sigma^{2}-Var[\mu_{MLE}]\nonumber\&amp;&#x3D;\sigma^{2}-Var[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}]&#x3D;\sigma^{2}-\frac{1}{N^{2} }\sum\limits <em>{i&#x3D;1}^{N}Var[x</em>{i}]&#x3D;\frac{N-1}{N}\sigma^{2}<br>\end{align}<br>$$<br>所以：<br>$$<br>\hat{\sigma}^{2}&#x3D;\frac{1}{N-1}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><h3 id="多维情况MLE"><a href="#多维情况MLE" class="headerlink" title="多维情况MLE"></a>多维情况MLE</h3><p>多维高斯分布表达式为：<br>$$<br>p(x|\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{p&#x2F;2}|\Sigma|^{1&#x2F;2} }e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$<br>其中 $x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times p}$ ，$\mu $是期望,$\Sigma$ 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵,以方便计算。</p><blockquote><p>什么是正定？<br>    当$A\in \mathbb{R}^{n\times n}, A^T&#x3D;A$时，对任意$x\neq0,x \in R^n$,都有$x^TAx\gt0$</p><p>什么是半正定？</p><p>​当$A\in \mathbb{R}^{n\times n}, A^T&#x3D;A$时，对任意$x\neq0,x \in R^n$,都有$x^TAx\ge0$</p><p>常用的正定判定有</p><ol><li>$\lambda(A)\gt0$</li><li>各阶主子式均大于0</li></ol></blockquote><p>首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，<br>$$<br>\Sigma&#x3D;U\Lambda U^{T}&#x3D;(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}&#x3D;\sum\limits <em>{i&#x3D;1}^{p}u</em>{i}\lambda_{i}u_{i}^{T}<br>$$<br>，于是：<br>$$<br>\Sigma^{-1}&#x3D;{(U\Lambda U^{T})}^{-1}&#x3D;U{\Lambda}^{-1} U^{T}&#x3D;\sum\limits <em>{i&#x3D;1}^{p}u</em>{i}\frac{1}{\lambda_{i} }u_{i}^{T}<br>$$</p><p>$$<br>\begin{align}<br>\Delta &amp;&#x3D;(x-\mu)^{T}\Sigma^{-1}(x-\mu)\<br>&amp;&#x3D;\sum\limits <em>{i&#x3D;1}^{p}(x-\mu)^{T}u</em>{i}\frac{1}{\lambda_{i} }u_{i}^{T}(x-\mu)\<br>&amp;&#x3D;\sum\limits <em>{i&#x3D;1}^{p}\frac{ {[(x-u)^{T}u_i]}^{2} }{\lambda</em>{i} }&#x3D;\sum\limits <em>{i&#x3D;1}^{p}\frac{y</em>{i}^{2} }{\lambda_{i} }<br>\end{align}<br>$$</p><p>我们注意到 $y_{i}$ 是 $x-\mu$ 在特征向量 $u_{i}$ 上的投影长度，相当于是坐标轴的平移与旋转。因此上式子就是 $\Delta$ 取不同值时的同心椭圆。</p><p><img src="/%5Csrc%5CIMG_0994.JPG" alt="IMG_0994"></p><p>下面我们看多维高斯模型在实际应用时的两个问题</p><ol><li><p>参数 $\Sigma,\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\Sigma$ 有 $\frac{p(p+1)}{2}$ 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA: Principal Component Analysis(p-PCA) 。</p><p>$\forall i,j \in 1 \cdots p,i\neq j,\lambda_{i}&#x3D;\lambda_{j},\Sigma$为对角矩阵,可判定各特征<strong>各自同性</strong>。</p></li><li><p>第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合模型 GMM。</p></li></ol><p>下面对多维高斯分布的常用定理进行介绍。</p><p>我们记 $x&#x3D;(x_1, x_2,\cdots,x_p)^T&#x3D;(x_{a,m\times 1}, x_{b,n\times1})^T,\mu&#x3D;(\mu_{a,m\times1}, \mu_{b,n\times1})^{T},\Sigma&#x3D;\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$。</p><p>首先是一个高斯分布的定理：&#96;</p><blockquote><p>  定理：已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，那么 $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)$。</p><p>  证明：$\mathbb{E}[y]&#x3D;\mathbb{E}[Ax+b]&#x3D;A\mathbb{E}[x]+b&#x3D;A\mu+b$，$Var[y]&#x3D;Var[Ax+b]&#x3D;Var[Ax]&#x3D;A\cdot Var[x]\cdot A^T$。</p></blockquote><p>下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。</p><ol><li><p>$x_a&#x3D;\begin{pmatrix}\mathbb{I}<em>{m\times m}&amp;\mathbb{O}</em>{m\times n})\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}$，代入定理中得到：<br>$$<br>\mathbb{E}[x_a]&#x3D;\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}&#x3D;\mu_a\<br>Var[x_a]&#x3D;\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\mathbb{O}\end{pmatrix}&#x3D;\Sigma_{aa}<br>$$<br>所以 $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$。</p></li><li><p>同样的，$x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$。</p></li><li><p>对于两个条件概率，我们引入三个量：<br>$$<br>x_{b\cdot a}&#x3D;x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\<br>\mu_{b\cdot a}&#x3D;\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\<br>\Sigma_{bb\cdot a}&#x3D;\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}<br>$$<br>特别的，最后一个式子叫做 $\Sigma_{bb}$ 的 Schur Complementary。可以看到：<br>$$<br>x_{b\cdot a}&#x3D;\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}<br>$$<br>所以：<br>$$<br>\mathbb{E}[x</em>{b\cdot a}]&#x3D;\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}&#x3D;\mu</em>{b\cdot a}\<br>Var[x_{b\cdot a}]&#x3D;\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma</em>{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\mathbb{I}<em>{n\times n}\end{pmatrix}&#x3D;\Sigma</em>{bb\cdot a}<br>$$<br>利用这三个量可以得到 $x_b&#x3D;x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a$。因此：<br>$$<br>\mathbb{E}[x_b|x_a]&#x3D;\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a<br>$$</p><p>$$<br>Var[x_b|x_a]&#x3D;\Sigma_{bb\cdot a}<br>$$</p><p>这里同样用到了定理。</p></li><li><p>同样：<br>$$<br>x_{a\cdot b}&#x3D;x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\<br>\mu_{a\cdot b}&#x3D;\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\<br>\Sigma_{aa\cdot b}&#x3D;\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}<br>$$<br>所以：<br>$$<br>\mathbb{E}[x_a|x_b]&#x3D;\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b<br>$$</p><p>$$<br>Var[x_a|x_b]&#x3D;\Sigma_{aa\cdot b}<br>$$</p></li></ol><p>下面利用上边四个量，求解线性模型：</p><blockquote><p>  已知：$p(x)&#x3D;\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)&#x3D;\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。</p><p>  解：令 $y&#x3D;Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，所以 $\mathbb{E}[y]&#x3D;\mathbb{E}[Ax+b+\epsilon]&#x3D;A\mu+b$，$Var[y]&#x3D;A \Lambda^{-1}A^T+L^{-1}$，因此：<br>  $$<br>  p(y)&#x3D;\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)<br>  $$<br>  引入 $z&#x3D;\begin{pmatrix}x\y\end{pmatrix}$，我们可以得到 $Cov[x,y]&#x3D;\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]$。对于这个协方差可以直接计算：<br>  $$<br>  \begin{align}<br>  Cov(x,y)&amp;&#x3D;\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]\ &amp;&#x3D;\mathbb{E}[(x-\mu)(x-\mu)^TA^T]\<br>  &amp;&#x3D;Var[x]A^T&#x3D;\Lambda^{-1}A^T<br>  \end{align}<br>  $$<br>  注意到协方差矩阵的对称性，所以 $p(z)&#x3D;\mathcal{N}\begin{pmatrix}\mu\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&amp;\Lambda^{-1}A^T\A\Lambda^{-1}&amp;L^{-1}+A\Lambda^{-1}A^T\end<br>  {pmatrix}$。根据之前的公式，我们可以得到：<br>  $$<br>  \mathbb{E}[x|y]&#x3D;\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)<br>  $$</p><p>  $$<br>  Var[x|y]&#x3D;\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}<br>  $$</p></blockquote><p>这里的$\Lambda^{-1}$是精度矩阵，$\mathit{L}^{-1}$是协方差矩阵。</p><p>$$<br>Precision Matrix &#x3D; Covariance Matrix ^ {-1}<br>$$<br>这里补充一些小知识</p><ol><li>一般情况下，两个随机变量$x ,y$独立不相关，不相关不一定独立</li><li>若$x,y\sim N(\mu,\Sigma)$，不相关等价于独立��</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——线性分类</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/LinearClassification/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/LinearClassification/</url>
    
    <content type="html"><![CDATA[<h1 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h1><h2 id="线性回归："><a href="#线性回归：" class="headerlink" title="线性回归："></a>线性回归：</h2><pre><code class=" mermaid">graph TD;    LinearRegression--&gt;non-linear;    LinearRegression--&gt;non-crude --&gt; PCA;    LinearRegression--&gt;non-globa --&gt; DecisionTree;    non-linear --&gt; Attribute --&gt; FeatureChange;    non-linear --&gt; Globa --&gt; Classification;    non-linear --&gt; Parameters --&gt; NeuralNetwork;    </code></pre><p>对于分类任务，线性回归模型就无能为力了，但是我们可以在线性模型的函数进行后再加入一层激活函数，这个函数是非线性的，<strong>激活函数的反函</strong>数叫做<strong>链接函数</strong>。我们有两种线性分类的方式：</p><ol><li>硬分类，我们直接需要输出观测对应的分类。这类模型的代表为：<ol><li>线性判别分析（Fisher 判别）</li><li>感知机</li></ol></li><li>软分类，产生不同类别的概率，这类算法根据概率方法的不同分为两种<ol><li>生成式（根据贝叶斯定理先计算参数后验，再进行推断）：高斯判别分析（GDA）和朴素贝叶斯等为代表<ol><li>Gaussian Discrimination Analysis</li><li>Naive Bayes</li></ol></li><li>判别式（直接对条件概率进行建模）：Logistic Regression</li></ol></li></ol><h2 id="两分类-硬分类-感知机算法-Perception-Linear-Algorithm"><a href="#两分类-硬分类-感知机算法-Perception-Linear-Algorithm" class="headerlink" title="两分类-硬分类-感知机算法 Perception Linear Algorithm"></a>两分类-硬分类-感知机算法 Perception Linear Algorithm</h2><p>我们选取激活函数为：<br>$$<br>sign(a)&#x3D;\left{\begin{matrix}+1,a\ge0\-1,a\lt0\end{matrix}\right.<br>$$<br>这样就可以将线性回归的结果映射到两分类的结果上了。</p><p>定义损失函数为<strong>错误分类</strong>的数目，比较直观的方式是使用指示函数，但是指示函数不可导，因此可以定义：<br>$$<br>L(w)&#x3D;\sum\limits_{x_i\in\mathcal{D}<em>{wrong}}-y_iw^Tx_i<br>$$<br>其中，$\mathcal{D}</em>{wrong}$是错误分类集合，实际在每一次训练的时候，我们采用梯度下降的算法。损失函数对 $w$ 的偏导为：<br>$$<br>\frac{\partial}{\partial w}L(w)&#x3D;\sum\limits_{x_i\in\mathcal{D}<em>{wrong}}-y_ix_i<br>$$<br>但是如果样本非常多的情况下，计算复杂度较高，但是，实际上我们并不需要绝对的损失函数下降的方向，我们只需要损失函数的期望值下降，但是计算期望需要知道真实的概率分布，我们实际只能根据训练数据抽样来估算这个概率分布（经验风险）：<br>$$<br>\mathbb{E}</em>{\mathcal D}[\mathbb{E}<em>\hat{p}[\nabla_wL(w)]]&#x3D;\mathbb{E}</em>{\mathcal D}[\frac{1}{N}\sum\limits_{i&#x3D;1}^N\nabla_wL(w)]<br>$$<br>我们知道， $N$ 越大，样本近似真实分布越准确，但是对于一个标准差为 $\sigma$ 的数据，可以确定的标准差仅和 $\sqrt{N}$ 成反比，而计算速度却和 $N$ 成正比。因此可以每次使用较少样本，则在数学期望的意义上损失降低的同时，有可以提高计算速度，如果每次只使用一个错误样本，我们有下面的更新策略（根据泰勒公式，在负方向）：<br>$$<br>w^{t+1}\leftarrow w^{t}+\lambda y_ix_i<br>$$<br>是可以收敛的，同时使用单个观测更新也可以在一定程度上增加不确定度，从而减轻陷入局部最小的可能。在更大规模的数据上，常用的是小批量随机梯度下降法。</p><h2 id="两分类-硬分类-线性判别分析-Linear-Discriminant-Analysis"><a href="#两分类-硬分类-线性判别分析-Linear-Discriminant-Analysis" class="headerlink" title="两分类-硬分类-线性判别分析 Linear Discriminant Analysis"></a>两分类-硬分类-线性判别分析 Linear Discriminant Analysis</h2><p>在 LDA 中，我们的基本想法是选定一个方向，将试验样本顺着这个方向投影，投影后的数据需要满足两个条件，从而可以更好地分类：</p><ol><li>相同类内部的试验样本距离接近。</li><li>不同类别之间的距离较大。</li></ol><p>首先是投影，我们假定原来的数据是向量 $x$，那么顺着 $ w$ 方向的投影就是标量：<br>$$<br>z&#x3D;w^T\cdot x(&#x3D;|w|\cdot|x|\cos\theta)<br>$$<br>对第一点，相同类内部的样本更为接近，我们假设属于两类的试验样本数量分别是 $N_1$和 $N_2$，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了协方差的定义，用 $S$ 表示原数据的协方差：<br>$$<br>\begin{align}<br>C_1:Var_z[C_1]&amp;&#x3D;\frac{1}{N_1}\sum\limits_{i&#x3D;1}^{N_1}(z_i-\overline{z_{c1}})(z_i-\overline{z_{c1}})^T\nonumber\<br>&amp;&#x3D;\frac{1}{N_1}\sum\limits_{i&#x3D;1}^{N_1}(w^Tx_i-\frac{1}{N_1}\sum\limits_{j&#x3D;1}^{N_1}w^Tx_j)(w^Tx_i-\frac{1}{N_1}\sum\limits_{j&#x3D;1}^{N_1}w^Tx_j)^T\nonumber\<br>&amp;&#x3D;w^T\frac{1}{N_1}\sum\limits_{i&#x3D;1}^{N_1}(x_i-\overline{x_{c1}})(x_i-\overline{x_{c1}})^Tw\nonumber\<br>&amp;&#x3D;w^TS_1w\<br>C_2:Var_z[C_2]&amp;&#x3D;\frac{1}{N_2}\sum\limits_{i&#x3D;1}^{N_2}(z_i-\overline{z_{c2}})(z_i-\overline{z_{c2}})^T\nonumber\<br>&amp;&#x3D;w^TS_2w<br>\end{align}<br>$$<br>所以类内距离可以记为：<br>$$<br>\begin{align}<br>Var_z[C_1]+Var_z[C_2]&#x3D;w^T(S_1+S_2)w<br>\end{align}<br>$$<br>对于第二点，我们可以用两类的均值表示这个距离：<br>$$<br>\begin{align}<br>(\overline{z_{c1}}-\overline{z_{c2}})^2&amp;&#x3D;(\frac{1}{N_1}\sum\limits_{i&#x3D;1}^{N_1}w^Tx_i-\frac{1}{N_2}\sum\limits_{i&#x3D;1}^{N_2}w^Tx_i)^2\nonumber\<br>&amp;&#x3D;(w^T(\overline{x_{c1}}-\overline{x_{c2}}))^2\nonumber\<br>&amp;&#x3D;w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw<br>\end{align}<br>$$<br>综合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值：<br>$$<br>\begin{align}<br>\hat{w}&#x3D;\mathop{argmax}\limits_wJ(w)&amp;&#x3D;\mathop{argmax}\limits_w\frac{(\overline{z_{c1}}-\overline{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\nonumber\<br>&amp;&#x3D;\mathop{argmax}\limits_w\frac{w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw}{w^T(S_1+S_2)w}\nonumber\<br>&amp;&#x3D;\mathop{argmax}\limits_w\frac{w^TS_bw}{w^TS_ww}<br>\end{align}<br>$$<br>这样，我们就把损失函数和原数据集以及参数结合起来了。下面对这个损失函数求偏导，注意我们其实对 $w$ 的绝对值没有任何要求，只对方向有要求，因此只要一个方程就可以求解了：<br>$$<br>\begin{align}<br>&amp;\frac{\partial}{\partial w}J(w)&#x3D;2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww&#x3D;0\nonumber\<br>&amp;\Longrightarrow S_bw(w^TS_ww)&#x3D;(w^TS_bw)S_ww\nonumber\<br>&amp;\Longrightarrow w\propto S_w^{-1}S_bw&#x3D;S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw\propto S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})<br>\end{align}<br>$$<br>于是 $ S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})$ 就是我们需要寻找的方向。最后可以归一化求得单位的 $w$ 值。</p><h2 id="两分类-软分类-概率判别模型-Logistic-Regression"><a href="#两分类-软分类-概率判别模型-Logistic-Regression" class="headerlink" title="两分类-软分类-概率判别模型-Logistic Regression"></a>两分类-软分类-概率判别模型-Logistic Regression</h2><p>有时候我们只要得到一个类别的概率，那么我们需要一种能输出 $[0,1]$ 区间的值的函数。考虑两分类模型，我们利用判别模型，希望对 $p(C|x)$ 建模，利用贝叶斯定理：<br>$$<br>p(C_1|x)&#x3D;\frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)}<br>$$<br>取 $a&#x3D;\ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}$，于是：<br>$$<br>p(C_1|x)&#x3D;\frac{1}{1+\exp(-a)}<br>$$<br>上面的式子叫 Logistic Sigmoid 函数，其参数表示了两类联合概率比值的对数。在判别式中，不关心这个参数的具体值，模型假设直接对 $a$ 进行。</p><p>Logistic 回归的模型假设是：<br>$$<br>a&#x3D;w^Tx<br>$$<br>于是，通过寻找 $  w$ 的最佳值可以得到在这个模型假设下的最佳模型。概率判别模型常用最大似然估计的方式来确定参数。</p><p>对于一次观测，获得分类 $y$ 的概率为（假定$C_1&#x3D;1,C_2&#x3D;0$）：<br>$$<br>p(y|x)&#x3D;p_1^yp_0^{1-y}<br>$$</p><p>那么对于 $N$ 次独立全同的观测 MLE为：<br>$$<br>\hat{w}&#x3D;\mathop{argmax}_wJ(w)&#x3D;\mathop{argmax}<em>w\sum\limits</em>{i&#x3D;1}^N(y_i\log p_1+(1-y_i)\log p_0)<br>$$<br>注意到，这个表达式是交叉熵表达式的相反数乘 $N$，MLE 中的对数也保证了可以和指数函数相匹配，从而在大的区间汇总获取稳定的梯度。</p><p>对这个函数求导数，注意到：<br>$$<br>p_1’&#x3D;(\frac{1}{1+\exp(-a)})’&#x3D;p_1(1-p_1)<br>$$<br>则：<br>$$<br>J’(w)&#x3D;\sum\limits_{i&#x3D;1}^Ny_i(1-p_1)x_i-p_1x_i+y_ip_1x_i&#x3D;\sum\limits_{i&#x3D;1}^N(y_i-p_1)x_i<br>$$<br>由于概率值的非线性，放在求和符号中时，这个式子无法直接求解。于是在实际训练的时候，和感知机类似，也可以使用不同大小的批量随机梯度上升（对于最小化就是梯度下降）来获得这个函数的极大值。</p><h2 id="两分类-软分类-概率生成模型-高斯判别分析-Gaussian-Discrimination-Analysis"><a href="#两分类-软分类-概率生成模型-高斯判别分析-Gaussian-Discrimination-Analysis" class="headerlink" title="两分类-软分类-概率生成模型-高斯判别分析 Gaussian Discrimination Analysis"></a>两分类-软分类-概率生成模型-高斯判别分析 Gaussian Discrimination Analysis</h2><p>生成模型中，我们对联合概率分布进行建模，然后采用 MAP 来获得参数的最佳值。两分类的情况，我们采用的假设：</p><ol><li>$y\sim Bernoulli(\phi)$</li><li>$x|y&#x3D;1\sim\mathcal{N}(\mu_1,\Sigma)$</li><li>$x|y&#x3D;0\sim\mathcal{N}(\mu_0,\Sigma)$</li></ol><p>那么独立全同的数据集最大后验概率可以表示为：<br>$$<br>\begin{align}<br>\mathop{argmax}<em>{\phi,\mu_0,\mu_1,\Sigma}\log p(X|Y)p(Y)&amp;&#x3D;\mathop{argmax}</em>{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i&#x3D;1}^N (\log p(x_i|y_i)+\log p(y_i))\nonumber\<br>&amp;&#x3D;\mathop{argmax}<em>{\phi,\mu_0,\mu_1,\Sigma}\sum\limits</em>{i&#x3D;1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log \mathcal{N}(\mu_1,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi))<br>\end{align}<br>$$</p><ul><li><p>首先对 $\phi$ 进行求解，将式子对 $\phi$ 求偏导：<br>$$<br>\begin{align}\sum\limits_{i&#x3D;1}^N\frac{y_i}{\phi}+\frac{y_i-1}{1-\phi}&#x3D;0\nonumber\<br>\Longrightarrow\phi&#x3D;\frac{\sum\limits_{i&#x3D;1}^Ny_i}{N}&#x3D;\frac{N_1}{N}<br>\end{align}<br>$$</p></li><li><p>然后求解 $\mu_1$：<br>$$<br>\begin{align}\hat{\mu_1}&amp;&#x3D;\mathop{argmax}<em>{\mu_1}\sum\limits</em>{i&#x3D;1}^Ny_i\log\mathcal{N}(\mu_1,\Sigma)\nonumber\<br>&amp;&#x3D;\mathop{argmin}<em>{\mu_1}\sum\limits</em>{i&#x3D;1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)<br>\end{align}<br>$$<br>由于：<br>$$<br>\sum\limits_{i&#x3D;1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\ &#x3D;\sum\limits_{i&#x3D;1}^Ny_ix_i^T\Sigma^{-1}x_i-2y_i\mu_1^T\Sigma^{-1}x_i+y_i\mu_1^T\Sigma^{-1}\mu_1<br>$$</p><p>求微分左边乘以 $\Sigma$ 可以得到：<br>$$<br>\begin{align}\sum\limits_{i&#x3D;1}^N-2y_i\Sigma^{-1}x_i+2y_i\Sigma^{-1}\mu_1&#x3D;0\nonumber\<br>\Longrightarrow\mu_1&#x3D;\frac{\sum\limits_{i&#x3D;1}^Ny_ix_i}{\sum\limits_{i&#x3D;1}^Ny_i}&#x3D;\frac{\sum\limits_{i&#x3D;1}^Ny_ix_i}{N_1}<br>\end{align}<br>$$</p></li><li><p>求解 $\mu_0$，由于正反例是对称的，所以：<br>$$<br>\mu_0&#x3D;\frac{\sum\limits_{i&#x3D;1}^N(1-y_i)x_i}{N_0}<br>$$</p></li><li><p>最为困难的是求解 $\Sigma$，我们的模型假设对正反例采用相同的协方差矩阵，当然从上面的求解中我们可以看到，即使采用不同的矩阵也不会影响之前的三个参数。首先我们有：<br>$$<br>\begin{align}<br>\sum\limits_{i&#x3D;1}^N\log\mathcal{N}(\mu,\Sigma)&amp;&#x3D;\sum\limits_{i&#x3D;1}^N\log(\frac{1}{(2\pi)^{p&#x2F;2}|\Sigma|^{1&#x2F;2}})+(-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\<br>&amp;&#x3D;Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}Trace((x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\<br>&amp;&#x3D;Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}Trace((x_i-\mu)(x_i-\mu)^T\Sigma^{-1})\nonumber\<br>&amp;&#x3D;Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}NTrace(S\Sigma^{-1})<br>\end{align}<br>$$<br>在这个表达式中，我们在标量上加入迹从而可以交换矩阵的顺序，对于包含绝对值和迹的表达式的导数，我们有：<br>$$<br>\begin{align}<br>\frac{\partial}{\partial A}(|A|)&amp;&#x3D;|A|A^{-1}\<br>\frac{\partial}{\partial A}Trace(AB)&amp;&#x3D;B^T<br>\end{align}<br>$$<br>因此：<br>$$<br>\begin{align}[\sum\limits_{i&#x3D;1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log \mathcal{N}(\mu_1,\Sigma)]’<br>\nonumber\&#x3D;Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}N_1Trace(S_1\Sigma^{-1})-\frac{1}{2}N_2Trace(S_2\Sigma^{-1})<br>\end{align}<br>$$<br>其中，$S_1,S_2$ 分别为两个类数据内部的协方差矩阵，于是：<br>$$<br>\begin{align}N\Sigma^{-1}-N_1S_1^T\Sigma^{-2}-N_2S_2^T\Sigma^{-2}&#x3D;0\nonumber<br>\\Longrightarrow\Sigma&#x3D;\frac{N_1S_1+N_2S_2}{N}<br>\end{align}<br>$$<br>这里应用了类协方差矩阵的对称性。</p></li></ul><p>于是我们就利用最大后验的方法求得了我们模型假设里面的所有参数，根据模型，可以得到联合分布，也就可以得到用于推断的条件分布了。</p><h2 id="两分类-软分类-概率生成模型-朴素贝叶斯-Naive-Bayes"><a href="#两分类-软分类-概率生成模型-朴素贝叶斯-Naive-Bayes" class="headerlink" title="两分类-软分类-概率生成模型-朴素贝叶斯 Naive Bayes"></a>两分类-软分类-概率生成模型-朴素贝叶斯 Naive Bayes</h2><p>上面的高斯判别分析的是对数据集的分布作出了高斯分布的假设，同时引入伯努利分布作为类先验，从而利用最大后验求得这些假设中的参数。</p><p>朴素贝叶斯队数据的属性之间的关系作出了假设，一般地，我们有需要得到 $p(x|y)$ 这个概率值，由于 $x$ 有 $p$ 个维度，因此需要对这么多的维度的联合概率进行采样，但是我们知道这么高维度的空间中采样需要的样本数量非常大才能获得较为准确的概率近似。</p><p>在一般的有向概率图模型中，对各个属性维度之间的条件独立关系作出了不同的假设，其中最为简单的一个假设就是在朴素贝叶斯模型描述中的条件独立性假设。<br>$$<br>p(x|y)&#x3D;\prod\limits_{i&#x3D;1}^pp(x_i|y)<br>$$<br>即：<br>$$<br>x_i\perp x_j|y,\forall\  i\ne j<br>$$<br>于是利用贝叶斯定理，对于单次观测：<br>$$<br>p(y|x)&#x3D;\frac{p(x|y)p(y)}{p(x)}&#x3D;\frac{\prod\limits_{i&#x3D;1}^pp(x_i|y)p(y)}{p(x)}<br>$$<br>对于单个维度的条件概率以及类先验作出进一步的假设：</p><ol><li>$x_i$ 为连续变量：$p(x_i|y)&#x3D;\mathcal{N}(\mu_i,\sigma_i^2)$</li><li>$x_i$ 为离散变量：类别分布（Categorical）：$p(x_i&#x3D;i|y)&#x3D;\theta_i,\sum\limits_{i&#x3D;1}^K\theta_i&#x3D;1$</li><li>$p(y)&#x3D;\phi^y(1-\phi)^{1-y}$</li></ol><p>对这些参数的估计，常用 MLE 的方法直接在数据集上估计，由于不需要知道各个维度之间的关系，因此，所需数据量大大减少了。估算完这些参数，再代入贝叶斯定理中得到类别的后验分布。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>分类任务分为两类，对于需要直接输出类别的任务，感知机算法中我们在线性模型的基础上加入符号函数作为激活函数，那么就能得到这个类别，但是符号函数不光滑，于是我们采用错误驱动的方式，引入 $\sum\limits_{x_i\in\mathcal{D}<em>{wrong}}-y_iw^Tx_i$ 作为损失函数，然后最小化这个误差，采用批量随机梯度下降的方法来获取最佳的参数值。而在线性判别分析中，我们将线性模型看作是数据点在某一个方向的投影，采用类内小，类间大的思路来定义损失函数，其中类内小定义为两类数据的方差之和，类间大定义为两类数据中心点的间距，对损失函数求导得到参数的方向，这个方向就是 $S_w^{-1}(\overline x</em>{c1}-\overline x_{c2})$，其中 $S_w$ 为原数据集两类的方差之和。</p><p>另一种任务是输出分类的概率，对于概率模型，我们有两种方案，第一种是判别模型，也就是直接对类别的条件概率建模，将线性模型套入 Logistic 函数中，我们就得到了 Logistic 回归模型，这里的概率解释是两类的联合概率比值的对数是线性的，我们定义的损失函数是交叉熵（等价于 MLE），对这个函数求导得到 $\frac{1}{N}\sum\limits_{i&#x3D;1}^N(y_i-p_1)x_i$，同样利用批量随机梯度（上升）的方法进行优化。第二种是生成模型，生成模型引入了类别的先验，在高斯判别分析中，我们对数据集的数据分布作出了假设，其中类先验是二项分布，而每一类的似然是高斯分布，对这个联合分布的对数似然进行最大化就得到了参数， $\frac{\sum\limits_{i&#x3D;1}^Ny_ix_i}{N_1},\frac{\sum\limits_{i&#x3D;1}^N(1-y_i)x_i}{N_0},\frac{N_1S_1+N_2S_2}{N},\frac{N_1}{N}$。在朴素贝叶斯中，我们进一步对属性的各个维度之间的依赖关系作出假设，条件独立性假设大大减少了数据量的需求。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——数学引入</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/Intro_Math/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/Intro_Math/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：<br>$$<br>\mathcal{X}<em>{N\times p}&#x3D;(x</em>{1},x_{2},\cdots,x_{N})^{T},x_{i}&#x3D;(x_{i1},x_{i2},\cdots,x_{ip})^{T}<br>$$<br> 这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\theta)$ 生成的。</p><h2 id="频率派的观点"><a href="#频率派的观点" class="headerlink" title="频率派的观点"></a>频率派的观点</h2><p>$p(x|\theta)$中的 $\theta$ 是一个未知常量，$x$是一个随机变量。对于 $N$ 个观测来说观测集的概率为 $p(\mathcal{X}|\theta)\mathop{&#x3D;}\limits <em>{i.i.d}\prod\limits <em>{i&#x3D;1}^{N}p(x</em>{i}|\theta))$ 。$i.i.d$指的是独立同分布 Indentical Independent Distribution。为了求 $\theta$ 的大小，我们采用对数估计MLE的方法：<br>$$<br>\begin{equation}<br>    \begin{split}<br>    \theta</em>{MLE}&amp;&#x3D;\mathop{argmax}\limits _{\theta}\log p(\mathcal{X}|\theta) \<br>    &amp;\mathop{&#x3D;}\limits _{i.i.d}\mathop{argmax}\limits _{\theta}\log \prod\limits <em>{i&#x3D;1}^{N}p(x</em>{i}|\theta) \<br>    &amp;\mathop{&#x3D;}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)<br>    \end{split}<br>\end{equation}<br>$$<br>连乘会导致下限溢出，所以用对数将连乘转化成连加。</p><h2 id="贝叶斯派的观点"><a href="#贝叶斯派的观点" class="headerlink" title="贝叶斯派的观点"></a>贝叶斯派的观点</h2><p>贝叶斯派则认为 $p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满足一个预设的先验的分布 $\theta\sim p(\theta)$ 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p><p>$$<br>p(\theta|\mathcal{X})&#x3D;\frac{p(\mathcal{X}|\theta)\cdot p(\theta)}{p(\mathcal{X})}&#x3D;\frac{p(\mathcal{X}|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(\mathcal{X}|\theta)\cdot p(\theta)d\theta}\propto p(\mathcal{X}|\theta)\cdot p(\theta)<br>$$<br>为了求 $\theta$ 的值，我们要最大化这个参数后验MAP Maximum Posteriori Estimation：</p><p>$$<br>\theta_{MAP}&#x3D;\mathop{argmax}\limits _{\theta}p(\theta|\mathcal{X})\propto\mathop{argmax}\limits _{\theta}p(\mathcal{X}|\theta)\cdot p(\theta)<br>$$<br>其中第二个等号是由于分母和 $\theta$ 没有关系。求解这个 $\theta$ 值后计算$\frac{p(\mathcal{X}|\theta)\cdot p(\theta)}{\int\limits <em>{\theta}p(\mathcal{X}|\theta)\cdot p(\theta)d\theta}$ ，就得到了参数的后验概率。其中 $p(\mathcal{X}|\theta)$ 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测贝叶斯预测：<br>$$<br>p(x</em>{new}|\mathcal{X})&#x3D;\int\limits <em>{\theta}p(x</em>{new}|\theta)\cdot p(\theta|\mathcal{X})d\theta<br>$$<br> 其中积分中的被乘数是模型，乘数是后验分布。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时<strong>最优化理论</strong>占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时<strong>积分</strong>占有重要地位。因此采样积分方法如Monte Carlo Method 、Bayes Prediction等很多方面应用。</p><h1 id="Math-Basics"><a href="#Math-Basics" class="headerlink" title="Math Basics"></a>Math Basics</h1><h2 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h2><h3 id="一维情况-MLE-Maximum-Likelihood-Estimation"><a href="#一维情况-MLE-Maximum-Likelihood-Estimation" class="headerlink" title="一维情况 MLE: Maximum Likelihood Estimation"></a>一维情况 MLE: Maximum Likelihood Estimation</h3><p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p><p>$$<br>\theta&#x3D;(\mu,\Sigma)&#x3D;(\mu,\sigma^{2}),x_i \stackrel{i.i.d}{\longrightarrow} p(x|\theta),\ \theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{&#x3D;}\limits _{i.i.d}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)<br>$$<br>一般地，高斯分布的概率密度函数 PDF（Probability Density Function）写为：</p><p>$$<br>p(x|\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{p&#x2F;2}|\Sigma|^{1&#x2F;2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$<br>带入 MLE 中我们考虑一维的情况</p><p>$$<br>\log p(X|\theta)&#x3D;\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)&#x3D;\sum\limits <em>{i&#x3D;1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(\frac{-(x</em>{i}-\mu)^{2}}{2\sigma^{2}})<br>$$<br>首先对 $\mu$ 的极值可以得到 ：<br>$$<br>\mu_{MLE}&#x3D;\mathop{argmax}\limits <em>{\mu}\log p(X|\theta)&#x3D;\mathop{argmax}\limits <em>{\mu}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu)^{2}<br>$$<br> 于是：<br>$$<br>\frac{\partial}{\partial\mu}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu)^{2}&#x3D;0\longrightarrow\mu</em>{MLE}&#x3D;\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}<br>$$<br>其次对 $\theta$ 中的另一个参数 $\sigma$ ，有：<br>$$<br>\begin{align}<br>\sigma</em>{MLE}&#x3D;\mathop{argmax}\limits <em>{\sigma}\log p(X|\theta)&amp;&#x3D;\mathop{argmax}\limits <em>{\sigma}\sum\limits <em>{i&#x3D;1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]\nonumber\<br>&amp;&#x3D;\mathop{argmin}\limits <em>{\sigma}\sum\limits <em>{i&#x3D;1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]<br>\end{align}<br>$$<br>于是：<br>$$<br>\frac{\partial}{\partial\sigma}\sum\limits <em>{i&#x3D;1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]&#x3D;0\longrightarrow\sigma</em>{MLE}^{2}&#x3D;\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu)^{2}<br>$$<br>值得注意的是，上面的推导中，首先对 $\mu$ 求 MLE， 然后利用这个结果求 $\sigma</em>{MLE}$ ，因此可以预期的是对数据集求期望时 $\mathbb{E}</em>{\mathcal{D}}[\mu_{MLE}]$ 是无偏差的：<br>$$<br>\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}]&#x3D;\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}]&#x3D;\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}\mathbb{E}</em>{\mathcal{D}}[x</em>{i}]&#x3D;\mu<br>$$<br>但是当对 $\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\sigma_{MLE}$ 是 有偏的：</p><p>$$<br>\begin{align}<br>\mathbb{E}<em>{\mathcal{D}}[\sigma</em>{MLE}^{2}]&amp;&#x3D;\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu</em>{MLE})^{2}]&#x3D;\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}^{2}-2x</em>{i}\mu_{MLE}+\mu_{MLE}^{2})\nonumber<br>\&amp;&#x3D;\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}^{2}-\mu</em>{MLE}^{2}]&#x3D;\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}^{2}-\mu^{2}+\mu^{2}-\mu</em>{MLE}^{2}]\nonumber\<br>&amp;&#x3D; \mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}^{2}-\mu^{2}]-\mathbb{E}</em>{\mathcal{D}}[\mu_{MLE}^{2}-\mu^{2}]&#x3D;\sigma^{2}-(\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}^{2}]-\mu^{2})\nonumber\&amp;&#x3D;\sigma^{2}-(\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}^{2}]-\mathbb{E}<em>{\mathcal{D}}^{2}[\mu</em>{MLE}])&#x3D;\sigma^{2}-Var[\mu_{MLE}]\nonumber\&amp;&#x3D;\sigma^{2}-Var[\frac{1}{N}\sum\limits <em>{i&#x3D;1}^{N}x</em>{i}]&#x3D;\sigma^{2}-\frac{1}{N^{2}}\sum\limits <em>{i&#x3D;1}^{N}Var[x</em>{i}]&#x3D;\frac{N-1}{N}\sigma^{2}<br>\end{align}<br>$$<br>所以：<br>$$<br>\hat{\sigma}^{2}&#x3D;\frac{1}{N-1}\sum\limits <em>{i&#x3D;1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><h3 id="多维情况"><a href="#多维情况" class="headerlink" title="多维情况"></a>多维情况</h3><p>多维高斯分布表达式为：<br>$$<br>p(x|\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{p&#x2F;2}|\Sigma|^{1&#x2F;2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$<br>其中 $x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times p}$ ，$\mu $是期望,$\Sigma$ 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵,以方便计算。首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，$\Sigma&#x3D;U\Lambda U^{T}&#x3D;(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}&#x3D;\sum\limits <em>{i&#x3D;1}^{p}u</em>{i}\lambda_{i}u_{i}^{T}$ ，于是：</p><p>$$<br>\Sigma^{-1}&#x3D;{(U\Lambda U^{T})}^{-1}&#x3D;U{\Lambda}^{-1} U^{T}&#x3D;\sum\limits <em>{i&#x3D;1}^{p}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}<br>$$</p><p>$$<br>\begin{align}<br>\Delta &amp;&#x3D;(x-\mu)^{T}\Sigma^{-1}(x-\mu)\<br>&amp;&#x3D;\sum\limits <em>{i&#x3D;1}^{p}(x-\mu)^{T}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)\<br>&amp;&#x3D;\sum\limits <em>{i&#x3D;1}^{p}\frac{ { [(x-u)^{T}u_i] }^{2} }{\lambda</em>{i} }&#x3D;\sum\limits <em>{i&#x3D;1}^{p}\frac{ y</em>{i}^{2} }{\lambda_{i} }<br>\end{align}<br>$$</p><p>我们注意到 $y_{i}$ 是 $x-\mu$ 在特征向量 $u_{i}$ 上的投影长度，相当于是坐标轴的平移与旋转。因此上式子就是 $\Delta$ 取不同值时的同心椭圆。</p><p>下面我们看多维高斯模型在实际应用时的两个问题</p><ol><li><p>参数 $\Sigma,\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\Sigma$ 有 $\frac{p(p+1)}{2}$ 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA: Principal Component Analysis(p-PCA) 。</p><p>$\forall i,j \in 1 \cdots p,i\neq j,\lambda_{i}&#x3D;\lambda_{j},\Sigma$为对角矩阵,可判定各特征<strong>各自同性</strong>。</p></li><li><p>第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合模型 GMM。</p></li></ol><p>下面对多维高斯分布的常用定理进行介绍。</p><p>我们记 $x&#x3D;(x_1, x_2,\cdots,x_p)^T&#x3D;(x_{a,m\times 1}, x_{b,n\times1})^T,\mu&#x3D;(\mu_{a,m\times1}, \mu_{b,n\times1})^{T},\Sigma&#x3D;\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$。</p><p>首先是一个高斯分布的定理：&#96;</p><blockquote><p>  定理：已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，那么 $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)$。</p><p>  证明：$\mathbb{E}[y]&#x3D;\mathbb{E}[Ax+b]&#x3D;A\mathbb{E}[x]+b&#x3D;A\mu+b$，$Var[y]&#x3D;Var[Ax+b]&#x3D;Var[Ax]&#x3D;A\cdot Var[x]\cdot A^T$。</p></blockquote><p>下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。</p><ol><li><p>$x_a&#x3D;\begin{pmatrix}\mathbb{I}<em>{m\times m}&amp;\mathbb{O}</em>{m\times n})\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}$，代入定理中得到：<br>$$<br>\mathbb{E}[x_a]&#x3D;\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}&#x3D;\mu_a\<br>Var[x_a]&#x3D;\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\mathbb{O}\end{pmatrix}&#x3D;\Sigma_{aa}<br>$$<br>所以 $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$。</p></li><li><p>同样的，$x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$。</p></li><li><p>对于两个条件概率，我们引入三个量：<br>$$<br>x_{b\cdot a}&#x3D;x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\<br>\mu_{b\cdot a}&#x3D;\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\<br>\Sigma_{bb\cdot a}&#x3D;\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}<br>$$<br>特别的，最后一个式子叫做 $\Sigma_{bb}$ 的 Schur Complementary。可以看到：<br>$$<br>x_{b\cdot a}&#x3D;\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}<br>$$<br>所以：<br>$$<br>\mathbb{E}[x</em>{b\cdot a}]&#x3D;\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}&#x3D;\mu</em>{b\cdot a}\<br>Var[x_{b\cdot a}]&#x3D;\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma</em>{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\mathbb{I}<em>{n\times n}\end{pmatrix}&#x3D;\Sigma</em>{bb\cdot a}<br>$$<br>利用这三个量可以得到 $x_b&#x3D;x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a$。因此：<br>$$<br>\mathbb{E}[x_b|x_a]&#x3D;\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a<br>$$</p><p>$$<br>Var[x_b|x_a]&#x3D;\Sigma_{bb\cdot a}<br>$$</p><p>这里同样用到了定理。</p></li><li><p>同样：<br>$$<br>x_{a\cdot b}&#x3D;x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\<br>\mu_{a\cdot b}&#x3D;\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\<br>\Sigma_{aa\cdot b}&#x3D;\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}<br>$$<br>所以：<br>$$<br>\mathbb{E}[x_a|x_b]&#x3D;\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b<br>$$</p><p>$$<br>Var[x_a|x_b]&#x3D;\Sigma_{aa\cdot b}<br>$$</p></li></ol><p>下面利用上边四个量，求解线性模型：</p><blockquote><p>  已知：$p(x)&#x3D;\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)&#x3D;\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。</p><p>  解：令 $y&#x3D;Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，所以 $\mathbb{E}[y]&#x3D;\mathbb{E}[Ax+b+\epsilon]&#x3D;A\mu+b$，$Var[y]&#x3D;A \Lambda^{-1}A^T+L^{-1}$，因此：<br>  $$<br>  p(y)&#x3D;\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)<br>  $$<br>  引入 $z&#x3D;\begin{pmatrix}x\y\end{pmatrix}$，我们可以得到 $Cov[x,y]&#x3D;\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]$。对于这个协方差可以直接计算：<br>  $$<br>  \begin{align}<br>  Cov(x,y)&amp;&#x3D;\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]\ &amp;&#x3D;\mathbb{E}[(x-\mu)(x-\mu)^TA^T]\<br>  &amp;&#x3D;Var[x]A^T&#x3D;\Lambda^{-1}A^T<br>  \end{align}<br>  $$<br>  注意到协方差矩阵的对称性，所以 $p(z)&#x3D;\mathcal{N}\begin{pmatrix}\mu\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&amp;\Lambda^{-1}A^T\A\Lambda^{-1}&amp;L^{-1}+A\Lambda^{-1}A^T\end<br>  {pmatrix}$。根据之前的公式，我们可以得到：<br>  $$<br>  \mathbb{E}[x|y]&#x3D;\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)<br>  $$</p><p>  $$<br>  Var[x|y]&#x3D;\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}<br>  $$</p></blockquote><p>这里的$\Lambda^{-1}$是精度矩阵，$\mathit{L}^{-1}$是协方差矩阵。</p><p>Precision matrix &#x3D; Covariance matrix ^ {-1}</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——总结</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/cheatsheet/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/cheatsheet/</url>
    
    <content type="html"><![CDATA[<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><ol><li><p>MLE<br>$$<br>\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{&#x3D;}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)<br>$$</p></li><li><p>MAP<br>$$<br>\theta_{MAP}&#x3D;\mathop{argmax}\limits _{\theta} \  p(\theta|X)&#x3D;\mathop{argmax}\limits _{\theta}\  p(X|\theta)\cdot p(\theta)<br>$$</p></li><li><p>Gaussian Distribution<br>$$<br>\begin{align}&amp;p(x|\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{p&#x2F;2}|\Sigma|^{1&#x2F;2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}\<br>&amp;\Delta&#x3D;(x-\mu)^{T}\Sigma^{-1}(x-\mu)&#x3D;\sum\limits <em>{i&#x3D;1}^{p}(x-\mu)^{T}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)&#x3D;\sum\limits <em>{i&#x3D;1}^{p}\frac{y</em>{i}^{2}}{\lambda_{i}}<br>\end{align}<br>$$</p></li><li><p>已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，有：<br>$$<br>\begin{align}y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)<br>\end{align}<br>$$</p></li><li><p>记 $x&#x3D;(x_1, x_2,\cdots,x_p)^T&#x3D;(x_{a,m\times 1}, x_{b,n\times1})^T,\mu&#x3D;(\mu_{a,m\times1}, \mu_{b,n\times1}),\Sigma&#x3D;\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$，则：<br>$$<br>\begin{align}&amp;x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\<br>&amp;x_b|x_a\sim\mathcal{N}(\mu_{b|a},\Sigma_{b|a})\<br>&amp;\mu_{b|a}&#x3D;\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\<br>&amp;\Sigma_{b|a}&#x3D;\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}<br>\end{align}<br>$$</p></li></ol><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ol><li><p>Dataset:<br>$$<br>\mathcal{D}&#x3D;{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)}<br>$$</p></li><li><p>Notation:<br>$$<br>X&#x3D;(x_1,x_2,\cdots,x_N)^T,Y&#x3D;(y_1,y_2,\cdots,y_N)^T<br>$$</p></li><li><p>Model:<br>$$<br>f(w)&#x3D;w^Tx<br>$$</p></li></ol><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><ol><li>最小二乘误差&#x2F;高斯噪声的MLE<br>$$<br>L(w)&#x3D;\sum\limits_{i&#x3D;1}^N||w^Tx_i-y_i||^2_2<br>$$</li></ol><h3 id="闭式解"><a href="#闭式解" class="headerlink" title="闭式解"></a>闭式解</h3><p>$$<br>\begin{align}\hat{w}&#x3D;(X^TX)^{-1}X^TY&#x3D;X^+Y\<br>X&#x3D;U\Sigma V^T\<br>X^+&#x3D;V\Sigma^{-1}U^T<br>\end{align}<br>$$</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>$$<br>\begin{align}<br>L1-Gaussian \ priori&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\<br>L2-Laplasian\ priori-Sparsity&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0<br>\end{align}<br>$$</p><h2 id="Linear-Classification"><a href="#Linear-Classification" class="headerlink" title="Linear Classification"></a>Linear Classification</h2><h3 id="Hard"><a href="#Hard" class="headerlink" title="Hard"></a>Hard</h3><h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><ol><li><p>Idea: 在线性模型上加入激活函数</p></li><li><p>Loss Function:</p></li></ol><p>$$<br>L(w)&#x3D;\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i<br>$$</p><ol start="3"><li>Parameters:</li></ol><p>$$<br>w^{t+1}\leftarrow w^{t}+\lambda y_ix_i<br>$$</p><h4 id="Fisher"><a href="#Fisher" class="headerlink" title="Fisher"></a>Fisher</h4><ol><li><p>Idea: 投影，类内小，类间大。</p></li><li><p>Loss Function:<br>$$<br>\begin{align}&amp;J(w)&#x3D;\frac{w^TS_bw}{w^TS_ww}\<br>&amp;S_b&#x3D;(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^T\<br>&amp;S_w&#x3D;S_1+S_2<br>\end{align}<br>$$</p></li><li><p>闭式解，投影方向:<br>$$<br>S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})<br>$$</p></li></ol><h3 id="Soft"><a href="#Soft" class="headerlink" title="Soft"></a>Soft</h3><h4 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h4><h5 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h5><ol><li><p>Idea，激活函数:<br>$$<br>\begin{align}p(C_1|x)&amp;&#x3D;\frac{1}{1+\exp(-a)}\<br>a&amp;&#x3D;w^Tx<br>\end{align}<br>$$</p></li><li><p>Loss Function(交叉熵):<br>$$<br>\hat{w}&#x3D;\mathop{argmax}_wJ(w)&#x3D;\mathop{argmax}<em>w\sum\limits</em>{i&#x3D;1}^N(y_i\log p_1+(1-y_i)\log p_0)<br>$$</p></li><li><p>解法，SGD<br>$$<br>J’(w)&#x3D;\sum\limits_{i&#x3D;1}^N(y_i-p_1)x_i<br>$$</p></li></ol><h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><h5 id="GDA"><a href="#GDA" class="headerlink" title="GDA"></a>GDA</h5><ol><li><p>Model</p><ol><li>$y\sim Bernoulli(\phi)$</li><li>$x|y&#x3D;1\sim\mathcal{N}(\mu_1,\Sigma)$</li><li>$x|y&#x3D;0\sim\mathcal{N}(\mu_0,\Sigma)$</li></ol></li><li><p>MAP<br>$$<br>\begin{align}<br>&amp;\mathop{argmax}<em>{\phi,\mu_0,\mu_1,\Sigma}\log p(X|Y)p(Y)\nonumber\<br>&amp;&#x3D;\mathop{argmax}</em>{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i&#x3D;1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log \mathcal{N}(\mu_1,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi))<br>\end{align}<br>$$</p></li><li><p>解<br>$$<br>\begin{align}\phi&amp;&#x3D;\frac{N_1}{N}\<br>\mu_1&amp;&#x3D;\frac{\sum\limits_{i&#x3D;1}^Ny_ix_i}{N_1}\<br>\mu_0&amp;&#x3D;\frac{\sum\limits_{i&#x3D;1}^N(1-y_i)x_i}{N_0}\<br>\Sigma&amp;&#x3D;\frac{N_1S_1+N_2S_2}{N}<br>\end{align}<br>$$</p></li></ol><h5 id="Naive-Bayesian"><a href="#Naive-Bayesian" class="headerlink" title="Naive Bayesian"></a>Naive Bayesian</h5><ol><li><p>Model, 对单个数据点的各个维度作出限制<br>$$<br>x_i\perp x_j|y,\forall\  i\ne j<br>$$</p><ol><li>$x_i$ 为连续变量：$p(x_i|y)&#x3D;\mathcal{N}(\mu_i,\sigma_i^2)$</li><li>$x_i$ 为离散变量：类别分布（Categorical）：$p(x_i&#x3D;i|y)&#x3D;\theta_i,\sum\limits_{i&#x3D;1}^K\theta_i&#x3D;1$</li><li>$p(y)&#x3D;\phi^y(1-\phi)^{1-y}$</li></ol></li><li><p>解：和GDA相同</p></li></ol><h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><p>中心化：<br>$$<br>\begin{align}S<br>&amp;&#x3D;\frac{1}{N}X^T(E_N-\frac{1}{N}\mathbb{I}<em>{N1}\mathbb{I}</em>{1N})(E_N-\frac{1}{N}\mathbb{I}<em>{N1}\mathbb{I}</em>{1N})^TX\nonumber\<br>&amp;&#x3D;\frac{1}{N}X^TH^2X&#x3D;\frac{1}{N}X^THX<br>\end{align}<br>$$</p><h3 id="PCA-1"><a href="#PCA-1" class="headerlink" title="PCA"></a>PCA</h3><ol><li><p>Idea: 坐标变换，寻找线性无关的新基矢，取信息损失最小的前几个维度</p></li><li><p>Loss Function:<br>$$<br>\begin{align}J<br>&amp;&#x3D;\sum\limits_{j&#x3D;1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j&#x3D;1<br>\end{align}<br>$$</p></li><li><p>解：</p><ol><li><p>特征分解法<br>$$<br>S&#x3D;U\Lambda U^T<br>$$</p></li><li><p>SVD for X&#x2F;S<br>$$<br>\begin{align}HX&#x3D;U\Sigma V^T\<br>S&#x3D;\frac{1}{N}V\Sigma^T\Sigma V^T<br>\new\ co&#x3D;HX\cdot V\end{align}<br>$$</p></li><li><p>SVD for T<br>$$<br>\begin{align}T&#x3D;HXX^TH&#x3D;U\Sigma\Sigma^TU^T\<br>new\ co&#x3D;U\Sigma<br>\end{align}<br>$$</p></li></ol></li></ol><h3 id="p-PCA"><a href="#p-PCA" class="headerlink" title="p-PCA"></a>p-PCA</h3><ol><li><p>Model:<br>$$<br>\begin{align}<br>z&amp;\sim\mathcal{N}(\mathbb{O}<em>{q1},\mathbb{I}</em>{qq})\<br>x&amp;&#x3D;Wz+\mu+\varepsilon\<br>\varepsilon&amp;\sim\mathcal{N}(0,\sigma^2\mathbb{I}_{pp})<br>\end{align}<br>$$</p></li><li><p>Learning: E-M</p></li><li><p>Inference:<br>$$<br>p(z|x)&#x3D;\mathcal{N}(W^T(WW^T+\sigma^2\mathbb{I})^{-1}(x-\mu),\mathbb{I}-W^T(WW^T+\sigma^2\mathbb{I})^{-1}W)<br>$$</p></li></ol><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><ol><li>强对偶关系：凸优化+（松弛）Slater 条件-&gt;强对偶。</li><li>参数求解：KKT条件<ol><li>可行域</li><li>互补松弛+梯度为0</li></ol></li></ol><h3 id="Hard-margin"><a href="#Hard-margin" class="headerlink" title="Hard-margin"></a>Hard-margin</h3><ol><li><p>Idea: 最大化间隔</p></li><li><p>Model:<br>$$<br>\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\ y_i(w^Tx_i+b)\ge1,i&#x3D;1,2,\cdots,N<br>$$</p></li><li><p>对偶问题<br>$$<br>\max_{\lambda}-\frac{1}{2}\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i&#x3D;1}^N\lambda_i,\ s.t.\ \lambda_i\ge0<br>$$</p></li><li><p>模型参数<br>$$<br>\hat{w}&#x3D;\sum\limits_{i&#x3D;1}^N\lambda_iy_ix_i\<br>\hat{b}&#x3D;y_k-w^Tx_k&#x3D;y_k-\sum\limits_{i&#x3D;1}^N\lambda_iy_ix_i^Tx_k,\exist k,1-y_k(w^Tx_k+b)&#x3D;0<br>$$</p></li></ol><h3 id="Soft-margin"><a href="#Soft-margin" class="headerlink" title="Soft-margin"></a>Soft-margin</h3><ol><li><p>Idea:允许少量错误</p></li><li><p>Model:<br>$$<br>error&#x3D;\sum\limits_{i&#x3D;1}^N\max{0,1-y_i(w^Tx_i+b)}\<br>\mathop{argmin}<em>{w,b}\frac{1}{2}w^Tw+C\sum\limits</em>{i&#x3D;1}^N\xi_i\ s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i&#x3D;1,2,\cdots,N<br>$$</p></li></ol><h3 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h3><p>对称的正定函数都可以作为正定核。</p><h2 id="Exp-Family"><a href="#Exp-Family" class="headerlink" title="Exp Family"></a>Exp Family</h2><ol><li><p>表达式<br>$$<br>p(x|\eta)&#x3D;h(x)\exp(\eta^T\phi(x)-A(\eta))&#x3D;\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))<br>$$</p></li><li><p>对数配分函数<br>$$<br>\begin{align}<br>A’(\eta)&#x3D;\mathbb{E}<em>{p(x|\eta)}[\phi(x)]\<br>A’’(\eta)&#x3D;Var</em>{p(x|\eta)}[\phi(x)]<br>\end{align}<br>$$</p></li><li><p>指数族分布满足最大熵定理</p></li></ol><h2 id="PGM"><a href="#PGM" class="headerlink" title="PGM"></a>PGM</h2><h3 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h3><ol><li><p>有向图<br>$$<br>p(x_1,x_2,\cdots,x_p)&#x3D;\prod\limits_{i&#x3D;1}^pp(x_i|x_{parent(i)})<br>$$<br>D-separation<br>$$<br>p(x_i|x_{-i})&#x3D;\frac{p(x)}{\int p(x)dx_{i}}&#x3D;\frac{\prod\limits_{j&#x3D;1}^pp(x_j|x_{parents(j)})}{\int\prod\limits_{j&#x3D;1}^pp(x_j|x_{parents(j)})dx_i}&#x3D;\frac{p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)}{\int p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)dx_i}<br>$$</p></li><li><p>无向图<br>$$<br>\begin{align}p(x)&#x3D;\frac{1}{Z}\prod\limits_{i&#x3D;1}^{K}\phi(x_{ci})\<br>Z&#x3D;\sum\limits_{x\in\mathcal{X}}\prod\limits_{i&#x3D;1}^{K}\phi(x_{ci})\<br>\phi(x_{ci})&#x3D;\exp(-E(x_{ci}))<br>\end{align}<br>$$</p></li><li><p>有向转无向</p><ol><li>将每个节点的父节点两两相连</li><li>将有向边替换为无向边</li></ol></li></ol><h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>参数学习-EM</p><ol><li><p>目的：解决具有隐变量的混合模型的参数估计（极大似然估计）</p></li><li><p>参数：<br>$$<br>\theta_{MLE}&#x3D;\mathop{argmax}\limits_\theta\log p(x|\theta)<br>$$</p></li><li><p>迭代求解：<br>$$<br>\theta^{t+1}&#x3D;\mathop{argmax}\limits_{\theta}\int_z\log [p(x,z|\theta)]p(z|x,\theta^t)dz&#x3D;\mathbb{E}_{z|x,\theta^t}[\log p(x,z|\theta)]<br>$$</p></li><li><p>原理<br>$$<br>\log p(x|\theta^t)\le\log p(x|\theta^{t+1})<br>$$</p></li><li><p>广义EM</p><ol><li><p>E step：<br>$$<br>\hat{q}^{t+1}(z)&#x3D;\mathop{argmax}_q\int_zq^t(z)\log\frac{p(x,z|\theta)}{q^t(z)}dz,fixed\ \theta<br>$$</p></li><li><p>M step：<br>$$<br>\hat{\theta}&#x3D;\mathop{argmax}_\theta \int_zq^{t+1}(z)\log\frac{p(x,z|\theta)}{q^{t+1}(z)}dz,fixed\ \hat{q}<br>$$</p></li></ol></li></ol><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ol><li><p>精确推断</p><ol><li><p>VE</p></li><li><p>BP<br>$$<br>m_{j\to i}(i)&#x3D;\sum\limits_j\phi_j(j)\phi_{ij}(ij)\prod\limits_{k\in Neighbour(j)-i}m_{k\to j}(j)<br>$$</p></li><li><p>MP<br>$$<br>m_{j\to i}&#x3D;\max\limits_{j}\phi_j\phi_{ij}\prod\limits_{k\in Neighbour(j)-i}m_{k\to j}<br>$$</p></li></ol></li><li><p>近似推断</p><ol><li><p>确定性近似，VI</p><ol><li><p>变分表达式<br>$$<br>\hat{q}(Z)&#x3D;\mathop{argmax}_{q(Z)}L(q)<br>$$</p></li><li><p>平均场近似下的 VI-坐标上升<br>$$<br>\mathbb{E}<em>{\prod\limits</em>{i\ne j}q_i(Z_i)}[\log p(X,Z)]&#x3D;\log \hat{p}(X,Z_j)\<br>q_j(Z_j)&#x3D;\hat{p}(X,Z_j)<br>$$</p></li><li><p>SGVI-变成优化问题，重参数法<br>$$<br>\mathop{argmax}<em>{q(Z)}L(q)&#x3D;\mathop{argmax}</em>{\phi}L(\phi)\<br>\nabla_\phi L(\phi)&#x3D;\mathbb{E}<em>{q_\phi}[(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))]\<br>&#x3D;\mathbb{E}</em>{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log q_\phi(z)]\nabla_\phi g_\phi(\varepsilon,x^i)]\<br>z&#x3D;g_\phi(\varepsilon,x^i),\varepsilon\sim p(\varepsilon)<br>$$</p></li></ol></li><li><p>随机性近似</p><ol><li><p>蒙特卡洛方法采样</p><ol><li><p>CDF 采样</p></li><li><p>拒绝采样， $q(z)$，使得 $\forall z_i,Mq(z_i)\ge p(z_i)$，拒绝因子：$\alpha&#x3D;\frac{p(z^i)}{Mq(z^i)}\le1$</p></li><li><p>重要性采样<br>$$<br>\mathbb{E}<em>{p(z)}[f(z)]&#x3D;\int p(z)f(z)dz&#x3D;\int \frac{p(z)}{q(z)}f(z)q(z)dz\simeq\frac{1}{N}\sum\limits</em>{i&#x3D;1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}<br>$$</p></li><li><p>重要性重采样：重要性采样+重采样</p></li></ol></li><li><p>MCMC：构建马尔可夫链概率序列，使其收敛到平稳分布 $p(z)$。</p><ol><li><p>转移矩阵（提议分布）<br>$$<br>p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)&#x3D;p(z^*)\cdot Q_{z^<em>\to z}\alpha(z^</em>,z)\<br>\alpha(z,z^*)&#x3D;\min{1,\frac{p(z^*)Q_{z^<em>\to z}}{p(z)Q_{z\to z^</em>}}}<br>$$</p></li><li><p>算法（MH）：</p><ol><li>通过在0，1之间均匀分布取点 $u$</li><li>生成 $z^<em>\sim Q(z^</em>|z^{i-1})$</li><li>计算 $\alpha$ 值</li><li>如果 $\alpha\ge u$，则 $z^i&#x3D;z^*$，否则 $z^{i}&#x3D;z^{i-1}$</li></ol></li></ol></li><li><p>Gibbs 采样：给定初始值 $z_1^0,z_2^0,\cdots$在 $t+1$ 时刻，采样 $z_i^{t+1}\sim p(z_i|z_{-i})$，从第一个维度一个个采样。</p></li></ol></li></ol></li></ol><h2 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h2><ol><li><p>Model<br>$$<br>p(x)&#x3D;\sum\limits_{k&#x3D;1}^Kp_k\mathcal{N}(x|\mu_k,\Sigma_k)<br>$$</p></li><li><p>求解-EM<br>$$<br>\begin{align}Q(\theta,\theta^t)&amp;&#x3D;\sum\limits_z[\log\prod\limits_{i&#x3D;1}^Np(x_i,z_i|\theta)]\prod \limits_{i&#x3D;1}^Np(z_i|x_i,\theta^t)\nonumber\<br>&amp;&#x3D;\sum\limits_z[\sum\limits_{i&#x3D;1}^N\log p(x_i,z_i|\theta)]\prod \limits_{i&#x3D;1}^Np(z_i|x_i,\theta^t)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^N\sum\limits_{z_i}\log p(x_i,z_i|\theta)p(z_i|x_i,\theta^t)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^N\sum\limits_{z_i}\log p_{z_i}\mathcal{N(x_i|\mu_{z_i},\Sigma_{z_i})}\frac{p_{z_i}^t\mathcal{N}(x_i|\mu_{z_i}^t,\Sigma_{z_i}^t)}{\sum\limits_kp_k^t\mathcal{N}(x_i|\mu_k^t,\Sigma_k^t)}<br>\end{align}<br>$$</p><p>$$<br>p_k^{t+1}&#x3D;\frac{1}{N}\sum\limits_{i&#x3D;1}^Np(z_i&#x3D;k|x_i,\theta^t)<br>$$</p></li></ol><h2 id="序列模型-HMM，LDS，Particle"><a href="#序列模型-HMM，LDS，Particle" class="headerlink" title="序列模型-HMM，LDS，Particle"></a>序列模型-HMM，LDS，Particle</h2><ol><li><p>假设：</p><ol><li><p>齐次 Markov 假设（未来只依赖于当前）：<br>$$<br>p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)&#x3D;p(i_{t+1}|i_t)<br>$$</p></li><li><p>观测独立假设：<br>$$<br>p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)&#x3D;p(o_t|i_t)<br>$$</p></li></ol></li><li><p>参数<br>$$<br>\lambda&#x3D;(\pi,A,B)<br>$$</p></li></ol><h3 id="离散线性隐变量-HMM"><a href="#离散线性隐变量-HMM" class="headerlink" title="离散线性隐变量-HMM"></a>离散线性隐变量-HMM</h3><ol><li><p>Evaluation：$p(O|\lambda)$，Forward-Backward 算法<br>$$<br>p(O|\lambda)&#x3D;\sum\limits_{i&#x3D;1}^Np(O,i_T&#x3D;q_i|\lambda)&#x3D;\sum\limits_{i&#x3D;1}^N\alpha_T(i)&#x3D;\sum\limits_{i&#x3D;1}^Nb_i(o_1)\pi_i\beta_1(i)\<br>\alpha_{t+1}(j)&#x3D;\sum\limits_{i&#x3D;1}^Nb_{j}(o_t)a_{ij}\alpha_t(i)\<br>\beta_t(i)&#x3D;\sum\limits_{j&#x3D;1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)<br>$$</p></li><li><p>Learning：$\lambda&#x3D;\mathop{argmax}\limits_{\lambda}p(O|\lambda)$，EM 算法（Baum-Welch）<br>$$<br>\lambda^{t+1}&#x3D;\mathop{argmax}<em>\lambda\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)\&#x3D;\sum\limits_I[\log \pi</em>{i_1}+\sum\limits_{t&#x3D;2}^T\log a_{i_{t-1},i_t}+\sum\limits_{t&#x3D;1}^T\log b_{i_t}(o_t)]p(O,I|\lambda^t)<br>$$</p></li><li><p>Decoding：$I&#x3D;\mathop{argmax}\limits_{I}p(I|O,\lambda)$，Viterbi 算法-动态规划<br>$$<br>\delta_{t}(j)&#x3D;\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t&#x3D;q_i)\\delta_{t+1}(j)&#x3D;\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})\\psi_{t+1}(j)&#x3D;\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}<br>$$</p></li></ol><h3 id="连续线性隐变量-LDS"><a href="#连续线性隐变量-LDS" class="headerlink" title="连续线性隐变量-LDS"></a>连续线性隐变量-LDS</h3><ol><li><p>Model<br>$$<br>\begin{align}<br>p(z_t|z_{t-1})&amp;\sim\mathcal{N}(A\cdot z_{t-1}+B,Q)\<br>p(x_t|z_t)&amp;\sim\mathcal{N}(C\cdot z_t+D,R)\<br>z_1&amp;\sim\mathcal{N}(\mu_1,\Sigma_1)<br>\end{align}<br>$$</p></li><li><p>滤波<br>$$<br>p(z_t|x_{1:t})&#x3D;p(x_{1:t},z_t)&#x2F;p(x_{1:t})\propto p(x_{1:t},z_t)\&#x3D;p(x_t|z_t)p(z_t|x_{1:t-1})p(x_{1:t-1})\propto p(x_t|z_t)p(z_t|x_{1:t-1})<br>$$</p></li><li><p>递推求解-线性高斯模型</p><ol><li><p>Prediction<br>$$<br>p(z_t|x_{1:t-1})&#x3D;\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}&#x3D;\int_{z_{t-1}}\mathcal{N}(Az_{t-1}+B,Q)\mathcal{N}(\mu_{t-1},\Sigma_{t-1})dz_{t-1}<br>$$</p></li><li><p>Update:<br>$$<br>p(z_t|x_{1:t})\propto p(x_t|z_t)p(z_t|x_{1:t-1}<br>$$</p></li></ol></li></ol><h3 id="连续非线性隐变量-粒子滤波"><a href="#连续非线性隐变量-粒子滤波" class="headerlink" title="连续非线性隐变量-粒子滤波"></a>连续非线性隐变量-粒子滤波</h3><p>通过采样(SIR)解决：<br>$$<br>\mathbb{E}[f(z)]&#x3D;\int_zf(z)p(z)dz&#x3D;\int_zf(z)\frac{p(z)}{q(z)}q(z)dz&#x3D;\sum\limits_{i&#x3D;1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}<br>$$</p><ol><li><p>采样<br>$$<br>w_t^i\propto\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}^i\<br>q(z_t|z_{1:t-1},x_{1:t})&#x3D;p(z_t|z_{t-1})<br>$$</p></li><li><p>重采样</p></li></ol><h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><ol><li><p>PDF<br>$$<br>p(Y&#x3D;y|X&#x3D;x)&#x3D;\frac{1}{Z(x,\theta)}\exp[\theta^TH(y_t,y_{t-1},x)]<br>$$</p></li><li><p>边缘概率<br>$$<br>p(y_t&#x3D;i|x)&#x3D;\sum\limits_{y_{1:t-1}}\sum\limits_{y_{t+1:T}}\frac{1}{Z}\prod\limits_{t’&#x3D;1}^T\phi_{t’}(y_{t’-1},y_{t’},x)\<br>p(y_t&#x3D;i|x)&#x3D;\frac{1}{Z}\Delta_l\Delta_r\<br>\Delta_l&#x3D;\sum\limits_{y_{1:t-1}}\phi_{1}(y_0,y_1,x)\phi_2(y_1,y_2,x)\cdots\phi_{t-1}(y_{t-2},y_{t-1},x)\phi_t(y_{t-1},y_t&#x3D;i,x)\<br>\Delta_r&#x3D;\sum\limits_{y_{t+1:T}}\phi_{t+1}(y_t&#x3D;i,y_{t+1},x)\phi_{t+2}(y_{t+1},y_{t+2},x)\cdots\phi_T(y_{T-1},y_T,x)<br>$$</p><p>$$<br>\alpha_t(i)&#x3D;\Delta_l&#x3D;\sum\limits_{j\in S}\phi_t(y_{t-1}&#x3D;j,y_t&#x3D;i,x)\alpha_{t-1}(j)\<br>\Delta_r&#x3D;\beta_t(i)&#x3D;\sum\limits_{j\in S}\phi_{t+1}(y_t&#x3D;i,y_{t+1}&#x3D;j,x)\beta_{t+1}(j)<br>$$</p></li><li><p>学习<br>$$<br>\nabla_\lambda L&#x3D;\sum\limits_{i&#x3D;1}^N\sum\limits_{t&#x3D;1}^T[f(y_{t-1},y_t,x^i)-\sum\limits_{y_{t-1}}\sum\limits_{y_t}p(y_{t-1},y_t|x^i)f(y_{t-1},y_t,x^i)]<br>$$</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——高斯混合模型</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/9.GMM/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/9.GMM/</url>
    
    <content type="html"><![CDATA[<h1 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h1><p>为了解决高斯模型的单峰性的问题，我们引入多个高斯模型的加权平均来拟合多峰数据：<br>$$<br>p(x)&#x3D;\sum\limits_{k&#x3D;1}^K\alpha_k\mathcal{N}(\mu_k,\Sigma_k)<br>$$<br>引入隐变量 $z$，这个变量表示对应的样本 $x$ 属于哪一个高斯分布，这个变量是一个离散的随机变量：<br>$$<br>p(z&#x3D;i)&#x3D;p_i,\sum\limits_{i&#x3D;1}^kp(z&#x3D;i)&#x3D;1<br>$$<br>作为一个生成式模型，高斯混合模型通过隐变量 $z$ 的分布来生成样本。用概率图来表示：</p><pre><code class=" mermaid">graph LR;z((z))--&gt;x((x))</code></pre><p>其中，节点 $z$ 就是上面的概率，$x$ 就是生成的高斯分布。于是对 $p(x)$：<br>$$<br>p(x)&#x3D;\sum\limits_zp(x,z)&#x3D;\sum\limits_{k&#x3D;1}^Kp(x,z&#x3D;k)&#x3D;\sum\limits_{k&#x3D;1}^Kp(z&#x3D;k)p(x|z&#x3D;k)<br>$$<br>因此：<br>$$<br>p(x)&#x3D;\sum\limits_{k&#x3D;1}^Kp_k\mathcal{N}(x|\mu_k,\Sigma_k)<br>$$</p><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>样本为 $X&#x3D;(x_1,x_2,\cdots,x_N)$，$ (X,Z)$ 为完全参数，参数为 $\theta&#x3D;{p_1,p_2,\cdots,p_K,\mu_1,\mu_2,\cdots,\mu_K\Sigma_1,\Sigma_2,\cdots,\Sigma_K}$。我们通过极大似然估计得到 $\theta$ 的值：<br>$$<br>\begin{align}\theta_{MLE}&amp;&#x3D;\mathop{argmax}\limits_{\theta}\log p(X)&#x3D;\mathop{argmax}<em>{\theta}\sum\limits</em>{i&#x3D;1}^N\log p(x_i)\nonumber\<br>&amp;&#x3D;\mathop{argmax}<em>\theta\sum\limits</em>{i&#x3D;1}^N\log \sum\limits_{k&#x3D;1}^Kp_k\mathcal{N}(x_i|\mu_k,\Sigma_k)<br>\end{align}<br>$$<br>这个表达式直接通过求导，由于连加号的存在，无法得到解析解。因此需要使用 EM 算法。</p><h2 id="EM-求解-GMM"><a href="#EM-求解-GMM" class="headerlink" title="EM 求解 GMM"></a>EM 求解 GMM</h2><p>EM 算法的基本表达式为：$\theta^{t+1}&#x3D;\mathop{argmax}\limits_{\theta}\mathbb{E}<em>{z|x,\theta_t}[p(x,z|\theta)]$。套用 GMM 的表达式，对数据集来说：<br>$$<br>\begin{align}Q(\theta,\theta^t)&amp;&#x3D;\sum\limits_z[\log\prod\limits</em>{i&#x3D;1}^Np(x_i,z_i|\theta)]\prod \limits_{i&#x3D;1}^Np(z_i|x_i,\theta^t)\nonumber\<br>&amp;&#x3D;\sum\limits_z[\sum\limits_{i&#x3D;1}^N\log p(x_i,z_i|\theta)]\prod \limits_{i&#x3D;1}^Np(z_i|x_i,\theta^t)<br>\end{align}<br>$$<br>对于中间的那个求和号，展开，第一项为：<br>$$<br>\begin{align}<br>\sum\limits_z\log p(x_1,z_1|\theta)\prod\limits_{i&#x3D;1}^Np(z_i|x_i,\theta^t)&amp;&#x3D;\sum\limits_z\log p(x_1,z_1|\theta)p(z_1|x_1,\theta^t)\prod\limits_{i&#x3D;2}^Np(z_i|x_i,\theta^t)\nonumber\<br>&amp;&#x3D;\sum\limits_{z_1}\log p(x_1,z_1|\theta)<br>p(z_1|x_1,\theta^t)\sum\limits_{z_2,\cdots,z_K}\prod\limits_{i&#x3D;2}^Np(z_i|x_i,\theta^t)\nonumber\<br>&amp;&#x3D;\sum\limits_{z_1}\log p(x_1,z_1|\theta)p(z_1|x_1,\theta^t)\end{align}<br>$$<br>类似地，$Q$ 可以写为：<br>$$<br>Q(\theta,\theta^t)&#x3D;\sum\limits_{i&#x3D;1}^N\sum\limits_{z_i}\log p(x_i,z_i|\theta)p(z_i|x_i,\theta^t)<br>$$<br>对于 $p(x,z|\theta)$：<br>$$<br>p(x,z|\theta)&#x3D;p(z|\theta)p(x|z,\theta)&#x3D;p_z\mathcal{N}(x|\mu_z,\Sigma_z)<br>$$<br>对 $p(z|x,\theta^t)$：<br>$$<br>p(z|x,\theta^t)&#x3D;\frac{p(x,z|\theta^t)}{p(x|\theta^t)}&#x3D;\frac{p_z^t\mathcal{N}(x|\mu_z^t,\Sigma_z^t)}{\sum\limits_kp_k^t\mathcal{N}(x|\mu_k^t,\Sigma_k^t)}<br>$$<br>代入 $Q$：<br>$$<br>Q&#x3D;\sum\limits_{i&#x3D;1}^N\sum\limits_{z_i}\log p_{z_i}\mathcal{N(x_i|\mu_{z_i},\Sigma_{z_i})}\frac{p_{z_i}^t\mathcal{N}(x_i|\mu_{z_i}^t,\Sigma_{z_i}^t)}{\sum\limits_kp_k^t\mathcal{N}(x_i|\mu_k^t,\Sigma_k^t)}<br>$$<br>下面需要对 $Q$ 值求最大值：<br>$$<br>Q&#x3D;\sum\limits_{k&#x3D;1}^K\sum\limits_{i&#x3D;1}^N[\log p_k+\log \mathcal{N}(x_i|\mu_k,\Sigma_k)]p(z_i&#x3D;k|x_i,\theta^t)<br>$$</p><ol><li><p>$p_k^{t+1}$：<br>$$<br>p_k^{t+1}&#x3D;\mathop{argmax}<em>{p_k}\sum\limits</em>{k&#x3D;1}^K\sum\limits_{i&#x3D;1}^N[\log p_k+\log \mathcal{N}(x_i|\mu_k,\Sigma_k)]p(z_i&#x3D;k|x_i,\theta^t)\ s.t.\ \sum\limits_{k&#x3D;1}^Kp_k&#x3D;1<br>$$<br>即：<br>$$<br>p_k^{t+1}&#x3D;\mathop{argmax}<em>{p_k}\sum\limits</em>{k&#x3D;1}^K\sum\limits_{i&#x3D;1}^N\log p_kp(z_i&#x3D;k|x_i,\theta^t)\ s.t.\ \sum\limits_{k&#x3D;1}^Kp_k&#x3D;1<br>$$<br>引入 Lagrange 乘子：$L(p_k,\lambda)&#x3D;\sum\limits_{k&#x3D;1}^K\sum\limits_{i&#x3D;1}^N\log p_kp(z_i&#x3D;k|x_i,\theta^t)-\lambda(1-\sum\limits_{k&#x3D;1}^Kp_k)$。所以：<br>$$<br>\frac{\partial}{\partial p_k}L&#x3D;\sum\limits_{i&#x3D;1}^N\frac{1}{p_k}p(z_i&#x3D;k|x_i,\theta^t)+\lambda&#x3D;0\<br>\Rightarrow \sum\limits_k\sum\limits_{i&#x3D;1}^N\frac{1}{p_k}p(z_i&#x3D;k|x_i,\theta^t)+\lambda\sum\limits_kp_k&#x3D;0\<br>\Rightarrow\lambda&#x3D;-N<br>$$</p><p>于是有：<br>$$<br>p_k^{t+1}&#x3D;\frac{1}{N}\sum\limits_{i&#x3D;1}^Np(z_i&#x3D;k|x_i,\theta^t)<br>$$</p></li><li><p>$\mu_k,\Sigma_k$，这两个参数是无约束的，直接求导即可。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——期望最大</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/8.EM/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/8.EM/</url>
    
    <content type="html"><![CDATA[<h1 id="期望最大"><a href="#期望最大" class="headerlink" title="期望最大"></a>期望最大</h1><p>期望最大算法的目的是解决具有隐变量的混合模型的参数估计（极大似然估计）。MLE 对 $p(x|\theta)$ 参数的估计记为：$\theta_{MLE}&#x3D;\mathop{argmax}\limits_\theta\log p(x|\theta)$。EM 算法对这个问题的解决方法是采用迭代的方法：<br>$$<br>\theta^{t+1}&#x3D;\mathop{argmax}\limits_{\theta}\int_z\log [p(x,z|\theta)]p(z|x,\theta^t)dz&#x3D;\mathbb{E}_{z|x,\theta^t}[\log p(x,z|\theta)]<br>$$<br>这个公式包含了迭代的两步：</p><ol><li>E step：计算 $\log p(x,z|\theta)$ 在概率分布 $p(z|x,\theta^t)$ 下的期望</li><li>M step：计算使这个期望最大化的参数得到下一个 EM 步骤的输入</li></ol><blockquote><p>  求证：$\log p(x|\theta^t)\le\log p(x|\theta^{t+1})$</p><p>  证明：$\log p(x|\theta)&#x3D;\log p(z,x|\theta)-\log p(z|x,\theta)$，对左右两边求积分：<br>  $$<br>  Left:\int_zp(z|x,\theta^t)\log p(x|\theta)dz&#x3D;\log p(x|\theta)<br>  $$</p><p>  $$<br>  Right:\int_zp(z|x,\theta^t)\log p(x,z|\theta)dz-\int_zp(z|x,\theta^t)\log p(z|x,\theta)dz&#x3D;Q(\theta,\theta^t)-H(\theta,\theta^t)<br>  $$</p><p>  所以：<br>  $$<br>  \log p(x|\theta)&#x3D;Q(\theta,\theta^t)-H(\theta,\theta^t)<br>  $$<br>  由于 $Q(\theta,\theta^t)&#x3D;\int_zp(z|x,\theta^t)\log p(x,z|\theta)dz$，而 $\theta^{t+1}&#x3D;\mathop{argmax}\limits_{\theta}\int_z\log [p(x,z|\theta)]p(z|x,\theta^t)dz$，所以 $Q(\theta^{t+1},\theta^t)\ge Q(\theta^t,\theta^t)$。要证 $\log p(x|\theta^t)\le\log p(x|\theta^{t+1})$，需证：$H(\theta^t,\theta^t)\ge H(\theta^{t+1},\theta^t)$：<br>  $$<br>  \begin{align}H(\theta^{t+1},\theta^t)-H(\theta^{t},\theta^t)&amp;&#x3D;\int_zp(z|x,\theta^{t})\log p(z|x,\theta^{t+1})dz-\int_zp(z|x,\theta^t)\log p(z|x,\theta^{t})dz\nonumber\<br>  &amp;&#x3D;\int_zp(z|x,\theta^t)\log\frac{p(z|x,\theta^{t+1})}{p(z|x,\theta^t)}&#x3D;-KL(p(z|x,\theta^t),p(z|x,\theta^{t+1}))\le0<br>  \end{align}<br>  $$<br>  综合上面的结果：<br>  $$<br>  \log p(x|\theta^t)\le\log p(x|\theta^{t+1})<br>  $$</p></blockquote><p>根据上面的证明，我们看到，似然函数在每一步都会增大。进一步的，我们看 EM 迭代过程中的式子是怎么来的：<br>$$<br>\log p(x|\theta)&#x3D;\log p(z,x|\theta)-\log p(z|x,\theta)&#x3D;\log \frac{p(z,x|\theta)}{q(z)}-\log \frac{p(z|x,\theta)}{q(z)}<br>$$<br>分别对两边求期望 $\mathbb{E}<em>{q(z)}$：<br>$$<br>\begin{align}<br>&amp;Left:\int_zq(z)\log p(x|\theta)dz&#x3D;\log p(x|\theta)\<br>&amp;Right:\int_zq(z)\log \frac{p(z,x|\theta)}{q(z)}dz-\int_zq(z)\log \frac{p(z|x,\theta)}{q(z)}dz&#x3D;ELBO+KL(q(z),p(z|x,\theta))<br>\end{align}<br>$$<br>上式中，Evidence Lower Bound(ELBO)，是一个下界，所以 $\log p(x|\theta)\ge ELBO$，等于号取在 KL 散度为0是，即：$q(z)&#x3D;p(z|x,\theta)$，EM 算法的目的是将 ELBO 最大化，根据上面的证明过程，在每一步 EM 后，求得了最大的ELBO，并根据这个使 ELBO 最大的参数代入下一步中：<br>$$<br>\hat{\theta}&#x3D;\mathop{argmax}</em>{\theta}ELBO&#x3D;\mathop{argmax}<em>\theta\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz<br>$$<br>由于 $ q(z)&#x3D;p(z|x,\theta^t)$ 的时候，这一步的最大值才能取等号，所以：<br>$$<br>\hat{\theta}&#x3D;\mathop{argmax}</em>{\theta}ELBO&#x3D;\mathop{argmax}_\theta\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz&#x3D;\mathop{argmax}_\theta\int_zp(z|x,\theta^t)\log\frac{p(x,z|\theta)}{p(z|x,\theta^t)}d z\<br>&#x3D;\mathop{argmax}_\theta\int_z p(z|x,\theta^t)\log p(x,z|\theta)<br>$$<br>这个式子就是上面 EM 迭代过程中的式子。</p><p>从 Jensen 不等式出发，也可以导出这个式子：<br>$$<br>\log p(x|\theta)&#x3D;\log\int_zp(x,z|\theta)dz&#x3D;\log\int_z\frac{p(x,z|\theta)q(z)}{q(z)}dz\<br>&#x3D;\log \mathbb{E}<em>{q(z)}[\frac{p(x,z|\theta)}{q(z)}]\ge \mathbb{E}</em>{q(z)}[\log\frac{p(x,z|\theta)}{q(z)}]<br>$$<br>其中，右边的式子就是 ELBO，等号在 $ p(x,z|\theta)&#x3D;Cq(z)$ 时成立。于是：<br>$$<br>\int_zq(z)dz&#x3D;\frac{1}{C}\int_zp(x,z|\theta)dz&#x3D;\frac{1}{C}p(x|\theta)&#x3D;1\<br>\Rightarrow q(z)&#x3D;\frac{1}{p(x|\theta)}p(x,z|\theta)&#x3D;p(z|x,\theta)<br>$$<br>我们发现，这个过程就是上面的最大值取等号的条件。</p><h2 id="广义-EM"><a href="#广义-EM" class="headerlink" title="广义 EM"></a>广义 EM</h2><p>EM 模型解决了概率生成模型的参数估计的问题，通过引入隐变量 $z$，来学习 $\theta$，具体的模型对 $z$ 有不同的假设。对学习任务 $p(x|\theta)$，就是学习任务 $\frac{p(x,z|\theta)}{p(z|x,\theta)}$。在这个式子中，我们假定了在 E 步骤中，$q(z)&#x3D;p(z|x,\theta)$，但是这个$p(z|x,\theta)$ 如果无法求解，那么必须使用采样（MCMC）或者变分推断等方法来近似推断这个后验。我们观察 KL 散度的表达式，为了最大化 ELBO，在固定的 $\theta$ 时，我们需要最小化 KL 散度，于是：<br>$$<br>\hat{q}(z)&#x3D;\mathop{argmin}_qKL(p,q)&#x3D;\mathop{argmax}_qELBO<br>$$<br>这就是广义 EM 的基本思路：</p><ol><li><p>E step：<br>$$<br>\hat{q}^{t+1}(z)&#x3D;\mathop{argmax}_q\int_zq^t(z)\log\frac{p(x,z|\theta)}{q^t(z)}dz,fixed\ \theta<br>$$</p></li><li><p>M step：<br>$$<br>\hat{\theta}&#x3D;\mathop{argmax}_\theta \int_zq^{t+1}(z)\log\frac{p(x,z|\theta)}{q^{t+1}(z)}dz,fixed\ \hat{q}<br>$$</p></li></ol><p>对于上面的积分：<br>$$<br>ELBO&#x3D;\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz&#x3D;\mathbb{E}_{q(z)}[p(x,z|\theta)]+Entropy(q(z))<br>$$<br>因此，我们看到，广义 EM 相当于在原来的式子中加入熵这一项。</p><h2 id="EM-的推广"><a href="#EM-的推广" class="headerlink" title="EM 的推广"></a>EM 的推广</h2><p>EM 算法类似于坐标上升法，固定部分坐标，优化其他坐标，再一遍一遍的迭代。如果在 EM 框架中，无法求解 $z$ 后验概率，那么需要采用一些变种的 EM 来估算这个后验。</p><ol><li>基于平均场的变分推断，VBEM&#x2F;VEM</li><li>基于蒙特卡洛的EM，MCEM</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——概率图模型</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/7.PGMIntro/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/7.PGMIntro/</url>
    
    <content type="html"><![CDATA[<h1 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h1><p>概率图模型使用图的方式表示概率分布。为了在图中添加各种概率，首先总结一下随机变量分布的一些规则：<br>$$<br>\begin{align}<br>&amp;Sum\ Rule:p(x_1)&#x3D;\int p(x_1,x_2)dx_2\<br>&amp;Product\ Rule:p(x_1,x_2)&#x3D;p(x_1|x_2)p(x_2)\<br>&amp;Chain\ Rule:p(x_1,x_2,\cdots,x_p)&#x3D;\prod\limits_{i&#x3D;1}^pp(x_i|x_{i+1,x_{i+2} \cdots}x_p)\<br>&amp;Bayesian\ Rule:p(x_1|x_2)&#x3D;\frac{p(x_2|x_1)p(x_1)}{p(x_2)}<br>\end{align}<br>$$<br>可以看到，在链式法则中，如果数据维度特别高，那么的采样和计算非常困难，我们需要在一定程度上作出简化，在朴素贝叶斯中，作出了条件独立性假设。在 Markov 假设中，给定数据的维度是以时间顺序出现的，给定当前时间的维度，那么下一个维度与之前的维度独立。在 HMM 中，采用了齐次 Markov 假设。在 Markov 假设之上，更一般的，加入条件独立性假设，对维度划分集合 $A,B,C$，使得 $X_A\perp X_B|X_C$。</p><p>概率图模型采用图的特点表示上述的条件独立性假设，节点表示随机变量，边表示条件概率。概率图模型可以分为三大理论部分：</p><ol><li>表示：<ol><li>有向图（离散）：贝叶斯网络</li><li>高斯图（连续）：高斯贝叶斯和高斯马尔可夫网路</li><li>无向图（离散）：马尔可夫网络</li></ol></li><li>推断<ol><li>精确推断</li><li>近似推断<ol><li>确定性近似（如变分推断）</li><li>随机近似（如 MCMC）</li></ol></li></ol></li><li>学习<ol><li>参数学习<ol><li>完备数据</li><li>隐变量：E-M 算法</li></ol></li><li>结构学习</li></ol></li></ol><h2 id="有向图-贝叶斯网络"><a href="#有向图-贝叶斯网络" class="headerlink" title="有向图-贝叶斯网络"></a>有向图-贝叶斯网络</h2><p>已知联合分布中，各个随机变量之间的依赖关系，那么可以通过拓扑排序（根据依赖关系）可以获得一个有向图。而如果已知一个图，也可以直接得到联合概率分布的因子分解：<br>$$<br>p(x_1,x_2,\cdots,x_p)&#x3D;\prod\limits_{i&#x3D;1}^pp(x_i|x_{parent(i)})<br>$$<br>那么实际的图中条件独立性是如何体现的呢？在局部任何三个节点，可以有三种结构：</p><ol><li><pre><code class="mermaid">graph TB;    A((A))--&gt;B((B));    B--&gt;C((C));<figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs coq"><br>    $$<br>    p(A,B,C)=p(A)p(B|<span class="hljs-type">A</span>)p(C|<span class="hljs-type">B</span>)=p(A)p(B|<span class="hljs-type">A</span>)p(C|<span class="hljs-type">B</span>,A)\\<br>    \Longrightarrow p(C|<span class="hljs-type">B</span>)=p(C|<span class="hljs-type">B</span>,A)\\<br>    \Leftrightarrow p(C|<span class="hljs-type">B</span>)p(A|<span class="hljs-type">B</span>)=p(C|<span class="hljs-type">A</span>,B)p(A|<span class="hljs-type">B</span>)=p(C,A|<span class="hljs-type">B</span>)\\<br>    \Longrightarrow C\perp A|<span class="hljs-type">B</span><br>    $$<br><br><span class="hljs-number">2.</span>  ```mermaid<br>    graph TB;<br>    B((B))--&gt;A((A));<br>    B--&gt;C((C));<br></code></pre></td></tr></table></figure>$$p(A,B,C)=p(A|B)p(B)p(C|B)=p(B)p(A|B)p(C|A,B)\\\Longrightarrow p(C|B)=p(C|B,A)\\\Leftrightarrow p(C|B)p(A|B)=p(C|A,B)p(A|B)=p(C,A|B)\\\Longrightarrow C\perp A|B$$</code></pre></li><li><pre><code class="mermaid">graph TB;    A((A))--&gt;B((B));    C((C))--&gt;B<figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs gams"><br>    <span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br>    p(A,B,C)=p(A)p(C)p(B|C,A)=p(A)p(C|A)p(B|C,A)\\<br>    \Longrightarrow p(C)=p(C|A)\\<br>    \Leftrightarrow C\perp A\\<br>    <span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br><br>    对这种结构，<span class="hljs-symbol">$</span>A,C<span class="hljs-symbol">$</span> 不与 <span class="hljs-symbol">$</span>B<span class="hljs-symbol">$</span> 条件独立。<br><br>从整体的图来看，可以引入 D 划分的概念。对于类似上面图 <span class="hljs-number">1</span>和图 <span class="hljs-number">2</span>的关系，引入集合A，B，那么满足 <span class="hljs-symbol">$</span>A\perp B|C<span class="hljs-symbol">$</span> 的 <span class="hljs-symbol">$</span>C<span class="hljs-symbol">$</span> 集合中的点与 <span class="hljs-symbol">$</span>A,B<span class="hljs-symbol">$</span>  中的点的关系都满足图 <span class="hljs-number">1</span>，<span class="hljs-number">2</span>，满足图<span class="hljs-number">3</span> 关系的点都不在 <span class="hljs-symbol">$</span>C<span class="hljs-symbol">$</span> 中。D 划分应用在贝叶斯定理中：<br><span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br>p(x_i|x_&#123;-i&#125;)=\<span class="hljs-built_in">frac</span>&#123;p(x)&#125;&#123;\int p(x)dx_&#123;i&#125;&#125;=\<span class="hljs-built_in">frac</span>&#123;\<span class="hljs-keyword">prod</span>\limits_&#123;j=<span class="hljs-number">1</span>&#125;^pp(x_j|x_&#123;parents(j)&#125;)&#125;&#123;\int\<span class="hljs-keyword">prod</span>\limits_&#123;j=<span class="hljs-number">1</span>&#125;^pp(x_j|x_&#123;parents(j)&#125;)dx_i&#125;<br><span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br>可以发现，上下部分可以分为两部分，一部分是和 <span class="hljs-symbol">$</span>x_i<span class="hljs-symbol">$</span> 相关的，另一部分是和 <span class="hljs-symbol">$</span>x_i<span class="hljs-symbol">$</span> 无关的，而这个无关的部分可以相互约掉。于是计算只涉及和 <span class="hljs-symbol">$</span>x_i<span class="hljs-symbol">$</span> 相关的部分。<br><br>与 <span class="hljs-symbol">$</span>x_i<span class="hljs-symbol">$</span> 相关的部分可以写成：<br><span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br>p(x_i|x_&#123;parents(i)&#125;)p(x_&#123;child(i)&#125;|x_i)<br><span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br>这些相关的部分又叫做 Markov 毯。<br><br>实际应用的模型中，对这些条件独立性作出了假设，从单一到混合，从有限到无限（时间，空间）可以分为：<br><br><span class="hljs-number">1.</span>  朴素贝叶斯，单一的条件独立性假设 <span class="hljs-symbol">$</span>p(x|y)=\<span class="hljs-keyword">prod</span>\limits_&#123;i=<span class="hljs-number">1</span>&#125;^pp(x_i|y)<span class="hljs-symbol">$</span>，在 D 划分后，所有条件依赖的集合就是单个元素。<br><span class="hljs-number">2.</span>  高斯混合模型：混合的条件独立。引入多类别的隐变量 <span class="hljs-symbol">$</span>z_1, z_2,\cdots,z_k<span class="hljs-symbol">$</span>， <span class="hljs-symbol">$</span>p(x|z)=\mathcal&#123;N&#125;(\mu,\Sigma)<span class="hljs-symbol">$</span>，条件依赖集合为多个元素。<br><span class="hljs-number">3.</span>  与时间相关的条件依赖<br>    <span class="hljs-number">1.</span>  Markov 链<br>    <span class="hljs-number">2.</span>  高斯过程（无限维高斯分布）<br><span class="hljs-number">4.</span>  连续：高斯贝叶斯网络<br><span class="hljs-number">5.</span>  组合上面的分类<br>    *   GMM 与时序结合：动态模型<br>        *   HMM（离散）<br>        *   线性动态系统 LDS（Kalman 滤波）<br>        *   粒子滤波（非高斯，非线性）<br><br>## 无向图-马尔可夫网络（马尔可夫随机场）<br><br>无向图没有了类似有向图的局部不同结构，在马尔可夫网络中，也存在 D 划分的概念。直接将条件独立的集合 <span class="hljs-symbol">$</span>x_A\perp x_B|x_C<span class="hljs-symbol">$</span> 划分为三个集合。这个也叫全局 Markov。对局部的节点，<span class="hljs-symbol">$</span>x\perp (X-Neighbour(\mathcal&#123;x&#125;))|Neighbour(x)<span class="hljs-symbol">$</span>。这也叫局部 Markov。对于成对的节点：<span class="hljs-symbol">$</span>x_i\perp x_j|x_&#123;-i-j&#125;<span class="hljs-symbol">$</span>，其中 <span class="hljs-symbol">$</span>i,j<span class="hljs-symbol">$</span> 不能相邻。这也叫成对 Markov。事实上上面三个点局部全局成对是相互等价的。<br><br>有了这个条件独立性的划分，还需要因子分解来实际计算。引入团的概念：<br><br>&gt;   团，最大团：图中节点的集合，集合中的节点之间相互都是连接的叫做团，如果不能再添加节点，那么叫最大团。<br><br>利用这个定义进行的 <span class="hljs-symbol">$</span>x<span class="hljs-symbol">$</span> 所有维度的联合概率分布的因子分解为，假设有 <span class="hljs-symbol">$</span>K<span class="hljs-symbol">$</span> 个团，<span class="hljs-symbol">$</span>Z<span class="hljs-symbol">$</span> 就是对所有可能取值求和：<br><span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br>\begin&#123;align&#125;p(x)=\<span class="hljs-built_in">frac</span>&#123;<span class="hljs-number">1</span>&#125;&#123;Z&#125;\<span class="hljs-keyword">prod</span>\limits_&#123;i=<span class="hljs-number">1</span>&#125;^&#123;K&#125;\phi(x_&#123;ci&#125;)\\<br>Z=\<span class="hljs-keyword">sum</span>\limits_&#123;x\in\mathcal&#123;X&#125;&#125;\<span class="hljs-keyword">prod</span>\limits_&#123;i=<span class="hljs-number">1</span>&#125;^&#123;K&#125;\phi(x_&#123;ci&#125;)<br>\end&#123;align&#125;<br><span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br>其中 <span class="hljs-symbol">$</span>\phi(x_&#123;ci&#125;)<span class="hljs-symbol">$</span> 叫做势函数，它必须是一个正值，可以记为：<br><span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br>\phi(x_&#123;ci&#125;)=\<span class="hljs-built_in">exp</span>(-E(x_&#123;ci&#125;))<br><span class="hljs-symbol">$</span><span class="hljs-symbol">$</span><br> 这个分布叫做 Gibbs 分布（玻尔兹曼分布）。于是也可以记为：<span class="hljs-symbol">$</span>p(x)=\<span class="hljs-built_in">frac</span>&#123;<span class="hljs-number">1</span>&#125;&#123;Z&#125;\<span class="hljs-built_in">exp</span>(-\<span class="hljs-keyword">sum</span>\limits_&#123;i=<span class="hljs-number">1</span>&#125;^KE(x_&#123;ci&#125;))<span class="hljs-symbol">$</span>。这个分解和条件独立性等价（Hammesley-Clifford 定理），这个分布的形式也和指数族分布形式上相同，于是满足最大熵原理。<br><br>## 两种图的转换-道德图<br><br>我们常常想将有向图转为无向图，从而应用更一般的表达式。<br><br><span class="hljs-number">1.</span>  链式：<br><br>    ```mermaid<br>    graph TB;<br>    A((A))--&gt;B((B));<br>    B--&gt;C((C));<br></code></pre></td></tr></table></figure>直接去掉箭头，$p(a,b,c)=p(a)p(b|a)p(c|b)=\phi(a,b)\phi(b,c)$：<pre><code class=" mermaid">graph TB;A((A))---B((B));B---C((C));</code></pre></code></pre></li><li><p>V 形：</p><pre><code class=" mermaid">graph TB;B((B))--&gt;A((A));B--&gt;C((C));</code></pre><p>由于 $p(a,b,c)&#x3D;p(b)p(a|b)p(c|b)&#x3D;\phi(a,b)\phi(b,c)$，直接去掉箭头：</p><pre><code class=" mermaid">graph TB;B((B))---A((A));B---C((C));</code></pre></li><li><p>倒 V 形：</p><pre><code class=" mermaid">graph TB;A((A))--&gt;B((B));C((C))--&gt;B</code></pre><p>由于 $p(a,b,c)&#x3D;p(a)p(c)p(b|a,c)&#x3D;\phi(a,b,c)$，于是在 $a,c$ 之间添加线：</p><pre><code class=" mermaid">graph TD;a((a))---b((b));b---c((c));a---c;</code></pre><p>观察着三种情况可以概括为：</p><ol><li>将每个节点的父节点两两相连</li><li>将有向边替换为无向边</li></ol></li></ol><h2 id="更精细的分解-因子图"><a href="#更精细的分解-因子图" class="headerlink" title="更精细的分解-因子图"></a>更精细的分解-因子图</h2><p>对于一个有向图，可以通过引入环的方式，可以将其转换为无向图（Tree-like graph），这个图就叫做道德图。但是我们上面的 BP 算法只对无环图有效，通过因子图可以变为无环图。</p><p>考虑一个无向图：</p><pre><code class=" mermaid">graph TD;a((a))---b((b));b---c((c));a---c;</code></pre><p>可以将其转为：</p><pre><code class=" mermaid">graph TD;a((a))---f;f---b((b));f---c((c))</code></pre><p>其中 $f&#x3D;f(a,b,c)$。因子图不是唯一的，这是由于因式分解本身就对应一个特殊的因子图，将因式分解：$p(x)&#x3D;\prod\limits_{s}f_s(x_s)$ 可以进一步分解得到因子图。</p><h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>推断的主要目的是求各种概率分布，包括边缘概率，条件概率，以及使用 MAP 来求得参数。通常推断可以分为：</p><ol><li>精确推断<ol><li>Variable Elimination(VE)</li><li>Belief Propagation(BP, Sum-Product Algo)，从 VE 发展而来</li><li>Junction Tree，上面两种在树结构上应用，Junction Tree 在图结构上应用</li></ol></li><li>近似推断<ol><li>Loop Belief Propagation（针对有环图）</li><li>Mente Carlo Interference：例如 Importance Sampling，MCMC</li><li>Variational Inference</li></ol></li></ol><h3 id="推断-变量消除（VE）"><a href="#推断-变量消除（VE）" class="headerlink" title="推断-变量消除（VE）"></a>推断-变量消除（VE）</h3><p>变量消除的方法是在求解概率分布的时候，将相关的条件概率先行求和或积分，从而一步步地消除变量，例如在马尔可夫链中：</p><pre><code class=" mermaid">graph LR;a((a))--&gt;b((b));b--&gt;c((c));c--&gt;d((d))</code></pre><p>$$<br>p(d)&#x3D;\sum\limits_{a,b,c}p(a,b,c,d)&#x3D;\sum\limits_cp(d|c)\sum\limits_bp(c|b)\sum\limits_ap(b|a)p(a)<br>$$</p><p>变量消除的缺点很明显：</p><ol><li>计算步骤无法存储</li><li>消除的最优次序是一个 NP-hard 问题</li></ol><h3 id="推断-信念传播（BP）"><a href="#推断-信念传播（BP）" class="headerlink" title="推断-信念传播（BP）"></a>推断-信念传播（BP）</h3><p>为了克服 VE 的第一个缺陷-计算步骤无法存储。我们进一步地对上面的马尔可夫链进行观察：</p><pre><code class=" mermaid">graph LR;a((a))--&gt;b((b));b--&gt;c((c));c--&gt;d((d));d--&gt;e((e));</code></pre><p>要求 $p(e)$，当然使用 VE，从 $a$ 一直消除到 $d$，记 $\sum\limits_ap(a)p(b|a)&#x3D;m_{a\to b(b)}$，表示这是消除 $a$ 后的关于 $b$ 的概率，类似地，记 $\sum\limits_bp(c|b)m_{a\to b}(b)&#x3D;m_{b\to c}(c)$。于是 $p(e)&#x3D;\sum\limits_dp(e|d)m_{b\to c}(c)$。</p><p>进一步观察，对 $p(c)$：<br>$$<br>p(c)&#x3D;[\sum\limits_bp(c|b)\sum\limits_ap(b|a)p(a)]\cdot[\sum\limits_dp(d|c)\sum\limits_ep(e)p(e|d)]<br>$$<br>我们发现了和上面计算 $p(e)$ 类似的结构，这个式子可以分成两个部分，一部分是从 $a$ 传播过来的概率，第二部分是从 $ e$ 传播过来的概率。</p><p>一般地，对于图（只对树形状的图）：</p><pre><code class=" mermaid">graph TD;a((a))---b((b));b---c((c));b---d((d));</code></pre><p>这四个团（对于无向图是团，对于有向图就是概率为除了根的节点为1），有四个节点，三个边：<br>$$<br>p(a,b,c,d)&#x3D;\frac{1}{Z}\phi_a(a)\phi_b(b)\phi_c(c)\phi_d(d)\cdot\phi_{ab}(a,b)\phi_{bc}(c,b)\phi_{bd}(d,b)<br>$$<br>套用上面关于有向图的观察，如果求解边缘概率 $p(a)$，定义 $m_{c\to b}(b)&#x3D;\sum\limits_c\phi_c(c)\phi_{bc}(bc)$，$m_{d\to b}(b)&#x3D;\sum\limits_d\phi_d(d)\phi_{bd}(bd)$，$m_{b\to a}(a)&#x3D;\sum\limits_b\phi_{ba}(ba)\phi_b(b)m_{c\to b}(b)<em>{d\to b}m(b)$，这样概率就一步步地传播到了 $a$：<br>$$<br>p(a)&#x3D;\phi_a(a)m</em>{b\to a}(a)<br>$$<br>写成一般的形式，对于相邻节点 $i,j$：<br>$$<br>m_{j\to i}(i)&#x3D;\sum\limits_j\phi_j(j)\phi_{ij}(ij)\prod\limits_{k\in Neighbour(j)-i}m_{k\to j}(j)<br>$$<br>这个表达式，就可以保存计算过程了，只要对每条边的传播分别计算，对于一个无向树形图可以递归并行实现：</p><ol><li>任取一个节点 $a$ 作为根节点</li><li>对这个根节点的邻居中的每一个节点，收集信息（计算入信息）</li><li>对根节点的邻居，分发信息（计算出信息）</li></ol><h3 id="推断-Max-Product-算法"><a href="#推断-Max-Product-算法" class="headerlink" title="推断-Max-Product 算法"></a>推断-Max-Product 算法</h3><p>在推断任务中，MAP 也是常常需要的，MAP 的目的是寻找最佳参数：<br>$$<br>(\hat{a},\hat{b},\hat{c},\hat{d})&#x3D;\mathop{argmax}<em>{a,b,c,d}p(a,b,c,d|E)<br>$$<br>类似 BP，我们采用信息传递的方式来求得最优参数，不同的是，我们在所有信息传递中，传递的是最大化参数的概率，而不是将所有可能求和：<br>$$<br>m</em>{j\to i}&#x3D;\max\limits_{j}\phi_j\phi_{ij}\prod\limits_{k\in Neighbour(j)-i}m_{k\to j}<br>$$<br>于是对于上面的图：<br>$$<br>\max_a p(a,b,c,d)&#x3D;\max_a\phi_a\phi_{ab}m_{c\to b}m_{d\to b}<br>$$<br>这个算法是 Sum-Product 算法的改进，也是在 HMM 中应用给的 Viterbi 算法的推广。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——指数族分布</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/6.Exponentialfamily/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/6.Exponentialfamily/</url>
    
    <content type="html"><![CDATA[<h1 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h1><p>指数族是一类分布，包括高斯分布、伯努利分布、二项分布、泊松分布、Beta 分布、Dirichlet 分布、Gamma 分布等一系列分布。指数族分布可以写为统一的形式：<br>$$<br>p(x|\eta)&#x3D;h(x)\exp(\eta^T\phi(x)-A(\eta))&#x3D;\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))<br>$$<br>其中，$\eta$ 是参数向量，$A(\eta)$ 是对数配分函数（归一化因子）。</p><p>在这个式子中，$ \phi(x)$ 叫做充分统计量，包含样本集合所有的信息，例如高斯分布中的均值和方差。充分统计量在在线学习中有应用，对于一个数据集，只需要记录样本的充分统计量即可。</p><p>对于一个模型分布假设（似然），那么我们在求解中，常常需要寻找一个共轭先验，使得先验与后验的形式相同，例如选取似然是二项分布，可取先验是 Beta 分布，那么后验也是 Beta 分布。指数族分布常常具有共轭的性质，于是我们在模型选择以及推断具有很大的便利。</p><p>共轭先验的性质便于计算，同时，指数族分布满足最大熵的思想（无信息先验），也就是说对于经验分布利用最大熵原理导出的分布就是指数族分布。</p><p>观察到指数族分布的表达式类似线性模型，事实上，指数族分布很自然地导出广义线性模型：<br>$$<br>y&#x3D;f(w^Tx)\<br>y|x\sim Exp Family<br>$$<br>在更复杂的概率图模型中，例如在无向图模型中如受限玻尔兹曼机中，指数族分布也扮演着重要作用。</p><p>在推断的算法中，例如变分推断中，指数族分布也会大大简化计算。</p><h2 id="一维高斯分布"><a href="#一维高斯分布" class="headerlink" title="一维高斯分布"></a>一维高斯分布</h2><p>一维高斯分布可以写成：<br>$$<br>p(x|\theta)&#x3D;\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2})<br>$$<br>将这个式子改写：<br>$$<br>\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x^2-2\mu x+\mu^2))\<br>&#x3D;\exp(\log(2\pi\sigma^2)^{-1&#x2F;2})\exp(-\frac{1}{2\sigma^2}\begin{pmatrix}-2\mu&amp;1\end{pmatrix}\begin{pmatrix}x\x^2\end{pmatrix}-\frac{\mu^2}{2\sigma^2})<br>$$<br>所以：<br>$$<br>\eta&#x3D;\begin{pmatrix}\frac{\mu}{\sigma^2}\-\frac{1}{2\sigma^2}\end{pmatrix}&#x3D;\begin{pmatrix}\eta_1\\eta_2\end{pmatrix}<br>$$<br>于是 $A(\eta)$：<br>$$<br>A(\eta)&#x3D;-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\log(-\frac{\pi}{\eta_2})<br>$$</p><h2 id="充分统计量和对数配分函数的关系"><a href="#充分统计量和对数配分函数的关系" class="headerlink" title="充分统计量和对数配分函数的关系"></a>充分统计量和对数配分函数的关系</h2><p>对概率密度函数求积分：<br>$$<br>\begin{align}<br>\exp(A(\eta))&amp;&#x3D;\int h(x)\exp(\eta^T\phi(x))dx\nonumber<br>\end{align}<br>$$<br>两边对参数求导：<br>$$<br>\exp(A(\eta))A’(\eta)&#x3D;\int h(x)\exp(\eta^T\phi(x))\phi(x)dx\<br>\Longrightarrow A’(\eta)&#x3D;\mathbb{E}<em>{p(x|\eta)}[\phi(x)]<br>$$<br>类似的：<br>$$<br>A’’(\eta)&#x3D;Var</em>{p(x|\eta)}[\phi(x)]<br>$$<br>由于方差为正，于是 $A(\eta)$ 一定是凸函数。</p><h2 id="充分统计量和极大似然估计"><a href="#充分统计量和极大似然估计" class="headerlink" title="充分统计量和极大似然估计"></a>充分统计量和极大似然估计</h2><p>对于独立全同采样得到的数据集 $\mathcal{D}&#x3D;{x_1,x_2,\cdots,x_N}$。<br>$$<br>\begin{align}\eta_{MLE}&amp;&#x3D;\mathop{argmax}<em>\eta\sum\limits</em>{i&#x3D;1}^N\log p(x_i|\eta)\nonumber\<br>&amp;&#x3D;\mathop{argmax}<em>\eta\sum\limits</em>{i&#x3D;1}^N(\eta^T\phi(x_i)-A(\eta))\nonumber\<br>&amp;\Longrightarrow A’(\eta_{MLE})&#x3D;\frac{1}{N}\sum\limits_{i&#x3D;1}^N\phi(x_i)</p><p>\end{align}<br>$$<br>由此可以看到，为了估算参数，只需要知道充分统计量就可以了。</p><h2 id="最大熵"><a href="#最大熵" class="headerlink" title="最大熵"></a>最大熵</h2><p>信息熵记为：<br>$$<br>Entropy&#x3D;\int-p(x)\log(p(x))dx<br>$$</p><blockquote><p>   一般地，对于完全随机的变量（等可能），信息熵最大。</p><p>  我们的假设为最大熵原则，假设数据是离散分布的，$k$ 个特征的概率分别为 $p_k$，最大熵原理可以表述为：<br>  $$<br>  \max{H(p)}&#x3D;\min{\sum\limits_{k&#x3D;1}^Kp_k\log p_k}\ s.t.\ \sum\limits_{k&#x3D;1}^Kp_k&#x3D;1<br>  $$<br>  利用 Lagrange 乘子法：<br>  $$<br>  L(p,\lambda)&#x3D;\sum\limits_{k&#x3D;1}^Kp_k\log p_k+\lambda(1-\sum\limits_{k&#x3D;1}^Kp_k)<br>  $$<br>  于是可得：<br>  $$<br>  p_1&#x3D;p_2&#x3D;\cdots&#x3D;p_K&#x3D;\frac{1}{K}<br>  $$<br>  因此等可能的情况熵最大。</p></blockquote><p>一个数据集 $\mathcal{D}$，在这个数据集上的经验分布为 $\hat{p}(x)&#x3D;\frac{Count(x)}{N}$，实际不可能满足所有的经验概率相同，于是在上面的最大熵原理中还需要加入这个经验分布的约束。</p><p>对任意一个函数，经验分布的经验期望可以求得为：<br>$$<br>\mathbb{E}<em>\hat{p}[f(x)]&#x3D;\Delta<br>$$<br>于是：<br>$$<br>\max{H(p)}&#x3D;\min{\sum\limits</em>{k&#x3D;1}^Np_k\log p_k}\ s.t.\ \sum\limits_{k&#x3D;1}^Np_k&#x3D;1,\mathbb{E}<em>p[f(x)]&#x3D;\Delta<br>$$<br>Lagrange 函数为：<br>$$<br>L(p,\lambda_0,\lambda)&#x3D;\sum\limits</em>{k&#x3D;1}^Np_k\log p_k+\lambda_0(1-\sum\limits_{k&#x3D;1}^Np_k)+\lambda^T(\Delta-\mathbb{E}<em>p[f(x)])<br>$$<br>求导得到：<br>$$<br>\frac{\partial}{\partial p(x)}L&#x3D;\sum\limits</em>{k&#x3D;1}^N(\log p(x)+1)-\sum\limits_{k&#x3D;1}^N\lambda_0-\sum\limits_{k&#x3D;1}^N\lambda^Tf(x)\<br>\Longrightarrow\sum\limits_{k&#x3D;1}^N\log p(x)+1-\lambda_0-\lambda^Tf(x)&#x3D;0<br>$$<br>由于数据集是任意的，对数据集求和也意味着求和项里面的每一项都是0：<br>$$<br>p(x)&#x3D;\exp(\lambda^Tf(x)+\lambda_0-1)<br>$$<br>这就是指数族分布。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——降维</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/4.DimentionReduction/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/4.DimentionReduction/</url>
    
    <content type="html"><![CDATA[<h1 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h1><p>我们知道，解决过拟合的问题除了正则化和添加数据之外，降维就是最好的方法。降维的思路来源于维度灾难的问题，我们知道 $n$ 维球的体积为：<br>$$<br>CR^n<br>$$<br>那么在球体积与边长为 $2R$ 的超立方体比值为：<br>$$<br>\lim\limits_{n\rightarrow0}\frac{CR^n}{2^nR^n}&#x3D;0<br>$$</p><p>这就是所谓的维度灾难，在高维数据中，主要样本都分布在立方体的边缘，所以数据集更加稀疏。</p><p>降维的算法分为：</p><ol><li>直接降维，特征选择</li><li>线性降维，PCA，MDS等</li><li>分线性，流形包括 Isomap，LLE 等</li></ol><p>为了方便，我们首先将协方差矩阵（数据集）写成中心化的形式：<br>$$<br>\begin{align}S&amp;&#x3D;\frac{1}{N}\sum\limits_{i&#x3D;1}^N(x_i-\overline{x})(x_i-\overline{x})^T\nonumber\<br>&amp;&#x3D;\frac{1}{N}(x_1-\overline{x},x_2-\overline{x},\cdots,x_N-\overline{x})(x_1-\overline{x},x_2-\overline{x},\cdots,x_N-\overline{x})^T\nonumber\<br>&amp;&#x3D;\frac{1}{N}(X^T-\frac{1}{N}X^T\mathbb{I}<em>{N1}\mathbb{I}</em>{N1}^T)(X^T-\frac{1}{N}X^T\mathbb{I}<em>{N1}\mathbb{I}</em>{N1}^T)^T\nonumber\<br>&amp;&#x3D;\frac{1}{N}X^T(E_N-\frac{1}{N}\mathbb{I}<em>{N1}\mathbb{I}</em>{1N})(E_N-\frac{1}{N}\mathbb{I}<em>{N1}\mathbb{I}</em>{1N})^TX\nonumber\<br>&amp;&#x3D;\frac{1}{N}X^TH_NH_N^TX\nonumber\<br>&amp;&#x3D;\frac{1}{N}X^TH_NH_NX&#x3D;\frac{1}{N}X^THX<br>\end{align}<br>$$<br>这个式子利用了中心矩阵 $ H$的对称性，这也是一个投影矩阵。</p><h2 id="线性降维-主成分分析-PCA"><a href="#线性降维-主成分分析-PCA" class="headerlink" title="线性降维-主成分分析 PCA"></a>线性降维-主成分分析 PCA</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>主成分分析中，我们的基本想法是将所有数据投影到一个字空间中，从而达到降维的目标，为了寻找这个子空间，我们基本想法是：</p><ol><li>所有数据在子空间中更为分散</li><li>损失的信息最小，即：在补空间的分量少</li></ol><p>原来的数据很有可能各个维度之间是相关的，于是我们希望找到一组 $p$ 个新的线性无关的单位基 $u_i$，降维就是取其中的 $q$ 个基。于是对于一个样本 $x_i$，经过这个坐标变换后：<br>$$<br>\hat{x_i}&#x3D;\sum\limits_{i&#x3D;1}^p(u_i^Tx_i)u_i&#x3D;\sum\limits_{i&#x3D;1}^q(u_i^Tx_i)u_i+\sum\limits_{i&#x3D;q+1}^p(u_i^Tx_i)u_i<br>$$<br>对于数据集来说，我们首先将其中心化然后再去上面的式子的第一项，并使用其系数的平方平均作为损失函数并最大化：<br>$$<br>\begin{align}J&amp;&#x3D;\frac{1}{N}\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;1}^q((x_i-\overline{x})^Tu_j)^2\nonumber\<br>&amp;&#x3D;\sum\limits_{j&#x3D;1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j&#x3D;1<br>\end{align}<br>$$<br>由于每个基都是线性无关的，于是每一个 $u_j$ 的求解可以分别进行，使用拉格朗日乘子法：<br>$$<br>\mathop{argmax}<em>{u_j}L(u_j,\lambda)&#x3D;\mathop{argmax}</em>{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)<br>$$<br>于是：<br>$$<br>Su_j&#x3D;\lambda u_j<br>$$<br>可见，我们需要的基就是协方差矩阵的本征矢。损失函数最大取在本征值前 $q$ 个最大值。</p><p>下面看其损失的信息最少这个条件，同样适用系数的平方平均作为损失函数，并最小化：<br>$$<br>\begin{align}J&amp;&#x3D;\frac{1}{N}\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;q+1}^p((x_i-\overline{x})^Tu_j)^2\nonumber\<br>&amp;&#x3D;\sum\limits_{j&#x3D;q+1}^pu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j&#x3D;1<br>\end{align}<br>$$<br>同样的：<br>$$<br>\mathop{argmin}<em>{u_j}L(u_j,\lambda)&#x3D;\mathop{argmin}</em>{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)<br>$$<br>损失函数最小取在本征值剩下的个最小的几个值。数据集的协方差矩阵可以写成 $S&#x3D;U\Lambda U^T$，直接对这个表达式当然可以得到本征矢。</p><h3 id="SVD-与-PCoA"><a href="#SVD-与-PCoA" class="headerlink" title="SVD 与 PCoA"></a>SVD 与 PCoA</h3><p>下面使用实际训练时常常使用的 SVD 直接求得这个 $q$ 个本征矢。</p><p>对中心化后的数据集进行奇异值分解：<br>$$<br>HX&#x3D;U\Sigma V^T,U^TU&#x3D;E_N,V^TV&#x3D;E_p,\Sigma:N\times p<br>$$</p><p>于是：<br>$$<br>S&#x3D;\frac{1}{N}X^THX&#x3D;\frac{1}{N}X^TH^THX&#x3D;\frac{1}{N}V\Sigma^T\Sigma V^T<br>$$<br>因此，我们直接对中心化后的数据集进行 SVD，就可以得到特征值和特征向量 $V$，在新坐标系中的坐标就是：<br>$$<br>HX\cdot V<br>$$<br>由上面的推导，我们也可以得到另一种方法 PCoA 主坐标分析，定义并进行特征值分解：<br>$$<br>T&#x3D;HXX^TH&#x3D;U\Sigma\Sigma^TU^T<br>$$<br>由于：<br>$$<br>TU\Sigma&#x3D;U\Sigma(\Sigma^T\Sigma)<br>$$<br>于是可以直接得到坐标。这两种方法都可以得到主成分，但是由于方差矩阵是 $p\times p$ 的，而 $T$ 是 $N\times N$ 的，所以对样本量较少的时候可以采用 PCoA的方法。</p><h3 id="p-PCA"><a href="#p-PCA" class="headerlink" title="p-PCA"></a>p-PCA</h3><p>下面从概率的角度对 PCA 进行分析，概率方法也叫 p-PCA。我们使用线性模型，类似之前 LDA，我们选定一个方向，对原数据 $x\in\mathbb{R}^p$ ，降维后的数据为 $z\in\mathbb{R}^q,q&lt;p$。降维通过一个矩阵变换（投影）进行：<br>$$<br>\begin{align}<br>z&amp;\sim\mathcal{N}(\mathbb{O}<em>{q1},\mathbb{I}</em>{qq})\<br>x&amp;&#x3D;Wz+\mu+\varepsilon\<br>\varepsilon&amp;\sim\mathcal{N}(0,\sigma^2\mathbb{I}<em>{pp})<br>\end{align}<br>$$<br>对于这个模型，我么可以使用期望-最大（EM）的算法进行学习，在进行推断的时候需要求得 $p(z|x)$，推断的求解过程和线性高斯模型类似。<br>$$<br>\begin{align}<br>&amp;p(z|x)&#x3D;\frac{p(x|z)p(z)}{p(x)}\<br>&amp;\mathbb{E}[x]&#x3D;\mathbb{E}[Wz+\mu+\varepsilon]&#x3D;\mu\<br>&amp;Var[x]&#x3D;WW^T+\sigma^2\mathbb{I}</em>{pp}\<br>\Longrightarrow p(z|x)&#x3D;\mathcal{N}(W^T(WW^T+&amp;\sigma^2\mathbb{I})^{-1}(x-\mu),\mathbb{I}-W^T(WW^T+\sigma^2\mathbb{I})^{-1}W)<br>\end{align}<br>$$</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>降维是解决维度灾难和过拟合的重要方法，除了直接的特征选择外，我们还可以采用算法的途径对特征进行筛选，线性的降维方法以 PCA 为代表，在 PCA 中，我们只要直接对数据矩阵进行中心化然后求奇异值分解或者对数据的协方差矩阵进行分解就可以得到其主要维度。非线性学习的方法如流形学习将投影面从平面改为超曲面。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——近似推断</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/23.ApproInference/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/23.ApproInference/</url>
    
    <content type="html"><![CDATA[<h1 id="近似推断"><a href="#近似推断" class="headerlink" title="近似推断"></a>近似推断</h1><p>这一讲中的近似推断具体描述在深度生成模型中的近似推断。推断的目的有下面几个部分：</p><ol><li>推断本身，根据结果（观测）得到原因（隐变量）。</li><li>为参数的学习提供帮助。</li></ol><p>但是推断本身是一个困难的额任务，计算复杂度往往很高，对于无向图，由于节点之间的联系过多，那么因子分解很难进行，并且相互之间都有耦合，于是很难求解，仅仅在某些情况如 RBM 中可解，在有向图中，常常由于条件独立性问题，如两个节点之间条件相关（explain away），于是求解这些节点的条件概率就很困难，仅仅在某些概率假设情况下可解如高斯模型，于是需要近似推断。</p><p>事实上，我们常常讲推断问题变为优化问题，即：<br>$$<br>Log-likehood:\sum\limits_{v\in V}\log p(v)<br>$$<br>对上面这个问题，由于：<br>$$<br>\log p(v)&#x3D;\log\frac{p(v,h)}{p(h|v)}&#x3D;\log\frac{p(v,h)}{q(h|v)}+\log\frac{q(h|v)}{p(h|v)}<br>$$<br>左右两边对 $h$ 积分：<br>$$<br>\int_h\log p(v)\cdot q(h|v)dh&#x3D;\log p(v)<br>$$<br>右边积分有：<br>$$<br>\mathbb{E}<em>{q(h|v)}[\log\frac{p(v,h)}{q(h|v)}]+KL(q(h|v)||p(h|v))&#x3D;\mathbb{E}</em>{q(h|v)}[\log p(v,h)]+H(q)+KL(q||p)<br>$$<br>其中前两项是 ELBO，于是这就变成一个优化 ELBO 的问题。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——配分函数</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/22.PartitionFunction/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/22.PartitionFunction/</url>
    
    <content type="html"><![CDATA[<h1 id="配分函数"><a href="#配分函数" class="headerlink" title="配分函数"></a>配分函数</h1><p>在学习和推断中，对于一个概率的归一化因子很难处理，这个归一化因子和配分函数相关。假设一个概率分布：<br>$$<br>p(x|\theta)&#x3D;\frac{1}{Z(\theta)}\hat{p}(x|\theta),Z(\theta)&#x3D;\int\hat{p}(x|\theta)dx<br>$$</p><h2 id="包含配分函数的-MLE"><a href="#包含配分函数的-MLE" class="headerlink" title="包含配分函数的 MLE"></a>包含配分函数的 MLE</h2><p>在学习任务中，采用最大似然：<br>$$<br>\begin{align}<br>\hat{\theta}&amp;&#x3D;\mathop{argmax}<em>{\theta}p(x|\theta)&#x3D;\mathop{argmax}<em>\theta\sum\limits</em>{i&#x3D;1}^N\log p(x_i|\theta)\nonumber\<br>&amp;&#x3D;\mathop{argmax}<em>\theta\sum\limits</em>{i&#x3D;1}^N\log \hat{p}(x|\theta)-N\log Z(\theta)\nonumber\<br>&amp;&#x3D;\mathop{argmax}</em>{\theta}\frac{1}{N}\sum\limits_{i&#x3D;1}^N\log \hat{p}(x|\theta)-\log Z(\theta)&#x3D;\mathop{argmax}<em>\theta l(\theta)<br>\end{align}<br>$$<br>求导：<br>$$<br>\begin{align}\nabla_\theta\log Z(\theta)&amp;&#x3D;\frac{1}{Z(\theta)}\nabla_\theta Z(\theta)\nonumber\<br>&amp;&#x3D;\frac{p(x|\theta)}{\hat{p}(x|\theta)}\int\nabla_\theta \hat{p}(x|\theta)dx\nonumber\<br>&amp;&#x3D;\int\frac{p(x|\theta)}{\hat{p}(x|\theta)}\nabla_\theta\hat{p}(x|\theta)dx\nonumber\<br>&amp;&#x3D;\mathbb{E}</em>{p(x|\theta)}[\nabla_\theta\log\hat{p}(x|\theta)]<br>\end{align}<br>$$<br>由于这个表达式和未知的概率相关，于是无法直接精确求解，需要近似采样，如果没有这一项，那么可以采用梯度下降，但是存在配分函数就无法直接采用梯度下降了。</p><p>上面这个期望值，是对模型假设的概率分布，定义真实概率分布为 $p_{data}$，于是，$l(\theta)$ 中的第一项的梯度可以看成是从这个概率分布中采样出来的 $N$ 个点求和平均，可以近似期望值。<br>$$<br>\nabla_\theta l(\theta)&#x3D;\mathbb{E}<em>{p</em>{data}}[\nabla_\theta\log\hat{p}(x|\theta)]-\mathbb{E}_{p(x|\theta)}[\nabla_\theta\log\hat{p}(x|\theta)]<br>$$<br>于是，相当于真实分布和模型假设越接近越好。上面这个式子第一项叫做正相，第二项叫做负相。为了得到负相的值，需要采用各种采样方法，如 MCMC。</p><p>采样得到 $\hat{x}<em>{1-m}\sim p</em>{model}(x|\theta^t)$，那么：<br>$$<br>\theta^{t+1}&#x3D;\theta^t+\eta(\sum\limits_{i&#x3D;1}^m\nabla_\theta \log \hat{p}(x_i|\theta^t)-\sum\limits_{i&#x3D;1}^m\nabla_\theta\log \hat{p}(\hat{x_i}|\theta^t))<br>$$<br>这个算法也叫做基于 MCMC 采样的梯度上升。每次通过采样得到的样本叫做幻想粒子，如果这些幻想粒子区域的概率高于实际分布，那么最大化参数的结果就是降低这些部分的概率。</p><h2 id="对比散度-CD-Learning"><a href="#对比散度-CD-Learning" class="headerlink" title="对比散度-CD Learning"></a>对比散度-CD Learning</h2><p>上面对于负相的采样，最大的问题是，采样到达平稳分布的步骤数量是未知的。对比散度的方法，是对上述的采样是的初始值作出限制，直接采样 $\hat{x}_i&#x3D;x_i$，这样可以缩短采样的混合时间。这个算法叫做 CD-k 算法，$k$ 就是初始化后进行的演化时间，很多时候，即使 $k&#x3D;1$ 也是可以的。</p><p>我们看 MLE 的表达式：<br>$$<br>\begin{align}\hat{\theta}&amp;&#x3D;\mathop{argmax}<em>{\theta}p(x|\theta)&#x3D;\mathop{argmax}</em>{\theta}\frac{1}{N}\sum\limits_{i&#x3D;1}^N\log p(x_i|\theta)&#x3D;\mathbb{E}<em>{p</em>{data}}[\log p_{model}(x|\theta)]\nonumber\<br>&amp;&#x3D;\mathop{argmax}<em>\theta\int p</em>{data}\log p_{model}dx\nonumber\<br>&amp;&#x3D;\mathop{argmax}<em>\theta\int p</em>{data}\log \frac{p_{model}}{p_{data}}dx\nonumber\<br>&amp;&#x3D;\mathop{argmin}<em>\theta KL(p</em>{data}||p_{model})<br>\end{align}<br>$$<br>对于 CD-k 的采样过程，可以将初始值这些点表示为：<br>$$<br>p^0&#x3D;p_{data}<br>$$<br>而我们的模型需要采样过程达到平稳分布：<br>$$<br>p^\infin&#x3D;p_{model}<br>$$<br>因此，我们需要的是 $KL(p^0||p^\infin)$。定义 CD：<br>$$<br>KL(p^0||p^\infin)-KL(p^k||p^\infin)<br>$$<br>这就是 CD-k 算法第 $k$ 次采样的目标函数。</p><h2 id="RBM-的学习问题"><a href="#RBM-的学习问题" class="headerlink" title="RBM 的学习问题"></a>RBM 的学习问题</h2><p>RBM 的参数为：<br>$$<br>\begin{align}<br>h&#x3D;(h_1,\cdots,h_m)^T\<br>v&#x3D;(v_1,\cdots,v_n)^T\<br>w&#x3D;(w_{ij})<em>{mn}\<br>\alpha&#x3D;(\alpha_1,\cdots,\alpha_n)^T\<br>\beta&#x3D;(\beta_1,\cdots,\beta_m)^T<br>\end{align}<br>$$<br>学习问题关注的概率分布为：<br>$$<br>\begin{align}<br>\log p(v)&amp;&#x3D;\log\sum\limits</em>{h}p(h,v)\nonumber\<br>&amp;&#x3D;\log\sum\limits_h\frac{1}{Z}\exp(-E(v,h))\nonumber\<br>&amp;&#x3D;\log\sum\limits_{h}\exp(-E(v,h))-\log\sum\limits_{v,h}\exp(-E(h,v))<br>\end{align}<br>$$<br>对上面这个式子求导第一项：<br>$$<br>\frac{\partial \log\sum\limits_{h}\exp(-E(v,h))}{\partial\theta}&#x3D;-\frac{\sum\limits_h\exp(-E(v,h))\frac{\partial E(v,h)}{\partial\theta}}{\sum\limits_{h}\exp(-E(v,h))}\<br>&#x3D;-\sum\limits_h\frac{\exp(-E(v,h))\frac{\partial E(v,h)}{\partial\theta}}{\sum\limits_{h}\exp(-E(v,h))}&#x3D;-\sum\limits_hp(h|v)\frac{\partial E(v,h)}{\partial\theta}<br>$$<br>第二项：<br>$$<br>\frac{\partial \log\sum\limits_{v,h}\exp(-E(h,v))}{\partial\theta}&#x3D;-\sum\limits_{h,v}\frac{\exp(-E(v,h))\frac{\partial E(v,h)}{\partial\theta}}{\sum\limits_{h,v}\exp(-E(v,h))}&#x3D;-\sum\limits_{v,h}p(v,h)\frac{\partial E(v,h)}{\partial\theta}<br>$$<br>所以有：<br>$$<br>\frac{\partial}{\partial\theta}\log p(v)&#x3D;-\sum\limits_hp(h|v)\frac{\partial E(v,h)}{\partial\theta}+\sum\limits_{v,h}p(v,h)\frac{\partial E(v,h)}{\partial\theta}<br>$$<br>将 RBM 的模型假设代入：<br>$$<br>E(v,h)&#x3D;-(h^Twv+\alpha^Tv+\beta^Th)<br>$$</p><ol><li><p>$w_{ij}$：<br>$$<br>\frac{\partial}{\partial w_{ij}}E(v,h)&#x3D;-h_iv_j<br>$$<br>于是：<br>$$<br>\frac{\partial}{\partial\theta}\log p(v)&#x3D;\sum\limits_{h}p(h|v)h_iv_j-\sum\limits_{h,v}p(h,v)h_iv_j<br>$$<br>第一项：<br>$$<br>\sum\limits_{h_1,h_2,\cdots,h_m}p(h_1,h_2,\cdots,h_m|v)h_iv_j&#x3D;\sum\limits_{h_i}p(h_i|v)h_iv_j&#x3D;p(h_i&#x3D;1|v)v_j<br>$$<br>这里假设了 $h_i$ 是二元变量。</p><p>第二项：<br>$$<br>\sum\limits_{h,v}p(h,v)h_iv_j&#x3D;\sum\limits_{h,v}p(v)p(h|v)h_iv_j&#x3D;\sum\limits_vp(v)p(h_i&#x3D;1|v)v_j<br>$$<br>这个求和是指数阶的，于是需要采样解决，我么使用 CD-k 方法。</p><p>对于第一项，可以直接使用训练样本得到，第二项采用 CD-k 采样方法，首先使用样本 $v^0&#x3D;v$，然后采样得到 $h^0$，然后采样得到 $v^1$，这样顺次进行，最终得到 $v^k$，对于每个样本都得到一个 $v^k$，最终采样得到 $N$ 个 $v^k $，于是第二项就是：<br>$$<br>p(h_i&#x3D;1|v^k)v_j^k<br>$$<br>具体的算法为：</p><ol><li>对每一个样本中的 $v$，进行采样：<ol><li>使用这个样本初始化采样</li><li>进行 $k$ 次采样（0-k-1）：<ol><li>$h_i^l\sim p(h_i|v^l)$</li><li>$v_i^{l+1}\sim p(v_i|h^l)$</li></ol></li><li>将这些采样出来的结果累加进梯度中</li></ol></li><li>重复进行上述过程，最终的梯度除以 $N$</li></ol></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——前馈神经网络</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/21.NN/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/21.NN/</url>
    
    <content type="html"><![CDATA[<h1 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h1><p>机器学习我们已经知道可以分为两大流派：</p><ol><li><p>频率派，这个流派的方法叫做统计学习，根据具体问题有下面的算法：</p><ol><li><p>正则化，L1，L2 等</p></li><li><p>核化，如核支撑向量机</p></li><li><p>集成化，AdaBoost，RandomForest</p></li><li><p>层次化，神经网络，神经网络有各种不同的模型，有代表性的有：</p><ol><li>多层感知机</li><li>Autoencoder</li><li>CNN</li><li>RNN</li></ol><p>这几种模型又叫做深度神经网络。</p></li></ol></li><li><p>贝叶斯派，这个流派的方法叫概率图模型，根据图特点分为：</p><ol><li>有向图-贝叶斯网络，加入层次化后有深度有向网络，包括<ol><li>Sigmoid Belief Network</li><li>Variational Autoencoder</li><li>GAN</li></ol></li><li>无向图-马尔可夫网络，加入层次化后有深度玻尔兹曼机。</li><li>混合，加入层次化后有深度信念网络</li></ol><p>这几个加入层次化后的模型叫做深度生成网络。</p></li></ol><p>从广义来说，深度学习包括深度生成网络和深度神经网络。</p><h2 id="From-PLA-to-DL"><a href="#From-PLA-to-DL" class="headerlink" title="From PLA to DL"></a>From PLA to DL</h2><ul><li>1958，PLA</li><li>1969，PLA 不能解决 XOR 等非线性数据</li><li>1981，MLP，多层感知机的出现解决了上面的问题</li><li>1986，BP 算法应用在 MLP 上，RNN</li><li>1989，CNN，Univeral Approximation Theorem，但是于此同时，由于深度和宽度的相对效率不知道，并且无法解决 BP 算法的梯度消失问题</li><li>1993，1995，SVM + kernel，AdaBoost，RandomForest，这些算法的发展，DL 逐渐没落</li><li>1997，LSTM</li><li>2006，基于 RBM 的 深度信念网络和深度自编码</li><li>2009，GPU的发展</li><li>2011，在语音方面的应用</li><li>2012，ImageNet</li><li>2013，VAE</li><li>2014，GAN</li><li>2016，AlphaGo</li><li>2018，GNN</li></ul><p>DL 不是一个新的东西，其近年来的大发展主要原因如下：</p><ol><li>数据量变大</li><li>分布式计算的发展</li><li>硬件算力的发展</li></ol><h2 id="非线性问题"><a href="#非线性问题" class="headerlink" title="非线性问题"></a>非线性问题</h2><p>对于非线性的问题，有三种方法：</p><ol><li>非线性转换，将低维空间转换到高维空间（Cover 定理），从而变为一个线性问题。</li><li>核方法，由于非线性转换是变换为高维空间，因此可能导致维度灾难，并且可能很难得到这个变换函数，核方法不直接寻找这个转换，而是寻找一个内积。</li><li>神经网络方法，将复合运算变为基本的线性运算的组合。</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——谱聚类</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/20.Spectral/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/20.Spectral/</url>
    
    <content type="html"><![CDATA[<h1 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h1><p>聚类问题可以分为两种思路：</p><ol><li>Compactness，这类有 K-means，GMM 等，但是这类算法只能处理凸集，为了处理非凸的样本集，必须引入核技巧。</li><li>Connectivity，这类以谱聚类为代表。</li></ol><p>谱聚类是一种基于无向带权图的聚类方法。这个图用 $G&#x3D;(V,E)$ 表示，其中 $V&#x3D;{1,2,\cdots,N}$，$E&#x3D;{w_{ij}}$，这里 $w_{ij}$ 就是边的权重，这里权重取为相似度，$W&#x3D;(w_{ij})$ 是相似度矩阵，定义相似度（径向核）：<br>$$<br>w_{ij}&#x3D;k(x_i,x_j)&#x3D;\exp(-\frac{||x_i-x_j||<em>2^2}{2\sigma^2}),(i,j)\in E\<br>w</em>{ij}&#x3D;0,(i,j)\notin E<br>$$<br>下面定义图的分割，这种分割就相当于聚类的结果。定义 $w(A,B)$：<br>$$<br>A\sub V,B\sub V,A\cap B&#x3D;\emptyset,w(A,B)&#x3D;\sum\limits_{i\in A,j\in B}w_{ij}<br>$$<br>假设一共有 $K$ 个类别，对这个图的分割 $CUT(V)&#x3D;CUT(A_1,A_2,\cdots,A_K)&#x3D;\sum\limits_{k&#x3D;1}^Kw(A_k,\overline{A_k})&#x3D;\sum\limits_{k&#x3D;1}^K[w(A_k,V)-w(A_k,A_k)]$</p><p>于是，我们的目标就是 $\min\limits_{A_k}CUT(V)$。</p><p>为了平衡每一类内部的权重不同，我们做归一化的操作，定义每一个集合的度，首先，对单个节点的度定义：<br>$$<br>d_i&#x3D;\sum\limits_{j&#x3D;1}^Nw_{ij}<br>$$<br>其次，每个集合：<br>$$<br>\Delta_k&#x3D;degree(A_k)&#x3D;\sum\limits_{i\in A_k}d_i<br>$$<br>于是：<br>$$<br>N(CUT)&#x3D;\sum\limits_{k&#x3D;1}^K\frac{w(A_k,\overline{A_k})}{\sum\limits_{i\in A_k}d_i}<br>$$<br>所以目标函数就是最小化这个式子。</p><p>谱聚类的模型就是：<br>$$<br>{\hat{A}<em>k}</em>{k&#x3D;1}^K&#x3D;\mathop{argmin}<em>{A_k}N(CUT)<br>$$<br>引入指示向量：<br>$$<br>\left{<br>\begin{align}y_i\in {0,1}^K\<br>\sum\limits</em>{j&#x3D;1}^Ky_{ij}&#x3D;1\end{align}<br>\right.<br>$$<br>其中，$y_{ij}$ 表示第 $i$ 个样本属于 $j$ 个类别，记：$Y&#x3D;(y_1,y_2,\cdots,y_N)^T$。所以：<br>$$<br>\hat{Y}&#x3D;\mathop{argmin}<em>YN(CUT)<br>$$<br>将 $N(CUT)$ 写成对角矩阵的形式，于是：<br>$$<br>\begin{align}N(CUT)&amp;&#x3D;Trace[diag(\frac{w(A_1,\overline{A_1})}{\sum\limits</em>{i\in A_1}d_i},\frac{w(A_2,\overline{A_2})}{\sum\limits_{i\in A_2}d_i},\cdots,\frac{w(A_K,\overline{A_K})}{\sum\limits_{i\in A_K}d_i})]\nonumber\<br>&amp;&#x3D;Trace[diag(w(A_1,\overline{A_1}),w(A_2,\overline{A_2}),\cdots,w(A_K,\overline{A_K}))\cdot diag(\sum\limits_{i\in A_1}d_i,\cdots,\sum\limits_{i\in A_K}d_i)^{-1}]\nonumber\<br>&amp;&#x3D;Trace[O\cdot P^{-1}]<br>\end{align}<br>$$<br>我们已经知道 $Y,w$ 这两个矩阵，我们希望求得 $O,P$。</p><p>由于：<br>$$<br>Y^TY&#x3D;\sum\limits_{i&#x3D;1}^Ny_iy_i^T<br>$$<br>对于 $y_iy_i^T$，只在对角线上的 $k\times k$ 处为 1，所以：<br>$$<br>Y^TY&#x3D;diag(N_1,N_2,\cdots,N_K)<br>$$<br>其中，$N_i$ 表示有 $N_i$ 个样本属于 $i$，即 $N_k&#x3D;\sum\limits_{k\in A_k}1$。</p><p>引入对角矩阵，根据 $d_i$ 的定义， $D&#x3D;diag(d_1,d_2,\cdots,d_N)&#x3D;diag(w_{NN}\mathbb{I}<em>{N1})$，于是：<br>$$<br>P&#x3D;Y^TDY<br>$$<br>对另一项 $O&#x3D;diag(w(A_1,\overline{A_1}),w(A_2,\overline{A_2}),\cdots,w(A_K,\overline{A_K})$：<br>$$<br>O&#x3D;diag(w(A_i,V))-diag(w(A_i,A_i))&#x3D;diag(\sum\limits</em>{j\in A_i}d_j)-diag(w(A_i,A_i))<br>$$<br>其中，第一项已知，第二项可以写成 $Y^TwY$，这是由于：<br>$$<br>Y^TwY&#x3D;\sum\limits_{i&#x3D;1}^N\sum\limits_{j&#x3D;1}^Ny_iy_j^Tw_{ij}<br>$$<br>于是这个矩阵的第 $lm$ 项可以写为：<br>$$<br>\sum\limits_{i\in A_l,j\in A_m}w_{ij}<br>$$<br>这个矩阵的对角线上的项和 $w(A_i,A_i)$ 相同，所以取迹后的取值不会变化。</p><p>所以：<br>$$<br>N(CUT)&#x3D;Trace[(Y^T(D-w))Y)\cdot(Y^TDY)^{-1}]<br>$$<br>其中，$ L&#x3D;D-w$ 叫做拉普拉斯矩阵。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——受限玻尔兹曼机</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/19.RBM/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/19.RBM/</url>
    
    <content type="html"><![CDATA[<h1 id="受限玻尔兹曼机"><a href="#受限玻尔兹曼机" class="headerlink" title="受限玻尔兹曼机"></a>受限玻尔兹曼机</h1><p>玻尔兹曼机是一种存在隐节点的无向图模型。在图模型中最简单的是朴素贝叶斯模型（朴素贝叶斯假设），引入单个隐变量后，发展出了 GMM，如果单个隐变量变成序列的隐变量，就得到了状态空间模型（引入齐次马尔可夫假设和观测独立假设就有HMM，Kalman Filter，Particle Filter），为了引入观测变量之间的关联，引入了一种最大熵模型-MEMM，为了克服 MEMM 中的局域问题，又引入了 CRF，CRF 是一个无向图，其中，破坏了齐次马尔可夫假设，如果隐变量是一个链式结构，那么又叫线性链 CRF。</p><p>在无向图的基础上，引入隐变量得到了玻尔兹曼机，这个图模型的概率密度函数是一个指数族分布。对隐变量和观测变量作出一定的限制，就得到了受限玻尔兹曼机（RBM）。</p><p>我们看到，不同的概率图模型对下面几个特点作出假设：</p><ol><li>方向-边的性质</li><li>离散&#x2F;连续&#x2F;混合-点的性质</li><li>条件独立性-边的性质</li><li>隐变量-节点的性质</li><li>指数族-结构特点</li></ol><p>将观测变量和隐变量分别记为 $v,h,h&#x3D;{h_1,\cdots,h_m},v&#x3D;{v_1,\cdots,v_n}$。我们知道，无向图根据最大团的分解，可以写为玻尔兹曼分布的形式 $p(x)&#x3D;\frac{1}{Z}\prod\limits_{i&#x3D;1}^K\psi_i(x_{ci})&#x3D;\frac{1}{Z}\exp(-\sum\limits_{i&#x3D;1}^KE(x_{ci}))$，这也是一个指数族分布。</p><p>一个玻尔兹曼机存在一系列的问题，在其推断任务中，想要精确推断，是无法进行的，想要近似推断，计算量过大。为了解决这个问题，一种简化的玻尔兹曼机-受限玻尔兹曼机作出了假设，所有隐变量内部以及观测变量内部没有连接，只在隐变量和观测变量之间有连接，这样一来：<br>$$<br>p(x)&#x3D;p(h,v)&#x3D;\frac{1}{Z}\exp(-E(v,h))<br>$$<br>其中能量函数 $E(v,h)$ 可以写出三个部分，包括与节点集合相关的两项以及与边 $w$ 相关的一项，记为：<br>$$<br>E(v,h)&#x3D;-(h^Twv+\alpha^T v+\beta^T h)<br>$$<br>所以：<br>$$<br>p(x)&#x3D;\frac{1}{Z}\exp(h^Twv)\exp(\alpha^T v)\exp(\beta^T h)&#x3D;\frac{1}{Z}\prod_{i&#x3D;1}^m\prod_{j&#x3D;1}^n\exp(h_iw_{ij}v_j)\prod_{j&#x3D;1}^n\exp(\alpha_jv_j)\prod_{i&#x3D;1}^m\exp(\beta_ih_i)<br>$$<br>上面这个式子也和 RBM 的因子图一一对应。</p><h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>推断任务包括求后验概率 $ p(v|h),p(h|v)$ 以及求边缘概率 $p(v)$。</p><h3 id="p-h-v"><a href="#p-h-v" class="headerlink" title="$p(h|v)$"></a>$p(h|v)$</h3><p>对于一个无向图，满足局域的 Markov 性质，即 $p(h_1|h-{h_1},v)&#x3D;p(h_1|Neighbour(h_1))&#x3D;p(h_1|v)$。我们可以得到：<br>$$<br>p(h|v)&#x3D;\prod_{i&#x3D;1}^mp(h_i|v)<br>$$<br>考虑 Binary RBM，所有的隐变量只有两个取值 $0,1$：<br>$$<br>p(h_l&#x3D;1|v)&#x3D;\frac{p(h_l&#x3D;1,h_{-l},v)}{p(h_{-l},v)}&#x3D;\frac{p(h_l&#x3D;1,h_{-l},v)}{p(h_l&#x3D;1,h_{-l},v)+p(h_l&#x3D;0,h_{-l},v)}<br>$$<br>将能量函数写成和 $l$ 相关或不相关的两项：<br>$$<br>E(v,h)&#x3D;-(\sum\limits_{i&#x3D;1,i\ne l}^m\sum\limits_{j&#x3D;1}^nh_iw_{ij}v_j+h_l\sum\limits_{j&#x3D;1}^nw_{lj}v_j+\sum\limits_{j&#x3D;1}^n\alpha_j v_j+\sum\limits_{i&#x3D;1,i\ne l}^m\beta_ih_i+\beta_lh_l)<br>$$<br>定义：$h_lH_l(v)&#x3D;h_l\sum\limits_{j&#x3D;1}^nw_{lj}v_j+\beta_lh_l,\overline{H}(h_{-l},v)&#x3D;\sum\limits_{i&#x3D;1,i\ne l}^m\sum\limits_{j&#x3D;1}^nh_iw_{ij}v_j+\sum\limits_{j&#x3D;1}^n\alpha_j v_j+\sum\limits_{i&#x3D;1,i\ne l}^m\beta_ih_i$。</p><p>代入，有：<br>$$<br>p(h_l&#x3D;1|v)&#x3D;\frac{\exp(H_l(v)+\overline{H}(h_{-l},v))}{\exp(H_l(v)+\overline{H}(h_{-l},v))+\exp(\overline{H}(h_{-l},v))}&#x3D;\frac{1}{1+\exp(-H_l(v))}&#x3D;\sigma(H_l(v))<br>$$<br>于是就得到了后验概率。对于 $v$ 的后验是对称的，所以类似的可以求解。</p><h3 id="p-v"><a href="#p-v" class="headerlink" title="$p(v)$"></a>$p(v)$</h3><p>$$<br>\begin{align}p(v)&amp;&#x3D;\sum\limits_hp(h,v)&#x3D;\sum\limits_h\frac{1}{Z}\exp(h^Twv+\alpha^Tv+\beta^Th)\nonumber\<br>&amp;&#x3D;\exp(\alpha^Tv)\frac{1}{Z}\sum\limits_{h_1}\exp(h_1w_1v+\beta_1h_1)\cdots\sum\limits_{h_m}\exp(h_mw_mv+\beta_mh_m)\nonumber\<br>&amp;&#x3D;\exp(\alpha^Tv)\frac{1}{Z}(1+\exp(w_1v+\beta_1))\cdots(1+\exp(w_mv+\beta_m))\nonumber\<br>&amp;&#x3D;\frac{1}{Z}\exp(\alpha^Tv+\sum\limits_{i&#x3D;1}^m\log(1+\exp(w_iv+\beta_i)))<br>\end{align}<br>$$</p><p>其中，$\log(1+\exp(x))$ 叫做 Softplus 函数。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——高斯过程回归</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/18.GaussianProcess/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/18.GaussianProcess/</url>
    
    <content type="html"><![CDATA[<h1 id="高斯过程回归"><a href="#高斯过程回归" class="headerlink" title="高斯过程回归"></a>高斯过程回归</h1><p>将一维高斯分布推广到多变量中就得到了高斯网络，将多变量推广到无限维，就得到了高斯过程，高斯过程是定义在连续域（时间空间）上的无限多个高维随机变量所组成的随机过程。</p><p>在时间轴上的任意一个点都满足高斯分布吗，将这些点的集合叫做高斯过程的一个样本。</p><blockquote><p>  对于时间轴上的序列 $\xi_t$，如果 $\forall n\in N^+，t_i\in T$，有 $\xi_{t_1-t_n}\sim \mathcal{N}(\mu_{t_1-t_n},\Sigma_{t_1-t_n})$，  那么 ${\xi_t}_{t\in T}$ 是一个高斯过程。</p><p>  高斯过程有两个参数（高斯过程存在性定理），均值函数 $m(t)&#x3D;\mathbb{E}[\xi_t]$ 和协方差函数 $k(s,t)&#x3D;\mathbb{E}[(\xi_s-\mathbb{E}[\xi_s])(\xi_t-\mathbb{E}[\xi_t])]$。</p></blockquote><p>我们将贝叶斯线性回归添加核技巧的这个模型叫做高斯过程回归，高斯过程回归分为两种视角：</p><ol><li>权空间的视角-核贝叶斯线性回归，相当于 $x$ 为 $t$，在每个时刻的高斯分布来源于权重，根据上面的推导，预测的函数依然是高斯分布。</li><li>函数空间的视角-高斯分布通过函数 $f(x)$ 来体现。</li></ol><h2 id="核贝叶斯线性回归"><a href="#核贝叶斯线性回归" class="headerlink" title="核贝叶斯线性回归"></a>核贝叶斯线性回归</h2><p>贝叶斯线性回归可以通过加入核函数的方法来解决非线性函数的问题，将 $f(x)&#x3D;x^Tw$ 这个函数变为 $f(x)&#x3D;\phi(x)^Tw$（当然这个时候，$ \Sigma_p$ 也要变为更高维度的），变换到更高维的空间，有：<br>$$<br>\begin{align}f(x^*)\sim \mathcal{N}(\phi(x^*)^{T}\sigma^{-2}A^{-1}\Phi^TY,\phi(x^*)^{T}A^{-1}\phi(x^*))\<br>A&#x3D;\sigma^{-2}\Phi^T\Phi+\Sigma_p^{-1}<br>\end{align}<br>$$<br>其中，$\Phi&#x3D;(\phi(x_1),\phi(x_2),\cdots,\phi(x_N))^T$。</p><p>为了求解 $A^{-1}$，可以利用 Woodbury Formula，$A&#x3D;\Sigma_p^{-1},C&#x3D;\sigma^{-2}\mathbb{I}$：<br>$$<br>(A+UCV)^{-1}&#x3D;A^{-1}-A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}<br>$$<br>所以 $A^{-1}&#x3D;\Sigma_p-\Sigma_p\Phi^T(\sigma^2\mathbb{I}+\Phi\Sigma_p\Phi^T)^{-1}\Phi\Sigma_p$</p><p>也可以用另一种方法：<br>$$<br>\begin{align}<br>A&amp;&#x3D;\sigma^{-2}\Phi^T\Phi+\Sigma_p^{-1}\nonumber\<br>\Leftrightarrow A\Sigma_p&amp;&#x3D;\sigma^{-2}\Phi^T\Phi\Sigma_p+\mathbb{I}\nonumber\<br>\Leftrightarrow A\Sigma_p\Phi^T&amp;&#x3D;\sigma^{-2}\Phi^T\Phi\Sigma_p\Phi^T+\Phi^T&#x3D;\sigma^{-2}\Phi^T(k+\sigma^2\mathbb{I})\nonumber\<br>\Leftrightarrow \Sigma_p\Phi^T&amp;&#x3D;\sigma^{-2}A^{-1}\Phi^T(k+\sigma^2\mathbb{I})\nonumber\<br>\Leftrightarrow \sigma^{-2}A^{-1}\Phi^T&amp;&#x3D;\Sigma_p\Phi^T(k+\sigma^2\mathbb{I})^{-1}\nonumber\<br>\Leftrightarrow \phi(x^*)^T\sigma^{-2}A^{-1}\Phi^T&amp;&#x3D;\phi(x^*)^T\Sigma_p\Phi^T(k+\sigma^2\mathbb{I})^{-1}<br>\end{align}<br>$$<br>上面的左边的式子就是变换后的均值，而右边的式子就是不含 $A^{-1}$ 的式子，其中 $k&#x3D;\Phi\Sigma_p\Phi^T$。</p><p>根据 $A^{-1}$ 得到方差为：<br>$$<br>\phi(x^*)^T\Sigma_p\phi(x^*)-\phi(x^*)^T\Sigma_p\Phi^T(\sigma^2\mathbb{I}+k)^{-1}\Phi\Sigma_p\phi(x^*)<br>$$<br>上面定义了：<br>$$<br>k&#x3D;\Phi\Sigma_p\Phi^T<br>$$<br>我们看到，在均值和方差中，含有下面四项：<br>$$<br>\phi(x^*)^T\Sigma_p\Phi^T,\phi(x^*)^T\Sigma_p\phi(x^*),\phi(x^*)^T\Sigma_p\Phi^T,\Phi\Sigma_p\phi(x^*)<br>$$<br>展开后，可以看到，有共同的项：$k(x,x’)&#x3D;\phi(x)^T\Sigma_p\phi(x‘)$。由于 $\Sigma_p$ 是正定对称的方差矩阵，所以，这是一个核函数。</p><p>对于高斯过程中的协方差：<br>$$<br>k(t,s)&#x3D;Cov[f(x),f(x’)]&#x3D;\mathbb{E}[\phi(x)^Tww^T\phi(x’)]&#x3D;\phi(x)^T\mathbb{E}[ww^T]\phi(x’)&#x3D;\phi(x)^T\Sigma_p\phi(x’)<br>$$<br>我们可以看到，这个就对应着上面的核函数。因此我们看到 ${f(x)}$ 组成的组合就是一个高斯过程。</p><h2 id="函数空间的观点"><a href="#函数空间的观点" class="headerlink" title="函数空间的观点"></a>函数空间的观点</h2><p>相比权重空间，我们也可以直接关注 $f$ 这个空间，对于预测任务，这就是类似于求：<br>$$<br>p(y^*|X,Y,x^*)&#x3D;\int_fp(y^*|f,X,Y,x^*)p(f|X,Y,x^*)df<br>$$<br>对于数据集来说，取 $f(X)\sim\mathcal{N}(\mu(X),k(X,X)),Y&#x3D;f(X)+\varepsilon\sim\mathcal{N}(\mu(X),k(X,X)+\sigma^2\mathbb{I})$。预测任务的目的是给定一个新数据序列 $X^*&#x3D;(x_1^*,\cdots,x_M^*)^T$，得到 $Y^*&#x3D;f(X^*)+\varepsilon$。我们可以写出：<br>$$<br>\begin{pmatrix}Y\f(X^*)\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu(X)\\mu(X^*)\end{pmatrix},\begin{pmatrix}k(X,X)+\sigma^2\mathbb{I}&amp;k(X,X^*)\k(X^*,X)&amp;k(X^*,X^*)\end{pmatrix}\right)<br>$$<br>根据高斯分布的方法：<br>$$<br>\begin{align}x&#x3D;\begin{pmatrix}x_a\x_b\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu_a\\mu_b\end{pmatrix},\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\right)\<br>x_b|x_a\sim\mathcal{N}(\mu_{b|a},\Sigma_{b|a})\<br>\mu_{b|a}&#x3D;\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\<br>\Sigma_{b|a}&#x3D;\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}<br>\end{align}<br>$$<br>可以直接写出：<br>$$<br>p(f(X^*)|X,Y,X^*)&#x3D;p(f(X^*)|Y)\<br>&#x3D;\mathcal{N}(k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{-1}(Y-\mu(X))+\mu(X^*),\<br>k(X^*,X^*)-k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{1}k(X,X^*))<br>$$<br>所以对于 $Y&#x3D;f(X^*)+\varepsilon$：<br>$$<br>\mathcal{N}(k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{-1}(Y-\mu(X))+\mu(X^*),\<br>k(X^*,X^*)-k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{1}k(X,X^*)+\sigma^2\mathbb{I})<br>$$<br>我们看到，函数空间的观点更加简单易于求解。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——贝叶斯线性回归</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/17.BayesianLR/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/17.BayesianLR/</url>
    
    <content type="html"><![CDATA[<h1 id="贝叶斯线性回归"><a href="#贝叶斯线性回归" class="headerlink" title="贝叶斯线性回归"></a>贝叶斯线性回归</h1><p> 我们知道，线性回归当噪声为高斯分布的时候，最小二乘损失导出的结果相当于对概率模型应用 MLE，引入参数的先验时，先验分布是高斯分布，那么 MAP的结果相当于岭回归的正则化，如果先验是拉普拉斯分布，那么相当于 Lasso 的正则化。这两种方案都是点估计方法。我们希望利用贝叶斯方法来求解参数的后验分布。</p><p>线性回归的模型假设为：<br>$$<br>\begin{align}f(x)&#x3D;w^Tx<br>\y&#x3D;f(x)+\varepsilon\<br>\varepsilon\sim\mathcal{N}(0,\sigma^2)<br>\end{align}<br>$$<br>在贝叶斯方法中，需要解决推断和预测两个问题。</p><h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>引入高斯先验：<br>$$<br>p(w)&#x3D;\mathcal{N}(0,\Sigma_p)<br>$$<br>对参数的后验分布进行推断：<br>$$<br>p(w|X,Y)&#x3D;\frac{p(w,Y|X)}{p(Y|X)}&#x3D;\frac{p(Y|w,X)p(w|X)}{\int p(Y|w,X)p(w|X)dw}<br>$$<br>分母和参数无关，由于 $p(w|X)&#x3D;p(w)$，代入先验得到：<br>$$<br>p(w|X,Y)\propto \prod\limits_{i&#x3D;1}^N\mathcal{N}(y_i|w^Tx_i,\sigma^2)\cdot\mathcal{N}(0,\Sigma_p)<br>$$<br>高斯分布取高斯先验的共轭分布依然是高斯分布，于是可以得到后验分布也是一个高斯分布。第一项：<br>$$<br>\begin{align}\prod\limits_{i&#x3D;1}^N\mathcal{N}(y_i|w^Tx_i,\sigma^2)&amp;&#x3D;\frac{1}{(2\pi)^{N&#x2F;2}\sigma^N}\exp(-\frac{1}{2\sigma^2}\sum\limits_{i&#x3D;1}^N(y_i-w^Tx_i)^2)\nonumber\<br>&amp;&#x3D;\frac{1}{(2\pi)^{N&#x2F;2}\sigma^N}\exp(-\frac{1}{2}(Y-Xw)^T(\sigma^{-2}\mathbb{I})(Y-Xw))<br>\nonumber\&amp;&#x3D;\mathcal{N}(Xw,\sigma^2\mathbb{I})<br>\end{align}<br>$$<br>代入上面的式子：<br>$$<br>p(w|X,Y)\propto\exp(-\frac{1}{2\sigma^2}(Y-Xw)^T\sigma^{-2}\mathbb{I}(Y-Xw)-\frac{1}{2}w^T\Sigma_p^{-1}w)<br>$$<br>假定最后得到的高斯分布为：$\mathcal{N}(\mu_w,\Sigma_w)$。对于上面的分布，采用配方的方式来得到最终的分布，指数上面的二次项为：<br>$$<br>-\frac{1}{2\sigma^2}w^TX^TXw-\frac{1}{2}w^T\Sigma_p^{-1}w<br>$$<br>于是：<br>$$<br>\Sigma_w^{-1}&#x3D;\sigma^{-2}X^TX+\Sigma_p^{-1}&#x3D;A<br>$$<br>一次项：<br>$$<br>\frac{1}{2\sigma^2}2Y^TXw&#x3D;\sigma^{-2}Y^TXw<br>$$<br>于是：<br>$$<br>\mu_w^T\Sigma_w^{-1}&#x3D;\sigma^{-2}Y^TX\Rightarrow\mu_w&#x3D;\sigma^{-2}A^{-1}X^TY<br>$$</p><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>给定一个 $x^*$，求解 $y^*$，所以 $f(x^*)&#x3D;x^{<em>T}w$，代入参数后验，有 $x^{<em>T}w\sim \mathcal{N}(x^{<em>T}\mu_w,x^{<em>T}\Sigma_wx^</em>)$，添上噪声项：<br>$$<br>p(y^</em>|X,Y,x^</em>)&#x3D;\int_wp(y^</em>|w,X,Y,x^*)p(w|X,Y,x^*)dw&#x3D;\int_wp(y^*|w,x^*)p(w|X,Y)dw\<br>&#x3D;\mathcal{N}(x^{*T}\mu_w,x^{<em>T}\Sigma_wx^</em>+\sigma^2)<br>$$</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——高斯网络</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/16.GaussianNetwork/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/16.GaussianNetwork/</url>
    
    <content type="html"><![CDATA[<h1 id="高斯网络"><a href="#高斯网络" class="headerlink" title="高斯网络"></a>高斯网络</h1><p>高斯图模型（高斯网络）是一种随机变量为连续的有向或者无向图。有向图版本的高斯图是高斯贝叶斯网络，无向版本的叫高斯马尔可夫网络。</p><p>高斯网络的每一个节点都是高斯分布：$\mathcal{N}(\mu_i,\Sigma_i)$，于是所有节点的联合分布就是一个高斯分布，均值为 $\mu$，方差为 $\Sigma$。</p><p>对于边缘概率，我们有下面三个结论：</p><ol><li><p>对于方差矩阵，可以得到独立性条件：$x_i\perp x_j\Leftrightarrow\sigma_{ij}&#x3D;0$，这个叫做全局独立性。</p></li><li><p>我们看方差矩阵的逆（精度矩阵或信息矩阵）：$\Lambda&#x3D;\Sigma^{-1}&#x3D;(\lambda_{ij})_{pp}$，有定理：</p><blockquote><p>  $x_i\perp x_j|(X-{x_i,x_j})\Leftrightarrow\lambda_{ij}&#x3D;0$</p></blockquote><p>因此，我们使用精度矩阵来表示条件独立性。</p></li><li><p>对于任意一个无向图中的节点 $x_i$，$x_i|(X-x_i)\sim \mathcal{N}(\sum\limits_{j\ne i}\frac{\lambda_{ij}}{\lambda_{ii}}x_j,\lambda_{ii}^{-1})$</p><p>也就是其他所有分量的线性组合，即所有与它有链接的分量的线性组合。</p></li></ol><h2 id="高斯贝叶斯网络-GBN"><a href="#高斯贝叶斯网络-GBN" class="headerlink" title="高斯贝叶斯网络 GBN"></a>高斯贝叶斯网络 GBN</h2><p>高斯贝叶斯网络可以看成是 LDS 的一个推广，LDS 的假设是相邻时刻的变量之间的依赖关系，因此是一个局域模型，而高斯贝叶斯网络，每一个节点的父亲节点不一定只有一个，因此可以看成是一个全局的模型。根据有向图的因子分解：<br>$$<br>p(x)&#x3D;\prod\limits_{i&#x3D;1}^pp(x_i|x_{Parents(i)})<br>$$<br>对里面每一项，假设每一个特征是一维的，可以写成线性组合：<br>$$<br>p(x_i|x_{Parents(i)})&#x3D;\mathcal{N}(x_i|\mu_i+W_i^Tx_{Parents(i)},\sigma^2_i)<br>$$<br>将随机变量写成：<br>$$<br>x_i&#x3D;\mu_i+\sum\limits_{j\in x_{Parents(i)}}w_{ij}(x_j-\mu_j)+\sigma_i\varepsilon_i,\varepsilon_i\sim \mathcal{N}(0,1)<br>$$<br>写成矩阵形式，并且对 $w$ 进行扩展：<br>$$<br>x-\mu&#x3D;W(x-\mu)+S\varepsilon<br>$$<br>其中，$S&#x3D;diag(\sigma_i)$。所以有：$x-\mu&#x3D;(\mathbb{I}-W)^{-1}S\varepsilon$</p><p>由于：<br>$$<br>Cov(x)&#x3D;Cov(x-\mu)<br>$$<br>可以得到协方差矩阵。</p><h2 id="高斯马尔可夫网络-GMN"><a href="#高斯马尔可夫网络-GMN" class="headerlink" title="高斯马尔可夫网络 GMN"></a>高斯马尔可夫网络 GMN</h2><p>对于无向图版本的高斯网络，可以写成：<br>$$<br>p(x)&#x3D;\frac{1}{Z}\prod\limits_{i&#x3D;1}^p\phi_i(x_i)\prod\limits_{i,j\in X}\phi_{i,j}(x_i,x_j)<br>$$<br>为了将高斯分布和这个式子结合，我们写出高斯分布和变量相关的部分：<br>$$<br>\begin{align}p(x)&amp;\propto \exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\nonumber\<br>&amp;&#x3D;\exp(-\frac{1}{2}(x^T\Lambda x-2\mu^T\Lambda x+\mu^T\Lambda\mu))\nonumber\<br>&amp;&#x3D;\exp(-\frac{1}{2}x^T\Lambda x+(\Lambda\mu)^Tx)<br>\end{align}<br>$$<br>可以看到，这个式子与无向图分解中的两个部分对应，我们记 $h&#x3D;\Lambda\mu$为 Potential Vector。其中和 $x_i$ 相关的为：$x_i:-\frac{1}{2}\lambda_{ii}x_i^2+h_ix_i$，与 $x_i,x_j$ 相关的是：$x_i,x_j:-\lambda_{ij}x_ix_j$，这里利用了精度矩阵为对称矩阵的性质。我们看到，这里也可以看出，$x_i,x_j$ 构成的一个势函数，只和 $\lambda_{ij}$ 有关，于是 $x_i\perp x_j|(X-{x_i,x_j})\Leftrightarrow\lambda_{ij}&#x3D;0 $。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——条件随机场</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/15.CRF/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/15.CRF/</url>
    
    <content type="html"><![CDATA[<h1 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h1><p>我们知道，分类问题可以分为硬分类和软分类两种，其中硬分类有 SVM，PLA，LDA 等。软分类问题大体上可以分为概率生成和概率判别模型，其中较为有名的概率判别模型有 Logistic 回归，生成模型有朴素贝叶斯模型。Logistic 回归模型的损失函数为交叉熵，这类模型也叫对数线性模型，一般地，又叫做最大熵模型，这类模型和指数族分布的概率假设是一致的。对朴素贝叶斯假设，如果将其中的单元素的条件独立性做推广到一系列的隐变量，那么，由此得到的模型又被称为动态模型，比较有代表性的如 HMM，从概率意义上，HMM也可以看成是 GMM 在时序上面的推广。</p><p>我们看到，一般地，如果将最大熵模型和 HMM相结合，那么这种模型叫做最大熵 Markov 模型（MEMM）：</p><pre><code class=" mermaid">graph LR;x4((x4))--&gt;y4x2((x2))--&gt;y2x1((x1))--&gt;y1x3((x3))--&gt;y3y1--&gt;y2;y2--&gt;y3;y3--&gt;y4;</code></pre><p>这个图就是将 HMM 的图中观测变量和隐变量的边方向反向，应用在分类中，隐变量就是输出的分类，这样 HMM 中的两个假设就不成立了，特别是观测之间不是完全独立的了。</p><p>HMM 是一种生成式模型，其建模对象为 $p(X,Y|\lambda)$，根据 HMM 的概率图，$p(X,Y|\lambda)&#x3D;\prod\limits_{t&#x3D;1}^Tp(x_t,y_t|\lambda,y_{t-1})$。我们看到，观测独立性假设是一个很强的假设，如果我们有一个文本样本，那么观测独立性假设就假定了所有的单词之间没有关联。</p><p>在 MEMM 中，建模对象是 $p(Y|X,\lambda)$，我们看概率图，给定 $y_t$，$x_t,x_{t-1}$ 是不独立的，这样，观测独立假设就不成立了。根据概率图，$p(Y|X,\lambda)&#x3D;\prod\limits_{t&#x3D;1}^Tp(y_t|y_{t-1},X,\lambda)$。</p><p>MEMM 的缺陷是其必须满足局域的概率归一化（Label Bias Problem），我们看到，在上面的概率图中，$p(y_t|y_{t-1},x_t)$， 这个概率，如果 $p(y_t|y_{t-1})$ 非常接近1，那么事实上，观测变量是什么就不会影响这个概率了。</p><p>对于这个问题，我们将 $y$ 之间的箭头转为直线转为无向图（线性链条件随机场），这样就只要满足全局归一化了（破坏齐次 Markov 假设）。</p><pre><code class=" mermaid">graph LR;x4((x4))--&gt;y4x2((x2))--&gt;y2x1((x1))--&gt;y1x3((x3))--&gt;y3y1---y2;y2---y3;y3---y4;</code></pre><h2 id="CRF-的-PDF"><a href="#CRF-的-PDF" class="headerlink" title="CRF 的 PDF"></a>CRF 的 PDF</h2><p>线性链的 CRF 的 PDF 为 $p(Y|X)&#x3D;\frac{1}{Z}\exp\sum\limits_{t&#x3D;1}^T(F_t(y_{t-1},y_t,x_{1:T}))$，两两形成了最大团，其中 $y_0$ 是随意外加的一个元素。作为第一个简化，我们假设每个团的势函数相同 $F_t&#x3D;F$。</p><p>对于这个 $F$，我们进一步，可以将其写为 $ F(y_{t-1},y_t,X)&#x3D;\Delta_{y_{t-1},X}+\Delta_{y_{t},X}+\Delta_{y_t,y_{t-1},X}$这三个部分，分别表示状态函数已经转移函数，由于整体的求和，可以简化为 $ F(y_{t-1},y_t,X)&#x3D;\Delta_{y_{t},X}+\Delta_{y_t,y_{t-1},X}$。</p><p>我们可以设计一个表达式将其参数化：<br>$$<br>\begin{align}<br>\Delta_{y_t,y_{t-1},X}&amp;&#x3D;\sum\limits_{k&#x3D;1}^K\lambda_kf_k(y_{t-1},y_t,X)\<br>\Delta_{y_{t},X}&amp;&#x3D;\sum\limits_{l&#x3D;1}^L\eta_lg_l(y_t,X)<br>\end{align}<br>$$<br>其中 $g,f $ 叫做特征函数，对于 $y$ 有 $S$ 种元素，那么 $K\le S^2,L\le S$。</p><p>代入概率密度函数中：<br>$$<br>p(Y|X)&#x3D;\frac{1}{Z}\exp\sum\limits_{t&#x3D;1}^T[\sum\limits_{k&#x3D;1}^K\lambda_kf_k(y_{t-1},y_t,X)+\sum\limits_{l&#x3D;1}^L\eta_lg_l(y_t,X)]<br>$$<br>对于单个样本，将其写成向量的形式。定义 $y&#x3D;(y_1,y_2,\cdots,y_T)^T,x&#x3D;(x_1,x_2,\cdots,x_T)^T,\lambda&#x3D;(\lambda_1,\lambda_2,\cdots,\lambda_K)^T,\eta&#x3D;(\eta_1,\eta_2,\cdots,\eta_L)^T$。并且有 $f&#x3D;(f_1,f_2,\cdots,f_K)^T,g&#x3D;(g_1,g_2,\cdots,g_L)^T$。于是：<br>$$<br>p(Y&#x3D;y|X&#x3D;x)&#x3D;\frac{1}{Z}\exp\sum\limits_{t&#x3D;1}^T[\lambda^Tf(y_{t-1},y_t,x)+\eta^Tg(y_t,x)]<br>$$<br>不妨记：$\theta&#x3D;(\lambda,\eta)^T,H&#x3D;(\sum\limits_{t&#x3D;1}^Tf,\sum\limits_{t&#x3D;1}^Tg)^T$：<br>$$<br>p(Y&#x3D;y|X&#x3D;x)&#x3D;\frac{1}{Z(x,\theta)}\exp[\theta^TH(y_t,y_{t-1},x)]<br>$$<br>上面这个式子是一个指数族分布，于是 $Z$ 是配分函数。</p><p>CRF 需要解决下面几个问题：</p><ol><li><p>Learning：参数估计问题，对 $N$ 个 $T$ 维样本，$\hat{\theta}&#x3D;\mathop{argmax}\limits_{\theta}\prod\limits_{i&#x3D;1}^Np(y^i|x^i)$，这里用上标表示样本的编号。</p></li><li><p>Inference：</p><ol><li>边缘概率：<br>$$<br>p(y_t|x)<br>$$</li></ol></li><li><p>条件概率：一般在生成模型中较为关注，CRF 中不关注</p></li><li><p>MAP 推断：<br>$$<br>\hat{y}&#x3D;\mathop{argmax}p(y|x)<br>$$</p></li></ol><h2 id="边缘概率"><a href="#边缘概率" class="headerlink" title="边缘概率"></a>边缘概率</h2><p>边缘概率这个问题描述为，根据学习任务得到的参数，给定了 $p(Y&#x3D;y|X&#x3D;x)$，求解 $p(y_t&#x3D;i|x)$。根据无向图可以给出：<br>$$<br>p(y_t&#x3D;i|x)&#x3D;\sum\limits_{y_{1:t-1},y_{t+1:T}}p(y|x)&#x3D;\sum\limits_{y_{1:t-1}}\sum\limits_{y_{t+1:T}}\frac{1}{Z}\prod\limits_{t’&#x3D;1}^T\phi_{t’}(y_{t’-1},y_{t’},x)<br>$$<br>我们看到上面的式子，直接计算的复杂度很高，这是由于求和的复杂度在 $O(S^T)$，求积的复杂度在 $O(T)$，所以整体复杂度为 $O(TS^T)$。我们需要调整求和符号的顺序，从而降低复杂度。</p><p>首先，将两个求和分为：<br>$$<br>\begin{align}&amp;p(y_t&#x3D;i|x)&#x3D;\frac{1}{Z}\Delta_l\Delta_r\<br>&amp;\Delta_l&#x3D;\sum\limits_{y_{1:t-1}}\phi_{1}(y_0,y_1,x)\phi_2(y_1,y_2,x)\cdots\phi_{t-1}(y_{t-2},y_{t-1},x)\phi_t(y_{t-1},y_t&#x3D;i,x)\<br>&amp;\Delta_r&#x3D;\sum\limits_{y_{t+1:T}}\phi_{t+1}(y_t&#x3D;i,y_{t+1},x)\phi_{t+2}(y_{t+1},y_{t+2},x)\cdots\phi_T(y_{T-1},y_T,x)<br>\end{align}<br>$$<br>对于 $\Delta_l$，从左向右，一步一步将 $y_t$ 消掉：<br>$$<br>\Delta_l&#x3D;\sum\limits_{y_{t-1}}\phi_t(y_{t-1},y_t&#x3D;i,x)\sum\limits_{y_{t-2}}\phi_{t-1}(y_{t-2},y_{t-1},x)\cdots\sum\limits_{y_0}\phi_1(y_0,y_1,x)<br>$$<br>引入：<br>$$<br>\alpha_t(i)&#x3D;\Delta_l<br>$$<br>于是：<br>$$<br>\alpha_{t}(i)&#x3D;\sum\limits_{j\in S}\phi_t(y_{t-1}&#x3D;j,y_t&#x3D;i,x)\alpha_{t-1}(j)<br>$$<br>这样我们得到了一个递推式。</p><p>类似地，$\Delta_r&#x3D;\beta_t(i)&#x3D;\sum\limits_{j\in S}\phi_{t+1}(y_t&#x3D;i,y_{t+1}&#x3D;j,x)\beta_{t+1}(j)$。这个方法和 HMM 中的前向后向算法类似，就是概率图模型中精确推断的变量消除算法（信念传播）。</p><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>在进行各种类型的推断之前，还需要对参数进行学习：<br>$$<br>\begin{align}\hat{\theta}&amp;&#x3D;\mathop{argmax}<em>{\theta}\prod\limits</em>{i&#x3D;1}^Np(y^i|x^i)\<br>&amp;&#x3D;\mathop{argmax}<em>\theta\sum\limits</em>{i&#x3D;1}^N\log p(y^i|x^i)\<br>&amp;&#x3D;\mathop{argmax}<em>\theta\sum\limits</em>{i&#x3D;1}^N[-\log Z(x^i,\lambda,\eta)+\sum\limits_{t&#x3D;1}^T[\lambda^Tf(y_{t-1},y_t,x)+\eta^Tg(y_t,x)]]<br>\end{align}<br>$$<br>上面的式子中，第一项是对数配分函数，根据指数族分布的结论：<br>$$<br>\nabla_\lambda(\log Z(x^i,\lambda,\eta))&#x3D;\mathbb{E}<em>{p(y^i|x^i)}[\sum\limits</em>{t&#x3D;1}^Tf(y_{t-1},y_t,x^i)]<br>$$<br>其中，和 $\eta$ 相关的项相当于一个常数。求解这个期望值：<br>$$<br>\mathbb{E}<em>{p(y^i|x^i)}[\sum\limits</em>{t&#x3D;1}^Tf(y_{t-1},y_t,x^i)]&#x3D;\sum\limits_{y}p(y|x^i)\sum\limits_{t&#x3D;1}^Tf(y_{t-1},y_t,x^i)<br>$$<br>第一个求和号的复杂度为 $O(S^T)$，重新排列求和符号：<br>$$<br>\begin{align}\mathbb{E}<em>{p(y^i|x^i)}[\sum\limits</em>{t&#x3D;1}^Tf(y_{t-1},y_t,x^i)]&amp;&#x3D;\sum\limits_{t&#x3D;1}^T\sum\limits_{y_{1:t-2}}\sum\limits_{y_{t-1}}\sum\limits_{y_t}\sum\limits_{y_{t+1:T}}p(y|x^i)f(y_{t-1},y_t,x^i)\nonumber\<br>&amp;&#x3D;\sum\limits_{t&#x3D;1}^T\sum\limits_{y_{t-1}}\sum\limits_{y_t}p(y_{t-1},y_t|x^i)f(y_{t-1},y_t,x^i)<br>\end{align}<br>$$<br>和上面的边缘概率类似，也可以通过前向后向算法得到上面式子中的边缘概率。</p><p>于是：<br>$$<br>\nabla_\lambda L&#x3D;\sum\limits_{i&#x3D;1}^N\sum\limits_{t&#x3D;1}^T[f(y_{t-1},y_t,x^i)-\sum\limits_{y_{t-1}}\sum\limits_{y_t}p(y_{t-1},y_t|x^i)f(y_{t-1},y_t,x^i)]<br>$$<br>利用梯度上升算法可以求解。对于 $\eta$ 也是类似的过程。</p><h2 id="译码"><a href="#译码" class="headerlink" title="译码"></a>译码</h2><p>译码问题和 HMM 中的 Viterbi 算法类似，同样采样动态规划的思想一层一层求解最大值。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——粒子滤波</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/14.particleFilter/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/14.particleFilter/</url>
    
    <content type="html"><![CDATA[<h1 id="粒子滤波"><a href="#粒子滤波" class="headerlink" title="粒子滤波"></a>粒子滤波</h1><p>Kalman 滤波根据线性高斯模型可以求得解析解，但是在非线性，非高斯的情况，是无法得到解析解的，对这类一般的情况，我们叫做粒子滤波，我们需要求得概率分布，需要采用采样的方式。</p><p>我们希望应用 Monte Carlo 方法来进行采样，对于一个概率分布，如果我们希望计算依这个分布的某个函数 $f(z)$ 的期望，可以利用某种抽样方法，在这个概率分布中抽取 $N$ 个样本，则 $\mathbb{E}[f(z)]\simeq\frac{1}{N}\sum\limits_{i&#x3D;1}^Nf(z_i)$。但是如果这个概率十分复杂，那么采样比较困难。对于复杂的概率分布，我们可以通过一个简单的概率分布 $q(z)$ 作为桥梁（重要值采样）:<br>$$<br>\mathbb{E}[f(z)]&#x3D;\int_zf(z)p(z)dz&#x3D;\int_zf(z)\frac{p(z)}{q(z)}q(z)dz&#x3D;\sum\limits_{i&#x3D;1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}<br>$$<br>于是直接通过对 $q(z)$ 采样，然后对每一个采样的样本应用权重就得到了期望的近似，当然为了概率分布的特性，我们需要对权重进行归一化。</p><p>在滤波问题中，需要求解 $p(z_t|x_{1:t})$，其权重为：<br>$$<br>w_t^i&#x3D;\frac{p(z_t^i|x_{1:t})}{q(z_t^i|x_{1:t})},i&#x3D;1,2,\cdots,N<br>$$<br>于是在每一个时刻 $t$，都需要采样 $N$ 个点，但是即使采样了这么多点，分子上面的那一项也十分难求，于是希望找到一个关于权重的递推公式。为了解决这个问题，引入序列重要性采样（SIS）。</p><h2 id="SIS"><a href="#SIS" class="headerlink" title="SIS"></a>SIS</h2><p>在 SIS 中，解决的问题是 $p(z_{1:t}|x_{1:t})$。<br>$$<br>w_t^i\propto\frac{p(z_{1:t}|x_{1:t})}{q(z_{1:t}|x_{1:t})}<br>$$<br>根据 LDS 中的推导：<br>$$<br>\begin{align}p(z_{1:t}|x_{1:t})\propto p(x_{1:t},z_{1:t})&amp;&#x3D;p(x_t|z_{1:t},x_{1:t-1})p(z_{1:t},x_{1:t-1})\nonumber\<br>&amp;&#x3D;p(x_t|z_t)p(z_t|x_{1:t-1},z_{1:t-1})p(x_{1:t-1},z_{1:t-1})\nonumber\<br>&amp;&#x3D;p(x_t|z_t)p(z_t|z_{t-1})p(x_{1:t-1},z_{1:t-1})\nonumber\<br>&amp;\propto p(x_t|z_t)p(z_t|z_{t-1})p(z_{1:t-1}|x_{1:t-1})<br>\end{align}<br>$$<br>于是分子的递推式就得到了。对于提议分布的分母，可以取：<br>$$<br>q(z_{1:t}|x_{1:t})&#x3D;q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t-1})<br>$$<br>所以有：<br>$$<br>w_t^i\propto\frac{p(z_{1:t}|x_{1:t})}{q(z_{1:t}|x_{1:t})}\propto \frac{p(x_t|z_t)p(z_t|z_{t-1})p(z_{1:t-1}|x_{1:t-1})}{q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t-1})}&#x3D;\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}^i<br>$$<br>我们得到的对权重的算法为：</p><ol><li>$t-1$ 时刻，采样完成并计算得到权重</li><li>t 时刻，根据 $q(z_t|z_{1:t-1},x_{1:t})$ 进行采样得到 $z_t^i$。然后计算得到 $N$ 个权重。</li><li>最后对权重归一化。</li></ol><p>SIS 算法会出现权值退化的情况，在一定时间后，可能会出现大部分权重都逼近0的情况，这是由于空间维度越来越高，需要的样本也越来越多。解决这个问题的方法有：</p><ol><li>重采样，以权重作为概率分布，重新在已经采样的样本中采样，然后所有样本的权重相同，这个方法的思路是将权重作为概率分布，然后得到累积密度函数，在累积密度上取点（阶梯函数）。</li><li>选择一个合适的提议分布，$q(z_t|z_{1:t-1},x_{1:t})&#x3D;p(z_t|z_{t-1})$，于是就消掉了一项，并且采样的概率就是 $p(z_t|z_{t-1})$，这就叫做生成与测试方法。</li></ol><p>采用重采样的 SIS 算法就是基本的粒子滤波算法。如果像上面那样选择提议分布，这个算法叫做 SIR 算法。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——线性动态系统</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/13.LDS/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/13.LDS/</url>
    
    <content type="html"><![CDATA[<h1 id="线性动态系统"><a href="#线性动态系统" class="headerlink" title="线性动态系统"></a>线性动态系统</h1><p>HMM 模型适用于隐变量是离散的值的时候，对于连续隐变量的 HMM，常用线性动态系统描述线性高斯模型的态变量，使用粒子滤波来表述非高斯非线性的态变量。</p><p>LDS 又叫卡尔曼滤波，其中，线性体现在上一时刻和这一时刻的隐变量以及隐变量和观测之间：<br>$$<br>\begin{align}<br>z_t&amp;&#x3D;A\cdot z_{t-1}+B+\varepsilon\<br>x_t&amp;&#x3D;C\cdot z_t+D+\delta\<br>\varepsilon&amp;\sim\mathcal{N}(0,Q)\<br>\delta&amp;\sim\mathcal{N}(0,R)<br>\end{align}<br>$$<br>类比 HMM 中的几个参数：<br>$$<br>\begin{align}<br>p(z_t|z_{t-1})&amp;\sim\mathcal{N}(A\cdot z_{t-1}+B,Q)\<br>p(x_t|z_t)&amp;\sim\mathcal{N}(C\cdot z_t+D,R)\<br>z_1&amp;\sim\mathcal{N}(\mu_1,\Sigma_1)<br>\end{align}<br>$$<br>在含时的概率图中，除了对参数估计的学习问题外，在推断任务中，包括译码，证据概率，滤波，平滑，预测问题，LDS 更关心滤波这个问题：$p(z_t|x_1,x_2,\cdots,x_t)$。类似 HMM 中的前向算法，我们需要找到一个递推关系。<br>$$<br>p(z_t|x_{1:t})&#x3D;p(x_{1:t},z_t)&#x2F;p(x_{1:t})&#x3D;Cp(x_{1:t},z_t)<br>$$<br>对于 $p(x_{1:t},z_t)$：<br>$$<br>\begin{align}p(x_{1:t},z_t)&amp;&#x3D;p(x_t|x_{1:t-1},z_t)p(x_{1:t-1},z_t)&#x3D;p(x_t|z_t)p(x_{1:t-1},z_t)\nonumber\<br>&amp;&#x3D;p(x_t|z_t)p(z_t|x_{1:t-1})p(x_{1:t-1})&#x3D;Cp(x_t|z_t)p(z_t|x_{1:t-1})\<br>\end{align}<br>$$<br>我们看到，右边除了只和观测相关的常数项，还有一项是预测任务需要的概率。对这个值：<br>$$<br>\begin{align}<br>p(z_t|x_{1:t-1})&amp;&#x3D;\int_{z_{t-1}}p(z_t,z_{t-1}|x_{1:t-1})dz_{t-1}\nonumber\<br>&amp;&#x3D;\int_{z_{t-1}}p(z_t|z_{t-1},x_{1:t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}\nonumber\<br>&amp;&#x3D;\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}<br>\end{align}<br>$$<br>我们看到，这又化成了一个滤波问题。于是我们得到了一个递推公式：</p><ol><li>$t&#x3D;1$，$p(z_1|x_1)$，称为 update 过程，然后计算 $p(z_2|x_1)$，通过上面的积分进行，称为 prediction 过程。</li><li>$t&#x3D;2$，$p(z_2|x_2,x_1)$ 和 $p(z_3|x_1,x_2)$</li></ol><p>我们看到，这个过程是一个 Online 的过程，对于我们的线性高斯假设，这个计算过程都可以得到解析解。</p><ol><li><p>Prediction：<br>$$<br>p(z_t|x_{1:t-1})&#x3D;\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}&#x3D;\int_{z_{t-1}}\mathcal{N}(Az_{t-1}+B,Q)\mathcal{N}(\mu_{t-1},\Sigma_{t-1})dz_{t-1}<br>$$<br>其中第二个高斯分布是上一步的 Update 过程，所以根据线性高斯模型，直接可以写出这个积分：<br>$$<br>p(z_t|x_{1:t-1})&#x3D;\mathcal{N}(A\mu_{t-1}+B,Q+A\Sigma_{t-1}A^T)<br>$$</p></li><li><p>Update:<br>$$<br>p(z_t|x_{1:t})\propto p(x_t|z_t)p(z_t|x_{1:t-1})<br>$$<br>同样利用线性高斯模型，也可以直接写出这个高斯分布。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——隐马尔可夫模型</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/12.HMM/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/12.HMM/</url>
    
    <content type="html"><![CDATA[<h2 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h2><p>隐马尔可夫模型是一种概率图模型。我们知道，机器学习模型可以从频率派和贝叶斯派两个方向考虑，在频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC 等。概率图模型最基本的模型可以分为有向图（贝叶斯网络）和无向图（马尔可夫随机场）两个方面，例如 GMM，在这些基本的模型上，如果样本之间存在关联，可以认为样本中附带了时序信息，从而样本之间不独立全同分布的，这种模型就叫做动态模型，隐变量随着时间发生变化，于是观测变量也发生变化：</p><pre><code class=" mermaid">graph LR;z1--&gt;z2--&gt;z3;</code></pre><p>根据状态变量的特点，可以分为：</p><ol><li>HMM，状态变量（隐变量）是离散的</li><li>Kalman 滤波，状态变量是连续的，线性的</li><li>粒子滤波，状态变量是连续，非线性的</li></ol><h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><p>HMM 用概率图表示为：</p><pre><code class=" mermaid">graph TD;t1--&gt;t2;subgraph fourt4--&gt;x4((x4))endsubgraph threet3--&gt;x3((x3))endsubgraph twot2--&gt;x2((x2))endsubgraph onet1--&gt;x1((x1))endt2--&gt;t3;t3--&gt;t4;</code></pre><p>上图表示了四个时刻的隐变量变化。用参数 $\lambda&#x3D;(\pi,A,B)$ 来表示，其中 $\pi$ 是开始的概率分布，$A$ 为状态转移矩阵，$B$ 为发射矩阵。</p><p>下面使用 $ o_t$ 来表示观测变量，$O$ 为观测序列，$V&#x3D;{v_1,v_2,\cdots,v_M}$ 表示观测的值域，$i_t$ 表示状态变量，$I$ 为状态序列，$Q&#x3D;{q_1,q_2,\cdots,q_N}$ 表示状态变量的值域。定义 $A&#x3D;(a_{ij}&#x3D;p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i))$ 表示状态转移矩阵，$B&#x3D;(b_j(k)&#x3D;p(o_t&#x3D;v_k|i_t&#x3D;q_j))$ 表示发射矩阵。</p><p>在 HMM 中，有两个基本假设：</p><ol><li><p>齐次 Markov 假设（未来只依赖于当前）：<br>$$<br>p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)&#x3D;p(i_{t+1}|i_t)<br>$$</p></li><li><p>观测独立假设：<br>$$<br>p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)&#x3D;p(o_t|i_t)<br>$$</p></li></ol><p>HMM 要解决三个问题：</p><ol><li>Evaluation：$p(O|\lambda)$，Forward-Backward 算法</li><li>Learning：$\lambda&#x3D;\mathop{argmax}\limits_{\lambda}p(O|\lambda)$，EM 算法（Baum-Welch）</li><li>Decoding：$I&#x3D;\mathop{argmax}\limits_{I}p(I|O,\lambda)$，Vierbi 算法<ol><li>预测问题：$p(i_{t+1}|o_1,o_2,\cdots,o_t)$</li><li>滤波问题：$p(i_t|o_1,o_2,\cdots,o_t)$</li></ol></li></ol><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>$$<br>p(O|\lambda)&#x3D;\sum\limits_{I}p(I,O|\lambda)&#x3D;\sum\limits_{I}p(O|I,\lambda)p(I|\lambda)<br>$$</p><p>$$<br>p(I|\lambda)&#x3D;p(i_1,i_2,\cdots,i_t|\lambda)&#x3D;p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)p(i_1,i_2,\cdots,i_{t-1}|\lambda)<br>$$</p><p>根据齐次 Markov 假设：<br>$$<br>p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)&#x3D;p(i_t|i_{t-1})&#x3D;a_{i_{t-1}i_t}<br>$$<br>所以：<br>$$<br>p(I|\lambda)&#x3D;\pi_1\prod\limits_{t&#x3D;2}^Ta_{i_{t-1},i_t}<br>$$<br>又由于：<br>$$<br>p(O|I,\lambda)&#x3D;\prod\limits_{t&#x3D;1}^Tb_{i_t}(o_t)<br>$$<br>于是：<br>$$<br>p(O|\lambda)&#x3D;\sum\limits_{I}\pi_{i_1}\prod\limits_{t&#x3D;2}^Ta_{i_{t-1},i_t}\prod\limits_{t&#x3D;1}^Tb_{i_t}(o_t)<br>$$<br>我们看到，上面的式子中的求和符号是对所有的观测变量求和，于是复杂度为 $O(N^T)$。</p><p>下面，记 $\alpha_t(i)&#x3D;p(o_1,o_2,\cdots,o_t,i_t&#x3D;q_i|\lambda)$，所以，$\alpha_T(i)&#x3D;p(O,i_T&#x3D;q_i|\lambda)$。我们看到：<br>$$<br>p(O|\lambda)&#x3D;\sum\limits_{i&#x3D;1}^Np(O,i_T&#x3D;q_i|\lambda)&#x3D;\sum\limits_{i&#x3D;1}^N\alpha_T(i)<br>$$<br>对 $\alpha_{t+1}(j)$：<br>$$<br>\begin{align}\alpha_{t+1}(j)&amp;&#x3D;p(o_1,o_2,\cdots,o_{t+1},i_{t+1}&#x3D;q_j|\lambda)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^Np(o_1,o_2,\cdots,o_{t+1},i_{t+1}&#x3D;q_j,i_t&#x3D;q_i|\lambda)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^Np(o_{t+1}|o_1,o_2,\cdots,i_{t+1}&#x3D;q_j,i_t&#x3D;q_i|\lambda)p(o_1,\cdots,o_t,i_t&#x3D;q_i,i_{t+1}&#x3D;q_j|\lambda)<br>\end{align}<br>$$<br>利用观测独立假设：<br>$$<br>\begin{align}\alpha_{t+1}(j)&amp;&#x3D;\sum\limits_{i&#x3D;1}^Np(o_{t+1}|i_{t+1}&#x3D;q_j)p(o_1,\cdots,o_t,i_t&#x3D;q_i,i_{t+1}&#x3D;q_j|\lambda)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^Np(o_{t+1}|i_{t+1}&#x3D;q_j)p(i_{t+1}&#x3D;q_j|o_1,\cdots,o_t,i_t&#x3D;q_i,\lambda)p(o_1,\cdots,o_t,i_t&#x3D;q_i|\lambda)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^Nb_{j}(o_t)a_{ij}\alpha_t(i)<br>\end{align}<br>$$<br>上面利用了齐次 Markov 假设得到了一个递推公式，这个算法叫做前向算法。</p><p>还有一种算法叫做后向算法，定义 $\beta_t(i)&#x3D;p(o_{t+1},o_{t+1},\cdots，o_T|i_t&#x3D;i,\lambda)$：<br>$$<br>\begin{align}p(O|\lambda)&amp;&#x3D;p(o_1,\cdots,o_T|\lambda)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^Np(o_1,o_2,\cdots,o_T,i_1&#x3D;q_i|\lambda)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^Np(o_1,o_2,\cdots,o_T|i_1&#x3D;q_i,\lambda)\pi_i\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^Np(o_1|o_2,\cdots,o_T,i_1&#x3D;q_i,\lambda)p(o_2,\cdots,o_T|i_1&#x3D;q_i,\lambda)\pi_i\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^Nb_i(o_1)\pi_i\beta_1(i)<br>\end{align}<br>$$<br>对于这个 $\beta_1(i)$：<br>$$<br>\begin{align}\beta_t(i)&amp;&#x3D;p(o_{t+1},\cdots,o_T|i_t&#x3D;q_i)\nonumber\<br>&amp;&#x3D;\sum\limits_{j&#x3D;1}^Np(o_{t+1},o_{t+2},\cdots,o_T,i_{t+1}&#x3D;q_j|i_t&#x3D;q_i)\nonumber\<br>&amp;&#x3D;\sum\limits_{j&#x3D;1}^Np(o_{t+1},\cdots,o_T|i_{t+1}&#x3D;q_j,i_t&#x3D;q_i)p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i)\nonumber\<br>&amp;&#x3D;\sum\limits_{j&#x3D;1}^Np(o_{t+1},\cdots,o_T|i_{t+1}&#x3D;q_j)a_{ij}\nonumber\<br>&amp;&#x3D;\sum\limits_{j&#x3D;1}^Np(o_{t+1}|o_{t+2},\cdots,o_T,i_{t+1}&#x3D;q_j)p(o_{t+2},\cdots,o_T|i_{t+1}&#x3D;q_j)a_{ij}\nonumber\<br>&amp;&#x3D;\sum\limits_{j&#x3D;1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)<br>\end{align}<br>$$<br>于是后向地得到了第一项。</p><h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>为了学习得到参数的最优值，在 MLE 中：<br>$$<br>\lambda_{MLE}&#x3D;\mathop{argmax}<em>\lambda p(O|\lambda)<br>$$<br>我们采用 EM 算法（在这里也叫 Baum Welch 算法），用上标表示迭代：<br>$$<br>\theta^{t+1}&#x3D;\mathop{argmax}</em>{\theta}\int_z\log p(X,Z|\theta)p(Z|X,\theta^t)dz<br>$$<br>其中，$X$ 是观测变量，$Z$ 是隐变量序列。于是：<br>$$<br>\lambda^{t+1}&#x3D;\mathop{argmax}<em>\lambda\sum\limits_I\log p(O,I|\lambda)p(I|O,\lambda^t)\<br>&#x3D;\mathop{argmax}<em>\lambda\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)<br>$$<br> 这里利用了 $p(O|\lambda^t)$ 和$\lambda$ 无关。将 Evaluation 中的式子代入：<br>$$<br>\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)&#x3D;\sum\limits_I[\log \pi</em>{i_1}+\sum\limits</em>{t&#x3D;2}^T\log a_{i_{t-1},i_t}+\sum\limits_{t&#x3D;1}^T\log b_{i_t}(o_t)]p(O,I|\lambda^t)<br>$$<br>对 $\pi^{t+1}$：<br>$$<br>\begin{align}\pi^{t+1}&amp;&#x3D;\mathop{argmax}<em>\pi\sum\limits_I[\log \pi</em>{i_1}p(O,I|\lambda^t)]\nonumber\<br>&amp;&#x3D;\mathop{argmax}<em>\pi\sum\limits_I[\log \pi</em>{i_1}\cdot p(O,i_1,i_2,\cdots,i_T|\lambda^t)]<br>\end{align}<br>$$<br>上面的式子中，对 $i_2,i_2,\cdots,i_T$ 求和可以将这些参数消掉：<br>$$<br>\pi^{t+1}&#x3D;\mathop{argmax}<em>\pi\sum\limits</em>{i_1}[\log \pi_{i_1}\cdot p(O,i_1|\lambda^t)]<br>$$<br>上面的式子还有对 $\pi$ 的约束 $\sum\limits_i\pi_i&#x3D;1$。定义 Lagrange 函数：<br>$$<br>L(\pi,\eta)&#x3D;\sum\limits_{i&#x3D;1}^N\log \pi_i\cdot p(O,i_1&#x3D;q_i|\lambda^t)+\eta(\sum\limits_{i&#x3D;1}^N\pi_i-1)<br>$$<br>于是：<br>$$<br>\frac{\partial L}{\partial\pi_i}&#x3D;\frac{1}{\pi_i}p(O,i_1&#x3D;q_i|\lambda^t)+\eta&#x3D;0<br>$$<br>对上式求和：<br>$$<br>\sum\limits_{i&#x3D;1}^Np(O,i_1&#x3D;q_i|\lambda^t)+\pi_i\eta&#x3D;0\Rightarrow\eta&#x3D;-p(O|\lambda^t)<br>$$<br>所以：<br>$$<br>\pi_i^{t+1}&#x3D;\frac{p(O,i_1&#x3D;q_i|\lambda^t)}{p(O|\lambda^t)}<br>$$</p><h3 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h3><p>Decoding 问题表述为：<br>$$<br>I&#x3D;\mathop{argmax}\limits_{I}p(I|O,\lambda)<br>$$<br>我们需要找到一个序列，其概率最大，这个序列就是在参数空间中的一个路径，可以采用动态规划的思想。</p><p>定义：<br>$$<br>\delta_{t}(j)&#x3D;\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t&#x3D;q_i)<br>$$<br>于是：<br>$$<br>\delta_{t+1}(j)&#x3D;\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})<br>$$<br>这个式子就是从上一步到下一步的概率再求最大值。记这个路径为：<br>$$<br>\psi_{t+1}(j)&#x3D;\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}<br>$$</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>HMM 是一种动态模型，是由混合树形模型和时序结合起来的一种模型（类似 GMM + Time）。对于类似 HMM 的这种状态空间模型，普遍的除了学习任务（采用 EM ）外，还有推断任务，推断任务包括：</p><ol><li><p>译码 Decoding：$p(z_1,z_2,\cdots,z_t|x_1,x_2,\cdots,x_t)$</p></li><li><p>似然概率：$p(X|\theta)$</p></li><li><p>滤波：$ p(z_t|x_1,\cdots,x_t)$，Online<br>$$<br>p(z_t|x_{1:t})&#x3D;\frac{p(x_{1:t},z_t)}{p(x_{1:t})}&#x3D;C\alpha_t(z_t)<br>$$</p></li><li><p>平滑：$p(z_t|x_1,\cdots,x_T)$，Offline<br>$$<br>p(z_t|x_{1:T})&#x3D;\frac{p(x_{1:T},z_t)}{p(x_{1:T})}&#x3D;\frac{\alpha_t(z_t)p(x_{t+1:T}|x_{1:t},z_t)}{p(x_{1:T})}<br>$$<br>根据概率图的条件独立性，有：<br>$$<br>p(z_t|x_{1:T})&#x3D;\frac{\alpha_t(z_t)p(x_{t+1:T}|z_t)}{p(x_{1:T})}&#x3D;C\alpha_t(z_t)\beta_t(z_t)<br>$$<br>这个算法叫做前向后向算法。</p></li><li><p>预测：$p(z_{t+1},z_{t+2}|x_1,\cdots,x_t),p(x_{t+1},x_{t+2}|x_1,\cdots,x_t)$<br>$$<br>p(z_{t+1}|x_{1:t})&#x3D;\sum_{z_t}p(z_{t+1},z_t|x_{1:t})&#x3D;\sum\limits_{z_t}p(z_{t+1}|z_t)p(z_t|x_{1:t})<br>$$</p><p>$$<br>p(x_{t+1}|x_{1:t})&#x3D;\sum\limits_{z_{t+1}}p(x_{t+1},z_{t+1}|x_{1:t})&#x3D;\sum\limits_{z_{t+1}}p(x_{t+1}|z_{t+1})p(z_{t+1}|x_{1:t})<br>$$</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——马尔可夫链蒙特卡洛</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/11.MCMC/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/11.MCMC/</url>
    
    <content type="html"><![CDATA[<h1 id="马尔可夫链蒙特卡洛"><a href="#马尔可夫链蒙特卡洛" class="headerlink" title="马尔可夫链蒙特卡洛"></a>马尔可夫链蒙特卡洛</h1><p>MCMC 是一种随机的近似推断，其核心就是基于采样的随机近似方法蒙特卡洛方法。对于采样任务来说，有下面一些常用的场景：</p><ol><li>采样作为任务，用于生成新的样本</li><li>求和&#x2F;求积分</li></ol><p>采样结束后，我们需要评价采样出来的样本点是不是好的样本集：</p><ol><li>样本趋向于高概率的区域</li><li>样本之间必须独立</li></ol><p>具体采样中，采样是一个困难的过程：</p><ol><li>无法采样得到归一化因子，即无法直接对概率 $ p(x)&#x3D;\frac{1}{Z}\hat{p}(x)$ 采样，常常需要对 CDF 采样，但复杂的情况不行</li><li>如果归一化因子可以求得，但是对高维数据依然不能均匀采样（维度灾难），这是由于对 $p$ 维空间，总的状态空间是 $K^p$ 这么大，于是在这种情况下，直接采样也不行</li></ol><p>因此需要借助其他手段，如蒙特卡洛方法中的拒绝采样，重要性采样和 MCMC。</p><h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><p>蒙特卡洛方法旨在求得复杂概率分布下的期望值：$\mathbb{E}<em>{z|x}[f(z)]&#x3D;\int p(z|x)f(z)dz\simeq\frac{1}{N}\sum\limits</em>{i&#x3D;1}^Nf(z_i)$，也就是说，从概率分布中取 $N$ 个点，从而近似计算这个积分。采样方法有：</p><ol><li><p>概率分布采样，首先求得概率密度的累积密度函数 CDF，然后求得 CDF 的反函数，在0到1之间均匀采样，代入反函数，就得到了采样点。但是实际大部分概率分布不能得到 CDF。</p></li><li><p>Rejection Sampling 拒绝采样：对于概率分布 $p(z)$，引入简单的提议分布 $q(z)$，使得 $\forall z_i,Mq(z_i)\ge p(z_i)$。我们先在 $ q(z)$ 中采样，定义接受率：$\alpha&#x3D;\frac{p(z^i)}{Mq(z^i)}\le1$。算法描述为：</p><ol><li>取 $z^i\sim q(z)$。</li><li>在均匀分布中选取 $u$。</li><li>如果 $u\le\alpha$，则接受 $z^i$，否则，拒绝这个值。</li></ol></li><li><p>Importance Sampling：直接对期望：$\mathbb{E}<em>{p(z)}[f(z)]$ 进行采样。<br>$$<br>\mathbb{E}</em>{p(z)}[f(z)]&#x3D;\int p(z)f(z)dz&#x3D;\int \frac{p(z)}{q(z)}f(z)q(z)dz\simeq\frac{1}{N}\sum\limits_{i&#x3D;1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}<br>$$<br>于是采样在 $ q(z)$ 中采样，并通过权重计算和。重要值采样对于权重非常小的时候，效率非常低。重要性采样有一个变种 Sampling-Importance-Resampling，这种方法，首先和上面一样进行采样，然后在采样出来的 $N$ 个样本中，重新采样，这个重新采样，使用每个样本点的权重作为概率分布进行采样。</p></li></ol><h2 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h2><p>马尔可夫链式一种时间状态都是离散的随机变量序列。我们关注的主要是齐次的一阶马尔可夫链。马尔可夫链满足：$p(X_{t+1}|X_1,X_2,\cdots,X_t)&#x3D;p(X_{t+1}|X_t)$。这个式子可以写成转移矩阵的形式 $p_{ij}&#x3D;p(X_{t+1}&#x3D;j|X_t&#x3D;i)$。我们有：<br>$$<br>\pi_{t+1}(x^*)&#x3D;\int\pi_i(x)p_{x\to x^*}dx<br>$$<br>如果存在 $\pi&#x3D;(\pi(1),\pi(2),\cdots),\sum\limits_{i&#x3D;1}^{+\infin}\pi(i)&#x3D;1$，有上式成立，这个序列就叫马尔可夫链 $X_t$ 的平稳分布，平稳分布就是表示在某一个时刻后，分布不再改变。MCMC 就是通过构建马尔可夫链概率序列，使其收敛到平稳分布 $p(z)$。引入细致平衡：$\pi(x)p_{x\to x^*}&#x3D;\pi(x^*)p_{x^<em>\to x}$。如果一个分布满足细致平衡，那么一定满足平稳分布（反之不成立）：<br>$$<br>\int\pi(x)p_{x\to x^</em>}dx&#x3D;\int\pi(x^*)p_{x^<em>\to x}dx&#x3D;\pi(x^</em>)<br>$$<br>细致平衡条件将平稳分布的序列和马尔可夫链的转移矩阵联系在一起了，通过转移矩阵可以不断生成样本点。假定随机取一个转移矩阵 $(Q&#x3D;Q_{ij})$，作为一个提议矩阵。我们有：<br>$$<br>p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)&#x3D;p(z^*)\cdot Q_{z^<em>\to z}\alpha(z^</em>,z)<br>$$<br>取 ：<br>$$<br>\alpha(z,z^*)&#x3D;\min{1,\frac{p(z^*)Q_{z^<em>\to z}}{p(z)Q_{z\to z^</em>}}}<br>$$<br>则<br>$$<br>p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)&#x3D;\min{p(z)Q_{z\to z^*},p(z^*)Q_{z^<em>\to z}}&#x3D;p(z^</em>)\cdot Q_{z^<em>\to z}\alpha(z^</em>,z)<br>$$<br>于是，迭代就得到了序列，这个算法叫做 Metropolis-Hastings 算法：</p><ol><li>通过在0，1之间均匀分布取点 $u$</li><li>生成 $z^<em>\sim Q(z^</em>|z^{i-1})$</li><li>计算 $\alpha$ 值</li><li>如果 $\alpha\ge u$，则 $z^i&#x3D;z^*$，否则 $z^{i}&#x3D;z^{i-1}$</li></ol><p>这样取的样本就服从 $p(z)&#x3D;\frac{\hat{p}(z)}{z_p}\sim \hat{p}(z)$。</p><p>下面介绍另一种采样方式 Gibbs 采样，如果 $z$ 的维度非常高，那么通过固定被采样的维度其余的维度来简化采样过程：$z_i\sim p(z_i|z_{-i})$：</p><ol><li>给定初始值 $z_1^0,z_2^0,\cdots$</li><li>在 $t+1$ 时刻，采样 $z_i^{t+1}\sim p(z_i|z_{-i})$，从第一个维度一个个采样。</li></ol><p>Gibbs 采样方法是一种特殊的 MH 采样，可以计算 Gibbs 采样的接受率：<br>$$<br>\frac{p(z^*)Q_{z^<em>\to z}}{p(z)Q_{z\to z^</em>}}&#x3D;\frac{p(z_i^*|z^*_{-i})p(z^*_{-i})p(z_i|z_{-i}^*)}{p(z_i|z_{-i})p(z_{-i})p(z_i^*|z_{-i})}<br>$$<br>对于每个 Gibbs 采样步骤，$z_{-i}&#x3D;z_{-i}^*$，这是由于每个维度 $i$ 采样的时候，其余的参量保持不变。所以上式为1。于是 Gibbs 采样过程中，相当于找到了一个步骤，使得所有的接受率为 1。</p><h2 id="平稳分布"><a href="#平稳分布" class="headerlink" title="平稳分布"></a>平稳分布</h2><p>定义随机矩阵：<br>$$<br>Q&#x3D;\begin{pmatrix}Q_{11}&amp;Q_{12}&amp;\cdots&amp;Q_{1K}\\vdots&amp;\vdots&amp;\vdots&amp;\vdots\Q_{k1}&amp;Q_{k2}&amp;\cdots&amp;Q_{KK}\end{pmatrix}<br>$$<br>这个矩阵每一行或者每一列的和都是1。随机矩阵的特征值都小于等于1。假设只有一个特征值为 $\lambda_i&#x3D;1$。于是在马尔可夫过程中：<br>$$<br>q^{t+1}(x&#x3D;j)&#x3D;\sum\limits_{i&#x3D;1}^Kq^t(x&#x3D;i)Q_{ij}\<br>\Rightarrow q^{t+1}&#x3D;q^t\cdot Q&#x3D;q^1Q^t<br>$$<br>于是有：<br>$$<br>q^{t+1}&#x3D;q^1A\Lambda^t A^{-1}<br>$$<br>如果 $m$ 足够大，那么，$\Lambda^m&#x3D;diag(0,0,\cdots,1,\cdots,0)$，则：$q^{m+1}&#x3D;q^{m}$ ，则趋于平稳分布了。马尔可夫链可能具有平稳分布的性质，所以我们可以构建马尔可夫链使其平稳分布收敛于需要的概率分布（设计转移矩阵）。</p><p>在采样过程中，需要经历一定的时间（燃烧期&#x2F;混合时间）才能达到平稳分布。但是 MCMC 方法有一些问题：</p><ol><li>无法判断是否已经收敛</li><li>燃烧期过长（维度太高，并且维度之间有关，可能无法采样到某些维度），例如在 GMM 中，可能无法采样到某些峰。于是在一些模型中，需要对隐变量之间的关系作出约束，如 RBM 假设隐变量之间无关。</li><li>样本之间一定是有相关性的，如果每个时刻都取一个点，那么每个样本一定和前一个相关，这可以通过间隔一段时间采样。</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——变分推断</title>
    <link href="/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/10.VI/"/>
    <url>/2022/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%AC%94%E8%AE%B0/10.VI/</url>
    
    <content type="html"><![CDATA[<h1 id="变分推断"><a href="#变分推断" class="headerlink" title="变分推断"></a>变分推断</h1><p>我们已经知道概率模型可以分为，频率派的优化问题和贝叶斯派的积分问题。从贝叶斯角度来看推断，对于 $\hat{x}$ 这样的新样本，需要得到：<br>$$<br>p(\hat{x}|X)&#x3D;\int_\theta p(\hat{x},\theta|X)d\theta&#x3D;\int_\theta p(\theta|X)p(\hat{x}|\theta,X)d\theta<br>$$<br>如果新样本和数据集独立，那么推断就是概率分布依参数后验分布的期望。</p><p>我们看到，推断问题的中心是参数后验分布的求解，推断分为：</p><ol><li>精确推断</li><li>近似推断-参数空间无法精确求解<ol><li>确定性近似-如变分推断</li><li>随机近似-如 MCMC，MH，Gibbs</li></ol></li></ol><h2 id="基于平均场假设的变分推断"><a href="#基于平均场假设的变分推断" class="headerlink" title="基于平均场假设的变分推断"></a>基于平均场假设的变分推断</h2><p>我们记 $Z$ 为隐变量和参数的集合，$Z_i$ 为第 $i$ 维的参数，于是，回顾一下 EM 中的推导：<br>$$<br>\log p(X)&#x3D;\log p(X,Z)-\log p(Z|X)&#x3D;\log\frac{p(X,Z)}{q(Z)}-\log\frac{p(Z|X)}{q(Z)}<br>$$<br>左右两边分别积分：<br>$$<br>Left:\int_Zq(Z)\log p(X)dZ&#x3D;\log p(X)\<br>Right:\int_Z[\log \frac{p(X,Z)}{q(Z)}-\log \frac{p(Z|X)}{q(Z)}]q(Z)dZ&#x3D;ELBO+KL(q,p)<br>$$<br>第二个式子可以写为变分和 KL 散度的和：<br>$$<br>L(q)+KL(q,p)<br>$$<br>由于这个式子是常数，于是寻找 $q\simeq p$ 就相当于对 $L(q)$ 最大值。<br>$$<br>\hat{q}(Z)&#x3D;\mathop{argmax}<em>{q(Z)}L(q)<br>$$<br>假设 $q(Z)$ 可以划分为 $M$ 个组（平均场近似）：<br>$$<br>q(Z)&#x3D;\prod\limits</em>{i&#x3D;1}^Mq_i(Z_i)<br>$$<br>因此，在 $L(q)&#x3D;\int_Zq(Z)\log p(X,Z)dZ-\int_Zq(Z)\log{q(Z)}$ 中，看 $p(Z_j)$ ，第一项：<br>$$<br>\begin{align}\int_Zq(Z)\log p(X,Z)dZ&amp;&#x3D;\int_Z\prod\limits_{i&#x3D;1}^Mq_i(Z_i)\log p(X,Z)dZ\nonumber\<br>&amp;&#x3D;\int_{Z_j}q_j(Z_j)\int_{Z-Z_{j}}\prod\limits_{i\ne j}q_i(Z_i)\log p(X,Z)dZ\nonumber\<br>&amp;&#x3D;\int_{Z_j}q_j(Z_j)\mathbb{E}<em>{\prod\limits</em>{i\ne j}q_i(Z_i)}[\log p(X,Z)]dZ_j<br>\end{align}<br>$$</p><p>第二项：<br>$$<br>\int_Zq(Z)\log q(Z)dZ&#x3D;\int_Z\prod\limits_{i&#x3D;1}^Mq_i(Z_i)\sum\limits_{i&#x3D;1}^M\log q_i(Z_i)dZ<br>$$<br>展开求和项第一项为：<br>$$<br>\int_Z\prod\limits_{i&#x3D;1}^Mq_i(Z_i)\log q_1(Z_1)dZ&#x3D;\int_{Z_1}q_1(Z_1)\log q_1(Z_1)dZ_1<br>$$<br>所以：<br>$$<br>\int_Zq(Z)\log q(Z)dZ&#x3D;\sum\limits_{i&#x3D;1}^M\int_{Z_i}q_i(Z_i)\log q_i(Z_i)dZ_i&#x3D;\int_{Z_j}q_j(Z_j)\log q_j(Z_j)dZ_j+Const<br>$$<br>两项相减，令 $\mathbb{E}<em>{\prod\limits</em>{i\ne j}q_i(Z_i)}[\log p(X,Z)]&#x3D;\log \hat{p}(X,Z_j)$ 可以得到：<br>$$<br>-\int_{Z_j}q_j(Z_j)\log\frac{q_j(Z_j)}{\hat{p}(X,Z_j)}dZ_j\le 0<br>$$<br>于是最大的 $q_j(Z_j)&#x3D;\hat{p}(X,Z_j)$ 才能得到最大值。我们看到，对每一个 $q_j$，都是固定其余的 $q_i$，求这个值，于是可以使用坐标上升的方法进行迭代求解，上面的推导针对单个样本，但是对数据集也是适用的。</p><p>基于平均场假设的变分推断存在一些问题：</p><ol><li>假设太强，$Z$ 非常复杂的情况下，假设不适用</li><li>期望中的积分，可能无法计算</li></ol><h2 id="SGVI"><a href="#SGVI" class="headerlink" title="SGVI"></a>SGVI</h2><p>从 $Z$ 到 $X$ 的过程叫做生成过程或译码，反过来的额过程叫推断过程或编码过程，基于平均场的变分推断可以导出坐标上升的算法，但是这个假设在一些情况下假设太强，同时积分也不一定能算。我们知道，优化方法除了坐标上升，还有梯度上升的方式，我们希望通过梯度上升来得到变分推断的另一种算法。</p><p>我们的目标函数：<br>$$<br>\hat{q}(Z)&#x3D;\mathop{argmax}<em>{q(Z)}L(q)<br>$$<br>假定 $q(Z)&#x3D;q_\phi(Z)$，是和 $\phi$ 这个参数相连的概率分布。于是 $\mathop{argmax}</em>{q(Z)}L(q)&#x3D;\mathop{argmax}<em>{\phi}L(\phi)$，其中 $L(\phi)&#x3D;\mathbb{E}</em>{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]$，这里 $x^i$ 表示第 $i$ 个样本。<br>$$<br>\begin{align}\nabla_\phi L(\phi)&amp;&#x3D;\nabla_\phi\mathbb{E}<em>{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]\nonumber\<br>&amp;&#x3D;\nabla_\phi\int q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz\nonumber\<br>&amp;&#x3D;\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz+\int q_\phi(z)\nabla_\phi [\log p_\theta(x^i,z)-\log q_\phi(z)]dz\nonumber\<br>&amp;&#x3D;\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz-\int q_\phi(z)\nabla_\phi \log q_\phi(z)dz\nonumber\<br>&amp;&#x3D;\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz-\int \nabla_\phi q_\phi(z)dz\nonumber\<br>&amp;&#x3D;\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz\nonumber\<br>&amp;&#x3D;\int q_\phi(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))dz\nonumber\<br>&amp;&#x3D;\mathbb{E}</em>{q_\phi}[(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))]<br>\end{align}<br>$$<br>这个期望可以通过蒙特卡洛采样来近似，从而得到梯度，然后利用梯度上升的方法来得到参数：<br>$$<br>z^l\sim q_\phi(z)\<br>\mathbb{E}<em>{q_\phi}[(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))]\sim \frac{1}{L}\sum\limits</em>{l&#x3D;1}^L(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))<br>$$<br>但是由于求和符号中存在一个对数项，于是直接采样的方差很大，需要采样的样本非常多。为了解决方差太大的问题，我们采用 Reparameterization 的技巧。</p><p>考虑：<br>$$<br>\nabla_\phi L(\phi)&#x3D;\nabla_\phi\mathbb{E}<em>{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]<br>$$<br>我们取：$z&#x3D;g_\phi(\varepsilon,x^i),\varepsilon\sim p(\varepsilon)$，于是对后验：$z\sim q_\phi(z|x^i)$，有 $|q_\phi(z|x^i)dz|&#x3D;|p(\varepsilon)d\varepsilon|$。代入上面的梯度中：<br>$$<br>\begin{align}<br>\nabla_\phi L(\phi)&amp;&#x3D;\nabla_\phi\mathbb{E}</em>{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]\nonumber\<br>&amp;&#x3D;\nabla_\phi L(\phi)&#x3D;\nabla_\phi\int[\log p_\theta(x^i,z)-\log q_\phi(z)]q_\phi dz\nonumber\<br>&amp;&#x3D;\nabla_\phi\int[\log p_\theta(x^i,z)-\log q_\phi(z)]p_\varepsilon d\varepsilon\nonumber\<br>&amp;&#x3D;\mathbb{E}<em>{p(\varepsilon)}[\nabla_\phi[\log p_\theta(x^i,z)-\log q_\phi(z)]]\nonumber\<br>&amp;&#x3D;\mathbb{E}</em>{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log q_\phi(z)]\nabla_\phi z]\nonumber\<br>&amp;&#x3D;\mathbb{E}_{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log q_\phi(z)]\nabla_\phi g_\phi(\varepsilon,x^i)]<br>\end{align}<br>$$<br>对这个式子进行蒙特卡洛采样，然后计算期望，得到梯度。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——清华大学机器学习库</title>
    <link href="/2022/08/01/Toolkit/%E8%AE%BA%E6%96%87%E4%B8%AD%E6%8F%90%E5%88%B0%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%93/"/>
    <url>/2022/08/01/Toolkit/%E8%AE%BA%E6%96%87%E4%B8%AD%E6%8F%90%E5%88%B0%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%93/</url>
    
    <content type="html"><![CDATA[<h1 id="ZhuSuan"><a href="#ZhuSuan" class="headerlink" title="ZhuSuan "></a><a href="https://github.com/thu-ml/zhusuan">ZhuSuan </a></h1><p>清华大学机器学习</p><p><strong>ZhuSuan</strong>是一个用于贝叶斯深度学习的 Python 概率编程库，它结合了贝叶斯方法和深度学习的互补优势。ZhuSuan 建立在 <a href="https://www.tensorflow.org/">TensorFlow</a>之上。与现有的主要为确定性神经网络和监督任务设计的深度学习库不同，ZhuSuan 提供了深度学习风格的原语和算法，用于构建概率模型和应用贝叶斯推理。支持的推理算法包括：</p><ul><li>具有可编程变分后验、各种目标和高级梯度估计器（SGVB、REINFORCE、VIMCO 等）的<strong>变分推理 (VI )。</strong></li><li>用于学习和评估模型的<strong>重要性采样 (IS) ，具有可编程的建议。</strong></li><li>具有平行链和可选自动参数调整的<strong>哈密顿蒙特卡罗 (HMC) 。</strong></li><li>**随机梯度马尔可夫链蒙特卡罗 (SGMCMC)**：SGLD、PSGLD、SGHMC 和 SGNHT。</li></ul><h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><p>We provide examples on traditional hierarchical Bayesian models and recent deep generative models.</p><p>To run the provided examples, you may need extra dependencies to be installed. This can be done by</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> <span class="hljs-string">&quot;.[examples]&quot;</span><br></code></pre></td></tr></table></figure><ul><li>Gaussian: <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/toy_examples/gaussian.py">HMC</a></li><li>Toy 2D Intractable Posterior: <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/toy_examples/toy2d_intractable.py">SGVB</a></li><li>Bayesian Neural Networks: <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/bayesian_neural_nets/bnn_vi.py">SGVB</a>, <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/bayesian_neural_nets/bnn_sgmcmc.py">SGMCMC</a></li><li>Variational Autoencoder (VAE): <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/variational_autoencoders/vae.py">SGVB</a>, <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/variational_autoencoders/iwae.py">IWAE</a></li><li>Convolutional VAE: <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/variational_autoencoders/vae_conv.py">SGVB</a></li><li>Semi-supervised VAE (Kingma, 2014): <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/semi_supervised_vae/vae_ssl.py">SGVB</a>, <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/semi_supervised_vae/vae_ssl_adaptive_is.py">Adaptive IS</a></li><li>Deep Sigmoid Belief Networks <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/sigmoid_belief_nets/sbn_adaptive_is.py">Adaptive IS</a>, <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/sigmoid_belief_nets/sbn_vimco.py">VIMCO</a></li><li>Logistic Normal Topic Model: <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/topic_models/lntm_mcem.py">HMC</a></li><li>Probabilistic Matrix Factorization: <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/probabilistic_matrix_factorization/pmf_hmc.py">HMC</a></li><li>Sparse Variational Gaussian Process: <a href="https://github.com/thu-ml/zhusuan/blob/master/examples/gaussian_process/svgp.py">SGVB</a></li></ul><h1 id="ML-Essentials"><a href="#ML-Essentials" class="headerlink" title="ML-Essentials"></a><a href="https://github.com/haowen-xu/ml-essentials">ML-Essentials</a></h1><p>一套用于日常机器学习实验的基本工具包。</p>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在window下设计自运行脚本</title>
    <link href="/2022/08/01/Toolkit/%E8%87%AA%E8%BF%90%E8%A1%8C/"/>
    <url>/2022/08/01/Toolkit/%E8%87%AA%E8%BF%90%E8%A1%8C/</url>
    
    <content type="html"><![CDATA[<p>在window下设计自运行脚本 获取博客访问数据</p><p>![屏幕截图 2022-01-06 140130](&#x2F;src\屏幕截图 2022-01-06 140130.png)</p><p>![屏幕截图 2022-01-06 140233](&#x2F;src\屏幕截图 2022-01-06 140233.png)</p><p>在这个位置新建一个事件</p><p>确定好事件 给事件赋予一定的权限，触发器也要写好</p><p>执行的命令就是python.exe GetData.py 很简单 </p><p>注意在设置里面也要找一找命令�令</p>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——数据收集地址</title>
    <link href="/2022/08/01/Toolkit/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E6%96%B9%E6%B3%95/"/>
    <url>/2022/08/01/Toolkit/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="01-公开的数据库"><a href="#01-公开的数据库" class="headerlink" title="01 公开的数据库"></a><strong>01 公开的数据库</strong></h1><h2 id="－-kaggle-－"><a href="#－-kaggle-－" class="headerlink" title="－ kaggle －"></a>－ kaggle －</h2><p><a href="https://www.kaggle.com/datasets">https://www.kaggle.com/datasets</a></p><p>这是我最喜欢的数据集之一，每个数据集都对应于一个小型社区，你可以在其中讨论数据、查找公共代码，或者在其中创建自己的项目。这里包含了大量不同类型、不同结构的数据集内容。同时，还可以在其中获取到与每个数据集关联的资料，其中包含了许多数据科学家们提供的数据集分析笔记等。</p><h2 id="－-UCI－"><a href="#－-UCI－" class="headerlink" title="－ UCI－"></a>－ UCI－</h2><p><a href="https://archive.ics.uci.edu/ml/datasets.html">https://archive.ics.uci.edu/ml/datasets.html</a></p><p>这个数据集来自于加州大学信息与计算机科学学院，其中包含了100多个数据集。根据机器学习问题的类型对数据集进行分类，可找到单变量或多变量时间序列数据集，以及分类、回归或推荐系统的数据集。</p><h2 id="－-微软数据集－"><a href="#－-微软数据集－" class="headerlink" title="－ 微软数据集－"></a>－ 微软数据集－</h2><p><a href="https://msropendata.com/">https://msropendata.com/</a></p><p>它包含了云服务器中的数据存储库，致力于促进全球研究社区的协作，并在其中提供了一系列用于已发表研究的数据集内容。</p><h2 id="－-国家数据-－"><a href="#－-国家数据-－" class="headerlink" title="－ 国家数据 －"></a>－ 国家数据 －</h2><p><a href="http://data.stats.gov.cn/index.htm">http://data.stats.gov.cn/index.htm</a></p><p>数据来源于中国国家统计局，包含了我国经济民生等多个方面的数据，并且在月度、季度、年度都有覆盖，较为全面和权威，对于社会科学的研究不要太有帮助。最关键的是，网站简洁美观，还有专门的可视化读物。</p><h2 id="－-CEIC-－"><a href="#－-CEIC-－" class="headerlink" title="－ CEIC －"></a>－ CEIC －</h2><p><a href="http://www.ceicdata.com/zh-hans">http://www.ceicdata.com/zh-hans</a></p><p>最完整的一套超过128个国家的经济数据，能够精确查找GDP, CPI, 进口，出口，外资直接投资，零售，销售，以及国际利率等深度数据。其中的“中国经济数据库”收编了300,000多条时间序列数据，数据内容涵盖宏观经济数据、行业经济数据和地区经济数据。</p><h2 id="－-wind（万得）－"><a href="#－-wind（万得）－" class="headerlink" title="－ wind（万得）－"></a>－ wind（万得）－</h2><p><a href="http://www.wind.com.cn/">http://www.wind.com.cn/</a></p><p>万得被誉为中国的Bloomberg，在金融业有着全面的数据覆盖，金融数据的类目更新非常快，据说很受国内的商业分析者和投资人的亲睐。</p><h2 id="－-搜数网-－"><a href="#－-搜数网-－" class="headerlink" title="－ 搜数网 －"></a>－ 搜数网 －</h2><p><a href="http://www.soshoo.com/">http://www.soshoo.com/</a></p><p>已加载到搜数网站的统计资料达到7,874本,涵盖1,761,009张统计表格和364,580,479个统计数据，汇集了中国资讯行自92年以来收集的所有统计和调查数据，并提供多样化的搜索功能。</p><h2 id="－-中国统计信息网-－"><a href="#－-中国统计信息网-－" class="headerlink" title="－ 中国统计信息网 －"></a>－ 中国统计信息网 －</h2><p><a href="http://www.tjcn.org/">http://www.tjcn.org/</a></p><p>国家统计局的官方网站，汇集了海量的全国各级政府各年度的国民经济和社会发展统计信息，建立了以统计公报为主，统计年鉴、阶段发展数据、统计分析、经济新闻、主要统计指标排行等。</p><h2 id="－-亚马逊aws-－"><a href="#－-亚马逊aws-－" class="headerlink" title="－ 亚马逊aws －"></a>－ 亚马逊aws －</h2><p><a href="http://aws.amazon.com/cn/datasets/?nc1=h_ls">http://aws.amazon.com/cn/datasets/?nc1=h_ls</a>   &#x2F;       <a href="https://registry.opendata.aws/">https://registry.opendata.aws/</a></p><p>来自亚马逊的跨科学云数据平台，包含化学、生物、经济等多个领域的数据集。</p><h2 id="－-figshare-－"><a href="#－-figshare-－" class="headerlink" title="－ figshare －"></a>－ figshare －</h2><p><a href="https://figshare.com/">https://figshare.com/</a></p><p>研究成果共享平台，在这里你会发现来自世界的大牛们的研究成果分享，同时get其中的研究数据，内容很有启发性，网站颇具设计感。</p><h2 id="－-Visual-Data－"><a href="#－-Visual-Data－" class="headerlink" title="－ Visual Data－"></a>－ Visual Data－</h2><p><a href="https://www.visualdata.io/">https://www.visualdata.io/</a></p><p>视觉数据包含了一些用于构建计算机视觉模型的优秀数据集，用户可通过某个CV主题查询，例如语义分割、图像标题、图像生成、自动驾驶汽车等内容。</p><h2 id="－-github-－"><a href="#－-github-－" class="headerlink" title="－ github －"></a>－ github －</h2><p><a href="https://github.com/caesar0301/awesome-public-datasets">https://github.com/caesar0301/awesome-public-datasets</a></p><p><a href="https://github.com/awesomedata/awesome-public-datasets">https://github.com/awesomedata/awesome-public-datasets</a></p><p>如果觉得前面的数据源还不够，github上的大神已经为大家整理好了一个非常全面的数据获取渠道，包含各个细分领域的数据库资源，自然科学和社会科学的覆盖都很全面，简直是做研究和数据分析的利器。</p><h1 id="02-数据交易平台"><a href="#02-数据交易平台" class="headerlink" title="02 数据交易平台"></a><strong>02 数据交易平台</strong></h1><h2 id="－-优易数据-－"><a href="#－-优易数据-－" class="headerlink" title="－ 优易数据 －"></a>－ 优易数据 －</h2><p><a href="http://www.youedata.com/">http://www.youedata.com/</a></p><p>由国家信息中心发起，拥有国家级信息资源的数据平台，国内领先的数据交易平台。平台有B2B、B2C两种交易模式，包含政务、社会、社交、教育、消费、交通、能源、金融、健康等多个领域的数据资源。</p><h2 id="－-数据堂-－"><a href="#－-数据堂-－" class="headerlink" title="－ 数据堂 －"></a>－ 数据堂 －</h2><p><a href="http://www.datatang.com/">http://www.datatang.com/</a></p><p>专注于互联网综合数据交易，提供数据交易、处理和数据API服务，包含语音识别、医疗健康、交通地理、电子商务、社交网络、图像识别等方面的数据。</p><h1 id="03-网络指数"><a href="#03-网络指数" class="headerlink" title="03 网络指数"></a><strong>03 网络指数</strong></h1><h2 id="－-百度指数-－"><a href="#－-百度指数-－" class="headerlink" title="－ 百度指数 －"></a>－ 百度指数 －</h2><p><a href="http://index.baidu.com/">http://index.baidu.com/</a></p><p>大家都很熟悉的指数查询平台，可以根据指数的变化查看某个主题在各个时间段受关注的情况，进行趋势分析、舆情预测有很好的指导作用。除了关注趋势之外，还有需求分析、人群画像等精准分析的工具，对于市场调研来说具有很好的参考意义。同样的另外两个搜索引擎搜狗、360也有类似的产品，都可以作为参考。</p><h2 id="－-阿里指数-－"><a href="#－-阿里指数-－" class="headerlink" title="－ 阿里指数 －"></a>－ 阿里指数 －</h2><p><a href="https://alizs.taobao.com/">https://alizs.taobao.com/</a></p><p>国内权威的商品交易分析工具，可以按地域、按行业查看商品搜索和交易数据，基于淘宝、天猫和1688平台的交易数据基本能够看出国内商品交易的概况，对于趋势分析、行业观察意义不小。</p><h2 id="－-艾瑞咨询-－"><a href="#－-艾瑞咨询-－" class="headerlink" title="－ 艾瑞咨询 －"></a>－ 艾瑞咨询 －</h2><p><a href="http://www.iresearch.com.cn/">http://www.iresearch.com.cn/</a></p><p>艾瑞作为老牌的互联网研究机构，在数据的沉淀和数据分析上都有得天独厚的优势，在互联网的趋势和行业发展数据分析上面比较权威，艾瑞的互联网分析报告可以说是互联网研究的必读刊物</p><h2 id="－-友盟指数-－"><a href="#－-友盟指数-－" class="headerlink" title="－ 友盟指数 －"></a>－ 友盟指数 －</h2><p><a href="http://www.umeng.com/">http://www.umeng.com/</a></p><p>友盟在移动互联网应用数据统计和分析具有较为全面的统计和分析，对于研究移动端产品、做市场调研、用户行为分析很有帮助。除了友盟指数，友盟的互联网报告同样是了解互联网趋势的优秀读物。</p><h2 id="－-爱奇艺指数-－"><a href="#－-爱奇艺指数-－" class="headerlink" title="－ 爱奇艺指数 －"></a>－ 爱奇艺指数 －</h2><p><a href="http://index.iqiyi.com/">http://index.iqiyi.com/</a></p><p>爱奇艺指数是专门针对视频的播放行为、趋势的分析平台，对于互联网视频的播放有着全面的统计和分析，涉及到播放趋势、播放设备、用户画像、地域分布、等多个方面。由于爱奇艺庞大的用户基数，该指数基本可以说明实际情况。</p><h2 id="－-猫眼专业版－"><a href="#－-猫眼专业版－" class="headerlink" title="－ 猫眼专业版－"></a>－ 猫眼专业版－</h2><p><a href="http://piaofang.maoyan.com/">http://piaofang.maoyan.com/</a></p><p>电影票房统计分析平台，猫眼专业版有实时的票房统计，影片的排盘情况、上座率和影院数据，对于当前电影的分析是必不可少的。</p><h1 id="04-网络采集器"><a href="#04-网络采集器" class="headerlink" title="04 网络采集器"></a><strong>04 网络采集器</strong></h1><p>网络采集器是通过软件的形式实现简单快捷地采集网络上分散的内容，具有很好的内容收集作用，而且不需要技术成本，被很多用户作为初级的采集工具。</p><h2 id="－-火车采集器-－"><a href="#－-火车采集器-－" class="headerlink" title="－ 火车采集器 －"></a>－ 火车采集器 －</h2><p><a href="http://www.locoy.com/">http://www.locoy.com/</a></p><p>一款专业的互联网数据抓取、处理、分析，挖掘软件，可以灵活迅速地抓取网页上散乱分布的数据信息，并通过一系列的分析处理，准确挖掘出所需数据，最常用的就是采集某些网站的文字、图片、数据等在线资源。接口比较齐全，支持的扩展比较好用，懂代码的话，可以使用PHP或C#开发任意功能的扩展。</p><h2 id="－-八爪鱼-－"><a href="#－-八爪鱼-－" class="headerlink" title="－ 八爪鱼 －"></a>－ 八爪鱼 －</h2><p><a href="http://www.bazhuayu.com/">http://www.bazhuayu.com/</a></p><p>简单实用的采集器，功能齐全，操作简单，不用写规则。特有的云采集，关机也可以在云服务器上运行采集任务。</p><h2 id="－-集搜客-－"><a href="#－-集搜客-－" class="headerlink" title="－ 集搜客 －"></a>－ 集搜客 －</h2><p><a href="http://www.gooseeker.com/">http://www.gooseeker.com/</a></p><p>一款简单易用的网页信息抓取软件,能够抓取网页文字、图表、超链接等多种网页元素，提供好用的网页抓取软件、数据挖掘攻略、行业资讯和前沿科技等</p><h1 id="05网络爬虫"><a href="#05网络爬虫" class="headerlink" title="05网络爬虫"></a><strong>05网络爬虫</strong></h1><p>作为极客们最喜欢的数据收集方式，爬虫高度的自由性、自主性都使其成为数据挖掘的必备技能，当然精通python等语言是必要前提。</p><p>利用爬虫可以做很多有意思的事情，当然也可以获取一些从其它渠道获取不到的数据资源，更重要的是帮你打开寻找和搜集数据的思路。</p><h2 id="－-利用爬虫爬取网络图片-－"><a href="#－-利用爬虫爬取网络图片-－" class="headerlink" title="－ 利用爬虫爬取网络图片 －"></a>－ 利用爬虫爬取网络图片 －</h2><p>爬取的图像素材</p><p>你看到某个网站上的图片恰好是你需要的，但是量大单个下载太麻烦，那么利用爬虫你可以快速地进行抓取，并可以根据标签、特征、颜色等信息进行分类储存。从此不缺设计素材，不缺美女图片，连斗图都多了几分自信。</p><h2 id="－-利用爬虫爬取高质量资源-－"><a href="#－-利用爬虫爬取高质量资源-－" class="headerlink" title="－ 利用爬虫爬取高质量资源 －"></a>－ 利用爬虫爬取高质量资源 －</h2><p>爬取的音乐资源</p><p>我们总是想快速地去搜集高质量的网络资源，但是人工查找比对实在太麻烦，利用爬虫你就可以轻松解决。比如爬取知乎点赞最多的文章列表，爬取网易云音乐评论最多的音乐，爬取豆瓣网高评分的电影或图书……总之，你可以从此拒绝平庸</p><h2 id="－-利用爬虫获取舆情数据-－"><a href="#－-利用爬虫获取舆情数据-－" class="headerlink" title="－ 利用爬虫获取舆情数据 －"></a>－ 利用爬虫获取舆情数据 －</h2><p>爬取的某招聘网站职位信息</p><p>比如你可以批量爬取社交平台的数据资源，可以爬取网站的交易数据，爬取招聘网站的职位信息等，可以用于个性化的分析研究。</p><p>总之，爬虫是非常强大的，甚至有人说天下没有不能爬的网站，因而爬取数据也成为了很多极客的乐趣。开发出高效的爬虫工具可以帮助我们节省很多时间，可以完全按照自己的需求来订制，想想这个世界就太美好。</p><h1 id="06-小工具"><a href="#06-小工具" class="headerlink" title="06 小工具"></a><strong>06 小工具</strong></h1><h2 id="－-Web-Plot-Digitizer-－"><a href="#－-Web-Plot-Digitizer-－" class="headerlink" title="－ Web Plot Digitizer －"></a>－ Web Plot Digitizer －</h2><p><a href="http://arohatgi.info/WebPlotDigitizer/app/">http://arohatgi.info/WebPlotDigitizer/app/</a></p><p>比如我们在查看期刊文献的时候看到一张成型的图表，但其本身数据是缺失的，你想获得这个图表的相关数据怎么办？有了这个小工具就非常easy了。直接上传我们需要获得数据的图表，如下：</p><p>然后我们就会获得如下的数据反馈，感觉运筹帷幄有木有，对于一些不需要十分精确的分析研究足够使用。</p><p>当然并不推荐用这个作为量化分析的依赖，对于定性的分析，做ppt级的数据统计分析就足够了。</p><h2 id="－-you-get-－"><a href="#－-you-get-－" class="headerlink" title="－ you-get －"></a>－ you-get －</h2><p><a href="https://you-get.org/">https://you-get.org/</a></p><p>这是一个程序员基于python 3开发的项目，已经在github上面开源，支持64个网站，包括优酷、土豆、爱奇艺、b站、酷狗音乐、虾米……总之你能想到的网站都有! 还有一个黑科技的地方，即使是名单上没有的网站，当你输入链接，程序也会猜测你想要下载什么，然后帮你下载。</p><p>下载优酷视频</p><p>批量下载图片<br>当然you-get要在python3环境下进行安装，用pip安装好后，在终端输入“you get＋你想下载资源的链接”就可以等着收藏资源了。</p>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——博客运行命令及出现的问题</title>
    <link href="/2022/08/01/Toolkit/%E5%8D%9A%E5%AE%A2%E8%BF%90%E8%A1%8C%20%E5%8F%8A%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"/>
    <url>/2022/08/01/Toolkit/%E5%8D%9A%E5%AE%A2%E8%BF%90%E8%A1%8C%20%E5%8F%8A%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="github-hexo"><a href="#github-hexo" class="headerlink" title="github +  hexo"></a>github +  hexo</h1><p><code>hexo init </code> 初始化</p><p><code>npm install</code> 安装一些必要的东西 </p><p><code>hexo clean</code>清除了你之前生成的东西，也可以不加。<br><code>hexo generate</code> 顾名思义，生成静态文章，可以用 <code>hexo g</code>缩写<br><code>hexo deploy</code> 部署文章，可以用<code>hexo d</code>缩写</p><p><code>hexo server </code> 本地查看</p><h1 id="pandoc-exited-with-code-9-pandoc-Unknown-extension-smart"><a href="#pandoc-exited-with-code-9-pandoc-Unknown-extension-smart" class="headerlink" title="pandoc exited with code 9: pandoc: Unknown extension: smart"></a>pandoc exited with code 9: pandoc: Unknown extension: smart</h1><p>这个问题的原因是, 由于需要让Hexo支持数学公式渲染, 所以我在本地使用hexo-renderer-pandoc替换了hexo默认的markdown渲染引擎.</p><p>出现这个问题的原因是, pandoc从2.0版本之后, 取消了smart这个扩展, 说白了也就是gitlab-ci中安装的pandoc版本太低. 于是我们只要更换一个2.0版本以上的pandoc即可解决.</p><p>pandoc是在anaconda3内自带的，所以我们需要升级一下conda内的这个包就可以了</p><p><code>conda update pandoc</code></p><h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——博客评论工具 Waline及用户头像修改</title>
    <link href="/2022/08/01/Toolkit/%E5%8D%9A%E5%AE%A2%E8%AF%84%E8%AE%BAwaline/"/>
    <url>/2022/08/01/Toolkit/%E5%8D%9A%E5%AE%A2%E8%AF%84%E8%AE%BAwaline/</url>
    
    <content type="html"><![CDATA[<h1 id="博客相关"><a href="#博客相关" class="headerlink" title="博客相关"></a>博客相关</h1><p> 我这里使用的博客其实是Hexo+Fluid 搭建的。上手比较简单，用户手册在<a href="https://hexo.fluid-dev.com/docs/guide/">这里</a></p><p>他可以对本地的markdown文件进行一个页面化，自动生成一个博客。</p><p> 关于评论显示这个工具其实我一开使用的是cusdis，这个比较麻烦在哪呢，就是需要手动的判断comment，然后让他显示在网页上，显得比较傻瓜，而且交互性太少了</p><p>所以这里我改用了Waline。</p><p>其实在Hexo+Fluid 框架使用各个评论工具是比较友好的。我只需要 开启评论需要在<strong>主题配置</strong>中开启并指定评论模块，然后在下方还要设置对应评论模块的参数就是可以实现整个功能比较方便。</p><p>其实Waline这里最主要的还是一个关于serverURL的获取，获取的具体的流程其实在Waline的<a href="https://waline.js.org/guide/get-started.html">快速上手手册</a>中也有体现.</p><p>这里主要想记录的是关于waline这个评论头像的修改(一开始的图像 真的是难受)</p><h1 id="用户头像"><a href="#用户头像" class="headerlink" title="用户头像"></a>用户头像</h1><p>我们在很多博客或者网站留言，评论的时候会看到有的人头像很酷很个性化，但是这个博客和网站本身并没有提供设置头像的功能，感觉有点神奇，那么是怎么做到的呢？其实这是使用了Gravatar。</p><p>　　Gravatar是Globally Recognized Avatar的缩写，是gravatar推出的一项服务，意为“全球通用头像”。如果在Gravatar的服务器上放置了你自己的头像，那么在任何支持 Gravatar的blog或者留言本上留言时，只要提供你与这个头像关联的email地址，就能够显示出你的Gravatar头像来。Gravatar的官网地址在<a href="http://en.gravatar.com/">这里</a></p><p>然后注意这里使用的账户竟然是wordpress(emmm 恕我也不知道为什么会这样) 登陆以后就可以在个人主页里面添加你邮箱与图片。这里需要注意的是这里的图片可以上传多个，但是一个邮箱只会有一个头像哦</p><p><img src="/src/image-20220717191423839.png" alt="image-20220717191423839"></p><p>当然这个头像你还可以放在其他地方使用，例如在Waline的评论后台的图像设置啊，都可以直接使用这个图片的地址。</p><blockquote><p>waline的后台地址，其实就是你的serverURL加上&#x2F;ui&#x2F; &#x2F; </p></blockquote>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——删除无效文件、文件夹</title>
    <link href="/2022/08/01/Toolkit/%E5%88%A0%E9%99%A4%E6%97%A0%E6%95%88%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9/"/>
    <url>/2022/08/01/Toolkit/%E5%88%A0%E9%99%A4%E6%97%A0%E6%95%88%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9/</url>
    
    <content type="html"><![CDATA[<h1 id="删除无效文件-文件夹"><a href="#删除无效文件-文件夹" class="headerlink" title="删除无效文件\文件夹"></a>删除无效文件\文件夹</h1><p>有时候会发现自己电脑的文件夹 window找不到 改不动 删不了 </p><p>创建一个txt 插入如下代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">DEL /F /A /Q \\?\%1<br>RD /S /Q \\?\%1<br></code></pre></td></tr></table></figure><p>修改格式为bat  </p><p>将删除不了的文件或文件夹拖动到这个bat上 即可删除</p>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——Pip的安装路径修正</title>
    <link href="/2022/08/01/Toolkit/%E4%BF%AE%E6%94%B9pip%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84/"/>
    <url>/2022/08/01/Toolkit/%E4%BF%AE%E6%94%B9pip%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84/</url>
    
    <content type="html"><![CDATA[<h1 id="更改win下python-pip的安装路径"><a href="#更改win下python-pip的安装路径" class="headerlink" title="更改win下python pip的安装路径"></a>更改win下python pip的安装路径</h1><p><strong>前言</strong></p><p>声明：python版本3.6，以下讨论的Python也都是适用于3.x版本</p><p>在实际使用安装python的pip安装 依赖库是非常的便捷的。</p><p>而且一般大家使用的都是安装<a href="https://so.csdn.net/so/search?q=Anaconda">Anaconda</a> 来学习和实践python项目。</p><p>我们通常都是直接就是使用</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">pip install ****<br></code></pre></td></tr></table></figure><p>其中****代表就是安装的依赖库名或者包名。</p><p>但是简单的背后就是，我们的最重要的系统盘C盘都是逐渐满了。更关键是你满完之后还不好去删除或者将一些依赖库放在别的盘的路径下，这就是一件非常糟糕的事情了。</p><p>所以今天，为了拯救C盘强迫症的自己，我是必须要好好折腾折腾，将pip install 安装的依赖库和包名都是放在Anaconda（别的盘中）目录下的site-packages中去（全部归到这个类下）。</p><p><strong>总体步骤</strong></p><p><strong>第一步</strong>：先查看自己的默认安装路径到底是在哪？列出全局的packages包的安装路径在哪？</p><p>\1. 按键（win+R） -&gt;打开cmd命令窗口。</p><p>2.键入</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">python -m site<br></code></pre></td></tr></table></figure><p>得到这样的结果：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs html">sys.path = [<br>    &#x27;C:\\Users\\Administrator&#x27;,<br>    &#x27;F:\\Anaconda\\python36.zip&#x27;,<br>    &#x27;F:\\Anaconda\\DLLs&#x27;,<br>    &#x27;F:\\Anaconda\\lib&#x27;,<br>    &#x27;F:\\Anaconda&#x27;,<br>    &#x27;F:\\Anaconda\\lib\\site-packages&#x27;,<br>&#x27;F:\\Anaconda\\lib\\site-packages\\win32&#x27;,<br>&#x27;F:\\Anaconda\\lib\\site-packages\\win32\\lib&#x27;,<br>&#x27;F:\\Anaconda\\lib\\site-packages\\Pythonwin&#x27;]<br><br>USER_BASE: &#x27;C:\\Users\\Administrator\\AppData\\Roaming\\Python&#x27; (exists)<br><br>USER_SITE: &#x27;C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages&#x27; (exists)<br><br>ENABLE_USER_SITE: True<br></code></pre></td></tr></table></figure><p>这里有几点说明：</p><p>①我们看见这里的USER_BASE 和USER_SITE其实就是用户自定义的启用Python脚本和依赖安装包的基础路径。</p><p>②其中USER_BASE表示就是在C盘这个目录下的Python.exe启动程序路径以及pip,esay-install,markdown等脚本，（我们已经是安装好了Anaconda Python，这个C盘又是什么鬼，不想用它啊），而这个C盘的Python.exe启动程序路径其实就是我们在安装Anaconda的时候一个分身，更准确的说，其实就是简单的Python程序，并不是什么IDE这种级别的可以类似Eclipse这样去操作编译丰富的功能窗口，只是简单类似shell的一样的存在。</p><p>所以要改。</p><p>③其中的USER_SITE其实就是用户如果调用C盘路径下的python.exe中的脚本pip文件去下载，就会将site-package的默认安装到这个C盘路径下。</p><p>并且大家有时候可能在使用 pip install 命令行安装一些依赖的时候，总是会提示报错“PermissionError：[WinError 5 ] Denied Excess.”C”\.”这样的类似权限拒绝访问，并且提示你无管理员权限的话，使用pip -install . + [user-site] 这样的字眼。”</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">python -m site --user-site<br></code></pre></td></tr></table></figure><p>而我自己也曾经试验过了，如果是权限问题拒绝安装访问的话，安装cmd命令提示框中提示的user-site使用方法，就能解决权限安装问题。</p><p>所以，user_site其实就是个人的site-packages默认安装路径了。而如果使用的是Anaconda 目录下的Scripts中的pip 安装依赖库或包等，则是安装在Anaconda路径下的site-packages中去。</p><p><strong>第二步：</strong>确认了是什么原因，那么就要去对应的配置文件改了。</p><p>因为出现了类似以下</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs html">sys.path = [<br>    &#x27;C:\\Users\\Administrator&#x27;,<br>    &#x27;F:\\Anaconda\\python36.zip&#x27;,<br>    &#x27;F:\\Anaconda\\DLLs&#x27;,<br>    &#x27;F:\\Anaconda\\lib&#x27;,<br>    &#x27;F:\\Anaconda&#x27;,<br>    &#x27;F:\\Anaconda\\lib\\site-packages&#x27;,<br>    &#x27;F:\\Anaconda\\lib\\site-packages\\win32&#x27;,<br>    &#x27;F:\\Anaconda\\lib\\site-packages\\win32\\lib&#x27;,<br>    &#x27;F:\\Anaconda\\lib\\site-packages\\Pythonwin&#x27;]<br><br>USER_BASE: &#x27;C:\\Users\\Administrator\\AppData\\Roaming\\Python&#x27; (exists)<br><br>USER_SITE: &#x27;C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages&#x27; (exists)<br><br>ENABLE_USER_SITE: True<br></code></pre></td></tr></table></figure><p>那么肯定是有对应的配置文件去读取的，我们去寻找修改即可。</p><p>cmd命令行窗口下键入：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">python -m site -help<br></code></pre></td></tr></table></figure><p>就会出现</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs html">C:\Users\Administrator&gt;python -m site -help<br><br>F:\Anaconda\lib\site.py [--user-base] [--user-site]<br>Without arguments print some useful information<br>With arguments print the value of USER_BASE and/or USER_SITE separated by &#x27;;&#x27;.<br><br>Exit codes with --user-base or --user-site:<br>  0 - user site directory is enabled<br>  1 - user site directory is disabled by user<br>  2 - uses site directory is disabled by super user<br>      or for security reasons<br> &gt;2 - unknown error<br></code></pre></td></tr></table></figure><p>可见，管理这个文件竟然就是Anaconda目录下的site.py文件，之前说C盘中的Python是它的分身还真没说错了。</p><p>找到</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">F:\Anaconda\lib\site.py<br></code></pre></td></tr></table></figure><p>，修改其中的参数</p><p><img src="/%5Csrc%5C70.png" alt="img"></p><p>实际修改这两个就好了。</p><p>注意</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">USER_SITE = &quot;F:\Anaconda\lib\site-packages&quot;<br></code></pre></td></tr></table></figure><p>这个是你要安装下载的site-packages的路径。</p><p>而执行下载的pip等脚本路径是由USER_BASE决定，并且一定一定要换到同样的有pip脚本的路径下，哪怕你路径也请写到</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">F:\Anaconda<br></code></pre></td></tr></table></figure><p>当然这里最好写成</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">USER_BASE = &quot;F:\Anaconda\Scripts&quot;<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5C70-16460245804101.png" alt="img"></p><p>可以看见Scripts就是这些执行程序和脚本位置了。</p><p><strong>第三步：</strong>测试安装，保证你修改之后成功有效。</p><p>测试键入</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">pip install numpy<br></code></pre></td></tr></table></figure><p>出现，在这个路径下其实已经拥有了满足的依赖包和库了（之前已经安装过了，今天只是将步骤总结下），所以证明是修改成功了。</p><p><img src="/%5Csrc%5C70-16460245804102.png" alt="img">245804102.png)</p>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——网络不通github小记</title>
    <link href="/2022/08/01/Toolkit/ping%20%E4%B8%8D%E9%80%9Agithub/"/>
    <url>/2022/08/01/Toolkit/ping%20%E4%B8%8D%E9%80%9Agithub/</url>
    
    <content type="html"><![CDATA[<p>在进行搭建本地 git 时，需要通过 ssh 建立与 github 的连接，在进行ssh连接验证时，出现如下提示</p><p><code>ssh: Could not resolve hostname github.com: Name or service not known</code><br> 此时对目标地址进行 ping 操作如下</p><p><code>ping github.com</code></p><p>出现如下错误提示(git bash)</p><p><code>Ping request could not find host github.com. Please check the name and try again.</code></p><p>windows的cmd 我没复制，但大致提示无法访问远程主机，连接不可建立</p><p>解决方案：<br>首先获取 github.com IP 地址</p><p>IP 地址查询： <a href="https://ipaddress.com/website/github.com">Click</a></p><p>通过上述网站查询得到 github.com IP 地址如下</p><p>140.82.113.4<br>此时修改本地 DNS 文件 C:\Windows\System32\drivers\etc 目录下的 hosts 文件</p><p>由于该文件在 C 盘中，无法直接打开进行修改，需要使用管理员权限</p><p>此处操作方式为，搜索 记事本 软件，右键以管理员身份运行</p><p>通过左上角文件-&gt;打开，选择上述 hosts 文件打开，在文件内容最下方加入一行</p><p><code>140.82.113.4    github.com</code><br>完成后保存即可。</p>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——mermaid图小记</title>
    <link href="/2022/08/01/Toolkit/MermaidMethod/"/>
    <url>/2022/08/01/Toolkit/MermaidMethod/</url>
    
    <content type="html"><![CDATA[<h3 id="pie：饼图"><a href="#pie：饼图" class="headerlink" title="pie：饼图"></a>pie：饼图</h3><p>标题下面分别是区域名称及其百分比</p><pre><code class=" mermaid">pie    title Key elements in Product X    &quot;Calcium&quot; : 42.96    &quot;Potassium&quot; : 50.05    &quot;Magnesium&quot; : 10.01    &quot;Iron&quot; :  5</code></pre><h3 id="graph-：流程图"><a href="#graph-：流程图" class="headerlink" title="graph ：流程图"></a><code>graph</code> ：流程图</h3><p>语法解释：<code>TD</code> 表示的是方向，这里的含义是 Top-Down 由上至下。</p><pre><code class=" mermaid">graph TD;    A--&gt;B;    A--&gt;C;    B--&gt;D;</code></pre><h3 id="sequence-Diagram-时序图"><a href="#sequence-Diagram-时序图" class="headerlink" title="sequence Diagram: 时序图"></a>sequence Diagram: 时序图</h3><p>语法解释：<code>-&gt;&gt;</code> 代表实线箭头，<code>--&gt;&gt;</code> 则代表虚线。</p><pre><code class=" mermaid">sequenceDiagram    Alice-&gt;&gt;John: Hello John, how are you?    John--&gt;&gt;Alice: Great!</code></pre><h3 id="state-Diagram：状态图"><a href="#state-Diagram：状态图" class="headerlink" title="state Diagram：状态图"></a><strong>state Diagram：状态图</strong></h3><p>语法解释：<code>[*]</code> 表示开始或者结束，如果在箭头右边则表示结束。</p><pre><code class=" mermaid">stateDiagram    [*] --&gt; s1    s1 --&gt; [*]</code></pre><h3 id="classDiagram：类图"><a href="#classDiagram：类图" class="headerlink" title="classDiagram：类图"></a><strong>classDiagram：类图</strong></h3><p>语法解释：<code>&lt;|--</code> 表示继承，<code>+</code> 表示 <code>public</code>，<code>-</code> 表示 <code>private</code>，学过 Java 的应该都知道。</p><pre><code class=" mermaid">classDiagram      Animal &lt;|-- Duck      Animal &lt;|-- Fish      Animal &lt;|-- Zebra      Animal : +int age      Animal : +String gender      Animal: +isMammal()      Animal: +mate()      class Duck&#123;          +String beakColor          +swim()          +quack()      &#125;      class Fish&#123;          -int sizeInFeet          -canEat()      &#125;      class Zebra&#123;          +bool is_wild          +run()      &#125;</code></pre><h3 id="gantt：甘特图"><a href="#gantt：甘特图" class="headerlink" title="gantt：甘特图"></a><strong>gantt：甘特图</strong></h3><p>甘特图一般用来表示项目的计划排期，目前在工作中经常会用到。</p><p>语法也非常简单，从上到下依次是图片标题、日期格式、项目、项目细分的任务。</p><pre><code class=" mermaid">gantt    title 工作计划    dateFormat  YYYY-MM-DD    section Section    A task           :a1, 2020-01-01, 30d    Another task     :after a1  , 20d    section Another    Task in sec      :2020-01-12  , 12d    another task      : 24d</code></pre><pre><code class=" mermaid">graph LR;    Propagation--&gt;Convolution;    Convolution --&gt; Spectal    Convolution --&gt; Spatial    Spatial --&gt; Basic    Spatial --&gt; Attentional    Spatial --&gt; Framework    Propagation--&gt;Recurrent;    Recurrent --&gt; Convergence    Recurrent --&gt; Gate    Propagation--&gt;Skip    Sampling--&gt;Node;    Sampling--&gt;Layer;    Sampling--&gt;Subgraph;    Pooling--&gt;Direct;    Pooling--&gt;Hierarchical;</code></pre>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——latex 数学公式小记</title>
    <link href="/2022/08/01/Toolkit/latex%20math/"/>
    <url>/2022/08/01/Toolkit/latex%20math/</url>
    
    <content type="html"><![CDATA[<h1 id="关于导数-x2F-积分部分"><a href="#关于导数-x2F-积分部分" class="headerlink" title="关于导数&#x2F;积分部分"></a>关于导数&#x2F;积分部分</h1><p>$$<br>\partial x \<br>\mathrm{d}x \<br>x^{‘}  \quad {y}’ \quad f’’’(x)\<br>\nabla f \<br>\int_{-\infty}^{+\infty}<br>$$</p><h1 id="关于水平线或者下划线部分"><a href="#关于水平线或者下划线部分" class="headerlink" title="关于水平线或者下划线部分"></a>关于水平线或者下划线部分</h1><p>$$<br>\overline{m+n}  \   \underline{m+n}\<br>\underbrace{m+n}_{26} \<br>\hat{x} \  \check{x}\ \dot{x}\  \breve{x}\  \acute{x}\ \ddot{x} \<br>\grave{x}\ \tilde{x}\ \mathring{x}\ \bar{x}\  \vec{x}<br>$$</p><h1 id="关于变量名称部分"><a href="#关于变量名称部分" class="headerlink" title="关于变量名称部分"></a>关于变量名称部分</h1><p>$$<br>\  \alpha  \  \beta  \  \gamma  \   \delta  \  \epsilon\ \eta \ \sigma  \  \mu  \<br>\theta  \ \lambda  \  \rho  \  \phi  \  \varphi  \ \varepsilon \  \psi  \  \omega  \<br>\xi \  \pi \ \tau \ \chi \<br>\Gamma  \  \Omega  \  \Phi  \  \Lambda  \  \Delta  \  \Theta  \  \Psi<br>$$</p><h1 id="其他常用字符"><a href="#其他常用字符" class="headerlink" title="其他常用字符"></a>其他常用字符</h1><p>$$<br>\infty\ \triangle\ \ell\ \emptyset\ \nabla\  \partial\ \forall\ \exists \<br>\neg \ \checkmark\ \Join<br>$$</p><h1 id="关系符"><a href="#关系符" class="headerlink" title="关系符"></a>关系符</h1><p>$$<br>\le  \  \lt   \  \ge  \  \gt \ \ll\  \gg \<br>\not\le  \  \not\lt   \  \not\ge  \  \not\gt \ \not\ll\  \not\gg \<br>\subset  \  \subseteq  \  \supset  \  \supseteq \<br>&#x3D;  \  \neq  \  \sim  \  \simeq  \  \approx  \  \propto \<br>\mid  \  \parallel  \  \perp  \  \<br>\in  \  \ni \quad \varnothing \<br>\bigcup \quad \bigcap \quad \cup \quad \cap<br>$$</p><h1 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h1><p>$$</p><ul><li>\  -  \  \pm  \  \mp  \   \cdot  \  \dots \<br>\div  \  \times  \  *  \  \<br>\oplus \ \otimes \ \ominus\ \odot \ \oslash\<br>\sum  \  \prod  \  \bigoplus  \  \bigotimes  \  \bigodot  \  \<br>$$</li></ul><h1 id="箭头"><a href="#箭头" class="headerlink" title="箭头"></a>箭头</h1><p>$$<br>\gets  \  \to  \  \longleftarrow  \  \longrightarrow \<br>\Leftarrow  \  \Rightarrow  \  \Longleftarrow  \  \Longrightarrow \<br>\Longleftrightarrow   \  \iff<br>$$</p><h1 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h1><p>$$<br>\mathbb{ABC}\<br>\mathit{ABC} \<br>\mathcal{ABC} \<br>\mathrm{ABC} \<br>\mathscr{ABC}<br>$$</p><h1 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h1><p>$$<br>f(x)&#x3D;<br>\left{\begin{array}{ll}<br>        {\frac{1}{b-a}} &amp; {\text { if } a \leq x \leq b} \<br>        {0} &amp; {\text { otherwise }}<br>        \end{array}\right.<br>$$</p><p>$$<br>A&#x3D;\left[\begin{array}{cccc}<br>{a_{11}} &amp; {a_{12}} &amp; {\cdots} &amp; {a_{1 n}} \<br>{a_{21}} &amp; {a_{22}} &amp; {\cdots} &amp; {a_{2 n}} \<br>{\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \<br>{a_{m 1}} &amp; {a_{m 2}} &amp; {\cdots} &amp; {a_{m n}}<br>\end{array}\right]<br>$$</p><p>$$<br>\begin{vmatrix}<br>1 &amp; 1 &amp; \ldots &amp; 1 \<br>x_{1} &amp; x_{2} &amp; \ldots &amp; x_{n} \<br>\ldots &amp; \ldots &amp; \ldots &amp; \ldots \<br>x_{1}^{n - 1} &amp; x_{2}^{n 1} &amp; \ldots &amp; x_{n}^{n - 1} \ \end{vmatrix}<br>$$</p><p>$$<br>\begin{pmatrix}<br>A_{11} &amp; A_{12} &amp; \ldots &amp; A_{1n} \<br>A_{21} &amp; A_{22} &amp; \ldots &amp; A_{2n} \<br>\ldots &amp; \ldots &amp; \ldots &amp; \ldots \<br>A_{n1} &amp; A_{n2} &amp; \ldots &amp; A_ \<br>\end{pmatrix}<br>$$</p><p>$$<br>\begin{align}<br>\max_{\theta,\phi}ELBO(q,x;\theta,\phi)&amp;&#x3D;\max_{\theta,\phi}E_{z \sim q(z;\phi)} [ log\frac{p(x|z;\theta)p(z;\theta)}{q(z;\theta)}] \<br>                    &amp;&#x3D;\max_{\theta,\phi}E_{z \sim q(z;\phi)}[ logp(x\vert z;\theta)] - D_{KL}[q(z\vert x;\phi)\vert\vert p(zl\theta)]<br>                    \end{align}<br>$$</p>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——关于怎么读论文</title>
    <link href="/2022/08/01/Toolkit/HowToReadThesis/"/>
    <url>/2022/08/01/Toolkit/HowToReadThesis/</url>
    
    <content type="html"><![CDATA[<h1 id="怎么去读一篇论文呢："><a href="#怎么去读一篇论文呢：" class="headerlink" title="怎么去读一篇论文呢："></a>怎么去读一篇论文呢：</h1><p>首先你需要了解一般论文的架构：title -&gt; abstract -&gt; introduce -&gt; method -&gt; experiment -&gt; conclusion</p><p>需要三遍：</p><ul><li>Pass1：看title、abstract、conclusion了解文章是否你是感兴趣的</li><li>Pass2：看文章的图、表，弄清楚文章大致在做什么 判定你是否需要去深读这篇文章</li><li>Pass3：逐字逐句的理解这论文，要能详细知道论文的原理和实现。判断他在什么场景下处理了什么问题， 着重看架构，运用了什么方法，损失函数，达到了什么效果。</li></ul><p><strong>会发现问题，是具有独立研究能力的标志</strong></p><h1 id="问题怎么发现呢？"><a href="#问题怎么发现呢？" class="headerlink" title="问题怎么发现呢？"></a>问题怎么发现呢？</h1><ol><li>导师给的<ul><li>只能说：你很幸运！！！</li><li>速度！！！（比的是效率）</li></ul></li><li>自己找的<ul><li>看文献，顺藤摸瓜</li><li>重要会议、期刊</li><li>重要大学、研究机构、研究组</li><li>重要研究者</li></ul></li></ol><h1 id="研究布局"><a href="#研究布局" class="headerlink" title="研究布局"></a>研究布局</h1><ul><li>可持续性、有全局性<br>不要盲目追热点，从长远目标考虑</li><li>可发展、前后一致<br>针对问题逐步展开，有继承性和发展性<img src="/src%5Cimage-20220324135233704.png" alt="image-20220324135233704"></li></ul><h2 id="关于想法这个问题？"><a href="#关于想法这个问题？" class="headerlink" title="关于想法这个问题？"></a>关于想法这个问题？</h2><p>靠自己，多读多项</p><p>面对每篇文章，套用这个万能公式</p><p><strong>针对什么问题，提出了什么方法，实现了什么效果</strong></p><h1 id="具体工作"><a href="#具体工作" class="headerlink" title="具体工作"></a>具体工作</h1><p>验证想法的过程</p><ul><li>理论<br>数学证明 需要有一定的数学功底</li><li>实验<br>合理设计实验<br>合理安排时间<br>切记陷入实验<br>善于分析实验结果</li></ul><h1 id="投稿流程"><a href="#投稿流程" class="headerlink" title="投稿流程"></a>投稿流程</h1><p>期刊：</p><ul><li>编辑部&#x2F;主编收到稿件，转给合适的AE</li><li>AE分配工作给Reviewer</li><li>Reviewer审稿完成，AE给出处理意见（改，小改，大改，拒后重投，拒）</li><li>主编审定</li></ul><p>会议：</p><ul><li>PC Member Bidding</li><li>分配论文到合适的PC Member</li><li>Rebuttal</li><li>AC给出推荐意见</li><li>AC Meeting给出最终意见</li></ul><h1 id="论文撰写"><a href="#论文撰写" class="headerlink" title="论文撰写"></a>论文撰写</h1><ol><li>是否与期刊、会议Topic相关</li><li>创新性</li><li>实验充分：SOTA？ Ablation？ Visualization？</li><li>写作</li></ol><p>主要内容：</p><ul><li>Problem X is important</li><li>Preview works A,B and C have been done</li><li>A, B and C have their weakness</li><li>Your work D</li><li>Theoretic analysis</li><li>Experimental comparison against A,B and C</li><li>Why D is better </li><li>Strength and Weakness of D</li><li>Futrue  work on D</li></ul><p>关于论文的几点建议</p><ul><li>模仿</li><li>读文献的时候注意摘录</li><li>多打磨</li><li>慢慢形成适合自己的风格</li></ul><p>对于一篇论文来说，印象力成为评价学术工作的一大标准，甚至越来越成为最重要的标准</p><ul><li>应用数量：谷歌引用、SCI应用</li><li>同行评价</li><li>ESI高被引论文</li></ul><p>怎么提高影响力</p><p>研究要专，瞄准问题，形成品牌</p><ul><li>长期坚持，研究问题要不断深入</li><li>做高质量研究：提出新的研究方向、建立数据集与评测基准，解决一个长期悬而未绝的问题，在实际应用中取得很好的效果</li></ul><p>展示科研成果</p><ul><li>科研论文：抓住会议、公众号等平台上的宣传机会进行交流</li><li>学术报告：摘选自己最亮点的工作，应适当不宜太多，坚持在科研一线</li><li>获得同行认可的重要手段，同时能后收获改进意见</li></ul>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toolkit——git相关操作</title>
    <link href="/2022/08/01/Toolkit/git%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/"/>
    <url>/2022/08/01/Toolkit/git%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="git拉取代码"><a href="#git拉取代码" class="headerlink" title="git拉取代码"></a>git拉取代码</h1><p>\1) 初始化：<code>git init</code></p><p>主要用来初始化一个空的<code>git</code>本地仓库。执行完上面的命令，当前目录下会自动生成<code>.git</code>隐藏文件夹，该隐藏文件夹就是<code>git</code>版本库</p><p>\2) 将本地仓库与远端仓库建立一个链接，并命名远端仓库为origin</p><p><code>git remote add origin master url</code></p><p>本地仓库默认为master分支，url为拉取的仓库地址</p><p>\3) <code>git pull origin master</code></p><p>拉取origin远端仓库到本地master分支</p><p>git拉取代码完成，下面介绍如何用git命令提交代码</p><h1 id="git提交代码"><a href="#git提交代码" class="headerlink" title="git提交代码"></a>git提交代码</h1><p>1） 在gitee官网新建仓库</p><p>2）对该仓库进行初始化</p><p><img src="/src%5C640-16460247334126.png" alt="图片"></p><p>得到如下图：</p><p><img src="/src%5C640-16460247334137.png" alt="图片"></p><p>3）在本地拉取该仓库，通过如下命令行实现：</p><p><code>git clone url</code> 其中url为仓库地址</p><p>4） 在本地拉取的仓库文件夹再复制您需要上传的代码</p><p>5） 将上传的代码添加到暂存区 <code>git add .</code></p><p>\6) 将暂存区的内容添加到本地仓库 <code>git commit -m &quot;第一次提交&quot;</code></p><p>\7) 将本地仓库推送到远端仓库<code>git push origin master</code></p><p><strong>拉取代码和提交代码大致是这个步骤，git可以在算法开发中大大提高开发效率。</strong>��**</p>]]></content>
    
    
    <categories>
      
      <category>tool</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sklearn——StandardScaler</title>
    <link href="/2022/08/01/Python/sklearn%E5%87%BD%E6%95%B0%E5%BA%93/preprocessing/"/>
    <url>/2022/08/01/Python/sklearn%E5%87%BD%E6%95%B0%E5%BA%93/preprocessing/</url>
    
    <content type="html"><![CDATA[<h2 id="StandardScaler类是一个用来对数据进行归一化和标准化的类。"><a href="#StandardScaler类是一个用来对数据进行归一化和标准化的类。" class="headerlink" title="StandardScaler类是一个用来对数据进行归一化和标准化的类。"></a>StandardScaler类是一个用来对数据进行归一化和标准化的类。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">scale_： 缩放比例，同时也是标准差</span><br><span class="hljs-string">mean_： 每个特征的平均值</span><br><span class="hljs-string">var_:每个特征的方差</span><br><span class="hljs-string">n_sample_seen_:样本数量，可以通过patial_fit 增加</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>x = np.array(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>ss = StandardScaler()<br>ss.fit(X=x)<br><span class="hljs-built_in">print</span>(x)<br><span class="hljs-built_in">print</span>(ss.n_samples_seen_)<br><span class="hljs-built_in">print</span>(ss.mean_)<br><span class="hljs-built_in">print</span>(ss.var_)<br><span class="hljs-built_in">print</span>(ss.scale_)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;标准化后的数据:&#x27;</span>)<br>y = ss.fit_transform(x)<br>y<br></code></pre></td></tr></table></figure><p>结果：<img src="/src/2022-02-22-13-38-06-image.png" alt="2022-02-22-13-38-06-image"></p><h2 id="关于StandardScaler-的api函数"><a href="#关于StandardScaler-的api函数" class="headerlink" title="关于StandardScaler()的api函数"></a>关于StandardScaler()的api函数</h2><table><thead><tr><th>api</th><th>describe</th></tr></thead><tbody><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit"><code>fit</code></a>(X[, y, sample_weight])</td><td>Compute the mean and std to be used for later scaling.</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform"><code>fit_transform</code></a>(X[, y])</td><td>Fit to data, then transform it.</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.get_feature_names_out"><code>get_feature_names_out</code></a>([input_features])</td><td>Get output feature names for transformation.</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.get_params"><code>get_params</code></a>([deep])</td><td>Get parameters for this estimator.</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.inverse_transform"><code>inverse_transform</code></a>(X[, copy])</td><td>Scale back the data to the original representation.</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.partial_fit"><code>partial_fit</code></a>(X[, y, sample_weight])</td><td>Online computation of mean and std on X for later scaling.</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.set_params"><code>set_params</code></a>(**params)</td><td>Set the parameters of this estimator.</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.transform"><code>transform</code></a>(X[, copy])</td><td>Perform standardization by centering and scaling.</td></tr></tbody></table><h2 id="常用api方法："><a href="#常用api方法：" class="headerlink" title="常用api方法："></a>常用api方法：</h2><p>1、fit</p><p>用于计算训练数据的均值和方差， 后面就会用均值和方差来转换训练数据</p><p>2、fit_transform</p><p>不仅计算训练数据的均值和方差，还会基于计算出来的均值和方差来转换训练数据，从而把数据转换成标准的正太分布</p><p>3、transform</p><p>很显然，它只是进行转换，只是把训练数据转换成标准的正态分布</p><p>对于上述方法的使用：</p><p>a) 先用fit</p><p><code>scaler = preprocessing.StandardScaler().fit(X)</code></p><p>这一步可以得到scaler，scaler里面存的有计算出来的均值和方差</p><p>b) 再用transform</p><p><code>scaler.transform(X)</code></p><p>这一步再用scaler中的均值和方差来转换X，使X标准化</p><p>c) 那么在预测的时候， 也要对数据做同样的标准化处理，即也要用上面的scaler中的均值和方差来对预测时候的特征进行标准化</p><p>注意：测试数据和预测数据的标准化的方式要和训练数据标准化的方式一样， 必须用同一个scaler来进行transform</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 错误集合</title>
    <link href="/2022/08/01/Python/%E9%94%99%E8%AF%AF%E9%9B%86%E5%90%88/"/>
    <url>/2022/08/01/Python/%E9%94%99%E8%AF%AF%E9%9B%86%E5%90%88/</url>
    
    <content type="html"><![CDATA[<h1 id="python程序在命令行执行提示ModuleNotFoundError-No-module-named-‘XXX’-解决方法"><a href="#python程序在命令行执行提示ModuleNotFoundError-No-module-named-‘XXX’-解决方法" class="headerlink" title="python程序在命令行执行提示ModuleNotFoundError: No module named ‘XXX’ 解决方法"></a>python程序在命令行执行提示ModuleNotFoundError: No module named ‘XXX’ 解决方法</h1><p>在模块中添加</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><br>curPath = os.path.abspath(os.path.dirname(__file__))<br>rootPath = os.path.split(curPath)[<span class="hljs-number">0</span>]<br>sys.path.append(rootPath)<br></code></pre></td></tr></table></figure><h1 id="interrupted-by-signal-9-SIGKILL"><a href="#interrupted-by-signal-9-SIGKILL" class="headerlink" title="interrupted by signal 9: SIGKILL"></a>interrupted by signal 9: SIGKILL</h1><p>终端top查看cpu内存未满，nvidia-smi查看gpu也未占满。<br>可能原因就是：pycharm配置运行内存太小。</p><ol><li>找到pycharm的安装目录下的 pycharm.vmoptions</li><li>右键用文本编辑器打开，修改前两项pycharm 的最小、最大内存：Xms和Xmx</li></ol><p>也有可能是你调用的变量太大，导致内存过载，建议使用批量处理方式。</p><h1 id="python中出现IndentationError-unindent-does-not-match-any-outer-indentation-level错误"><a href="#python中出现IndentationError-unindent-does-not-match-any-outer-indentation-level错误" class="headerlink" title="python中出现IndentationError:unindent does not match any outer indentation level错误"></a>python中出现IndentationError:unindent does not match any outer indentation level错误</h1><p>1.对于此错误，最常见的原因是，的确<strong>没有缩进</strong>。根据错误提示的行数，去代码中看了下，看起来没有什么问题呀，都有缩进，而且语法也没有错误呀。</p><p>2.仔细研究了下代码，发现真的看不出什么问题，突然想到了，把当前python脚本的所有字符（包括空格和tab字符）都显示出来看看到底有没有缩进或者是其他特殊的字符。</p><p>最简单的解决办法，格式化一个代码格式。如果是在pycharm内的话<code>Ctrl+Alt+L</code></p><h1 id="RuntimeError-unsupported-operation-some-elements-of-the-input-tensor-and-the-written-to-tensor-refer-to-a-single-memory-location-Please-clone-the-tensor-before-performing-the-operation"><a href="#RuntimeError-unsupported-operation-some-elements-of-the-input-tensor-and-the-written-to-tensor-refer-to-a-single-memory-location-Please-clone-the-tensor-before-performing-the-operation" class="headerlink" title="RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation."></a>RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.</h1><p>数据中对于张量的操作可能存在错误，所以需要对出现问题的张量进行操作，拷贝一下再进行下一步</p><p>这里即使把<code>new_adj += new_adj.t()</code> 替换成了<code>new_adj += new_adj.clone().t()</code></p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>Error</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python TrickNote in Code</title>
    <link href="/2022/08/01/Python/%E8%AE%BA%E6%96%87%E4%B8%AD%20%E4%BB%A3%E7%A0%81trick/"/>
    <url>/2022/08/01/Python/%E8%AE%BA%E6%96%87%E4%B8%AD%20%E4%BB%A3%E7%A0%81trick/</url>
    
    <content type="html"><![CDATA[<h2 id="pickel"><a href="#pickel" class="headerlink" title="pickel"></a>pickel</h2><p>pickel是能够实现任意对象与文本之间的相互转化，也可以实现任意对象与二进制之间的相互转化。也就是说，pickle 可以实现 Python 对象的存储及恢复。</p><p>pickle 模块提供了以下 4 个函数供我们使用：</p><ol><li>dumps()：将 Python 中的对象序列化成二进制对象，并返回；</li><li>loads()：读取给定的二进制对象数据，并将其转换为 Python 对象；</li><li>dump()：将 Python 中的对象序列化成二进制对象，并写入文件；</li><li>load()：读取指定的序列化数据文件，并返回对象。</li></ol><h2 id="pickel的读取的数据转成torch-float对象"><a href="#pickel的读取的数据转成torch-float对象" class="headerlink" title="pickel的读取的数据转成torch.float对象"></a>pickel的读取的数据转成torch.float对象</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">A = torch.from_numpy(data.todense()).<span class="hljs-built_in">type</span>(torch.FloatTensor)<br></code></pre></td></tr></table></figure><p><code>data</code>是pickel.load()方法读取出来的python对象。</p><p><code>todense()</code>和<code>toarray()</code>很类似，一个是把对象当作矩阵输出，一个是输出ndarray</p><p><img src="/src/image-20220710105542841.png" alt="image-20220710105542841"></p><p>python对象-&gt; 矩阵-&gt; ndarray -&gt; torch.float</p><h2 id="contiguous-函数引发的认知"><a href="#contiguous-函数引发的认知" class="headerlink" title="contiguous()函数引发的认知"></a>contiguous()函数引发的认知</h2><p>在pytorch中，只有很少几个操作是<strong>不改变tensor的内容本身</strong>，而只是<strong>重新定义下标与元素的对应关系</strong>的。换句话说，这种操作不进行数据拷贝和数据的改变，变的是<strong>元数据</strong>。</p><p>会改变元数据的操作是：narrow()、view()、expand()、transpose()<br>在使用transpose()进行转置操作时，pytorch并不会创建新的、转置后的tensor，而是修改了tensor中的一些属性（也就是元数据），使得此时的offset和stride是与转置tensor相对应的。转置的tensor和原tensor的内存是共享的！  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>y = torch.transpose(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;修改前：&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x-&quot;</span>, x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y-&quot;</span>, y)<br> <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n修改后：&quot;</span>)<br>y[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>] = <span class="hljs-number">11</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x-&quot;</span>, x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y-&quot;</span>, y)<br></code></pre></td></tr></table></figure><p>可以看到，<strong>改变了y的元素的值的同时，x的元素的值也发生了变化</strong>。因此可以说，x是contiguous的，但y不是（因为内部数据不是通常的布局方式）。注意不要被contiguous的字面意思“连续的”误解，tensor中数据还是在内存中一块区域里，只是布局的问题！</p><p>为什么这么说：因为，y里面数据布局的方式和从头开始创建一个常规的tensor布局的方式是不一样的。这个可能只是python中之前常用的浅拷贝，y还是指向x变量所处的位置，只是说记录了transpose这个变化的布局。</p><p>使用contiguous() ：如果想要断开这两个变量之间的依赖（x本身是contiguous的），就要使用contiguous()针对x进行变化，感觉上就是我们认为的深拷贝。</p><p> 当调用contiguous()时，会强制拷贝一份tensor，让它的布局和从头创建的一模一样，但是两个tensor完全没有联系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>y = torch.transpose(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>).contiguous()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;修改前：&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x-&quot;</span>, x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y-&quot;</span>, y)<br> <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n修改后：&quot;</span>)<br>y[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>] = <span class="hljs-number">11</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x-&quot;</span>, x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y-&quot;</span>, y)<br></code></pre></td></tr></table></figure><p>可以看到，当<strong>对y使用了.contiguous()<strong>后，</strong>改变y的值时，x没有任何影响</strong>！</p><p>一般来说这一点不用太担心，当遇到<strong>需要调用contiguous()的地方</strong>，运行时会提示你：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RuntimeError: <span class="hljs-built_in">input</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> contiguous<br></code></pre></td></tr></table></figure><p><strong>这时候只需要在该变量后面加上.contiguous()就可以了！</strong></p><h2 id="关于detach与clone之间的思考"><a href="#关于detach与clone之间的思考" class="headerlink" title="关于detach与clone之间的思考"></a>关于detach与clone之间的思考</h2><p>contiguous 这个操作和深拷贝的作用还是很类似的。clone()作为正常的深拷贝操作，一般可能会和detach()这个方法联用。</p><p>Torch 为了提高速度，向量或是矩阵的赋值是指向同一内存的，这不同于 Matlab。如果需要保存旧的tensor即需要开辟新的存储地址而不是引用，可以用 clone() 进行<strong>深拷贝</strong>，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>a = torch.tensor(<span class="hljs-number">1.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br>b = a.clone()<br>c = a.detach()<br>a.data *= <span class="hljs-number">3</span><br>b -= <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(a)   <span class="hljs-comment"># tensor(3., requires_grad=True)</span><br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-built_in">print</span>(c)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">输出结果：</span><br><span class="hljs-string">tensor(3., requires_grad=True)</span><br><span class="hljs-string">tensor(2., grad_fn=&lt;AddBackward0&gt;)</span><br><span class="hljs-string">tensor(3.)      # detach()后的值随着a的变化出现变化</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br></code></pre></td></tr></table></figure><p><code>grad_fn=&lt;CloneBackward&gt;</code>，表示新的tensor充当中间变量，会保留在计算图中，参与梯度计算（回传叠加），但是一般不会保留自身梯度。  clone操作在一定程度上可以视为是一个identity-mapping函数。detach()操作后的tensor与原始tensor共享数据内存，当原始tensor在计算图中数值发生反向传播等更新之后，detach()的tensor值也发生了改变。</p><p><code>所以在torch中使用id来直接判断tensor是否共享内存是不合适的。因为也许底层共享数据，但是存在一个变量是detach出来的新tensor。</code>     </p><p>同样可以发现的是detach()函数可以返回一个完全相同的tensor,与旧的tensor共享内存，脱离计算图，不会牵扯梯度计算。而clone充当中间变量，会将梯度传给源张量进行叠加，但是本身不保存其grad，即值为None。</p><p>使用torch.clone()获得的新tensor和原来的数据不再共享内存，但仍保留在计算图中，clone操作在不共享数据内存的同时支持梯度梯度传递与叠加，所以常用在神经网络中某个单元需要重复使用的场景下。</p><p>通常如果原tensor的requires_grad&#x3D;True，则：</p><ul><li>clone()操作后的tensor requires_grad&#x3D;True,  不共享内存</li><li>detach()操作后的tensor requires_grad&#x3D;False。共享内存</li></ul><p>如果原tensor的requires_grad&#x3D;False，则：</p><ul><li>clone()操作后的tensor requires_grad&#x3D;True, 不存在张量回传的现象。但是可以求得clone后的张量导数</li><li>detach()操作后的tensor requires_grad&#x3D;False</li></ul><h1 id="关于设置多GPU指定问题-torch-nn-DataParaller"><a href="#关于设置多GPU指定问题-torch-nn-DataParaller" class="headerlink" title="关于设置多GPU指定问题  torch.nn.DataParaller"></a>关于设置多GPU指定问题  torch.nn.DataParaller</h1><p>这里指的是单机多卡的问题。在终端中可以利用<code>nvidia-smi</code>查看相关信息</p><p>![屏幕截图 2022-07-18 093729](&#x2F;src&#x2F;屏幕截图 2022-07-18 093729.png)</p><p>可以看见前面对于每张显卡都是有序号的。一般情况下，序号为0的显卡是主卡（可以理解为最先使用的显卡）</p><p>我们通过<code>os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;]</code>来设定主卡是哪一张，使用空闲资源，避免内存资源分配不够。</p><p>例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;3,2,0,1&quot;</span><br><span class="hljs-comment"># os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &#x27;,&#x27;.join(map(str, [3,2,0,1]))</span><br>model = torch.nn.DataParallel(model, device_ids=[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]).cuda()<br></code></pre></td></tr></table></figure><p>则实际的显卡编号与torch中显卡编号的对应关系为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">实际显卡编号-----&gt;运算显卡编号<br>    <span class="hljs-number">3</span>     -----&gt;     <span class="hljs-number">0</span>（主卡）<br>    <span class="hljs-number">2</span>     -----&gt;     <span class="hljs-number">1</span><br>    <span class="hljs-number">0</span>     -----&gt;     <span class="hljs-number">2</span><br>    <span class="hljs-number">1</span>     -----&gt;     <span class="hljs-number">3</span><br></code></pre></td></tr></table></figure><p>当然如果不需要多卡并行计算的话，可以直接使用<code>torch.cuda.set_device(1)</code>来设定，但是这样设定后仍然会占据<code>device:0</code>的部分GPU资源</p><p>在使用<code>os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;]</code>一定要注意其放置的位置，要不然极有可能不生效。<strong>一定要放置在所有访问GPU代码之前，能放在<code>import torch</code>最好不过。</strong></p><p>这里需要注意的是当你设定了可见的显卡后，在torch中设定的显卡的时候注意一下显卡编号问题。<br>还有个小问题，就是当你设置多卡并行的时候，选定显卡需要包括主卡，要不然就会报错<code>报错RuntimeError: module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found</code></p><h1 id="关于self-dict-update"><a href="#关于self-dict-update" class="headerlink" title="关于self.__ dict __.update"></a>关于self.__ dict __.update</h1><p>__dict__是用来存储对象属性的一个<strong>字典</strong>，其键为属性名，值为属性的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Test</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.name = <span class="hljs-string">&#x27;xiaoming&#x27;</span><br>        self.age = <span class="hljs-number">18</span><br>        self.gender = <span class="hljs-string">&#x27;man&#x27;</span><br>    abc = <span class="hljs-number">666</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">print_dict</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(self.__dict__)<br></code></pre></td></tr></table></figure><p>如果我们调用<code>print_dict</code>来使用<code>self.__dict__</code>我们可以发现<code>__dict__</code>中存储了对象的属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;xiaoming&#x27;</span>, <span class="hljs-string">&#x27;age&#x27;</span>: <span class="hljs-number">18</span>, <span class="hljs-string">&#x27;gender&#x27;</span>: <span class="hljs-string">&#x27;man&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>这样写起来特别方便，在面对很大键值对的时候，直接调用update函数来实现自动化实例变量。这样直接输入一个字典，就可以自动的生成对应的变量了，还是很方便的</p><h1 id="torch-autograd-detect-anomaly"><a href="#torch-autograd-detect-anomaly" class="headerlink" title="torch.autograd.detect_anomaly()"></a>torch.autograd.detect_anomaly()</h1><p>为 autograd 引擎启用异常检测的上下文管理器。 这做了两件事：</p><ul><li>在启用检测的情况下运行正向传递将允许反向传递打印创建失败的反向函数的正向操作的回溯。</li><li>任何产生“nan”值的反向计算都会引发错误。</li></ul><blockquote><p>警告: 此模式应仅用于调试，因为不同的测试会减慢您的程序执行速度。</p></blockquote><h1 id="torch-spmm"><a href="#torch-spmm" class="headerlink" title="torch.spmm"></a>torch.spmm</h1><p>可以允许SparseMatrix * DenseMatrix，DenseMatrix* DenseMatrix， SparseMatrix * SparseMatrix </p><p>但是不可以DenseMatrix* SparseMatrix<br>x </p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>缺失值处理</title>
    <link href="/2022/08/01/Python/%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/"/>
    <url>/2022/08/01/Python/%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>诸多数据竞赛问题中，我们不可避免会碰到数据出现缺失的情况，可能是因为记录数据时候的失误，也有可能是数据本身就没有（例如一些资料用户没有填充，或者股票停牌了，那么那天的交易记录就是为空的）。对于这些缺失值的处理对于模型最终的预测可能起到至关重要的作用，因为缺失的数据会导致：</p><ul><li>数据集失真：大量缺失的数据可能导致变量分布失真，可能增加或减少数据集中特定类别的值。</li><li>影响最终模型的训练预测：缺失的数据会导致数据集中出现偏差，并可能导致模型训练和预测有偏差。</li></ul><p>本文我们介绍竞赛中常见的缺失值填充技巧。</p><p>不同类型数据的填充策略</p><h2 id="01-CCA-Complete-Case-Analysis"><a href="#01-CCA-Complete-Case-Analysis" class="headerlink" title="01 CCA(Complete Case Analysis)"></a>01 CCA(Complete Case Analysis)</h2><p>Complete Case Analysis(CCA)是一种非常简单的处理缺失数据的方法，它直接删除包含缺失数据的行，即我们只考虑包含完整数据的行。</p><p>这种策略较为直接，很容易实现，当然也会使我们损失非常多的数据信息。在预测存在缺失值的时候也会出现偏差。一般建议当我们的数据量足够大的时候，即缺失的数据占比非常小，而且有存在随机的情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">data_cca = df.dropna(axis=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h2 id="02-随机数填充"><a href="#02-随机数填充" class="headerlink" title="02  随机数填充"></a>02  随机数填充</h2><p>这是缺失值中填充的一项重要技术，它可以处理数值变量和分类变量。一般我们会对缺失值分组到一列中，并分配一个特殊的值，例如99999999或-999999等。这样我们保留了缺失值的特殊含义，在一些特殊的情况下，可以带来非常好的效果，但是缺点也很明显，我们这种填充策略对于目前的一些模型，例如NN等，进行预处理和训练预测带来一些挑战。</p><h2 id="03-类别特征的频率填充类别特征的频率填充"><a href="#03-类别特征的频率填充类别特征的频率填充" class="headerlink" title="03 类别特征的频率填充类别特征的频率填充"></a>03 类别特征的频率填充类别特征的频率填充</h2><p>用出现频率最高的变量值替换缺失的值，用该特征列的众数作为替换值。该方法实现简单，也较为常用，但是当数据缺失较大的时候，这样的填充会使得数据出现较大的偏差，使得预测结果较差。</p><h2 id="04-统计值填充"><a href="#04-统计值填充" class="headerlink" title="04 统计值填充"></a>04 统计值填充</h2><p>使用统计值进行填充，例如均值，中位数等。有的时候我们也可以使用正态分布的策略进行填充：</p><ul><li>使用与平均值相差2个标准差以内的值。在（平均值-2 std）&amp;（平均值+2 std）之间生成随机数来填充缺失值。</li></ul><h2 id="05-使用线性回归"><a href="#05-使用线性回归" class="headerlink" title="05 使用线性回归"></a>05 使用线性回归</h2><p>通过两个变量的强相关性来进行线性拟合填充。注意线性回归时需要注意异常值的影响。</p><h2 id="06-XGBoost迭代填充"><a href="#06-XGBoost迭代填充" class="headerlink" title="06 XGBoost迭代填充"></a>06 XGBoost迭代填充</h2><p>迭代填充就是将每个特征建模为其他特征的函数，例如预测缺失值的回归问题。这个可以认为是上面线性回归的扩展版本。该方法也在诸多的数据竞赛中获得过非常好的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># loading modules</span><br><span class="hljs-keyword">from</span> sklearn.experimental <span class="hljs-keyword">import</span> enable_iterative_imputer<br><span class="hljs-keyword">from</span> sklearn.impute <span class="hljs-keyword">import</span> IterativeImputer<br><span class="hljs-keyword">import</span> xgboost<br><br>data=df.copy()<br><br><span class="hljs-comment">#setting up the imputer</span><br><br>imp = IterativeImputer(<br>    estimator=xgboost.XGBRegressor(<br>        n_estimators=<span class="hljs-number">5</span>,<br>        random_state=<span class="hljs-number">1</span>,<br>        tree_method=<span class="hljs-string">&#x27;gpu_hist&#x27;</span>,<br>    ),<br>    missing_values=np.nan,<br>    max_iter=<span class="hljs-number">5</span>,<br>    initial_strategy=<span class="hljs-string">&#x27;mean&#x27;</span>,<br>    imputation_order=<span class="hljs-string">&#x27;ascending&#x27;</span>,<br>    verbose=<span class="hljs-number">2</span>,<br>    random_state=<span class="hljs-number">1</span><br>)<br><br>data[:] = imp.fit_transform(data)<br></code></pre></td></tr></table></figure><h2 id="07-最近邻填充"><a href="#07-最近邻填充" class="headerlink" title="07 最近邻填充"></a>07 最近邻填充</h2><p>顾名思义，就是采用最近邻的策略进行填充，大家常用的策略是KNNImputer。默认情况下，我们用缺失值的欧几里德距离度量来查找最近的点。每个缺失的特征都是使用最近N个样本的非缺失的特征值进行填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.impute <span class="hljs-keyword">import</span> KNNImputer<br>nan = np.nan<br>X = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, nan], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>], [nan, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>], [<span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>]]<br>imputer = KNNImputer(n_neighbors=<span class="hljs-number">2</span>, weights=<span class="hljs-string">&quot;uniform&quot;</span>)<br>imputed_x=imputer.fit_transform(X) <br></code></pre></td></tr></table></figure><h2 id="08-缺失值标记"><a href="#08-缺失值标记" class="headerlink" title="08  缺失值标记"></a>08  缺失值标记</h2><p>该方法也是最为常见的，就是将数据集中特征转换为相应的二进制矩阵，以标识该数据集中是否存在缺失值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.impute <span class="hljs-keyword">import</span> MissingIndicator<br>X = np.array([[-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>],<br>              [<span class="hljs-number">4</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>              [<span class="hljs-number">8</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])<br>indicator = MissingIndicator(missing_values=-<span class="hljs-number">1</span>)<br>mask_missing_values_only = indicator.fit_transform(X)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>特征选择示例</title>
    <link href="/2022/08/01/Python/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5/"/>
    <url>/2022/08/01/Python/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5/</url>
    
    <content type="html"><![CDATA[<p>太多的特征会增加模型的复杂性和过拟合，而太少的特征会导致模型的拟合不足。将模型优化为足够复杂以使其性能可推广，但又足够简单易于训练、维护和解释是特征选择的主要工作。</p><p>“特征选择”意味着可以保留一些特征并放弃其他一些特征。本文的目的是概述一些特征选择策略：</p><ol><li>删除未使用的列</li><li>删除具有缺失值的列</li><li>不相关的特征</li><li>低方差特征</li><li>多重共线性</li><li>特征系数</li><li>p 值</li><li>方差膨胀因子 (VIF)</li><li>基于特征重要性的特征选择</li><li>使用 sci-kit learn 进行自动特征选择</li><li>主成分分析 (PCA)</li></ol><p>该演示的数据集在 MIT 许可下发布，来自 PyCaret——一个开源的低代码机器学习库。</p><p>数据集相当干净，但我做了一些预处理。请注意，我使用此数据集来演示不同的特征选择策略如何工作，而不是构建最终模型，因此模型性能无关紧要。</p><p>首先加载数据集：</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pddata = &#x27;https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/automobile.csv&#x27;df = pd.read_csv(data)<br><span class="hljs-title">df</span>.sample(<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p><img src="/src/640-16561438083483.png" alt="图片"></p><p>该数据集包含 202 行和 26 列——每行代表一个汽车实例，每列代表其特征和相应的价格。这些列包括：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">df.columns<br>&gt;&gt; <span class="hljs-constructor">Index([&#x27;<span class="hljs-params">symboling</span>&#x27;, &#x27;<span class="hljs-params">normalized</span>-<span class="hljs-params">losses</span>&#x27;, &#x27;<span class="hljs-params">make</span>&#x27;, &#x27;<span class="hljs-params">fuel</span>-<span class="hljs-params">type</span>&#x27;, &#x27;<span class="hljs-params">aspiration</span>&#x27;, &#x27;<span class="hljs-params">num</span>-<span class="hljs-params">of</span>-<span class="hljs-params">doors</span>&#x27;, &#x27;<span class="hljs-params">body</span>-<span class="hljs-params">style</span>&#x27;, &#x27;<span class="hljs-params">drive</span>-<span class="hljs-params">wheels</span>&#x27;, &#x27;<span class="hljs-params">engine</span>-<span class="hljs-params">location</span>&#x27;,&#x27;<span class="hljs-params">wheel</span>-<span class="hljs-params">base</span>&#x27;, &#x27;<span class="hljs-params">length</span>&#x27;, &#x27;<span class="hljs-params">width</span>&#x27;, &#x27;<span class="hljs-params">height</span>&#x27;, &#x27;<span class="hljs-params">curb</span>-<span class="hljs-params">weight</span>&#x27;, &#x27;<span class="hljs-params">engine</span>-<span class="hljs-params">type</span>&#x27;, &#x27;<span class="hljs-params">num</span>-<span class="hljs-params">of</span>-<span class="hljs-params">cylinders</span>&#x27;, &#x27;<span class="hljs-params">engine</span>-<span class="hljs-params">size</span>&#x27;, &#x27;<span class="hljs-params">fuel</span>-<span class="hljs-params">system</span>&#x27;, &#x27;<span class="hljs-params">bore</span>&#x27;, &#x27;<span class="hljs-params">stroke</span>&#x27;, &#x27;<span class="hljs-params">compression</span>-<span class="hljs-params">ratio</span>&#x27;, &#x27;<span class="hljs-params">horsepower</span>&#x27;, &#x27;<span class="hljs-params">peak</span>-<span class="hljs-params">rpm</span>&#x27;, &#x27;<span class="hljs-params">city</span>-<span class="hljs-params">mpg</span>&#x27;, &#x27;<span class="hljs-params">highway</span>-<span class="hljs-params">mpg</span>&#x27;, &#x27;<span class="hljs-params">price</span>&#x27;], <span class="hljs-params">dtype</span>=&#x27;<span class="hljs-params">object</span>&#x27;)</span><br></code></pre></td></tr></table></figure><p>现在让我们深入研究特征选择的 11 种策略。</p><h2 id="1-删除未使用的列"><a href="#1-删除未使用的列" class="headerlink" title="1.删除未使用的列"></a><strong>1.删除未使用的列</strong></h2><p>当然，最简单的策略是你的直觉。虽然是直觉，但有时很有用的，某些列在最终模型中不会以任何形式使用（例如“ID”、“FirstName”、“LastName”等列）。如果您知道某个特定列将不会被使用，请随时将其删除。在我们的数据中，没有一列有这样的问题所以，我在此步骤中不删除任何列。</p><h2 id="2-删除具有缺失值的列"><a href="#2-删除具有缺失值的列" class="headerlink" title="2.删除具有缺失值的列"></a><strong>2.删除具有缺失值的列</strong></h2><p>缺失值在机器学习中是不可接受的，因此我们会采用不同的策略来清理缺失数据（例如插补）。但是如果列中缺少大量数据，那么完全删除它是非常好的方法。</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># total <span class="hljs-keyword">null</span> <span class="hljs-keyword">values</span> per columndf.<span class="hljs-keyword">isnull</span>().sum()<br>&gt;&gt;symboling             <span class="hljs-number">0</span>normalized-losses   <span class="hljs-number">35</span>make                 <span class="hljs-number">0</span>fuel-<span class="hljs-keyword">type</span>             <span class="hljs-number">0</span>aspiration           <span class="hljs-number">0</span>num-<span class="hljs-keyword">of</span>-doors         <span class="hljs-number">2</span>body-style           <span class="hljs-number">0</span>drive-wheels         <span class="hljs-number">0</span>engine-<span class="hljs-keyword">location</span>       <span class="hljs-number">0</span>wheel-base           <span class="hljs-number">0</span>length               <span class="hljs-number">0</span>width                 <span class="hljs-number">0</span>height               <span class="hljs-number">0</span>curb-weight           <span class="hljs-number">0</span>engine-<span class="hljs-keyword">type</span>           <span class="hljs-number">0</span>num-<span class="hljs-keyword">of</span>-cylinders     <span class="hljs-number">0</span>engine-size           <span class="hljs-number">0</span>fuel-<span class="hljs-keyword">system</span>           <span class="hljs-number">0</span>bore                 <span class="hljs-number">0</span>stroke               <span class="hljs-number">0</span>compression-ratio     <span class="hljs-number">0</span>horsepower           <span class="hljs-number">0</span>peak-rpm             <span class="hljs-number">0</span>city-mpg             <span class="hljs-number">0</span>highway-mpg           <span class="hljs-number">0</span>price                 <span class="hljs-number">0</span>dtype: int64<br></code></pre></td></tr></table></figure><h2 id="3-不相关的特征"><a href="#3-不相关的特征" class="headerlink" title="3.不相关的特征"></a><strong>3.不相关的特征</strong></h2><p>无论算法是回归（预测数字）还是分类（预测类别），特征都必须与目标相关。如果一个特征没有表现出相关性，它就是一个主要的消除目标。可以分别测试数值和分类特征的相关性。</p><p><strong>数值变量</strong></p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lisp"># correlation between target and features(<span class="hljs-name">df</span>.corr().loc[&#x27;price&#x27;].plot(<span class="hljs-name">kind=</span>&#x27;barh&#x27;, figsize=(<span class="hljs-number">4</span>,<span class="hljs-number">10</span>)))<br></code></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><p>在此示例中，peak-rpm, compression-ratio, stroke, bore, height , symboling 等特征与价格几乎没有相关性，因此我们可以删除它们。</p><p>可以手动删除列，但我更喜欢使用相关阈值（在本例中为 0.2）以编程方式进行：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stata"># <span class="hljs-keyword">drop</span> uncorrelated numeric features (threshold &lt;0.2)<span class="hljs-keyword">corr</span> = <span class="hljs-built_in">abs</span>(df.<span class="hljs-built_in">corr</span>().<span class="hljs-keyword">loc</span>[&#x27;price&#x27;])<span class="hljs-keyword">corr</span> = <span class="hljs-keyword">corr</span>[<span class="hljs-keyword">corr</span>&lt;0.2]cols_to_drop = <span class="hljs-keyword">corr</span>.index.to_list()df = df.<span class="hljs-keyword">drop</span>(cols_to_drop, axis=1)<br></code></pre></td></tr></table></figure><p><strong>分类变量</strong></p><p>可以使用箱线图查找目标和分类特征之间的相关性：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-title">sns</span>.boxplot(y = &#x27;price&#x27;, x = &#x27;fuel-<span class="hljs-class"><span class="hljs-keyword">type</span>&#x27;, <span class="hljs-keyword">data</span>=df)</span><br></code></pre></td></tr></table></figure><p>柴油车的中位价高于汽油车。这意味着这个分类变量可以解释汽车价格，所以应放弃它。可以像这样单独检查每个分类列。</p><h2 id="4-低方差特征"><a href="#4-低方差特征" class="headerlink" title="4.低方差特征"></a><strong>4.低方差特征</strong></h2><p>检查一下我们的特征的差异：</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br># variance <span class="hljs-keyword">of</span> numeric features(df.select_dtypes(include=np.number).var().astype(&#x27;str&#x27;))<br></code></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><p>这里的“bore”具有极低的方差，虽然这是删除的候选者。在这个特殊的例子中，我不愿意删除它，因为它的值在2.54和3.94之间，因此方差很低：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss">df<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;bore&#x27;</span>]</span><span class="hljs-selector-class">.describe</span>()<br></code></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><h2 id="5-多重共线性"><a href="#5-多重共线性" class="headerlink" title="5.多重共线性"></a><strong>5.多重共线性</strong></h2><p>当任何两个特征之间存在相关性时，就会出现多重共线性。在机器学习中，期望每个特征都应该独立于其他特征，即它们之间没有共线性。高马力车辆往往具有高发动机尺寸。所以你可能想消除其中一个，让另一个决定目标变量——价格。</p><p>我们可以分别测试数字和分类特征的多重共线性：</p><p><strong>数值变量</strong></p><p>Heatmap 是检查和寻找相关特征的最简单方法。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>sns.<span class="hljs-built_in">set</span>(rc=&#123;<span class="hljs-string">&#x27;figure.figsize&#x27;</span>:(16,10)&#125;)sns.heatmap(df.corr(),          <span class="hljs-attribute">annot</span>=<span class="hljs-literal">True</span>,          <span class="hljs-attribute">linewidths</span>=.5,          <span class="hljs-attribute">center</span>=0,          <span class="hljs-attribute">cbar</span>=<span class="hljs-literal">False</span>,          <span class="hljs-attribute">cmap</span>=<span class="hljs-string">&quot;PiYG&quot;</span>)plt.show()<br></code></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><p>大多数特征在某种程度上相互关联，但有些特征具有非常高的相关性，例如长度与轴距以及发动机尺寸与马力。</p><p>可以根据相关阈值手动或以编程方式删除这些功能。我将手动删除具有 0.80 共线性阈值的特征。</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sml"># drop correlated featuresdf = df.drop([<span class="hljs-symbol">&#x27;length&#x27;</span>, <span class="hljs-symbol">&#x27;width&#x27;</span>, <span class="hljs-symbol">&#x27;curb</span>-weight&#x27;, <span class="hljs-symbol">&#x27;engine</span>-size&#x27;, <span class="hljs-symbol">&#x27;city</span>-mpg&#x27;], axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>还可以使用称为方差膨胀因子 (VIF) 的方法来确定多重共线性并根据高 VIF 值删除特征。我稍后会展示这个例子。</p><p><strong>分类变量</strong></p><p>与数值特征类似，也可以检查分类变量之间的共线性。诸如独立性卡方检验之类的统计检验非常适合它。</p><p>让我们检查一下数据集中的两个分类列——燃料类型和车身风格——是独立的还是相关的。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lua">df_cat = df<span class="hljs-string">[[&#x27;fuel-type&#x27;, &#x27;body-style&#x27;]]</span>df_cat.sample(<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p><img src="/src/640-16561438083495.png" alt="图片"></p><p>然后我们将在每一列中创建一个类别的交叉表&#x2F;列联表。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">crosstab = pd<span class="hljs-selector-class">.crosstab</span>(df_cat<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;fuel-type&#x27;</span>]</span>, df_cat<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;body-style&#x27;</span>]</span>)crosstab<br></code></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><p>最后，我们将在交叉表上运行卡方检验，这将告诉我们这两个特征是否独立。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">from scipy<span class="hljs-selector-class">.stats</span> import chi2_contingency<br><span class="hljs-function"><span class="hljs-title">chi2_contingency</span><span class="hljs-params">(crosstab)</span></span><br></code></pre></td></tr></table></figure><p>输出依次是卡方值、p 值、自由度和预期频率数组。</p><p>p 值 &lt;0.05，因此我们可以拒绝特征之间没有关联的原假设，即两个特征之间存在统计上显着的关系。</p><p>由于这两个特征之间存在关联，我们可以选择删除其中一个。</p><p>到目前为止，我已经展示了在实现模型之前应用的特征选择策略。这些策略在第一轮特征选择以建立初始模型时很有用。但是一旦构建了模型，就可以获得有关模型性能中每个特征的适应度的更多信息。根据这些新信息，可以进一步确定要保留哪些功能。</p><p>下面我们使用最简单的线性模型展示其中的一些方法。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># drop columns with missing valuesdf = df.dropna()</span><br><span class="hljs-title">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split# get dummies for categorical featuresdf = pd.get_dummies(<span class="hljs-title">df</span>, <span class="hljs-title">drop_first</span>=<span class="hljs-type">True</span>)# X featuresX = df.drop(&#x27;<span class="hljs-title">price&#x27;</span>, <span class="hljs-title">axis</span>=1)# y targety = df[&#x27;price&#x27;]# split data into training and testing setX_train, X_test, y_train, y_test = train_test_split(<span class="hljs-type">X</span>, <span class="hljs-title">y</span>, <span class="hljs-title">test_size</span>=0.3, <span class="hljs-title">random_state</span>=42)<br><span class="hljs-title">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression# scalingfrom sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScalerscaler = StandardScaler()X_train = scaler.fit_transform(<span class="hljs-type">X_train</span>)X_test = scaler.fit_transform(<span class="hljs-type">X_test</span>)# convert back to dataframeX_train = pd.DataFrame(<span class="hljs-type">X_train</span>, <span class="hljs-title">columns</span> = <span class="hljs-type">X</span>.<span class="hljs-title">columns</span>.<span class="hljs-title">to_list</span>())X_test = pd.DataFrame(<span class="hljs-type">X_test</span>, <span class="hljs-title">columns</span> = <span class="hljs-type">X</span>.<span class="hljs-title">columns</span>.<span class="hljs-title">to_list</span>())# instantiate modelmodel = LinearRegression()# fitmodel.fit(<span class="hljs-type">X_train</span>, <span class="hljs-title">y_train</span>)<br></code></pre></td></tr></table></figure><p>现在我们已经拟合了模型，让我们进行另一轮特征选择。</p><h2 id="6-特征系数"><a href="#6-特征系数" class="headerlink" title="6.特征系数"></a><strong>6.特征系数</strong></h2><p>如果正在运行回归任务，则特征适应度的一个关键指标是回归系数（所谓的 beta 系数），它显示了模型中特征的相对贡献。有了这些信息，可以删除贡献很小或没有贡献的功能。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># feature coefficientscoeffs = model.coef_<br># visualizing coefficientsindex = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X_train</span>.</span></span>columns.tolist<span class="hljs-literal">()</span><br>(pd.<span class="hljs-constructor">DataFrame(<span class="hljs-params">coeffs</span>, <span class="hljs-params">index</span> = <span class="hljs-params">index</span>, <span class="hljs-params">columns</span> = [&#x27;<span class="hljs-params">coeff</span>&#x27;])</span>.sort<span class="hljs-constructor">_values(<span class="hljs-params">by</span> = &#x27;<span class="hljs-params">coeff</span>&#x27;)</span>.plot(kind=&#x27;barh&#x27;, figsize=(<span class="hljs-number">4</span>,<span class="hljs-number">10</span>)))<br></code></pre></td></tr></table></figure><p><img src="/src/640-16561438083496.png" alt="图片"></p><p>某些特征beta 系数很小，对汽车价格的预测贡献不大。可以过滤掉这些特征：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># <span class="hljs-keyword">filter</span> variables near zero coefficient valuetemp = pd.DataFrame(coeffs, <span class="hljs-keyword">index</span> = <span class="hljs-keyword">index</span>, <span class="hljs-keyword">columns</span> = [<span class="hljs-string">&#x27;coeff&#x27;</span>]).sort_values(<span class="hljs-keyword">by</span> = <span class="hljs-string">&#x27;coeff&#x27;</span>)<span class="hljs-keyword">temp</span> = <span class="hljs-keyword">temp</span>[(<span class="hljs-keyword">temp</span>[<span class="hljs-string">&#x27;coeff&#x27;</span>]&gt;<span class="hljs-number">1</span>) | (<span class="hljs-keyword">temp</span>[<span class="hljs-string">&#x27;coeff&#x27;</span>]&lt; <span class="hljs-number">-1</span>)]<br># <span class="hljs-keyword">drop</span> those featurescols_coeff = <span class="hljs-keyword">temp</span>.<span class="hljs-keyword">index</span>.to_list()X_train = X_train[cols_coeff]X_test = X_test[cols_coeff]<br></code></pre></td></tr></table></figure><h2 id="7-p-值"><a href="#7-p-值" class="headerlink" title="7.p 值"></a><strong>7.p 值</strong></h2><p>在回归中，p 值告诉我们预测变量和目标之间的关系是否具有统计显著性。statsmodels 库提供了带有特征系数和相关 p 值的回归输出的函数。</p><p>如果某些特征不显著，可以将它们一个一个移除，然后每次重新运行模型，直到找到一组具有显着 p 值的特征，并通过更高的调整 R2 提高性能。</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> smols = sm.OLS(y, <span class="hljs-type">X</span>).fit()print(ols.summary())<br></code></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><h2 id="8-方差膨胀因子-VIF"><a href="#8-方差膨胀因子-VIF" class="headerlink" title="8.方差膨胀因子 (VIF)"></a><strong>8.方差膨胀因子 (VIF)</strong></h2><p>方差膨胀因子 (VIF) 是衡量多重共线性的另一种方法。它被测量为整体模型方差与每个独立特征的方差的比率。一个特征的高 VIF 表明它与一个或多个其他特征相关。根据经验：</p><ul><li>VIF &#x3D; 1 表示无相关性</li><li>VIF &#x3D; 1-5 中等相关性</li><li>VIF &gt;5 高相关</li></ul><p>VIF 是一种消除多重共线性特征的有用技术。对于我们的演示，将所有 VIF 高于10的删除。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">from statsmodels.stats.outliers_influence import variance_inflation_factor<br># calculate VIFvif = pd.<span class="hljs-constructor">Series([<span class="hljs-params">variance_inflation_factor</span>(X.<span class="hljs-params">values</span>, <span class="hljs-params">i</span>)</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X</span>.</span></span>shape<span class="hljs-literal">[<span class="hljs-number">1</span>]</span>)], index=<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X</span>.</span></span>columns)<br># display VIFs <span class="hljs-keyword">in</span> a tableindex = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X_train</span>.</span></span>columns.tolist<span class="hljs-literal">()</span>vif_df = pd.<span class="hljs-constructor">DataFrame(<span class="hljs-params">vif</span>, <span class="hljs-params">index</span> = <span class="hljs-params">index</span>, <span class="hljs-params">columns</span> = [&#x27;<span class="hljs-params">vif</span>&#x27;])</span>.sort<span class="hljs-constructor">_values(<span class="hljs-params">by</span> = &#x27;<span class="hljs-params">vif</span>&#x27;, <span class="hljs-params">ascending</span>=False)</span>vif_df<span class="hljs-literal">[<span class="hljs-identifier">vif_df</span>[&#x27;<span class="hljs-identifier">vif</span>&#x27;]</span>&lt;<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><p><img src="/src/640-16561438083497.png" alt="图片"></p><h2 id="9-基于特征重要性选择"><a href="#9-基于特征重要性选择" class="headerlink" title="9.基于特征重要性选择"></a><strong>9.基于特征重要性选择</strong></h2><p>决策树&#x2F;随机森林使用一个特征来分割数据，该特征最大程度地减少了杂质(以基尼系数杂质或信息增益衡量)。找到最佳特征是算法如何在分类任务中工作的关键部分。我们可以通过 feature_importances_ 属性访问最好的特征。</p><p>让我们在我们的数据集上实现一个随机森林模型并过滤一些特征。</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br># instantiate modelmodel = RandomForestClassifier(n_estimators=<span class="hljs-number">200</span>, random_state=<span class="hljs-number">0</span>)# fit modelmodel.fit(X,y)<br></code></pre></td></tr></table></figure><p>现在让我们看看特征重要性：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># feature importanceimportances = model.feature_importances_<br># visualizationcols = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X</span>.</span></span>columns(pd.<span class="hljs-constructor">DataFrame(<span class="hljs-params">importances</span>, <span class="hljs-params">cols</span>, <span class="hljs-params">columns</span> = [&#x27;<span class="hljs-params">importance</span>&#x27;])</span>.sort<span class="hljs-constructor">_values(<span class="hljs-params">by</span>=&#x27;<span class="hljs-params">importance</span>&#x27;, <span class="hljs-params">ascending</span>=True)</span>.plot(kind=&#x27;barh&#x27;, figsize=(<span class="hljs-number">4</span>,<span class="hljs-number">10</span>)))<br></code></pre></td></tr></table></figure><p><img src="/src/640-16561438083508.png" alt="图片"></p><p>上面的输出显示了每个特征在减少每个节点&#x2F;拆分处的重要性。</p><p>由于随机森林分类器有很多估计量（例如上面例子中的 200 棵决策树），可以用置信区间计算相对重要性的估计值。</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs x86asm"># calculate standard deviation of feature importancesstd = np<span class="hljs-number">.</span><span class="hljs-keyword">std</span>([i<span class="hljs-number">.</span>feature_importances_ for i <span class="hljs-keyword">in</span> model<span class="hljs-number">.</span>estimators_], axis=<span class="hljs-number">0</span>)<br># visualizationfeat_with_importance = pd<span class="hljs-number">.</span>Series(importances, X<span class="hljs-number">.</span>columns)fig, <span class="hljs-built_in">ax</span> = plt<span class="hljs-number">.</span>subplots(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">5</span>))feat_with_importance<span class="hljs-number">.</span>plot<span class="hljs-number">.</span>bar(yerr=<span class="hljs-keyword">std</span>, <span class="hljs-built_in">ax</span>=<span class="hljs-built_in">ax</span>)<span class="hljs-built_in">ax</span><span class="hljs-number">.</span>set_title(<span class="hljs-string">&quot;Feature importances&quot;</span>)<span class="hljs-built_in">ax</span><span class="hljs-number">.</span>set_ylabel(<span class="hljs-string">&quot;Mean decrease in impurity&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><p>现在我们知道了每个特征的重要性，可以手动（或以编程方式）确定保留哪些特征以及删除哪些特征。</p><h2 id="10-使用-Scikit-Learn-自动选择特征"><a href="#10-使用-Scikit-Learn-自动选择特征" class="headerlink" title="10.使用 Scikit Learn 自动选择特征"></a><strong>10.使用 Scikit Learn 自动选择特征</strong></h2><p>sklearn 库中有一个完整的模块，只需几行代码即可处理特征选择。</p><p>sklearn 中有许多自动化流程，但这里我只展示一些：</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs elm"># <span class="hljs-keyword">import</span> modulesfrom sklearn.feature_selection <span class="hljs-keyword">import</span> (<span class="hljs-type">SelectKBest</span>, chi2, <span class="hljs-type">SelectPercentile</span>, <span class="hljs-type">SelectFromModel</span>, <span class="hljs-type">SequentialFeatureSelector</span>, <span class="hljs-type">SequentialFeatureSelector</span>)<br></code></pre></td></tr></table></figure><p><strong>基于卡方的技术</strong></p><p>基于卡方的技术根据一些预定义的分数选择特定数量的用户定义特征 (k)。这些分数是通过计算 X（独立）和 y（因）变量之间的卡方统计量来确定的。在 sklearn 中，需要做的就是确定要保留多少特征。如果想保留 10 个功能，实现将如下所示</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># select K best featuresX_best = <span class="hljs-constructor">SelectKBest(<span class="hljs-params">chi2</span>, <span class="hljs-params">k</span>=10)</span>.fit<span class="hljs-constructor">_transform(X,<span class="hljs-params">y</span>)</span><br># number <span class="hljs-keyword">of</span> best featuresX_best.shape<span class="hljs-literal">[<span class="hljs-number">1</span>]</span><br>&gt;&gt; <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>如果有大量特征，可以指定要保留的特征百分比。假设我们想要保留 75% 的特征并丢弃剩余的 25%：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># keep <span class="hljs-number">75</span>% top featuresX_top = <span class="hljs-constructor">SelectPercentile(<span class="hljs-params">chi2</span>, <span class="hljs-params">percentile</span> = 75)</span>.fit<span class="hljs-constructor">_transform(X,<span class="hljs-params">y</span>)</span><br># number <span class="hljs-keyword">of</span> best featuresX_top.shape<span class="hljs-literal">[<span class="hljs-number">1</span>]</span><br>&gt;&gt; <span class="hljs-number">36</span><br></code></pre></td></tr></table></figure><p><strong>正则化</strong></p><p>正则化减少了过拟合。如果你有太多的特征，正则化控制它们的效果，或者通过缩小特征系数（称为 L2 正则化）或将一些特征系数设置为零（称为 L1 正则化）。</p><p>一些模型具有内置的 L1&#x2F;L2 正则化作为超参数来惩罚特征。可以使用转换器 SelectFromModel 消除这些功能。</p><p>让我们实现一个带有惩罚 &#x3D; ‘l1’ 的 LinearSVC 算法。然后使用 SelectFromModel 删除一些功能。</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># implement algorithmfrom sklearn.svm <span class="hljs-keyword">import</span> LinearSVCmodel = LinearSVC(penalty= <span class="hljs-string">&#x27;l1&#x27;</span>, C = <span class="hljs-number">0.002</span>, dual=<span class="hljs-keyword">False</span>)model.fit(X,y)# <span class="hljs-keyword">select</span> features <span class="hljs-keyword">using</span> the meta transformerselector = SelectFromModel(estimator = model, prefit=<span class="hljs-keyword">True</span>)<br>X_new = selector.<span class="hljs-keyword">transform</span>(X)X_new.shape[<span class="hljs-number">1</span>]<br>&gt;&gt; <span class="hljs-number">2</span><br># names <span class="hljs-keyword">of</span> selected featuresfeature_names = np.<span class="hljs-keyword">array</span>(X.<span class="hljs-keyword">columns</span>)feature_names[selector.get_support()]<br>&gt;&gt; <span class="hljs-keyword">array</span>([<span class="hljs-string">&#x27;wheel-base&#x27;</span>, <span class="hljs-string">&#x27;horsepower&#x27;</span>], dtype=<span class="hljs-keyword">object</span>)<br></code></pre></td></tr></table></figure><p><strong>序贯法</strong></p><p>序贯法是一种经典的统计技术。在这种情况下一次添加&#x2F;删除一个功能并检查模型性能，直到它针对需求进行优化。</p><p>序贯法有两种变体。前向选择技术从 0 特征开始，然后添加一个最大程度地减少错误的特征；然后添加另一个特征，依此类推。</p><p>向后选择在相反的方向上起作用。模型从包含的所有特征开始并计算误差；然后它消除了一个可以进一步减少误差的特征。重复该过程，直到保留所需数量的特征。</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># instantiate modelmodel = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">0</span>)<br># <span class="hljs-keyword">select</span> featuresselector = SequentialFeatureSelector(estimator=model, n_features_to_select=<span class="hljs-number">10</span>, direction=<span class="hljs-string">&#x27;backward&#x27;</span>, cv=<span class="hljs-number">2</span>)selector.fit_transform(X,y)<br># <span class="hljs-keyword">check</span> names <span class="hljs-keyword">of</span> features selectedfeature_names = np.<span class="hljs-keyword">array</span>(X.<span class="hljs-keyword">columns</span>)feature_names[selector.get_support()]<br>&gt;&gt; <span class="hljs-keyword">array</span>([<span class="hljs-string">&#x27;bore&#x27;</span>, <span class="hljs-string">&#x27;make_mitsubishi&#x27;</span>, <span class="hljs-string">&#x27;make_nissan&#x27;</span>, <span class="hljs-string">&#x27;make_saab&#x27;</span>,      <span class="hljs-string">&#x27;aspiration_turbo&#x27;</span>, <span class="hljs-string">&#x27;num-of-doors_two&#x27;</span>, <span class="hljs-string">&#x27;body style_hatchback&#x27;</span>, <span class="hljs-string">&#x27;engine-type_ohc&#x27;</span>, <span class="hljs-string">&#x27;num-of-cylinders_twelve&#x27;</span>, <span class="hljs-string">&#x27;fuel-system_spdi&#x27;</span>], dtype=<span class="hljs-keyword">object</span>)<br></code></pre></td></tr></table></figure><h2 id="11-主成分分析-PCA"><a href="#11-主成分分析-PCA" class="headerlink" title="11.主成分分析 (PCA)"></a><strong>11.主成分分析 (PCA)</strong></h2><p>PCA的主要目的是降低高维特征空间的维数。原始特征被重新投影到新的维度（即主成分）。最终目标是找到最能解释数据方差的特征数量。</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stata"># import <span class="hljs-keyword">PCA</span> modulefrom sklearn.decomposition import <span class="hljs-keyword">PCA</span># scaling dataX_scaled = scaler.fit_transform(X)# <span class="hljs-keyword">fit</span> <span class="hljs-keyword">PCA</span> to datapca = <span class="hljs-keyword">PCA</span>()<span class="hljs-keyword">pca</span>.<span class="hljs-keyword">fit</span>(X_scaled)evr = <span class="hljs-keyword">pca</span>.explained_variance_ratio_<br># visualizing the variance explained <span class="hljs-keyword">by</span> each principal componentsplt.figure(figsize=(12, 5))plt.<span class="hljs-keyword">plot</span>(<span class="hljs-keyword">range</span>(0, len(evr)), evr.cumsum(), marker=<span class="hljs-string">&quot;o&quot;</span>, linestyle=<span class="hljs-string">&quot;--&quot;</span>)plt.xlabel(<span class="hljs-string">&quot;Number of components&quot;</span>)plt.ylabel(<span class="hljs-string">&quot;Cumulative explained variance&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/src/640-16561438083509.png" alt="图片"></p><p>20 个主成分解释了超过 80% 的方差，因此可以将模型拟合到这 20 个成分（特征）。可以预先确定方差阈值并选择所需的主成分数量。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>这是对可应用于特征选择的各种技术的有用指南。在拟合模型之前应用了一些技术，例如删除具有缺失值的列、不相关的列、具有多重共线性的列以及使用 PCA 进行降维，而在基本模型实现之后应用其他技术，例如特征系数、p 值、 VIF 等。虽然不会在一个项目中完全使用所有策略，这些策略都是我们进行测试的方向</p><blockquote><p><a href="https://mp.weixin.qq.com/s/GJy-7IEFNvYCnA5cBP4M9AYCnA5cBP4M9A">https://mp.weixin.qq.com/s/GJy-7IEFNvYCnA5cBP4M9AYCnA5cBP4M9A</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python trick</title>
    <link href="/2022/08/01/Python/%E5%A6%99%E7%94%A8/"/>
    <url>/2022/08/01/Python/%E5%A6%99%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="1-重复元素判定"><a href="#1-重复元素判定" class="headerlink" title="1. 重复元素判定"></a><strong>1. 重复元素判定</strong></h2><p>以下方法可以检查给定列表是不是存在重复元素，它会使用 set() 函数来移除所有重复元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">all_unique</span>(<span class="hljs-params">lst</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(lst) == <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(lst))<br>x = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]<br>y = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]<br>all_unique(x) <span class="hljs-comment"># False</span><br>all_unique(y) <span class="hljs-comment"># True</span><br></code></pre></td></tr></table></figure><h2 id="2-字符元素组成判定"><a href="#2-字符元素组成判定" class="headerlink" title="2. 字符元素组成判定"></a><strong>2. 字符元素组成判定</strong></h2><p>检查两个字符串的组成元素是不是一样的。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-keyword">from</span> collections import Counter<br><br>def anagram(<span class="hljs-keyword">first</span>, <span class="hljs-keyword">second</span>):<br><span class="hljs-built_in">    return</span> Counter(<span class="hljs-keyword">first</span>) == Counter(<span class="hljs-keyword">second</span>)<br>anagram(<span class="hljs-string">&quot;abcd3&quot;</span>, <span class="hljs-string">&quot;3acdb&quot;</span>) <span class="hljs-comment"># True</span><br></code></pre></td></tr></table></figure><h2 id="3-内存占用"><a href="#3-内存占用" class="headerlink" title="3. 内存占用"></a><strong>3. 内存占用</strong></h2><p>下面的代码块可以检查变量 variable 所占用的内存。</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-keyword">import</span> sys <br>variable = <span class="hljs-number">30</span> <br><span class="hljs-built_in">print</span>(sys.getsizeof(variable)) <span class="hljs-comment"># 24</span><br></code></pre></td></tr></table></figure><h2 id="4-字节占用"><a href="#4-字节占用" class="headerlink" title="4. 字节占用"></a><strong>4. 字节占用</strong></h2><p>下面的代码块可以检查字符串占用的字节数。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">byte_size</span>(string):<br>    <span class="hljs-built_in">return</span>(<span class="hljs-built_in">len</span>(string.<span class="hljs-built_in">encode</span>(<span class="hljs-string">&#x27;utf-8&#x27;</span>)))<br><br><span class="hljs-built_in">byte_size</span>(<span class="hljs-string">&#x27;😀&#x27;</span>) # <span class="hljs-number">4</span><br><span class="hljs-built_in">byte_size</span>(<span class="hljs-string">&#x27;Hello World&#x27;</span>) # <span class="hljs-number">11</span>   <br></code></pre></td></tr></table></figure><h2 id="5-打印-N-次字符串"><a href="#5-打印-N-次字符串" class="headerlink" title="5. 打印 N 次字符串"></a><strong>5. 打印 N 次字符串</strong></h2><p>该代码块不需要循环语句就能打印 N 次字符串。</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs makefile">n = 2; <br>s =<span class="hljs-string">&quot;Programming&quot;</span>; <br><br>print(s * n);<br><span class="hljs-comment"># ProgrammingProgramming  </span><br></code></pre></td></tr></table></figure><h2 id="6-大写第一个字母"><a href="#6-大写第一个字母" class="headerlink" title="6. 大写第一个字母"></a><strong>6. 大写第一个字母</strong></h2><p>以下代码块会使用 title() 方法，从而大写字符串中每一个单词的首字母。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">s = <span class="hljs-string">&quot;programming is awesome&quot;</span><br><br><span class="hljs-built_in">print</span>(s.title())<br><span class="hljs-comment"># Programming Is Awesome</span><br></code></pre></td></tr></table></figure><h2 id="7-分块"><a href="#7-分块" class="headerlink" title="7. 分块"></a><strong>7. 分块</strong></h2><p>给定具体的大小，定义一个函数以按照这个大小切割列表。</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs processing">from math <span class="hljs-keyword">import</span> <span class="hljs-built_in">ceil</span><br><br>def <span class="hljs-title function_">chunk</span>(lst, <span class="hljs-built_in">size</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-title function_">list</span>(<br>        <span class="hljs-built_in">map</span>(lambda x: lst[x * <span class="hljs-built_in">size</span>:x * <span class="hljs-built_in">size</span> + <span class="hljs-built_in">size</span>],<br>            <span class="hljs-title function_">list</span>(<span class="hljs-title function_">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">ceil</span>(<span class="hljs-title function_">len</span>(lst) / <span class="hljs-built_in">size</span>)))))<br><br><span class="hljs-title function_">chunk</span>([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>],<span class="hljs-number">2</span>)<br># [[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],<span class="hljs-number">5</span>]<br></code></pre></td></tr></table></figure><h2 id="8-压缩"><a href="#8-压缩" class="headerlink" title="8. 压缩"></a><strong>8. 压缩</strong></h2><p>这个方法可以将布尔型的值去掉，例如<code>(False, None, 0, &quot;&quot;)</code>，它使用 <code>filter()</code> 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compact</span>(<span class="hljs-params">lst</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">bool</span>, lst))<br>compact([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-literal">False</span>, <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-number">34</span>])<br><span class="hljs-comment"># [ 1, 2, 3, &#x27;a&#x27;, &#x27;s&#x27;, 34 ]</span><br></code></pre></td></tr></table></figure><h2 id="9-解包"><a href="#9-解包" class="headerlink" title="9. 解包"></a><strong>9. 解包</strong></h2><p>如下代码段可以将打包好的成对列表解开成两组不同的元组。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs prolog">array = [[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>], [<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>], [<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;f&#x27;</span>]]<br>transposed = zip(*array)<br>print(transposed)<br># [(<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>), (<span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;f&#x27;</span>)]<br></code></pre></td></tr></table></figure><h2 id="10-链式对比"><a href="#10-链式对比" class="headerlink" title="10. 链式对比"></a><strong>10. 链式对比</strong></h2><p>我们可以在一行代码中使用不同的运算符对比多个不同的元素。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">a = 3<br><span class="hljs-built_in">print</span>( 2 &lt; a &lt; 8) # <span class="hljs-literal">True</span><br><span class="hljs-built_in">print</span>(1 == a &lt; 2) # <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><h2 id="11-逗号连接"><a href="#11-逗号连接" class="headerlink" title="11. 逗号连接"></a><strong>11. 逗号连接</strong></h2><p>下面的代码可以将列表连接成单个字符串，且每一个元素间的分隔方式设置为了逗号。</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs axapta">hobbies = [<span class="hljs-string">&quot;basketball&quot;</span>, <span class="hljs-string">&quot;football&quot;</span>, <span class="hljs-string">&quot;swimming&quot;</span>]<br><br><span class="hljs-keyword">print</span>(<span class="hljs-string">&quot;My hobbies are: &quot;</span> + <span class="hljs-string">&quot;, &quot;</span>.<span class="hljs-keyword">join</span>(hobbies))<br><span class="hljs-meta"># My hobbies are: basketball, football, swimming</span><br></code></pre></td></tr></table></figure><h2 id="12-元音统计"><a href="#12-元音统计" class="headerlink" title="12. 元音统计"></a><strong>12. 元音统计</strong></h2><p>以下方法将统计字符串中的元音<code>(‘a’, ‘e’, ‘i’, ‘o’, ‘u’)</code>的个数，它是通过正则表达式做的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_vowels</span>(<span class="hljs-params"><span class="hljs-built_in">str</span></span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">len</span>(re.findall(<span class="hljs-string">r&#x27;[aeiou]&#x27;</span>, <span class="hljs-built_in">str</span>, re.IGNORECASE)))<br><br>count_vowels(<span class="hljs-string">&#x27;foobar&#x27;</span>) <span class="hljs-comment"># 3</span><br>count_vowels(<span class="hljs-string">&#x27;gym&#x27;</span>) <span class="hljs-comment"># 0</span><br></code></pre></td></tr></table></figure><h2 id="13-首字母小写"><a href="#13-首字母小写" class="headerlink" title="13. 首字母小写"></a><strong>13. 首字母小写</strong></h2><p>如下方法将令给定字符串的第一个字符统一为小写。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">decapitalize</span>(<span class="hljs-params">string</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">str</span>[:<span class="hljs-number">1</span>].lower() + <span class="hljs-built_in">str</span>[<span class="hljs-number">1</span>:]<br><br>decapitalize(<span class="hljs-string">&#x27;FooBar&#x27;</span>) <span class="hljs-comment"># &#x27;fooBar&#x27;</span><br>decapitalize(<span class="hljs-string">&#x27;FooBar&#x27;</span>) <span class="hljs-comment"># &#x27;fooBar&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="14-展开列表"><a href="#14-展开列表" class="headerlink" title="14. 展开列表"></a><strong>14. 展开列表</strong></h2><p>该方法将通过递归的方式将列表的嵌套展开为单个列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">spread</span>(<span class="hljs-params">arg</span>):<br>    ret = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> arg:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(i, <span class="hljs-built_in">list</span>):<br>            ret.extend(i)<br>        <span class="hljs-keyword">else</span>:<br>            ret.append(i)<br>    <span class="hljs-keyword">return</span> ret<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">deep_flatten</span>(<span class="hljs-params">lst</span>):<br>    result = []<br>    result.extend(<br>        spread(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: deep_flatten(x) <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(x) == <span class="hljs-built_in">list</span> <span class="hljs-keyword">else</span> x, lst))))<br>    <span class="hljs-keyword">return</span> result<br><br>deep_flatten([<span class="hljs-number">1</span>, [<span class="hljs-number">2</span>], [[<span class="hljs-number">3</span>], <span class="hljs-number">4</span>], <span class="hljs-number">5</span>]) <span class="hljs-comment"># [1,2,3,4,5]</span><br></code></pre></td></tr></table></figure><h2 id="15-列表的差"><a href="#15-列表的差" class="headerlink" title="15. 列表的差"></a><strong>15. 列表的差</strong></h2><p>该方法将返回第一个列表的元素，其不在第二个列表内。如果同时要反馈第二个列表独有的元素，还需要加一句 <code>set_b.difference(set_a)</code>。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">def <span class="hljs-built_in">difference</span>(<span class="hljs-keyword">a</span>, b):<br>    set_a = <span class="hljs-built_in">set</span>(<span class="hljs-keyword">a</span>)<br>    set_b = <span class="hljs-built_in">set</span>(b)<br>    comparison = set_a.<span class="hljs-built_in">difference</span>(set_b)<br>    <span class="hljs-literal">return</span> list(comparison)<br><br><span class="hljs-built_in">difference</span>([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>]) <span class="hljs-comment"># [3]</span><br></code></pre></td></tr></table></figure><h2 id="16-通过函数取差"><a href="#16-通过函数取差" class="headerlink" title="16. 通过函数取差"></a><strong>16. 通过函数取差</strong></h2><p>如下方法首先会应用一个给定的函数，然后再返回应用函数后结果有差别的列表元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">difference_by</span>(<span class="hljs-params">a, b, fn</span>):<br>    b = <span class="hljs-built_in">set</span>(<span class="hljs-built_in">map</span>(fn, b))<br>    <span class="hljs-keyword">return</span> [item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> a <span class="hljs-keyword">if</span> fn(item) <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> b]<br><br><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> floor<br>difference_by([<span class="hljs-number">2.1</span>, <span class="hljs-number">1.2</span>], [<span class="hljs-number">2.3</span>, <span class="hljs-number">3.4</span>],floor) <span class="hljs-comment"># [1.2]</span><br>difference_by([&#123; <span class="hljs-string">&#x27;x&#x27;</span>: <span class="hljs-number">2</span> &#125;, &#123; <span class="hljs-string">&#x27;x&#x27;</span>: <span class="hljs-number">1</span> &#125;], [&#123; <span class="hljs-string">&#x27;x&#x27;</span>: <span class="hljs-number">1</span> &#125;], <span class="hljs-keyword">lambda</span> v : v[<span class="hljs-string">&#x27;x&#x27;</span>])<br><span class="hljs-comment"># [ &#123; x: 2 &#125; ]</span><br></code></pre></td></tr></table></figure><h2 id="17-链式函数调用"><a href="#17-链式函数调用" class="headerlink" title="17. 链式函数调用"></a><strong>17. 链式函数调用</strong></h2><p>你可以在一行代码内调用多个函数。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">def <span class="hljs-built_in">add</span>(<span class="hljs-keyword">a</span>, b):<br>    <span class="hljs-literal">return</span> <span class="hljs-keyword">a</span> + b<br><br>def <span class="hljs-built_in">subtract</span>(<span class="hljs-keyword">a</span>, b):<br>    <span class="hljs-literal">return</span> <span class="hljs-keyword">a</span> - b<br><br><span class="hljs-keyword">a</span>, b = <span class="hljs-number">4</span>, <span class="hljs-number">5</span><br>print((<span class="hljs-built_in">subtract</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">a</span> &gt; b <span class="hljs-keyword">else</span> <span class="hljs-built_in">add</span>)(<span class="hljs-keyword">a</span>, b)) <span class="hljs-comment"># 9 </span><br></code></pre></td></tr></table></figure><h2 id="18-检查重复项"><a href="#18-检查重复项" class="headerlink" title="18. 检查重复项"></a><strong>18. 检查重复项</strong></h2><p>如下代码将检查两个列表是不是有重复项。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">has_duplicates</span>(<span class="hljs-params">lst</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(lst) != <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(lst))<br><br>x = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">5</span>]<br>y = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]<br>has_duplicates(x) <span class="hljs-comment"># True</span><br>has_duplicates(y) <span class="hljs-comment"># False</span><br></code></pre></td></tr></table></figure><h2 id="19-合并两个字典"><a href="#19-合并两个字典" class="headerlink" title="19. 合并两个字典"></a><strong>19. 合并两个字典</strong></h2><p>下面的方法将用于合并两个字典。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vim">def merge_two_dicts(<span class="hljs-keyword">a</span>, <span class="hljs-keyword">b</span>):<br>    <span class="hljs-keyword">c</span> = <span class="hljs-keyword">a</span>.<span class="hljs-keyword">copy</span>()   # <span class="hljs-keyword">make</span> <span class="hljs-keyword">a</span> <span class="hljs-keyword">copy</span> of <span class="hljs-keyword">a</span> <br>    <span class="hljs-keyword">c</span>.<span class="hljs-keyword">update</span>(<span class="hljs-keyword">b</span>)    # modify <span class="hljs-built_in">keys</span> <span class="hljs-built_in">and</span> <span class="hljs-built_in">values</span> of <span class="hljs-keyword">a</span> with the ones from <span class="hljs-keyword">b</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">c</span><br><br><span class="hljs-keyword">a</span> = &#123; <span class="hljs-string">&#x27;x&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;y&#x27;</span>: <span class="hljs-number">2</span>&#125;<br><span class="hljs-keyword">b</span> = &#123; <span class="hljs-string">&#x27;y&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;z&#x27;</span>: <span class="hljs-number">4</span>&#125;<br><span class="hljs-keyword">print</span>(merge_two_dicts(<span class="hljs-keyword">a</span>, <span class="hljs-keyword">b</span>))<br># &#123;<span class="hljs-string">&#x27;y&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;x&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;z&#x27;</span>: <span class="hljs-number">4</span>&#125;<br></code></pre></td></tr></table></figure><p>在 Python 3.5 或更高版本中，我们也可以用以下方式合并字典：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs gradle"><span class="hljs-keyword">def</span> merge_dictionaries(a, b)<br>   <span class="hljs-keyword">return</span> &#123;**a, **b&#125;<br><br>a = &#123; <span class="hljs-string">&#x27;x&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;y&#x27;</span>: <span class="hljs-number">2</span>&#125;<br>b = &#123; <span class="hljs-string">&#x27;y&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;z&#x27;</span>: <span class="hljs-number">4</span>&#125;<br><span class="hljs-keyword">print</span>(merge_dictionaries(a, b))<br># &#123;<span class="hljs-string">&#x27;y&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;x&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;z&#x27;</span>: <span class="hljs-number">4</span>&#125;<br></code></pre></td></tr></table></figure><p><strong>20. 将两个列表转化为字典</strong></p><p>如下方法将会把两个列表转化为单个字典。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs vim">def to_dictionary(<span class="hljs-built_in">keys</span>, <span class="hljs-built_in">values</span>):<br>    <span class="hljs-keyword">return</span> dict(zip(<span class="hljs-built_in">keys</span>, <span class="hljs-built_in">values</span>))<br><br><span class="hljs-built_in">keys</span> = [<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>]    <br><span class="hljs-built_in">values</span> = [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br><span class="hljs-keyword">print</span>(to_dictionary(<span class="hljs-built_in">keys</span>, <span class="hljs-built_in">values</span>))<br># &#123;<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;c&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">3</span>&#125;<br></code></pre></td></tr></table></figure><h2 id="21-使用枚举"><a href="#21-使用枚举" class="headerlink" title="21. 使用枚举"></a><strong>21. 使用枚举</strong></h2><p>我们常用 For 循环来遍历某个列表，同样我们也能枚举列表的索引与值。</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs autoit">list = [<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>]<br><span class="hljs-keyword">for</span> index, element <span class="hljs-keyword">in</span> enumerate(list): <br>    print(<span class="hljs-string">&quot;Value&quot;</span>, element, <span class="hljs-string">&quot;Index &quot;</span>, index, )<br><br><span class="hljs-meta"># (<span class="hljs-string">&#x27;Value&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;Index &#x27;</span>, 0)</span><br><span class="hljs-meta"># (<span class="hljs-string">&#x27;Value&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;Index &#x27;</span>, 1)</span><br><span class="hljs-meta">#(<span class="hljs-string">&#x27;Value&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;Index &#x27;</span>, 2)</span><br><span class="hljs-meta"># (<span class="hljs-string">&#x27;Value&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;Index &#x27;</span>, 3)    </span><br></code></pre></td></tr></table></figure><p><strong>22. 执行时间</strong></p><p>如下代码块可以用来计算执行特定代码所花费的时间。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import <span class="hljs-built_in">time</span><br><br>start_time = <span class="hljs-built_in">time</span>.<span class="hljs-built_in">time</span>()<br><br><span class="hljs-keyword">a</span> = <span class="hljs-number">1</span><br>b = <span class="hljs-number">2</span><br>c = <span class="hljs-keyword">a</span> + b<br>print(c) <span class="hljs-comment">#3</span><br><br>end_time = <span class="hljs-built_in">time</span>.<span class="hljs-built_in">time</span>()<br>total_time = end_time - start_time<br>print(<span class="hljs-string">&quot;Time: &quot;</span>, total_time)<br><br><span class="hljs-comment"># (&#x27;Time: &#x27;, 1.1205673217773438e-05)  </span><br></code></pre></td></tr></table></figure><h2 id="23-Try-else"><a href="#23-Try-else" class="headerlink" title="23.Try else"></a><strong>23.Try else</strong></h2><p>我们在使用 try&#x2F;except 语句的时候也可以加一个 else 子句，如果没有触发错误的话，这个子句就会被运行。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">try:<br>    2<span class="hljs-number">*3</span><br>except TypeError:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;An exception was raised&quot;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Thank God, no exceptions were raised.&quot;</span>)<br><br><span class="hljs-comment">#Thank God, no exceptions were raised.</span><br></code></pre></td></tr></table></figure><h2 id="24-元素频率"><a href="#24-元素频率" class="headerlink" title="24. 元素频率"></a><strong>24. 元素频率</strong></h2><p>下面的方法会根据元素频率取列表中最常见的元素。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gams">def most_frequent(list):<br>    return <span class="hljs-built_in">max</span>(<span class="hljs-keyword">set</span>(list), key <span class="hljs-comment">= list.count)</span><br><br>list <span class="hljs-comment">= [1,2,1,2,3,2,1,4,2]</span><br>most_frequent(list)  <br></code></pre></td></tr></table></figure><h2 id="25-回文序列"><a href="#25-回文序列" class="headerlink" title="25. 回文序列"></a><strong>25. 回文序列</strong></h2><p>以下方法会检查给定的字符串是不是回文序列，它首先会把所有字母转化为小写，并移除非英文字母符号。最后，它会对比字符串与反向字符串是否相等，相等则表示为回文序列。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">def palindrome(<span class="hljs-keyword">string</span>):<br>    <span class="hljs-built_in">from</span> re import sub<br>    s = sub(<span class="hljs-string">&#x27;[\W_]&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-keyword">string</span>.<span class="hljs-built_in">lower</span>())<br>    <span class="hljs-literal">return</span> s == s[::<span class="hljs-number">-1</span>]<br><br>palindrome(<span class="hljs-string">&#x27;taco cat&#x27;</span>) <span class="hljs-comment"># True</span><br></code></pre></td></tr></table></figure><h2 id="26-不使用-if-else-的计算子"><a href="#26-不使用-if-else-的计算子" class="headerlink" title="26. 不使用 if-else 的计算子"></a><strong>26. 不使用 if-else 的计算子</strong></h2><p>这一段代码可以不使用条件语句就实现加减乘除、求幂操作，它通过字典这一数据结构实现：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-keyword">import</span> <span class="hljs-keyword">operator</span><br>action = &#123;<br>    <span class="hljs-string">&quot;+&quot;</span>: <span class="hljs-keyword">operator</span>.add,<br>    <span class="hljs-string">&quot;-&quot;</span>: <span class="hljs-keyword">operator</span>.sub,<br>    <span class="hljs-string">&quot;/&quot;</span>: <span class="hljs-keyword">operator</span>.truediv,<br>    <span class="hljs-string">&quot;*&quot;</span>: <span class="hljs-keyword">operator</span>.mul,<br>    <span class="hljs-string">&quot;**&quot;</span>: pow<br>&#125;<br><span class="hljs-built_in">print</span>(action[<span class="hljs-string">&#x27;-&#x27;</span>](<span class="hljs-number">50</span>, <span class="hljs-number">25</span>)) # <span class="hljs-number">25</span><br></code></pre></td></tr></table></figure><h2 id="27-Shuffle"><a href="#27-Shuffle" class="headerlink" title="27.Shuffle"></a><strong>27.Shuffle</strong></h2><p>该算法会打乱列表元素的顺序，它主要会通过 Fisher-Yates 算法对新列表进行排序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> copy <span class="hljs-keyword">import</span> deepcopy<br><span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> randint<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">shuffle</span>(<span class="hljs-params">lst</span>):<br>    temp_lst = deepcopy(lst)<br>    m = <span class="hljs-built_in">len</span>(temp_lst)<br>    <span class="hljs-keyword">while</span> (m):<br>        m -= <span class="hljs-number">1</span><br>        i = randint(<span class="hljs-number">0</span>, m)<br>        temp_lst[m], temp_lst[i] = temp_lst[i], temp_lst[m]<br>    <span class="hljs-keyword">return</span> temp_lst<br><br><br>foo = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]<br>shuffle(foo) <span class="hljs-comment"># [2,3,1] , foo = [1,2,3]</span><br></code></pre></td></tr></table></figure><h2 id="28-展开列表"><a href="#28-展开列表" class="headerlink" title="28. 展开列表"></a><strong>28. 展开列表</strong></h2><p>将列表内的所有元素，包括子列表，都展开成一个列表。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">def</span> spread(arg):<br>    <span class="hljs-attribute">ret</span> =<span class="hljs-meta"> []</span><br>    <span class="hljs-attribute">for</span> i in arg:<br>        <span class="hljs-attribute">if</span> isinstance(i, list):<br>            <span class="hljs-attribute">ret</span>.extend(i)<br>        <span class="hljs-attribute">else</span>:<br>            <span class="hljs-attribute">ret</span>.append(i)<br>    <span class="hljs-attribute">return</span> ret<br><br><span class="hljs-attribute">spread</span>([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>],<span class="hljs-number">8</span>,<span class="hljs-number">9</span>]) #<span class="hljs-meta"> [1,2,3,4,5,6,7,8,9]</span><br></code></pre></td></tr></table></figure><h2 id="29-交换值"><a href="#29-交换值" class="headerlink" title="29. 交换值"></a><strong>29. 交换值</strong></h2><p>不需要额外的操作就能交换两个变量的值。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">def</span> swap(a, b):<br>  <span class="hljs-attribute">return</span> b, a<br><br><span class="hljs-attribute">a</span>, b = -<span class="hljs-number">1</span>, <span class="hljs-number">14</span><br><span class="hljs-attribute">swap</span>(a, b) # (<span class="hljs-number">14</span>, -<span class="hljs-number">1</span>)<br><span class="hljs-attribute">spread</span>([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>],<span class="hljs-number">8</span>,<span class="hljs-number">9</span>]) #<span class="hljs-meta"> [1,2,3,4,5,6,7,8,9]</span><br></code></pre></td></tr></table></figure><h2 id="30-字典默认值"><a href="#30-字典默认值" class="headerlink" title="30. 字典默认值"></a><strong>30. 字典默认值</strong></h2><p>通过 Key 取对应的 Value 值，可以通过以下方式设置默认值。如果 get() 方法没有设置默认值，那么如果遇到不存在的 Key，则会返回 None。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">d = &#123;<span class="hljs-string">&#x27;a&#x27;</span>: 1, <span class="hljs-string">&#x27;b&#x27;</span>: 2&#125;<br><br><span class="hljs-built_in">print</span>(d.<span class="hljs-built_in">get</span>(<span class="hljs-string">&#x27;c&#x27;</span>, 3)) # 3<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch Note</title>
    <link href="/2022/08/01/Python/Pytorch/"/>
    <url>/2022/08/01/Python/Pytorch/</url>
    
    <content type="html"><![CDATA[<h1 id="记录关于torch的用法"><a href="#记录关于torch的用法" class="headerlink" title="记录关于torch的用法"></a>记录关于torch的用法</h1><h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">optim.SGD([<br>                &#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.base.parameters()&#125;,<br>                &#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.classifier.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1e-3</span>&#125;<br>            ], lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p>优化器需要注意的是， 需要有个参数对象可用于优化。然后设置一些优化特定选项，例如：学习率、权值衰减等等，上述的写法是一种指定层不同的学习状态。这意味着<code>model.base</code>的参数将使用 的默认学习率<code>1e-2</code>， <code>model.classifier</code>的参数将使用 的学习率<code>1e-3</code>，并且所有参数都使用 的动量 <code>0.9</code>。</p><p>关于优化器常用的操作，也就是<code>Optimizer.load_state_dict</code>、<code>Optimizer.state_dict</code>、<code>Optimizer.step</code>、<code>Optimizer.zero_grad</code></p><h2 id="torch-cuda"><a href="#torch-cuda" class="headerlink" title="torch.cuda"></a>torch.cuda</h2><ol><li><code>torch.cuda.is_available()</code>判断GPU是否可用</li><li><code>torch.cuda.device_count()</code>     查看可用GPU数量</li><li><code>torch.cuda.current_device()</code>   查看当前使用的GPU序号</li><li><code>torch.cuda.get_device_capability(device)</code> 查看指定GPU的容量</li><li><code>torch.cuda.get_device_name(device)</code>         查看指定GPU的名称</li><li><code>torch.cuda.empty_cache()</code>       清空程序占用的GPU资源</li><li><code>torch.cuda.manual_seed(seed)</code> &#x2F; <code>torch.cuda.manual_seed_all(seed)</code> 为GPU设置随机种子</li></ol><h2 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a>torch.autograd</h2><p>torch.autograd 提供了实现任意标量值函数的自动微分的类和函数。它只需要对现有的代码做最小的改动–你只需要用require_grad&#x3D;True关键字来声明需要计算梯度的Tensor s。目前，我们只支持浮点张量类型（half, float, double and bfloat16）和复杂张量类型（cfloat, cdouble）的autograd。</p><ul><li><code>torch.autograd.function</code> （函数的反向传播）</li><li><code>torch.autograd.functional</code> （计算图的反向传播）</li><li><code>torch.autograd.gradcheck</code> （数值梯度检查）</li><li><code>torch.autograd.anomaly_mode</code> （在自动求导时检测错误产生路径）</li><li><code>torch.autograd.grad_mode</code> （设置是否需要梯度）</li><li><code>model.eval() </code>与 <code>torch.no_grad()</code></li><li><code>torch.autograd.profiler</code> （提供 function 级别的统计信息）</li></ul>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python Pyod</title>
    <link href="/2022/08/01/Python/Pyod/"/>
    <url>/2022/08/01/Python/Pyod/</url>
    
    <content type="html"><![CDATA[<h1 id="PYOD"><a href="#PYOD" class="headerlink" title="PYOD"></a>PYOD</h1><p> PyOD is the most comprehensive and scalable <strong>Python toolkit</strong> for <strong>detecting outlying objects</strong> in multivariate data. </p><p>link：<a href="https://github.com/yzhao062/pyod">https://github.com/yzhao062/pyod</a></p><p>使用SUOD框架可以加速实现</p><p>调用pyod</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train an ECOD detector</span><br><span class="hljs-keyword">from</span> pyod.models.ecod <span class="hljs-keyword">import</span> ECOD<br>clf = ECOD()<br>clf.fit(X_train)<br><br><span class="hljs-comment"># get outlier scores</span><br>y_train_scores = clf.decision_scores_  <span class="hljs-comment"># raw outlier scores on the train data</span><br>y_test_scores = clf.decision_function(X_test)  <span class="hljs-comment"># predict raw outlier scores on test</span><br></code></pre></td></tr></table></figure></blockquote><p><strong>Individual Detection Algorithms</strong> :</p><table><thead><tr><th>Type</th><th>Abbr</th><th>Algorithm</th><th>Year</th><th>Ref</th></tr></thead><tbody><tr><td>Probabilistic</td><td>ECOD</td><td>Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions</td><td>2022</td><td>[<a href="https://github.com/yzhao062/pyod#li2021ecod">23]</a></td></tr><tr><td>Probabilistic</td><td>ABOD</td><td>Angle-Based Outlier Detection</td><td>2008</td><td>[<a href="https://github.com/yzhao062/pyod#kriegel2008angle">17]</a></td></tr><tr><td>Probabilistic</td><td>FastABOD</td><td>Fast Angle-Based Outlier Detection using approximation</td><td>2008</td><td>[<a href="https://github.com/yzhao062/pyod#kriegel2008angle">17]</a></td></tr><tr><td>Probabilistic</td><td>COPOD</td><td>COPOD: Copula-Based Outlier Detection</td><td>2020</td><td>[<a href="https://github.com/yzhao062/pyod#li2020copod">22]</a></td></tr><tr><td>Probabilistic</td><td>MAD</td><td>Median Absolute Deviation (MAD)</td><td>1993</td><td>[<a href="https://github.com/yzhao062/pyod#iglewicz1993how">14]</a></td></tr><tr><td>Probabilistic</td><td>SOS</td><td>Stochastic Outlier Selection</td><td>2012</td><td>[<a href="https://github.com/yzhao062/pyod#janssens2012stochastic">15]</a></td></tr><tr><td>Probabilistic</td><td>KDE</td><td>Outlier Detection with Kernel Density Functions</td><td>2007</td><td>[<a href="https://github.com/yzhao062/pyod#latecki2007outlier">19]</a></td></tr><tr><td>Probabilistic</td><td>Sampling</td><td>Rapid distance-based outlier detection via sampling</td><td>2013</td><td>[<a href="https://github.com/yzhao062/pyod#sugiyama2013rapid">34]</a></td></tr><tr><td>Linear Model</td><td>PCA</td><td>Principal Component Analysis (the sum of weighted projected distances to the eigenvector hyperplanes)</td><td>2003</td><td>[<a href="https://github.com/yzhao062/pyod#shyu2003a">33]</a></td></tr><tr><td>Linear Model</td><td>MCD</td><td>Minimum Covariance Determinant (use the mahalanobis distances as the outlier scores)</td><td>1999</td><td>[<a href="https://github.com/yzhao062/pyod#hardin2004outlier">12]</a> [<a href="https://github.com/yzhao062/pyod#rousseeuw1999a">30]</a></td></tr><tr><td>Linear Model</td><td>CD</td><td>Use Cook’s distance for outlier detection</td><td>1977</td><td>[<a href="https://github.com/yzhao062/pyod#cook1977detection">9]</a></td></tr><tr><td>Linear Model</td><td>OCSVM</td><td>One-Class Support Vector Machines</td><td>2001</td><td>[<a href="https://github.com/yzhao062/pyod#scholkopf2001estimating">32]</a></td></tr><tr><td>Linear Model</td><td>LMDD</td><td>Deviation-based Outlier Detection (LMDD)</td><td>1996</td><td>[<a href="https://github.com/yzhao062/pyod#arning1996a">6]</a></td></tr><tr><td>Proximity-Based</td><td>LOF</td><td>Local Outlier Factor</td><td>2000</td><td>[<a href="https://github.com/yzhao062/pyod#breunig2000lof">7]</a></td></tr><tr><td>Proximity-Based</td><td>COF</td><td>Connectivity-Based Outlier Factor</td><td>2002</td><td>[<a href="https://github.com/yzhao062/pyod#tang2002enhancing">35]</a></td></tr><tr><td>Proximity-Based</td><td>(Incremental) COF</td><td>Memory Efficient Connectivity-Based Outlier Factor (slower but reduce storage complexity)</td><td>2002</td><td>[<a href="https://github.com/yzhao062/pyod#tang2002enhancing">35]</a></td></tr><tr><td>Proximity-Based</td><td>CBLOF</td><td>Clustering-Based Local Outlier Factor</td><td>2003</td><td>[<a href="https://github.com/yzhao062/pyod#he2003discovering">13]</a></td></tr><tr><td>Proximity-Based</td><td>LOCI</td><td>LOCI: Fast outlier detection using the local correlation integral</td><td>2003</td><td>[<a href="https://github.com/yzhao062/pyod#papadimitriou2003loci">26]</a></td></tr><tr><td>Proximity-Based</td><td>HBOS</td><td>Histogram-based Outlier Score</td><td>2012</td><td>[<a href="https://github.com/yzhao062/pyod#goldstein2012histogram">10]</a></td></tr><tr><td>Proximity-Based</td><td>kNN</td><td>k Nearest Neighbors (use the distance to the kth nearest neighbor as the outlier score)</td><td>2000</td><td>[<a href="https://github.com/yzhao062/pyod#ramaswamy2000efficient">29]</a></td></tr><tr><td>Proximity-Based</td><td>AvgKNN</td><td>Average kNN (use the average distance to k nearest neighbors as the outlier score)</td><td>2002</td><td>[<a href="https://github.com/yzhao062/pyod#angiulli2002fast">5]</a></td></tr><tr><td>Proximity-Based</td><td>MedKNN</td><td>Median kNN (use the median distance to k nearest neighbors as the outlier score)</td><td>2002</td><td>[<a href="https://github.com/yzhao062/pyod#angiulli2002fast">5]</a></td></tr><tr><td>Proximity-Based</td><td>SOD</td><td>Subspace Outlier Detection</td><td>2009</td><td>[<a href="https://github.com/yzhao062/pyod#kriegel2009outlier">18]</a></td></tr><tr><td>Proximity-Based</td><td>ROD</td><td>Rotation-based Outlier Detection</td><td>2020</td><td>[<a href="https://github.com/yzhao062/pyod#almardeny2020a">4]</a></td></tr><tr><td>Outlier Ensembles</td><td>IForest</td><td>Isolation Forest</td><td>2008</td><td>[<a href="https://github.com/yzhao062/pyod#liu2008isolation">24]</a></td></tr><tr><td>Outlier Ensembles</td><td>FB</td><td>Feature Bagging</td><td>2005</td><td>[<a href="https://github.com/yzhao062/pyod#lazarevic2005feature">20]</a></td></tr><tr><td>Outlier Ensembles</td><td>LSCP</td><td>LSCP: Locally Selective Combination of Parallel Outlier Ensembles</td><td>2019</td><td>[<a href="https://github.com/yzhao062/pyod#zhao2019lscp">38]</a></td></tr><tr><td>Outlier Ensembles</td><td>XGBOD</td><td>Extreme Boosting Based Outlier Detection <strong>(Supervised)</strong></td><td>2018</td><td>[<a href="https://github.com/yzhao062/pyod#zhao2018xgbod">37]</a></td></tr><tr><td>Outlier Ensembles</td><td>LODA</td><td>Lightweight On-line Detector of Anomalies</td><td>2016</td><td>[<a href="https://github.com/yzhao062/pyod#pevny2016loda">27]</a></td></tr><tr><td>Outlier Ensembles</td><td>SUOD</td><td>SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection <strong>(Acceleration)</strong></td><td>2021</td><td>[<a href="https://github.com/yzhao062/pyod#zhao2021suod">39]</a></td></tr><tr><td>Neural Networks</td><td>AutoEncoder</td><td>Fully connected AutoEncoder (use reconstruction error as the outlier score)</td><td></td><td>[<a href="https://github.com/yzhao062/pyod#aggarwal2015outlier">1]</a> [Ch.3]</td></tr><tr><td>Neural Networks</td><td>VAE</td><td>Variational AutoEncoder (use reconstruction error as the outlier score)</td><td>2013</td><td>[<a href="https://github.com/yzhao062/pyod#kingma2013auto">16]</a></td></tr><tr><td>Neural Networks</td><td>Beta-VAE</td><td>Variational AutoEncoder (all customized loss term by varying gamma and capacity)</td><td>2018</td><td>[<a href="https://github.com/yzhao062/pyod#burgess2018understanding">8]</a></td></tr><tr><td>Neural Networks</td><td>SO_GAAL</td><td>Single-Objective Generative Adversarial Active Learning</td><td>2019</td><td>[<a href="https://github.com/yzhao062/pyod#liu2019generative">25]</a></td></tr><tr><td>Neural Networks</td><td>MO_GAAL</td><td>Multiple-Objective Generative Adversarial Active Learning</td><td>2019</td><td>[<a href="https://github.com/yzhao062/pyod#liu2019generative">25]</a></td></tr><tr><td>Neural Networks</td><td>DeepSVDD</td><td>Deep One-Class Classification</td><td>2018</td><td>[<a href="https://github.com/yzhao062/pyod#ruff2018deep">31]</a></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python Pip using</title>
    <link href="/2022/08/01/Python/pip/"/>
    <url>/2022/08/01/Python/pip/</url>
    
    <content type="html"><![CDATA[<h1 id="pip——package-installer-for-python"><a href="#pip——package-installer-for-python" class="headerlink" title="pip——package installer for python"></a>pip——package installer for python</h1><p>这是一个用于对python的第三方库进行安装、更新、卸载等操作的包管理工具。十分方便</p><p><strong>「注：因为pip是一个命令行程序，所以pip一般都在命令行中执行各种操作」</strong></p><h3 id="1、安装pip"><a href="#1、安装pip" class="headerlink" title="1、安装pip"></a>1、安装pip</h3><p>从python3.4开始， pip已经默认安装到python内了，无需再次安装。若没有可以使用一下两种方式安装</p><ol><li>输入<code>easy_install pip</code></li><li>在<code>https://pypi.org/project/pip/#files</code>中下载好，然后安装到python scripts目录中，执行语句<code>python setup.py install</code>进行安装</li></ol><h3 id="2、查看pip版本"><a href="#2、查看pip版本" class="headerlink" title="2、查看pip版本"></a>2、查看pip版本</h3><p><code>pip --version</code></p><p><img src="/src/image-20220511201910084.png" alt="image-20220511201910084"></p><h3 id="3、升级pip"><a href="#3、升级pip" class="headerlink" title="3、升级pip"></a>3、升级pip</h3><p>如果pip的版本太低，可以升级当前版本<br><code>pip install --upgrade pip</code></p><h3 id="4、获取帮助"><a href="#4、获取帮助" class="headerlink" title="4、获取帮助"></a>4、获取帮助</h3><p>想了解如何使用pip，以及pip有哪些功能，执行下面语句可以获取详细教程:<br><code>pip help</code></p><h3 id="5、安装库"><a href="#5、安装库" class="headerlink" title="5、安装库"></a>5、安装库</h3><p>使用pip安装第三方库，执行下面语句<br><code>pip install package_name</code></p><p>指定package版本：<br><code>pip install package_name==1.1.2</code></p><p>比如说，我要安装3.4.1版本的matplotlib<br><code>pip install matplotlib==3.4.1</code></p><h3 id="6、批量安装库"><a href="#6、批量安装库" class="headerlink" title="6、批量安装库"></a>6、批量安装库</h3><p>如果一个项目需要安装很多库，那可以批量安装：<br><code>pip install -r requirements.txt</code></p><h3 id="7、使用wheel文件安装库"><a href="#7、使用wheel文件安装库" class="headerlink" title="7、使用wheel文件安装库"></a>7、使用wheel文件安装库</h3><p>这种方法适合离线安装，wheel文件是库的源文件，可以下载后放到本地安装。</p><p>步骤如下：</p><p>(1) 在下面网站里找相应库的.whl文件<br><a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/">https://www.lfd.uci.edu/~gohlke/pythonlibs/</a></p><p>(2) 下载.whl文件，注意对应的版本</p><p><img src="/src/640.png" alt="图片"></p><p>(3) 在.whl所在文件夹内，按Shift键+鼠标右键，打开CMD窗口或者PowerShell</p><p>(4) 输入命令：<br><code>pip install matplotlib‑3.4.1‑cp39‑cp39‑win_amd64.whl</code><br>即可完成安装</p><h3 id="8、卸载库"><a href="#8、卸载库" class="headerlink" title="8、卸载库"></a>8、卸载库</h3><p>安装好的库可以再卸载：<br><code>pip uninstall package_name</code></p><h3 id="9、升级库"><a href="#9、升级库" class="headerlink" title="9、升级库"></a>9、升级库</h3><p>对当前库进行版本升级:<br><code>pip install --upgrade package_name</code></p><h3 id="10、查看库信息"><a href="#10、查看库信息" class="headerlink" title="10、查看库信息"></a>10、查看库信息</h3><p><code>pip show -f package_name</code></p><h3 id="11、查看已安装的库"><a href="#11、查看已安装的库" class="headerlink" title="11、查看已安装的库"></a>11、查看已安装的库</h3><p>列出所有已安装的第三方库和对应版本<br><code>pip list</code></p><p><img src="/src/image-20220511202253586.png" alt="image-20220511202253586"></p><h3 id="12、将库列表保存到指定文件中"><a href="#12、将库列表保存到指定文件中" class="headerlink" title="12、将库列表保存到指定文件中"></a>12、将库列表保存到指定文件中</h3><p>把已经安装的库信息保存到到本地txt文件中：<br><code>pip freeze &gt; requirements.txt</code></p><p><img src="/src/image-20220511202414513-165694142759286.png" alt="image-20220511202414513"></p><h3 id="13、查看需要升级的库"><a href="#13、查看需要升级的库" class="headerlink" title="13、查看需要升级的库"></a>13、查看需要升级的库</h3><p>目前已经安装的库中，看哪些需要版本升级<br><code>pip list -o</code></p><p><img src="/src/image-20220511202717260-165694142975989.png" alt="image-20220511202717260"></p><h3 id="14、检查兼容问题"><a href="#14、检查兼容问题" class="headerlink" title="14、检查兼容问题"></a>14、检查兼容问题</h3><p>验证已安装的库是否有兼容依赖问题<br><code>pip check package-name</code></p><p><img src="/src/image-20220511202756903-165694143139792.png" alt="image-20220511202756903"></p><h3 id="15、下载库到本地"><a href="#15、下载库到本地" class="headerlink" title="15、下载库到本地"></a>15、下载库到本地</h3><p>将库下载到本地指定文件，保存为whl格式<br><code>pip download package_name -d &quot;要保存的文件路径&quot;</code></p><h3 id="附：更换pip源"><a href="#附：更换pip源" class="headerlink" title="附：更换pip源"></a>附：更换pip源</h3><p>很多人抱怨pip安装库有些时候太慢了，那是pip源的问题。</p><p>前面说过pip从PyPi中下载库文件，但由于PyPi服务器在国外，访问起来很慢。</p><p>但国内提供了很多镜像源，用来替代PyPi，像清华源、豆瓣源、阿里云源等。</p><p>这些镜像源备份了PyPi里的数据，由于服务器在国内，速度会快很多。</p><p>但镜像源数据有滞后性，比如说清华源的pypi 镜像每 5 分钟同步一次。</p><p>使用镜像源有两种方式，以清华源为例：</p><p>(1) 临时使用</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple <span class="hljs-keyword">some</span>-<span class="hljs-keyword">package</span> <span class="hljs-title">matplotlib</span><br></code></pre></td></tr></table></figure><p>除了matplotlib是要安装的库名外，其他都是固定格式</p><p>(2) 设为默认</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">pip config <span class="hljs-keyword">set</span> <span class="hljs-keyword">global</span>.<span class="hljs-built_in">index</span>-url https://pypi.tuna.tsinghua.edu.<span class="hljs-keyword">cn</span>/simple<br></code></pre></td></tr></table></figure><p>设为默认后，以后安装库都是从清华源下载，而且无需再加镜像源网址</p><p>附主流镜像源地址</p><p>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple/">http://mirrors.aliyun.com/pypi/simple/</a><br>中国科技大学：<a href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a><br>华中理工大学：<a href="http://pypi.hustunique.com/">http://pypi.hustunique.com/</a><br>山东理工大学：<a href="http://pypi.sdutlinux.org/">http://pypi.sdutlinux.org/</a><br>豆瓣：<a href="http://pypi.douban.com/simple/.com/simple/">http://pypi.douban.com/simple/.com/simple/</a></p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pandas duplicated data dealing</title>
    <link href="/2022/08/01/Python/Pandas%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86/"/>
    <url>/2022/08/01/Python/Pandas%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>本次来介绍重复值处理的常用方法。</p><hr><p>重复值处理主要涉及两个部分，一个是找出重复值，第二个是删除重复值，也就是根据自己设定的条件进行删除操作。</p><h2 id="定位重复值"><a href="#定位重复值" class="headerlink" title="定位重复值"></a>定位重复值</h2><p>对于重复值，我们首先需要查看这些重复值是什么样的形式，然后确定删除的范围，而查询重复值需要用到<code>duplicated</code>函数。</p><p><code>duplicated</code>的返回值是布尔值，返回<code>True</code>和<code>False</code>，默认情况下会按照一行的所有内容进行查重。</p><p>主要参数：</p><ul><li><p><code>subset</code>：如果不按照全部内容查重，那么需要指定按照哪些列进行查重。比如按照姓名进行查重<code>subset=[&#39;name&#39;]</code>，那么具有相同名字的人就只会保留一个，但很可能只是重名的原因，而并非真正同一个人，所以可以按照姓名和出生日期两列查重，<code>subset=[&#39;name&#39;,&#39;birthday&#39;]</code>，同理还可以再添加列，这样就可以基本保证去重效果了。</p></li><li><p><code>keep</code>：用来确定要标记的重复值，可以设置为<code>first</code>、<code>last</code>、<code>False</code>。</p></li><li><ul><li><code>first</code>：除第一次出现的重复值，其他都标记为True</li><li><code>last</code>：除最后一次出现的重复值，其他都标记为True</li><li><code>False</code>：所有重复值都标记为True</li></ul></li></ul><p>实例：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">import pandas as pd<br>import numpy as np<br><br>data = &#123;<br><span class="hljs-code">    &#x27;user&#x27; : [&#x27;zszxz&#x27;,&#x27;zszxz&#x27;,&#x27;rose&#x27;],</span><br><span class="hljs-code">    &#x27;price&#x27; : [100, 200, -300],</span><br><span class="hljs-code">    &#x27;hobby&#x27; : [&#x27;reading&#x27;,&#x27;reading&#x27;,&#x27;hiking&#x27;]</span><br>&#125;<br>frame  = pd.DataFrame(data)<br><span class="hljs-section">print(frame)</span><br><span class="hljs-section">------------------------</span><br><span class="hljs-code">    user  price    hobby</span><br>0  zszxz    100  reading<br>1  zszxz    200  reading<br><span class="hljs-section">2   rose   -300   hiking</span><br><span class="hljs-section">------------------------</span><br><br><span class="hljs-section">frame.duplicated()</span><br><span class="hljs-section">----------</span><br>0    False<br>1    False<br>2    False<br><span class="hljs-section">dtype: bool</span><br><span class="hljs-section">-----------</span><br></code></pre></td></tr></table></figure><p>上面提到<code>duplicated</code>返回布尔值，所以如果要想输出这些重复值，还需要和查询的方法配合使用<code>df[df.duplicated()]</code>，比如：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 1、按user变量筛选重复值<br>frame[frame.duplicated(subset=[<span class="hljs-emphasis">&#x27;user&#x27;</span>])]<br><span class="hljs-code">-------------------</span><br><span class="hljs-code">  user price hobby</span><br><span class="hljs-code">1 zszxz 200 reading</span><br><span class="hljs-code">-------------------</span><br></code></pre></td></tr></table></figure><p>上面按<code>user</code>一个变量进行查重，但没有设置<code>keep</code>参数，所以默认筛选出除了第一个以外的其它重复值。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 2、按user变量筛选重复值,保留全部重复值<br>frame[frame.duplicated(subset=[<span class="hljs-emphasis">&#x27;user&#x27;</span>], keep=False)]<br><span class="hljs-code">-------------------</span><br><span class="hljs-code">  user price hobby</span><br><span class="hljs-code">0 zszxz 100 reading</span><br><span class="hljs-code">1 zszxz 200 reading</span><br><span class="hljs-code">-------------------</span><br></code></pre></td></tr></table></figure><p>上面按<code>user</code>一个变量进行查重，并设置<code>keep</code>参数为<code>False</code>，所以保留了全部的重复值。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 3、按user和hobby变量筛选重复值,筛选出除最后一个重复值以外的其它重复值<br>frame[frame.duplicated(subset=[<span class="hljs-emphasis">&#x27;user&#x27;</span>,<span class="hljs-emphasis">&#x27;hobby&#x27;</span>], keep=<span class="hljs-emphasis">&#x27;last&#x27;</span>)]<br><span class="hljs-code">-------------------</span><br><span class="hljs-code">  user price hobby</span><br><span class="hljs-code">0 zszxz 100 reading</span><br><span class="hljs-code">-------------------</span><br></code></pre></td></tr></table></figure><p>上面按<code>user</code>和<code>hobby</code>两个变量进行查重，并设置<code>keep</code>参数为<code>last</code>，所以筛选出了除最后一个重复值以外的其它重复值。</p><p>通过两个参数的设置就可以查看自己想要的重复值了，以此判断要删除哪个，保留哪个。</p><h2 id="删除重复值"><a href="#删除重复值" class="headerlink" title="删除重复值"></a>删除重复值</h2><p>当确定好需要删除的重复值后，就进行进行删除的操作了。</p><p>删除重复值会用到<code>drop_duplicates</code>函数。</p><p>和<code>duplicated()</code>函数参数类似，主要有3个参数：</p><ul><li><p><code>subset</code>：同<code>duplicated()</code>，设置去重的字段</p></li><li><p><code>keep</code>: 这里稍有不同，<code>duplicated()</code>中是将除设置值以外重复值都返回<code>True</code>，而这里是保留的意思。同样可以设置<code>first</code>、<code>last</code>、<code>False</code></p></li><li><ul><li><code>first</code>：保留第一次出现的重复行，删除其他重复行</li><li><code>last</code>：保留最后一次出现的重复行，删除其他重复行</li><li><code>False</code>：删除所有重复行</li></ul></li><li><p><code>inplace</code>：布尔值，默认为<code>False</code>，是否直接在原数据上删除重复项或删除重复项后返回副本。</p></li></ul><p>实例：</p><p><strong>1、全部去重</strong></p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 按全部字段删除，在原数据frame上生效<br>frame.drop_duplicates(inplace=True)<br><span class="hljs-section">print(frame)</span><br><span class="hljs-section">------------------------</span><br><span class="hljs-code">    user  price    hobby</span><br>0  zszxz    100  reading<br>1  zszxz    200  reading<br><span class="hljs-section">2   rose   -300   hiking</span><br><span class="hljs-section">------------------------</span><br></code></pre></td></tr></table></figure><p>因为上面数据中没有全部重复的，因此没有可删除行。</p><p><strong>2、指定列去重</strong></p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 按user字段删除，在原数据frame上生效<br>frame.drop_duplicates(subset=[<span class="hljs-emphasis">&#x27;user&#x27;</span>],inplace=True)<br><span class="hljs-section">print(frame)</span><br><span class="hljs-section">------------------------</span><br><span class="hljs-code">    user  price    hobby</span><br>0  zszxz    100  reading<br><span class="hljs-section">2   rose   -300   hiking</span><br><span class="hljs-section">------------------------</span><br></code></pre></td></tr></table></figure><p>上面按<code>user</code>字段删除重复行，保留第一个重复行，因此第二行被删除了。但这里大家注意下，<strong>执行删除重复行操作后，表的索引也会被删掉。</strong></p><p>如需要重置可以加上<code>reset_index()</code>，设置<code>drop=True</code>，用索引替代被打乱的索引。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">frame.drop<span class="hljs-emphasis">_duplicates(subset=[&#x27;user&#x27;],inplace=True)</span><br><span class="hljs-emphasis">frame.reset_</span>index(drop=True)<br><span class="hljs-code">------------------------</span><br><span class="hljs-code">    user  price    hobby</span><br><span class="hljs-code">0  zszxz    100  reading</span><br><span class="hljs-code">1   rose   -300   hiking</span><br><span class="hljs-code">------------------------</span><br></code></pre></td></tr></table></figure><p><code>keep</code>默认为<code>first</code>，下面手动设置为<code>last</code>，只保留最后一个重复行。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 按全部字段删除，在原数据frame上生效<br>frame.drop_duplicates(subset=[<span class="hljs-emphasis">&#x27;user&#x27;</span>,<span class="hljs-emphasis">&#x27;hobby&#x27;</span>],keep=<span class="hljs-emphasis">&#x27;last&#x27;</span>,inplace=True)<br><span class="hljs-section">print(frame)</span><br><span class="hljs-section">------------------------</span><br><span class="hljs-code">    user  price    hobby</span><br>1  zszxz    200  reading<br><span class="hljs-section">2   rose   -300   hiking</span><br><span class="hljs-section">------------------------</span><br></code></pre></td></tr></table></figure><p><code>keep</code>手动设置为<code>False</code>，全部删除，这种一般很少用。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 按全部字段删除，在原数据frame上生效<br>frame.drop_duplicates(subset=[<span class="hljs-emphasis">&#x27;user&#x27;</span>,<span class="hljs-emphasis">&#x27;hobby&#x27;</span>],keep=False,inplace=True)<br><span class="hljs-section">print(frame)</span><br><span class="hljs-section">------------------------</span><br><span class="hljs-code">    user  price    hobby</span><br><span class="hljs-section">2   rose   -300   hiking</span><br><span class="hljs-section">------------------------</span><br></code></pre></td></tr></table></figure><p>以上就是重复值相关的所有操作。</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>在删除重复值时，要注意下删除的逻辑。</p><p>因为很多时候我们需要把这些离线的清洗操作在线上复现。</p><p>如果我们随机地删除重复行，没有明确的逻辑，那么对于这种随机性线上是无法复现的，即无法保证清洗后的数据一致性。</p><p><strong>所以我们在删除重复行前，可以把重复判断字段进行排序处理。</strong></p><p>比如上面例子中，如果要对<code>user</code>和<code>price</code>去重，那么比较严谨的做法是按照<code>user</code>和<code>price</code>进行排序。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">frame.sort_values(by=[<span class="hljs-emphasis">&#x27;user&#x27;</span>,<span class="hljs-emphasis">&#x27;price&#x27;</span>],ascending=True).reset_index(drop=True)<br><span class="hljs-code">--------------------</span><br><span class="hljs-code">  user price hobby</span><br><span class="hljs-code">0 rose -300 hiking</span><br><span class="hljs-code">1 zszxz 100 reading</span><br><span class="hljs-code">2 zszxz 200 reading</span><br><span class="hljs-code">--------------------</span><br></code></pre></td></tr></table></figure><p>因为有了排序性，只要按这个逻辑它的顺序是固定的，而不是随机的。所以无论我们设置<code>keep</code>为<code>first</code>还是<code>last</code>，都没有任何影响。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pandas  losing data deal</title>
    <link href="/2022/08/01/Python/pandas%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/"/>
    <url>/2022/08/01/Python/pandas%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="一、缺失值类型"><a href="#一、缺失值类型" class="headerlink" title="一、缺失值类型"></a>一、缺失值类型</h2><p>在<code>pandas</code>中，缺失数据显示为<strong>NaN</strong>。缺失值有3种表示方法，<code>np.nan</code>，<code>none</code>，<code>pd.NA</code>。</p><h3 id="1、np-nan"><a href="#1、np-nan" class="headerlink" title="1、np.nan"></a>1、np.nan</h3><p>缺失值有个特点（坑），它不等于任何值，连自己都不相等。如果用<code>nan</code>和任何其它值比较都会返回<code>nan</code>。</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs arcade">np.<span class="hljs-literal">nan</span> == np.<span class="hljs-literal">nan</span><br>&gt;&gt; <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>也正由于这个特点，在数据集读入以后，不论列是什么类型的数据，默认的缺失值全为<code>np.nan</code>。</p><p><strong>因为<code>nan</code>在<code>Numpy</code>中的类型是浮点，因此整型列会转为浮点；而字符型由于无法转化为浮点型，只能归并为object类型（’O’），原来是浮点型的则类型不变。</strong></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">type</span><span class="hljs-params">(np.nan)</span></span><br>&gt;&gt; <span class="hljs-attribute">float</span><br>pd<span class="hljs-selector-class">.Series</span>(<span class="hljs-selector-attr">[1,2,3]</span>)<span class="hljs-selector-class">.dtype</span><br>&gt;&gt; <span class="hljs-built_in">dtype</span>(<span class="hljs-string">&#x27;int64&#x27;</span>)<br>pd<span class="hljs-selector-class">.Series</span>(<span class="hljs-selector-attr">[1,np.nan,3]</span>)<span class="hljs-selector-class">.dtype</span><br>&gt;&gt; <span class="hljs-built_in">dtype</span>(<span class="hljs-string">&#x27;float64&#x27;</span>)<br></code></pre></td></tr></table></figure><p>初学者做数据处理遇见object类型会发懵，不知道这是个啥，明明是字符型，导入后就变了，其实是因为缺失值导致的。</p><p>除此之外，还要介绍一种针对时间序列的缺失值，它是单独存在的，用<strong>NaT</strong>表示，是<code>pandas</code>的内置类型，**可以视为时间序列版的<code>np.nan</code>**，也是与自己不相等。</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs subunit">s_time = pd.Series([pd.Timestamp(&#x27;20220101&#x27;)]*3)<br>s_time<br>&gt;&gt; 0 2022<span class="hljs-string">-01</span><span class="hljs-string">-01</span><br>   1 2022<span class="hljs-string">-01</span><span class="hljs-string">-01</span><br>   2 2022<span class="hljs-string">-01</span><span class="hljs-string">-01</span><br>   dtype:datetime64[ns]<br>-----------------<br>s_time[2] = pd.NaT<br>s_time<br>&gt;&gt; 0 2022<span class="hljs-string">-01</span><span class="hljs-string">-01</span><br>   1 2022<span class="hljs-string">-01</span><span class="hljs-string">-01</span><br>   2 NaT<br>   dtype:datetime64[ns]<br></code></pre></td></tr></table></figure><h3 id="2、None"><a href="#2、None" class="headerlink" title="2、None"></a>2、None</h3><p>还有一种就是<code>None</code>，它要比<code>nan</code>好那么一点，因为它至少自己与自己相等。</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">None</span> <span class="hljs-operator">=</span><span class="hljs-operator">=</span> None<br>&gt;&gt; True<br></code></pre></td></tr></table></figure><p>在传入数值类型后，会自动变为<code>np.nan</code>。</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">type</span>(pd.<span class="hljs-type">Series</span>([1,<span class="hljs-type">None</span>])[1])<br>&gt;&gt; numpy.float64<br></code></pre></td></tr></table></figure><p>只有当传入<code>object</code>类型时是不变的，因此可以认为如果不是人工命名为<code>None</code>的话，它基本不会自动出现在<code>pandas</code>中，所以<code>None</code>大家基本也看不到。</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">type</span>(pd.<span class="hljs-type">Series</span>([1,<span class="hljs-type">None</span>],dtype=&#x27;<span class="hljs-type">O</span>&#x27;)[1])<br>&gt;&gt; <span class="hljs-type">NoneType</span><br></code></pre></td></tr></table></figure><h3 id="3、NA标量"><a href="#3、NA标量" class="headerlink" title="3、NA标量"></a>3、NA标量</h3><p>pandas1.0以后的版本中引入了一个专门表示缺失值的标量<strong>pd.NA</strong>，它代表空整数、空布尔值、空字符，这个功能目前处于实验阶段。</p><p>开发者也注意到了这点，对于不同数据类型采取不同的缺失值表示会很乱。pd.NA就是为了统一而存在的。<strong>pd.NA的目标是提供一个缺失值指示器，可以在各种数据类型中一致使用(而不是np.nan、None或者NaT分情况使用)。</strong></p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-title">s_new</span> = pd.<span class="hljs-type">Series</span>([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], d<span class="hljs-keyword">type</span>=&quot;<span class="hljs-type">Int64</span>&quot;)<br><span class="hljs-title">s_new</span><br>&gt;&gt; <span class="hljs-number">0</span>   <span class="hljs-number">1</span><br>   <span class="hljs-number">1</span>   <span class="hljs-number">2</span><br>   d<span class="hljs-keyword">type</span>: <span class="hljs-type">Int64</span><br><span class="hljs-comment">-----------------</span><br><span class="hljs-title">s_new</span>[<span class="hljs-number">1</span>] = pd.<span class="hljs-type">NaT</span><br><span class="hljs-title">s_new</span><br>&gt;&gt; <span class="hljs-number">0</span>    <span class="hljs-number">1</span><br>   <span class="hljs-number">1</span>  &lt;<span class="hljs-type">NA</span>&gt;<br>   d<span class="hljs-keyword">type</span>: <span class="hljs-type">Int64</span><br></code></pre></td></tr></table></figure><p>同理，对于布尔型、字符型一样不会改变原有数据类型,这样就解决了原来动不动就变成<code>object</code>类型的麻烦了。</p><p>下面是pd.NA的一些常用算术运算和比较运算的示例：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">##### 算术运算<br># 加法<br>pd.NA + 1<br><span class="hljs-section">&gt;&gt; &lt;NA&gt;</span><br><span class="hljs-section">-----------</span><br># 乘法<br>&quot;a&quot; * pd.NA<br><span class="hljs-section">&gt;&gt; &lt;NA&gt;</span><br><span class="hljs-section">-----------</span><br># 以下两种其中结果为1<br>pd.NA <span class="hljs-strong">** 0</span><br><span class="hljs-strong">&gt;&gt; 1</span><br><span class="hljs-strong">-----------</span><br><span class="hljs-strong">1 **</span> pd.NA<br>&gt;&gt; 1<br><br>##### 比较运算<br>pd.NA == pd.NA<br><span class="hljs-section">&gt;&gt; &lt;NA&gt;</span><br><span class="hljs-section">-----------</span><br>pd.NA &lt; 2.5<br><span class="hljs-section">&gt;&gt; &lt;NA&gt;</span><br><span class="hljs-section">-----------</span><br>np.log(pd.NA)<br><span class="hljs-section">&gt;&gt; &lt;NA&gt;</span><br><span class="hljs-section">-----------</span><br>np.add(pd.NA, 1)<br>&gt;&gt; &lt;NA&gt;<br></code></pre></td></tr></table></figure><h2 id="二、缺失值判断"><a href="#二、缺失值判断" class="headerlink" title="二、缺失值判断"></a>二、缺失值判断</h2><p>了解了缺失值的几种形式后，我们要知道如何判断缺失值。对于一个<code>dataframe</code>而言，判断缺失的主要方法就是<code>isnull()</code>或者<code>isna()</code>，这两个方法会直接返回<code>True</code>和<code>False</code>的布尔值。可以是对整个<code>dataframe</code>或者某个列。</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs nsis">df = pd.DataFrame(&#123;<br>      <span class="hljs-string">&#x27;A&#x27;</span>:[<span class="hljs-string">&#x27;a1&#x27;</span>,<span class="hljs-string">&#x27;a1&#x27;</span>,<span class="hljs-string">&#x27;a2&#x27;</span>,<span class="hljs-string">&#x27;a3&#x27;</span>],<br>      <span class="hljs-string">&#x27;B&#x27;</span>:[<span class="hljs-string">&#x27;b1&#x27;</span>,<span class="hljs-literal">None</span>,<span class="hljs-string">&#x27;b2&#x27;</span>,<span class="hljs-string">&#x27;b3&#x27;</span>],<br>      <span class="hljs-string">&#x27;C&#x27;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],<br>      <span class="hljs-string">&#x27;D&#x27;</span>:[<span class="hljs-number">5</span>,<span class="hljs-literal">None</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>]&#125;)<br><span class="hljs-comment"># 将无穷设置为缺失值      </span><br>pd.options.mode.use_inf_as_na = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><h3 id="1、对整个dataframe判断缺失"><a href="#1、对整个dataframe判断缺失" class="headerlink" title="1、对整个dataframe判断缺失"></a>1、对整个dataframe判断缺失</h3><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-variable">df</span><span class="hljs-operator">.</span><span class="hljs-variable">isnull</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">)</span><br><span class="hljs-operator">&gt;&gt;</span> <span class="hljs-variable">A</span> <span class="hljs-variable">B</span> <span class="hljs-built_in">C</span> <span class="hljs-built_in">D</span><br><span class="hljs-number">0</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span><br><span class="hljs-number">1</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">True</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">True</span><br><span class="hljs-number">2</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span> <span class="hljs-built_in">False</span><br></code></pre></td></tr></table></figure><h3 id="2、对某个列判断缺失"><a href="#2、对某个列判断缺失" class="headerlink" title="2、对某个列判断缺失"></a>2、对某个列判断缺失</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">df[<span class="hljs-string">&#x27;C&#x27;</span>].<span class="hljs-keyword">isnull</span>()<br>&gt;&gt; <span class="hljs-number">0</span>    <span class="hljs-keyword">False</span><br>   <span class="hljs-number">1</span>    <span class="hljs-keyword">False</span><br>   <span class="hljs-number">2</span>    <span class="hljs-keyword">False</span><br>   <span class="hljs-number">3</span>    <span class="hljs-keyword">False</span><br><span class="hljs-type">Name</span>: C, dtype: <span class="hljs-type">bool</span><br></code></pre></td></tr></table></figure><p>如果想取非缺失可以用<code>notna()</code>，使用方法是一样的，结果相反。</p><h2 id="三、缺失值统计"><a href="#三、缺失值统计" class="headerlink" title="三、缺失值统计"></a>三、缺失值统计</h2><h3 id="1、列缺失"><a href="#1、列缺失" class="headerlink" title="1、列缺失"></a>1、列缺失</h3><p>一般我们会对一个<code>dataframe</code>的<strong>列</strong>进行缺失统计，查看每个列有多少缺失，如果缺失率过高再进行删除或者插值等操作。那么直接在上面的<code>isnull()</code>返回的结果上直接应用<code>.sum()</code>即可，<code>axis</code>默认等于0，0是列，1是行。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-comment">## 列缺失统计</span><br>isnull<span class="hljs-params">()</span><span class="hljs-string">.sum</span><span class="hljs-params">(<span class="hljs-attr">axis</span>=0)</span><br></code></pre></td></tr></table></figure><h3 id="2、行缺失"><a href="#2、行缺失" class="headerlink" title="2、行缺失"></a>2、行缺失</h3><p>但是很多情况下，我们也需要对<strong>行</strong>进行缺失值判断。比如一行数据可能一个值都没有，如果这个样本进入模型，会造成很大的干扰。因此，行列两个缺失率通常都要查看并统计。</p><p>操作很简单，只需要在<code>sum()</code>中设置<code>axis=1</code>即可。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-comment">## 行缺失统计</span><br>isnull<span class="hljs-params">()</span><span class="hljs-string">.sum</span><span class="hljs-params">(<span class="hljs-attr">axis</span>=1)</span><br></code></pre></td></tr></table></figure><h3 id="3、缺失率"><a href="#3、缺失率" class="headerlink" title="3、缺失率"></a>3、缺失率</h3><p>有时我不仅想要知道缺失的数量，我更想知道缺失的比例，即缺失率。正常可能会想到用上面求得数值再比上总行数。但其实这里有个小技巧可以一步就实现。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-comment">## 缺失率</span><br>df.isnull<span class="hljs-params">()</span><span class="hljs-string">.sum</span><span class="hljs-params">(<span class="hljs-attr">axis</span>=0)</span><span class="hljs-string">/df.shape</span>[0]<br><br><span class="hljs-comment">## 缺失率（一步到位）</span><br>isnull<span class="hljs-params">()</span><span class="hljs-string">.mean</span><span class="hljs-params">()</span><br></code></pre></td></tr></table></figure><h2 id="四、缺失值筛选"><a href="#四、缺失值筛选" class="headerlink" title="四、缺失值筛选"></a>四、缺失值筛选</h2><p>筛选需要loc配合完成，对于行和列的缺失筛选如下：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 筛选有缺失值的行<br>df.loc[df.isnull().any(1)]<br>&gt;&gt; A B C D<br><span class="hljs-section">1 a1 None 2 NaN</span><br><span class="hljs-section">-----------------</span><br># 筛选有缺失值的列<br>df.loc[:,df.isnull().any()]<br>&gt;&gt; B D<br>0 b1 5.0<br>1 None NaN<br>2 b2 9.0<br>3 b3 10.0<br></code></pre></td></tr></table></figure><p>如果要查询没有缺失值的行和列，可以对表达式用取反<code>~</code>操作：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">df.loc[~(df.isnull().any(<span class="hljs-number">1</span>))]<br>&gt;&gt; A <span class="hljs-keyword">B </span>C D<br><span class="hljs-number">0</span> <span class="hljs-built_in">a1</span> <span class="hljs-keyword">b1 </span><span class="hljs-number">1</span> <span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-number">2</span> <span class="hljs-built_in">a2</span> <span class="hljs-keyword">b2 </span><span class="hljs-number">3</span> <span class="hljs-number">9</span>.<span class="hljs-number">0</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">a3</span> <span class="hljs-keyword">b3 </span><span class="hljs-number">4</span> <span class="hljs-number">10</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>上面使用了<code>any</code>判断只要有缺失就进行筛选，也可以用<code>all</code>判断是否全部缺失，同样可以对行里进行判断，如果整列或者整行都是缺失值，那么这个变量或者样本就失去了分析的意义，可以考虑删除。</p><h2 id="五、缺失值填充"><a href="#五、缺失值填充" class="headerlink" title="五、缺失值填充"></a>五、缺失值填充</h2><p>一般我们对缺失值有两种处理方法，一种是直接删除，另外一种是保留并填充。下面先介绍填充的方法<code>fillna</code>。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 将dataframe所有缺失值填充为0<br>df.fillna(0)<br>&gt;&gt; A B C D<br>0 a1 b1 1 5.0<br>1 a1 0 2 0.0<br>2 a2 b2 3 9.0<br><span class="hljs-section">3 a3 b3 4 10.0</span><br><span class="hljs-section">--------------</span><br># 将D列缺失值填充为-999<br>df.D.fillna(<span class="hljs-emphasis">&#x27;-999&#x27;</span>)<br>&gt;&gt; 0       5<br><span class="hljs-code">   1    -999</span><br><span class="hljs-code">   2       9</span><br><span class="hljs-code">   3      10</span><br>Name: D, dtype: object<br></code></pre></td></tr></table></figure><p>方法很简单，但使用时需要注意一些参数。</p><ul><li>inplace：可以设置<code>fillna(0, inplace=True)</code>来让填充生效，原dataFrame被填充。</li><li>methond：可以设置<code>methond</code>方法来实现向前或者向后填充，<code>pad/ffill</code>为向前填充，<code>bfill/backfill</code>为向后填充，比如<code>df.fillna(methond=&#39;ffill&#39;)</code>，也可以简写为<code>df.ffill()</code>。</li></ul><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">df.ffill()<br>&gt;&gt; A <span class="hljs-keyword">B </span>C D<br><span class="hljs-number">0</span> <span class="hljs-built_in">a1</span> <span class="hljs-keyword">b1 </span><span class="hljs-number">1</span> <span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-number">1</span> <span class="hljs-built_in">a1</span> <span class="hljs-keyword">b1 </span><span class="hljs-number">2</span> <span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-number">2</span> <span class="hljs-built_in">a2</span> <span class="hljs-keyword">b2 </span><span class="hljs-number">3</span> <span class="hljs-number">9</span>.<span class="hljs-number">0</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">a3</span> <span class="hljs-keyword">b3 </span><span class="hljs-number">4</span> <span class="hljs-number">10</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>原缺失值都会按照前一个值来填充(B列1行，D列1行)。</p><p>除了用前后值来填充，也可以用整个列的均值来填充，比如对D列的其它非缺失值的平均值8来填充缺失值。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">df<span class="hljs-selector-class">.D</span><span class="hljs-selector-class">.fillna</span>(df<span class="hljs-selector-class">.D</span><span class="hljs-selector-class">.mean</span>())<br>&gt;&gt; <span class="hljs-number">0</span>     <span class="hljs-number">5.0</span><br>   <span class="hljs-number">1</span>     <span class="hljs-number">8.0</span><br>   <span class="hljs-number">2</span>     <span class="hljs-number">9.0</span><br>   <span class="hljs-number">3</span>    <span class="hljs-number">10.0</span><br>Name: D, dtype: float64<br></code></pre></td></tr></table></figure><h2 id="六、缺失值删除"><a href="#六、缺失值删除" class="headerlink" title="六、缺失值删除"></a>六、缺失值删除</h2><p>删除缺失值也非情况，比如是全删除还是删除比较高缺失率，这个要看自己的容忍程度，真实的数据必然会存在缺失的，这个无法避免。而且缺失在某些情况下也代表了一定的含义，要视情况而定。</p><h3 id="1、全部直接删除"><a href="#1、全部直接删除" class="headerlink" title="1、全部直接删除"></a>1、全部直接删除</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-comment"># 全部直接删除</span><br>df.dropna()<br>&gt;&gt; A <span class="hljs-keyword">B </span>C D<br><span class="hljs-number">0</span> <span class="hljs-built_in">a1</span> <span class="hljs-keyword">b1 </span><span class="hljs-number">1</span> <span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-number">2</span> <span class="hljs-built_in">a2</span> <span class="hljs-keyword">b2 </span><span class="hljs-number">3</span> <span class="hljs-number">9</span>.<span class="hljs-number">0</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">a3</span> <span class="hljs-keyword">b3 </span><span class="hljs-number">4</span> <span class="hljs-number">10</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h3 id="2、行缺失删除"><a href="#2、行缺失删除" class="headerlink" title="2、行缺失删除"></a>2、行缺失删除</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-comment"># 行缺失删除</span><br>df.dropna(axis=<span class="hljs-number">0</span>)<br>&gt;&gt; A <span class="hljs-keyword">B </span>C D<br><span class="hljs-number">0</span> <span class="hljs-built_in">a1</span> <span class="hljs-keyword">b1 </span><span class="hljs-number">1</span> <span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-number">2</span> <span class="hljs-built_in">a2</span> <span class="hljs-keyword">b2 </span><span class="hljs-number">3</span> <span class="hljs-number">9</span>.<span class="hljs-number">0</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">a3</span> <span class="hljs-keyword">b3 </span><span class="hljs-number">4</span> <span class="hljs-number">10</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h3 id="3、列缺失删除"><a href="#3、列缺失删除" class="headerlink" title="3、列缺失删除"></a>3、列缺失删除</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-comment"># 列缺失删除</span><br>df.dropna(axis=<span class="hljs-number">1</span>)<br>&gt;&gt; A C<br><span class="hljs-number">0</span> <span class="hljs-built_in">a1</span> <span class="hljs-number">1</span><br><span class="hljs-number">1</span> <span class="hljs-built_in">a1</span> <span class="hljs-number">2</span><br><span class="hljs-number">2</span> <span class="hljs-built_in">a2</span> <span class="hljs-number">3</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">a3</span> <span class="hljs-number">4</span><br>-------------<br><span class="hljs-comment"># 删除指定列范围内的缺失,因为C列无缺失，所以最后没有变化</span><br>df.dropna(<span class="hljs-keyword">subset=[&#x27;C&#x27;])</span><br><span class="hljs-keyword"></span>&gt;&gt; A <span class="hljs-keyword">B </span>C D<br><span class="hljs-number">0</span> <span class="hljs-built_in">a1</span> <span class="hljs-keyword">b1 </span><span class="hljs-number">1</span> <span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-number">1</span> <span class="hljs-built_in">a1</span> None <span class="hljs-number">2</span> NaN<br><span class="hljs-number">2</span> <span class="hljs-built_in">a2</span> <span class="hljs-keyword">b2 </span><span class="hljs-number">3</span> <span class="hljs-number">9</span>.<span class="hljs-number">0</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">a3</span> <span class="hljs-keyword">b3 </span><span class="hljs-number">4</span> <span class="hljs-number">10</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h3 id="4、按缺失率删除"><a href="#4、按缺失率删除" class="headerlink" title="4、按缺失率删除"></a>4、按缺失率删除</h3><p>这个可以考虑用筛选的方法来实现，比如要删除列缺失大于0.1的（即筛选小于0.1的）。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">df.loc[:,df.isnull().mean(axis=<span class="hljs-number">0</span>) &lt; <span class="hljs-number">0</span>.<span class="hljs-number">1</span>]<br>&gt;&gt; A C<br><span class="hljs-number">0</span> <span class="hljs-built_in">a1</span> <span class="hljs-number">1</span><br><span class="hljs-number">1</span> <span class="hljs-built_in">a1</span> <span class="hljs-number">2</span><br><span class="hljs-number">2</span> <span class="hljs-built_in">a2</span> <span class="hljs-number">3</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">a3</span> <span class="hljs-number">4</span><br>-------------<br><span class="hljs-comment"># 删除行缺失大于0.1的</span><br>df.loc[df.isnull().mean(axis=<span class="hljs-number">1</span>) &lt; <span class="hljs-number">0</span>.<span class="hljs-number">1</span>]<br>&gt;&gt; A <span class="hljs-keyword">B </span>C D<br><span class="hljs-number">0</span> <span class="hljs-built_in">a1</span> <span class="hljs-keyword">b1 </span><span class="hljs-number">1</span> <span class="hljs-number">5</span>.<span class="hljs-number">0</span><br><span class="hljs-number">2</span> <span class="hljs-built_in">a2</span> <span class="hljs-keyword">b2 </span><span class="hljs-number">3</span> <span class="hljs-number">9</span>.<span class="hljs-number">0</span><br><span class="hljs-number">3</span> <span class="hljs-built_in">a3</span> <span class="hljs-keyword">b3 </span><span class="hljs-number">4</span> <span class="hljs-number">10</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h2 id="七、缺失值参与计算"><a href="#七、缺失值参与计算" class="headerlink" title="七、缺失值参与计算"></a>七、缺失值参与计算</h2><p>如果不对缺失值处理，那么缺失值会按照什么逻辑进行计算呢？</p><p>下面我们一起看一下各种运算下缺失值的参与逻辑。</p><p><strong>1、加法</strong></p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">df<br>&gt;&gt;A B C D<br>0 a1 b1 1 5.0<br>1 a1 None 2 NaN<br>2 a2 b2 3 9.0<br><span class="hljs-section">3 a3 b3 4 10.0</span><br><span class="hljs-section">---------------</span><br># 对所有列求和<br>df.sum()<br>&gt;&gt; A    a1a1a2a3<br><span class="hljs-code">   C          10</span><br><span class="hljs-code">   D          24</span><br></code></pre></td></tr></table></figure><p>可以看到，加法是会忽略缺失值的。</p><p><strong>2、累加</strong></p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 对D列进行累加<br>df.D.cumsum()<br>&gt;&gt; 0     5.0<br><span class="hljs-code">   1     NaN</span><br><span class="hljs-code">   2    14.0</span><br><span class="hljs-code">   3    24.0</span><br><span class="hljs-section">Name: D, dtype: float64</span><br><span class="hljs-section">---------------</span><br>df.D.cumsum(skipna=False)<br>&gt;&gt; 0    5.0<br><span class="hljs-code">   1    NaN</span><br><span class="hljs-code">   2    NaN</span><br><span class="hljs-code">   3    NaN</span><br>Name: D, dtype: float64<br></code></pre></td></tr></table></figure><p><code>cumsum</code>累加会忽略NA，但值会保留在列中，可以使用<code>skipna=False</code>跳过有缺失值的计算并返回缺失值。</p><p><strong>3、计数</strong></p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs axapta"><span class="hljs-meta"># 对列计数</span><br>df.<span class="hljs-keyword">count</span>()<br>&gt;&gt; A    <span class="hljs-number">4</span><br>   B    <span class="hljs-number">3</span><br>   C    <span class="hljs-number">4</span><br>   D    <span class="hljs-number">3</span><br>dtype: <span class="hljs-built_in">int64</span><br></code></pre></td></tr></table></figure><p>缺失值不进入计数范围里。</p><p><strong>4、聚合分组</strong></p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">df.groupby(<span class="hljs-emphasis">&#x27;B&#x27;</span>).sum()<br>&gt;&gt; C D<br>B  <br>b1 1 5.0<br>b2 3 9.0<br><span class="hljs-section">b3 4 10.0</span><br><span class="hljs-section">---------------</span><br>df.groupby(<span class="hljs-emphasis">&#x27;B&#x27;</span>,dropna=False).sum()<br>&gt;&gt; C D<br>B  <br>b1 1 5.0<br>b2 3 9.0<br>b3 4 10.0<br>NaN 2 0.0<br></code></pre></td></tr></table></figure><p>聚合时会默认忽略缺失值，如果要缺失值计入到分组里，可以设置<code>dropna=False</code>。这个用法和其它比如<code>value_counts</code>是一样的，有的时候需要看缺失值的数量。</p><p>以上就是所有关于缺失值的常用操作了，从理解缺失值的3种表现形式开始，到缺失值判断、统计、处理、计算等。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pandas trick</title>
    <link href="/2022/08/01/Python/pandas%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    <url>/2022/08/01/Python/pandas%E5%B0%8F%E6%8A%80%E5%B7%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="Pandas-数据统计包的-6-种高效函数"><a href="#Pandas-数据统计包的-6-种高效函数" class="headerlink" title="Pandas 数据统计包的 6 种高效函数"></a><strong>Pandas 数据统计包的 6 种高效函数</strong></h1><p>Pandas 也是一个 Python 包，它提供了快速、灵活以及具有显著表达能力的数据结构，旨在使处理结构化 (表格化、多维、异构) 和时间序列数据变得既简单又直观 。</p><p><img src="/%5Csrc%5C640.webp" alt="图片"></p><p>Pandas 适用于以下各类数据:</p><ul><li>具有异构类型列的表格数据，如 SQL 表或 Excel 表；</li><li>有序和无序 (不一定是固定频率) 的时间序列数据；</li><li>带有行&#x2F;列标签的任意矩阵数据（同构类型或者是异构类型）；</li><li>其他任意形式的统计数据集。事实上，数据根本不需要标记就可以放入 Pandas 结构中。</li></ul><p>Pandas 擅长处理的类型如下所示：</p><ul><li>容易处理浮点数据和非浮点数据中的 缺失数据（用 NaN 表示）；</li><li>大小可调整性: 可以从 DataFrame 或者更高维度的对象中插入或者是删除列；</li><li>显式数据可自动对齐: 对象可以显式地对齐至一组标签内，或者用户可以简单地选择忽略标签，使 Series、 DataFrame 等自动对齐数据；</li><li>灵活的分组功能，对数据集执行拆分-应用-合并等操作，对数据进行聚合和转换；</li><li>简化将数据转换为 DataFrame 对象的过程，而这些数据基本是 Python 和 NumPy 数据结构中不规则、不同索引的数据；</li><li>基于标签的智能切片、索引以及面向大型数据集的子设定；</li><li>更加直观地合并以及连接数据集；</li><li>更加灵活地重塑、转置（pivot）数据集；</li><li>轴的分级标记 (可能包含多个标记)；</li><li>具有鲁棒性的 IO 工具，用于从平面文件 (CSV 和 delimited)、 Excel 文件、数据库中加在数据，以及从 HDF5 格式中保存 &#x2F; 加载数据；</li><li>时间序列的特定功能: 数据范围的生成以及频率转换、移动窗口统计、数据移动和滞后等。</li></ul><h2 id="read-csv-nrows-x3D-n"><a href="#read-csv-nrows-x3D-n" class="headerlink" title="read_csv(nrows&#x3D;n)"></a><strong>read_csv(nrows&#x3D;n)</strong></h2><p>大多数人都会犯的一个错误是，在不需要.csv 文件的情况下仍会完整地读取它。如果一个未知的.csv 文件有 10GB，那么读取整个.csv 文件将会非常不明智，不仅要占用大量内存，还会花很多时间。我们需要做的只是从.csv 文件中导入几行，之后根据需要继续导入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> io<br><span class="hljs-keyword">import</span> requests<span class="hljs-comment"># I am using this online data set just to make things easier for you guys</span><br>url = <span class="hljs-string">&quot;https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv&quot;</span><br>s = requests.get(url).content<span class="hljs-comment"># read only first 10 rows</span><br>df = pd.read_csv(io.StringIO(s.decode( utf-<span class="hljs-number">8</span> )),nrows=<span class="hljs-number">10</span> , index_col=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h2 id="map"><a href="#map" class="headerlink" title="map()"></a><strong>map()</strong></h2><p>map( ) 函数根据相应的输入来映射 Series 的值。用于将一个 Series 中的每个值替换为另一个值，该值可能来自一个函数、也可能来自于一个 dict 或 Series。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># create a dataframe</span><br>dframe = pd.DataFrame(np.random.randn(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>), columns=<span class="hljs-built_in">list</span>( bde ), index=[ India ,  USA ,  China ,  Russia ])<span class="hljs-comment">#compute a formatted string from each floating point value in frame</span><br>changefn = <span class="hljs-keyword">lambda</span> x:  %<span class="hljs-number">.2</span>f  % x<span class="hljs-comment"># Make changes element-wise</span><br>dframe[ d ].<span class="hljs-built_in">map</span>(changefn)<br></code></pre></td></tr></table></figure><h2 id="apply"><a href="#apply" class="headerlink" title="apply()"></a><strong>apply()</strong></h2><p>apply() 允许用户传递函数，并将其应用于 Pandas 序列中的每个值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># max minus mix lambda fn</span><br>fn = <span class="hljs-keyword">lambda</span> x: x.<span class="hljs-built_in">max</span>() - x.<span class="hljs-built_in">min</span>()<span class="hljs-comment"># Apply this on dframe that we ve just created above</span><br>dframe.apply(fn)<br></code></pre></td></tr></table></figure><h2 id="isin"><a href="#isin" class="headerlink" title="isin()"></a><strong>isin()</strong></h2><p>lsin () 用于过滤数据帧。Isin () 有助于选择特定列中具有特定（或多个）值的行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Using the dataframe we created for read_csv</span><br>filter1 = df[<span class="hljs-string">&quot;value&quot;</span>].isin([<span class="hljs-number">112</span>]) <br>filter2 = df[<span class="hljs-string">&quot;time&quot;</span>].isin([<span class="hljs-number">1949.000000</span>])<br>df [filter1 &amp; filter2]<br></code></pre></td></tr></table></figure><h2 id="copy"><a href="#copy" class="headerlink" title="copy()"></a><strong>copy()</strong></h2><p>Copy () 函数用于复制 Pandas 对象。当一个数据帧分配给另一个数据帧时，如果对其中一个数据帧进行更改，另一个数据帧的值也将发生更改。为了防止这类问题，可以使用 copy () 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># creating sample series </span><br>data = pd.Series([ India ,  Pakistan ,  China ,  Mongolia ])<span class="hljs-comment"># Assigning issue that we face</span><br>data1= data<br><span class="hljs-comment"># Change a value</span><br>data1[<span class="hljs-number">0</span>]= USA <br><span class="hljs-comment"># Also changes value in old dataframe</span><br>data<span class="hljs-comment"># To prevent that, we use</span><br><span class="hljs-comment"># creating copy of series </span><br>new = data.copy()<span class="hljs-comment"># assigning new values </span><br>new[<span class="hljs-number">1</span>]= Changed value <span class="hljs-comment"># printing data </span><br><span class="hljs-built_in">print</span>(new) <br><span class="hljs-built_in">print</span>(data)<br></code></pre></td></tr></table></figure><h2 id="select-dtypes"><a href="#select-dtypes" class="headerlink" title="select_dtypes()"></a><strong>select_dtypes()</strong></h2><p>select_dtypes() 的作用是，基于 dtypes 的列返回数据帧列的一个子集。这个函数的参数可设置为包含所有拥有特定数据类型的列，亦或者设置为排除具有特定数据类型的列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># We ll use the same dataframe that we used for read_csv</span><br>framex =  df.select_dtypes(include=<span class="hljs-string">&quot;float64&quot;</span>)<span class="hljs-comment"># Returns only time column</span><br></code></pre></td></tr></table></figure><p>最后，pivot_table( ) 也是 Pandas 中一个非常有用的函数。如果对 pivot_table( ) 在 excel 中的使用有所了解，那么就非常容易上手了。</p><pre><code class="python"># Create a sample dataframeschool = pd.DataFrame(&#123; A : [ Jay ,  Usher ,  Nicky ,  Romero ,  Will ],        B : [ Masters ,  Graduate ,  Graduate ,  Masters ,  Graduate ],        C : [26, 22, 20, 23, 24]&#125;)# Lets create a pivot table to segregate students based on age and coursetable = pd.pivot_table(school, values = A , index =[ B ,  C ],                          columns =[ B ], aggfunc = np.sum, fill_value=&quot;Not Available&quot;) table</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pandas using</title>
    <link href="/2022/08/01/Python/pandas%E5%A4%84%E7%90%862/"/>
    <url>/2022/08/01/Python/pandas%E5%A4%84%E7%90%862/</url>
    
    <content type="html"><![CDATA[<h2 id="导入工具包"><a href="#导入工具包" class="headerlink" title="导入工具包"></a>导入工具包</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs clean"># pandas 和numpy是两个基础的工具包<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br># matplotlib seaborn是作图工具包<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br># 通过os设置默认路径<br><span class="hljs-keyword">import</span> os<br>os.chdir(<span class="hljs-string">&#x27;C:/Users/用户/Desktop/&#x27;</span>)  # 桌面的路径<br># 图表中文显示问题<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>]=[<span class="hljs-string">&#x27;SimHei&#x27;</span>] #用来正常显示中文标签<br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="hljs-literal">False</span> #用来正常显示负号<br># 不显示预警<br><span class="hljs-keyword">import</span> warnings  <br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="读取excel数据"><a href="#读取excel数据" class="headerlink" title="读取excel数据"></a>读取excel数据</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 读取文件</span><br><span class="hljs-built_in">df</span> = pd.read_excel(<span class="hljs-string">&#x27;文件.xlsx&#x27;</span>)<br><span class="hljs-comment">#　读取文件同时筛选需要的列</span><br><span class="hljs-built_in">df</span> = pd.read_excel(<span class="hljs-string">&#x27;文件.xlsx&#x27;</span>)[[<span class="hljs-string">&#x27;&#x27;</span>,<span class="hljs-string">&#x27;&#x27;</span>]] <span class="hljs-comment"># 读取并筛选几列</span><br><span class="hljs-comment"># 读取特定的工作表</span><br><span class="hljs-built_in">df</span> = pd.read_excel(<span class="hljs-string">&#x27;文件.xlsx&#x27;</span>，sheet_name=<span class="hljs-string">&#x27;明细&#x27;</span>) <span class="hljs-comment"># 读取某个sheet表</span><br><span class="hljs-comment"># with方法读取</span><br>with pd.ExcelFile(<span class="hljs-string">&#x27;path_to_file.xls&#x27;</span>) as xls:<br>    df1 = pd.read_excel(xls, <span class="hljs-string">&#x27;Sheet1&#x27;</span>)<br>    df2 = pd.read_excel(xls, <span class="hljs-string">&#x27;Sheet2&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="读取csv或者txt"><a href="#读取csv或者txt" class="headerlink" title="读取csv或者txt"></a>读取csv或者txt</h2><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 分隔符: \s 表示空白字符; \s+多个空白字符; \r回车; \n换行; \t水平制表符; \v垂直制表符</span><br><span class="hljs-attr">df</span> = pd.read_csv(<span class="hljs-string">&#x27;文件.txt&#x27;</span>,sep=<span class="hljs-string">&#x27;\s+&#x27;</span>,error_bad_lines=<span class="hljs-literal">False</span>)    <br></code></pre></td></tr></table></figure><h2 id="批量读取同一文件夹下文件方式1"><a href="#批量读取同一文件夹下文件方式1" class="headerlink" title="批量读取同一文件夹下文件方式1"></a>批量读取同一文件夹下文件方式1</h2><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-keyword">for</span> root, dirs, <span class="hljs-built_in">files</span> <span class="hljs-keyword">in</span> os.walk(<span class="hljs-string">&#x27;.&#x27;</span>,topdown=False):<br>    print(<span class="hljs-built_in">files</span>)<br><span class="hljs-built_in">num</span> = <span class="hljs-built_in">len</span>(<span class="hljs-built_in">files</span>)   <span class="hljs-comment"># 获取文件个数</span><br>data = pd.DataFrame()  <span class="hljs-comment"># 定义一个空的dataframe</span><br><span class="hljs-comment"># 遍历所有文件</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-built_in">num</span>):<br>    datai = pd.read_excel(<span class="hljs-string">&#x27;./%s&#x27;</span> %<span class="hljs-built_in">files</span>[i])   <br>    datai_len = <span class="hljs-built_in">len</span>(datai)<br>    data = data.append(datai)   <span class="hljs-comment"># 添加到总的数据中</span><br>    print(<span class="hljs-string">&#x27;文件%i列, 第%i个表,读取%i行数据,名称：%s&#x27;</span>%(<span class="hljs-built_in">len</span>(data.columns),i,datai_len,<span class="hljs-built_in">files</span>[i]))     <span class="hljs-comment"># 查看是否全部读取，格式是否出错</span><br><br>data.reset_index(drop=True,inplace=True)<br></code></pre></td></tr></table></figure><h2 id="批量读取同一文件夹下的文件方式2"><a href="#批量读取同一文件夹下的文件方式2" class="headerlink" title="批量读取同一文件夹下的文件方式2"></a>批量读取同一文件夹下的文件方式2</h2><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 导入工具包</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-meta"># 路径</span><br><span class="hljs-title">path</span> = &#x27;d:/文件路径/&#x27;<br><br><span class="hljs-meta"># 文件列表</span><br><span class="hljs-title">files</span> = []<br><span class="hljs-title">for</span> file <span class="hljs-keyword">in</span> os.listdir(path):<br>    <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&quot;.csv&quot;</span>):<br>        files.append(path+file)<br><br><span class="hljs-meta"># 定义一个空的dataframe</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = pd.<span class="hljs-type">DataFrame</span>()  </span><br><br><span class="hljs-meta"># 遍历所有文件</span><br><span class="hljs-title">for</span> file <span class="hljs-keyword">in</span> files:<br>    datai = pd.read_csv(file,encoding=&#x27;gbk&#x27;)<br>    datai_len = len(datai)<br>    <span class="hljs-class"><span class="hljs-keyword">data</span> = <span class="hljs-keyword">data</span>.append(<span class="hljs-title">datai</span>)   # 添加到总的数据中</span><br>    print(&#x27;读取%i行数据,合并后文件%i列, 名称：%s&#x27;%(datai_len,len(<span class="hljs-class"><span class="hljs-keyword">data</span>.columns),file.split(&#x27;/&#x27;)[-1]))     </span><br>    # 查看是否全部读取，格式是否出错<br><span class="hljs-meta"># 重置索引    </span><br><span class="hljs-class"><span class="hljs-keyword">data</span>.reset_index(<span class="hljs-title">drop</span>=<span class="hljs-type">True</span>,<span class="hljs-title">inplace</span>=<span class="hljs-type">True</span>)</span><br></code></pre></td></tr></table></figure><h2 id="批量读取同一文件夹下得文件方式3-当txt文件不规范-读取会丢失数据或文件太大时"><a href="#批量读取同一文件夹下得文件方式3-当txt文件不规范-读取会丢失数据或文件太大时" class="headerlink" title="批量读取同一文件夹下得文件方式3(当txt文件不规范,读取会丢失数据或文件太大时)"></a>批量读取同一文件夹下得文件方式3(当txt文件不规范,读取会丢失数据或文件太大时)</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs vim"># 循环读取数据<br>n = <span class="hljs-number">1</span><br><span class="hljs-keyword">for</span> <span class="hljs-keyword">file</span> in <span class="hljs-keyword">files</span>:    <br>    with <span class="hljs-keyword">open</span>(<span class="hljs-keyword">file</span>, <span class="hljs-string">&#x27;r&#x27;</span>,encoding=<span class="hljs-string">&#x27;gbk&#x27;</span>) <span class="hljs-keyword">as</span> f_input:<br>        lisi= []<br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> in f_input:<br>            lisi.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">list</span>(<span class="hljs-built_in">line</span>.strip().<span class="hljs-keyword">split</span>(<span class="hljs-string">&#x27;|&#x27;</span>)))<br><br>    datai = pd.DataFrame(lisi)<br>    datai2 = guolv(datai)<br>    data = data.<span class="hljs-keyword">append</span>(datai2)<br><br>    <span class="hljs-keyword">print</span>(<span class="hljs-string">&#x27;读取第%i个文件,件名%s,文件%i行,文%i列.处理后文件%i行,%i列&#x27;</span> %(n,<span class="hljs-keyword">file</span>,datai.shape[<span class="hljs-number">0</span>],datai.shape[<span class="hljs-number">1</span>],datai2.shape[<span class="hljs-number">0</span>],datai2.shape[<span class="hljs-number">1</span>]))<br>    n = n + <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h2 id="文件导出-放在同一工作簿"><a href="#文件导出-放在同一工作簿" class="headerlink" title="文件导出,放在同一工作簿"></a>文件导出,放在同一工作簿</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">writer = pd.ExcelWriter(<span class="hljs-string">&#x27;文件.xlsx&#x27;</span>)<br><span class="hljs-keyword">for</span> <span class="hljs-type">name</span> , <span class="hljs-keyword">group</span> <span class="hljs-keyword">in</span> df.groupby(<span class="hljs-string">&#x27;名称&#x27;</span>):<br>    <span class="hljs-keyword">group</span>.to_excel(writer,sheet_name=<span class="hljs-type">name</span>,<span class="hljs-keyword">index</span>=<span class="hljs-keyword">False</span>)<br></code></pre></td></tr></table></figure><h2 id="文件导出-分组导出放在同一工作簿"><a href="#文件导出-分组导出放在同一工作簿" class="headerlink" title="文件导出, 分组导出放在同一工作簿"></a>文件导出, 分组导出放在同一工作簿</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">writer = pd.ExcelWriter(<span class="hljs-string">&#x27;文件.xlsx&#x27;</span>)<br><span class="hljs-keyword">for</span> <span class="hljs-type">name</span> , <span class="hljs-keyword">group</span> <span class="hljs-keyword">in</span> df.groupby(<span class="hljs-string">&#x27;名称&#x27;</span>):<br>    <span class="hljs-keyword">group</span>.to_excel(writer,sheet_name=<span class="hljs-type">name</span>,<span class="hljs-keyword">index</span>=<span class="hljs-keyword">False</span>)<br></code></pre></td></tr></table></figure><h2 id="文件导出-分组导出-放在不同工作簿"><a href="#文件导出-分组导出-放在不同工作簿" class="headerlink" title="文件导出, 分组导出, 放在不同工作簿"></a>文件导出, 分组导出, 放在不同工作簿</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">for</span> <span class="hljs-type">name</span> , <span class="hljs-keyword">group</span> <span class="hljs-keyword">in</span> df.groupby(<span class="hljs-string">&#x27;名称&#x27;</span>):<br>    <span class="hljs-keyword">group</span>.to_excel(<span class="hljs-type">name</span>+<span class="hljs-string">&#x27;.xlsx&#x27;</span>,<span class="hljs-keyword">index</span>=<span class="hljs-keyword">False</span>)<br></code></pre></td></tr></table></figure><h2 id="获取当前时间"><a href="#获取当前时间" class="headerlink" title="获取当前时间"></a>获取当前时间</h2><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs dos">import <span class="hljs-built_in">time</span><br>tim = <span class="hljs-built_in">time</span>.strftime(&quot;<span class="hljs-variable">%Y-%</span>m-<span class="hljs-variable">%d%</span>H<span class="hljs-variable">%M%</span>S&quot;, <span class="hljs-built_in">time</span>.localtime()) <br></code></pre></td></tr></table></figure><h2 id="保存图片分辨率设置"><a href="#保存图片分辨率设置" class="headerlink" title="保存图片分辨率设置"></a>保存图片分辨率设置</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">plt.savefig(<span class="hljs-string">&#x27;名称.png&#x27;</span>,<span class="hljs-attribute">dpi</span>=150)  <br></code></pre></td></tr></table></figure><h2 id="数据查看"><a href="#数据查看" class="headerlink" title="数据查看"></a>数据查看</h2><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs glsl">df.describe()   <span class="hljs-meta"># 描述统计</span><br>df.info()    <span class="hljs-meta"># 基本信息</span><br>df.dtypes    <span class="hljs-meta">#　列格式类型</span><br>df.head(<span class="hljs-number">2</span>)    ＃前n行<br>df.tail(<span class="hljs-number">2</span>)    <span class="hljs-meta">#后ｎ行</span><br>df.shape    ＃维度<br>df.<span class="hljs-keyword">index</span>  <span class="hljs-meta">#索引</span><br>df.columns  ＃列名<br>df.<span class="hljs-keyword">sample</span>(<span class="hljs-number">10</span>)   <span class="hljs-meta">#随机抽样</span><br>df.resample()　 <span class="hljs-meta">#随机抽样</span><br></code></pre></td></tr></table></figure><h2 id="行列处理-删除-排序"><a href="#行列处理-删除-排序" class="headerlink" title="行列处理(删除,排序)"></a>行列处理(删除,排序)</h2><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-comment"># 删除列</span><br>del df[<span class="hljs-string">&#x27;变量名&#x27;</span>]<br> <br><span class="hljs-comment"># 删除行</span><br>df.drop(<span class="hljs-string">&#x27;c&#x27;</span>)<br><span class="hljs-comment"># 更改列名</span><br>df.columns= [<span class="hljs-string">&#x27;var1&#x27;</span>,<span class="hljs-string">&#x27;var2&#x27;</span>,<span class="hljs-string">&#x27;var3&#x27;</span>]  <span class="hljs-comment"># 列名的个数 = 字段个数</span><br>df.<span class="hljs-keyword">rename</span>(columns = &#123;<span class="hljs-string">&#x27;名称前&#x27;</span>:<span class="hljs-string">&#x27;名称后&#x27;</span>&#125;,inplace=<span class="hljs-literal">True</span>)   <span class="hljs-comment"># columns 不能少</span><br><span class="hljs-comment"># series中改列名</span><br>s.<span class="hljs-keyword">rename</span>(<span class="hljs-string">&#x27;名称&#x27;</span>,inplace=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 调整列的顺序</span><br>df1 = df1[[<span class="hljs-string">&#x27;var1&#x27;</span>,<span class="hljs-string">&#x27;var2&#x27;</span>,<span class="hljs-string">&#x27;var3&#x27;</span>]]<br><span class="hljs-comment"># 调整行的顺序</span><br>df2 = df1.reindex([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>]) <span class="hljs-comment"># 返回一个新的DataFrame，按新的索引进行排序</span><br></code></pre></td></tr></table></figure><h2 id="缺失值-查看-替换-计数"><a href="#缺失值-查看-替换-计数" class="headerlink" title="缺失值(查看,替换,计数)"></a>缺失值(查看,替换,计数)</h2><p>缺失值：在<code>pandas</code>中，缺失数据显示为<strong>NaN</strong>。缺失值有3种表示方法，<code>np.nan</code>，<code>none</code>，<code>pd.NA</code>。</p><h3 id="1、np-nan"><a href="#1、np-nan" class="headerlink" title="1、np.nan"></a>1、np.nan</h3><p>缺失值有个特点（坑），它不等于任何值，连自己都不相等。如果用<code>nan</code>和任何其它值比较都会返回<code>nan</code>。</p><p>也正由于这个特点，在数据集读入以后，不论列是什么类型的数据，默认的缺失值全为<code>np.nan</code>。</p><p><strong>因为<code>nan</code>在<code>Numpy</code>中的类型是浮点，因此整型列会转为浮点；而字符型由于无法转化为浮点型，只能归并为object类型（’O’），原来是浮点型的则类型不变。</strong></p><p>初学者做数据处理遇见object类型会发懵，不知道这是个啥，明明是字符型，导入后就变了，其实是因为缺失值导致的。</p><p>除此之外，还要介绍一种针对时间序列的缺失值，它是单独存在的，用<strong>NaT</strong>表示，是<code>pandas</code>的内置类型，**可以视为时间序列版的<code>np.nan</code>**，也是与自己不相等。</p><h3 id="2、None"><a href="#2、None" class="headerlink" title="2、None"></a>2、None</h3><p>还有一种就是<code>None</code>，它要比<code>nan</code>好那么一点，因为它至少自己与自己相等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-literal">None</span> == <span class="hljs-literal">None</span><br>&gt;&gt; <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><p>在传入数值类型后，会自动变为<code>np.nan</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">type</span>(pd.Series([<span class="hljs-number">1</span>,<span class="hljs-literal">None</span>])[<span class="hljs-number">1</span>])<br>&gt;&gt; numpy.float64<br></code></pre></td></tr></table></figure><p>只有当传入<code>object</code>类型时是不变的，因此可以认为如果不是人工命名为<code>None</code>的话，它基本不会自动出现在<code>pandas</code>中，所以<code>None</code>大家基本也看不到。</p><h3 id="3、NA标量"><a href="#3、NA标量" class="headerlink" title="3、NA标量"></a>3、NA标量</h3><p>pandas1.0以后的版本中引入了一个专门表示缺失值的标量<strong>pd.NA</strong>，它代表空整数、空布尔值、空字符，这个功能目前处于实验阶段。</p><p>开发者也注意到了这点，对于不同数据类型采取不同的缺失值表示会很乱。pd.NA就是为了统一而存在的。<strong>pd.NA的目标是提供一个缺失值指示器，可以在各种数据类型中一致使用(而不是np.nan、None或者NaT分情况使用)。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">s_new = pd.Series([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=<span class="hljs-string">&quot;Int64&quot;</span>)<br>s_new<br>&gt;&gt; <span class="hljs-number">0</span>   <span class="hljs-number">1</span><br>   <span class="hljs-number">1</span>   <span class="hljs-number">2</span><br>   dtype: Int64<br>-----------------<br>s_new[<span class="hljs-number">1</span>] = pd.NaT<br>s_new<br>&gt;&gt; <span class="hljs-number">0</span>    <span class="hljs-number">1</span><br>   <span class="hljs-number">1</span>  &lt;NA&gt;<br>   dtype: Int64<br></code></pre></td></tr></table></figure><p>同理，对于布尔型、字符型一样不会改变原有数据类型,这样就解决了原来动不动就变成<code>object</code>类型的麻烦了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#将无穷值设置为缺失值</span><br>pd.options.mode.use_inf_as_na = <span class="hljs-literal">True</span><br><br><span class="hljs-comment"># 判断是否是缺失值</span><br>df.isnull()  <span class="hljs-comment">#不能省略括号</span><br>df[<span class="hljs-string">&#x27;colomu&#x27;</span>].isnull()<br>df.notnull()<br><span class="hljs-comment"># 计算缺失值个数</span><br>s.isnull().value_counts()<br><span class="hljs-comment"># 每列缺失值个数</span><br>df.isna().<span class="hljs-built_in">sum</span>()  <span class="hljs-comment"># 缺失值个数</span><br><br><span class="hljs-comment"># 填充缺失值</span><br>df[<span class="hljs-string">&#x27;A&#x27;</span>].fillna(<span class="hljs-string">&#x27;缺失数据&#x27;</span>,inplace=<span class="hljs-literal">True</span>)  <br>data1.fillna(method=<span class="hljs-string">&#x27;pad&#x27;</span>)   <span class="hljs-comment"># 类似于excel中用上一个单元格内容批量填充</span><br><br><span class="hljs-comment"># 删除指定列包含缺失值的行</span><br>data.dropna(subset=[<span class="hljs-string">&quot;C&quot;</span>],inplace=<span class="hljs-literal">True</span>)    <span class="hljs-comment"># []不能少</span><br>data.dropna(how=<span class="hljs-string">&quot;all&quot;</span>)   <span class="hljs-comment"># 删除全为空的行</span><br>data.dropna(thresh=<span class="hljs-number">2</span>)    <span class="hljs-comment">#删除有效数据小于2的数据</span><br><br><span class="hljs-comment"># 缺失值个数并排序</span><br>df.isnull().<span class="hljs-built_in">sum</span>().sort_values(ascending=<span class="hljs-literal">False</span>).head()<br></code></pre></td></tr></table></figure><h2 id="重复值-查看-删除"><a href="#重复值-查看-删除" class="headerlink" title="重复值(查看,删除)"></a>重复值(查看,删除)</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 不重复项</span><br>df[<span class="hljs-string">&#x27;A&#x27;</span>].unique()  <br>df[<span class="hljs-string">&#x27;A&#x27;</span>].nunique() #查看不重复值个数<br>df[<span class="hljs-string">&#x27;A&#x27;</span>].unique().tolist()   # 转list <br>df[<span class="hljs-string">&#x27;A&#x27;</span>].value_counts()  # 计数<br><span class="hljs-comment"># 去重</span><br><span class="hljs-built_in">set</span>(data2[<span class="hljs-string">&#x27;名称&#x27;</span>])<br><br><span class="hljs-comment"># 查看重复项，返回True False</span><br>df.duplicated()  <br><span class="hljs-comment"># 查看重复的数据</span><br>data4 = data3[data3.duplicated(<span class="hljs-attribute">subset</span>=<span class="hljs-string">&#x27;var1&#x27;</span>,keep=False)]  <br><br><span class="hljs-comment"># 删除重复项</span><br>df.drop_duplicates([<span class="hljs-string">&#x27;var&#x27;</span>],<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 按两列去重</span><br>data.drop_duplicates(subset=[<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-string">&#x27;B&#x27;</span>],<span class="hljs-attribute">keep</span>=<span class="hljs-string">&#x27;first&#x27;</span>,inplace=True)  <br><span class="hljs-comment"># 分组计算不重复项个数</span><br>result = df.groupby([<span class="hljs-string">&#x27;var1&#x27;</span>,<span class="hljs-string">&#x27;var2&#x27;</span>]).agg(&#123;<span class="hljs-string">&#x27;var3&#x27;</span>:<span class="hljs-string">&#x27;nunique&#x27;</span>,<span class="hljs-string">&#x27;var4&#x27;</span>:[<span class="hljs-string">&#x27;min&#x27;</span>,<span class="hljs-string">&#x27;max&#x27;</span>]&#125;)<span class="hljs-string">&#x27;b*--&#x27;</span>, <span class="hljs-attribute">alpha</span>=0.5, <span class="hljs-attribute">linewidth</span>=1<br></code></pre></td></tr></table></figure><h2 id="修改格式-格式转换-百分比-格式判断"><a href="#修改格式-格式转换-百分比-格式判断" class="headerlink" title="修改格式(格式转换, 百分比, 格式判断)**"></a>修改格式(格式转换, 百分比, 格式判断)**</h2><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs prolog"># 改变列的格式（文本型数值改成数值型数值）<br>df[<span class="hljs-string">&#x27;var1&#x27;</span>] = df[<span class="hljs-string">&#x27;var1&#x27;</span>].astype(<span class="hljs-string">&#x27;int&#x27;</span>)  # errors = <span class="hljs-string">&#x27;ignore&#x27;</span><br>df[<span class="hljs-string">&#x27;var1&#x27;</span>] = df[<span class="hljs-string">&#x27;var1&#x27;</span>].astype(np.float)<br># 多列转换<br>df[[<span class="hljs-string">&#x27;user_id&#x27;</span>,<span class="hljs-string">&#x27;merchant_id&#x27;</span>,<span class="hljs-string">&#x27;coupon_id&#x27;</span>]]=df[[<span class="hljs-string">&#x27;user_id&#x27;</span>,<span class="hljs-string">&#x27;merchant_id&#x27;</span>,<span class="hljs-string">&#x27;coupon_id&#x27;</span>]].astype(str)  <br>df.infer_objects()  # 根据数据特征自动转换<br><br># 百分比格式<br>data[<span class="hljs-string">&#x27;B_per%&#x27;</span>] = data[<span class="hljs-string">&#x27;B_per&#x27;</span>].apply(lambda x: <span class="hljs-string">&#x27;%.2f%%&#x27;</span> <span class="hljs-comment">% (x*100))</span><br><br># 判断格式，是否为字符串<br>data[<span class="hljs-string">&#x27;var&#x27;</span>].apply(lambda x:isinstance(x,str))<br></code></pre></td></tr></table></figure><h2 id="普通筛选"><a href="#普通筛选" class="headerlink" title="普通筛选"></a>普通筛选</h2><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs prolog"># loc筛选：选择符合条件的行<br>df.loc[df[<span class="hljs-string">&quot;grade&quot;</span>] == <span class="hljs-string">&quot;B&quot;</span>].head()<br><br># 选择符合条件的行和特定的列<br>df.loc[df[<span class="hljs-string">&quot;grade&quot;</span>] == <span class="hljs-string">&quot;B&quot;</span>, [<span class="hljs-string">&quot;member_id&quot;</span>, <span class="hljs-string">&quot;grade&quot;</span>]].head()<br># 选择符合条件的行和特定的列,并排序<br>df.loc[df[<span class="hljs-string">&quot;grade&quot;</span>] == <span class="hljs-string">&quot;B&quot;</span>, [<span class="hljs-string">&quot;loan_amnt&quot;</span>, <span class="hljs-string">&quot;grade&quot;</span>]].sort([<span class="hljs-string">&quot;loan_amnt&quot;</span>])<br># 多条件筛选<br>df.loc[[(df[<span class="hljs-string">&quot;gradd&quot;</span>]==<span class="hljs-string">&quot;B&quot;</span>) &amp; (df[<span class="hljs-string">&quot;loan_amnt&quot;</span>]&gt;<span class="hljs-number">5000</span>), [<span class="hljs-string">&quot;member_id&quot;</span>, <span class="hljs-string">&quot;term&quot;</span> ]].head()<br># 反选<br>data[~((data.发布时间.isnull()) &amp; (data.实际首试日期 &lt; <span class="hljs-string">&#x27;2018-11-1&#x27;</span>))]<br># 反转行<br>df.iloc[::-<span class="hljs-number">1</span>, :] <br></code></pre></td></tr></table></figure><h2 id="逻辑判断后筛选"><a href="#逻辑判断后筛选" class="headerlink" title="逻辑判断后筛选"></a>逻辑判断后筛选</h2><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs prolog"># isin筛选<br>df[<span class="hljs-string">&quot;A&quot;</span>].isin([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br>#　多条件<br>df[(df.<span class="hljs-symbol">AAA</span> &lt;= <span class="hljs-number">6</span>) &amp; (df.index.isin([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]))]<br># loc和isin组合　<br>df.loc[df[<span class="hljs-string">&#x27;sepal_length&#x27;</span>].isin([<span class="hljs-number">5.8</span>,<span class="hljs-number">5.1</span>])]<br># 字符串isin<br>df.loc[df[<span class="hljs-string">&#x27;grade&#x27;</span>].isin([<span class="hljs-string">&#x27;北京&#x27;</span>,<span class="hljs-string">&#x27;上海&#x27;</span>])]<br>#　字符串开头<br>df[df[<span class="hljs-string">&#x27;A&#x27;</span>].str.startswith(<span class="hljs-string">&#x27;北京&#x27;</span>)]<br><br># contains筛选<br>df[df[<span class="hljs-string">&#x27;商品名称&#x27;</span>].str.contains(<span class="hljs-string">&quot;四件套&quot;</span>)]<br># 多个筛选<br>df_re =df[df[<span class="hljs-string">&#x27;公司&#x27;</span>].str.contains(<span class="hljs-string">&#x27;公司A||公司B&#x27;</span>)]<br># 筛选含特殊字符的<br>df[df[<span class="hljs-string">&#x27;产业线&#x27;</span>].str.contains(<span class="hljs-string">&#x27;\?&#x27;</span>)] <br># 多条件筛选<br>data[(data[<span class="hljs-string">&#x27;var1&#x27;</span>].str[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;M&#x27;</span>)&amp;(data[<span class="hljs-string">&#x27;var2&#x27;</span>].apply(lambda x : str(x)[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;8&#x27;</span>))]<br><br># 索引特殊用法<br>df[[i.endswith(<span class="hljs-string">&#x27;A&#x27;</span>) for i in df.index]] <br><br># 按字符串长度筛选<br>data = df[df[<span class="hljs-string">&#x27;名称&#x27;</span>].str.len()&gt;<span class="hljs-number">5</span>]<br><br># 按字符串开头筛选<br>df[df[<span class="hljs-string">&#x27;名称&#x27;</span>].str[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;M&#x27;</span>]<br><br># 筛选列<br>data = data.loc[:,~(data.columns.str.contains(<span class="hljs-string">&#x27;集团&#x27;</span>))]<br><br># 筛选前几个最大值<br>data = df.iloc[df[<span class="hljs-string">&#x27;名称&#x27;</span>].nlargest(<span class="hljs-number">3</span>).index]<br><br># 筛选由汉字开头的数据<br>df = df[df[<span class="hljs-string">&#x27;名称&#x27;</span>].str.contains(r<span class="hljs-string">&#x27;^[\u4e00-\u9fa5]&#x27;</span>)]<br><br># 日期不是索引时, 按日期筛选 <br>data[data[<span class="hljs-string">&#x27;时间&#x27;</span>].apply(lambda x: x.year == <span class="hljs-number">2019</span>)]  <br>data[data[<span class="hljs-string">&#x27;时间&#x27;</span>].apply(lambda x: x.strftime(<span class="hljs-string">&#x27;%Y%m&#x27;</span>)==<span class="hljs-string">&#x27;202008&#x27;</span>)]<br></code></pre></td></tr></table></figure><h2 id="分组后筛选"><a href="#分组后筛选" class="headerlink" title="分组后筛选"></a>分组后筛选</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 分组后筛选前2个</span><br>df2 = df1.groupby([<span class="hljs-string">&#x27;class&#x27;</span>]).head(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 分组后筛选第2条或者倒数第2条数据</span><br>df1.groupby(<span class="hljs-string">&#x27;class&#x27;</span>).apply(<span class="hljs-keyword">lambda</span> i:i.iloc[<span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(i)&gt;<span class="hljs-number">1</span> <span class="hljs-keyword">else</span> np.nan)<br>df1.groupby(<span class="hljs-string">&#x27;class&#x27;</span>).apply(<span class="hljs-keyword">lambda</span> i:i.iloc[-<span class="hljs-number">2</span>] <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(i)&gt;<span class="hljs-number">1</span> <span class="hljs-keyword">else</span> np.nan)<br><br><span class="hljs-comment"># 分组后按条件筛选</span><br>data.groupby(<span class="hljs-string">&#x27;var1&#x27;</span>).<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[x[<span class="hljs-string">&#x27;var2&#x27;</span>]==<span class="hljs-string">&#x27;A&#x27;</span>])&gt;=<span class="hljs-number">1</span>)<br>data.groupby(<span class="hljs-string">&#x27;公司编码&#x27;</span>).<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">len</span>(x)!=<span class="hljs-number">3</span>)<br><br><span class="hljs-comment"># groupby筛选方法1（先分组，然后筛选另一个变量中最小值）</span><br>df.loc[df.groupby(<span class="hljs-string">&#x27;AA&#x27;</span>)[<span class="hljs-string">&#x27;BB&#x27;</span>].inxmin()]<br><span class="hljs-comment">#groupby筛选方法2：（先排序，分组，然后筛选每一组中的第一个）</span><br>df.sort_values(by=<span class="hljs-string">&#x27;BB&#x27;</span>).groupby(<span class="hljs-string">&#x27;AA&#x27;</span>,as_index=<span class="hljs-literal">False</span>).first()<br><span class="hljs-comment"># groupby筛选方法3：groupby与apply结合</span><br>df.groupby(<span class="hljs-string">&#x27;aa&#x27;</span>).apply(<span class="hljs-keyword">lambda</span> x :  x[<span class="hljs-string">&#x27;BB&#x27;</span>][x[<span class="hljs-string">&#x27;cc&#x27;</span>].idxmax()]<br><br><span class="hljs-comment"># 分组并排序</span><br>df[[<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-string">&#x27;B&#x27;</span>]].groupby(<span class="hljs-string">&#x27;A&#x27;</span>).mean().sort_values(<span class="hljs-string">&#x27;B&#x27;</span>,ascending=<span class="hljs-literal">False</span>).head()<br></code></pre></td></tr></table></figure><h2 id="替换"><a href="#替换" class="headerlink" title="替换"></a>替换</h2><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs prolog"># 单个替换<br>df[<span class="hljs-string">&#x27;判断&#x27;</span>].replace([<span class="hljs-string">&#x27;B&#x27;</span>,<span class="hljs-string">&#x27;C&#x27;</span>],np.nan,inplace=<span class="hljs-symbol">True</span>)   #支持正则表达式<br># 多个替换<br>data[<span class="hljs-string">&#x27;var&#x27;</span>].replace([<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-string">&#x27;B&#x27;</span>],[<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>],inplace=<span class="hljs-symbol">True</span>)<br># loc原地替换<br>df1.loc[df[<span class="hljs-string">&#x27;sex&#x27;</span>]==<span class="hljs-string">&#x27;female&#x27;</span>, <span class="hljs-string">&#x27;sex&#x27;</span>] = <span class="hljs-number">1</span><br>df1.loc[df[<span class="hljs-string">&#x27;sex&#x27;</span>]==<span class="hljs-string">&#x27;male&#x27;</span>, <span class="hljs-string">&#x27;sex&#x27;</span>] = <span class="hljs-number">0</span><br><br># 字典<br>df.replace(&#123;<span class="hljs-string">&#x27;female&#x27;</span>:<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;male&#x27;</span>:<span class="hljs-number">0</span>&#125;)<br># 适合二分法<br>df1[<span class="hljs-string">&#x27;sex&#x27;</span>] = df1[<span class="hljs-string">&#x27;sex&#x27;</span>].map(lambda x:<span class="hljs-number">1</span> if x==<span class="hljs-string">&#x27;female&#x27;</span> else <span class="hljs-number">0</span>)<br># apply<br>df_zb[<span class="hljs-string">&#x27;var&#x27;</span>] = df_zb[<span class="hljs-string">&#x27;var&#x27;</span>].apply(lambda x: x[:-<span class="hljs-number">1</span>] if x[<span class="hljs-number">-1</span>].isalpha() else x)<br>data3[<span class="hljs-string">&#x27;var&#x27;</span>] = data3[<span class="hljs-string">&#x27;var&#x27;</span>].apply(lambda x: x[:-<span class="hljs-number">1</span>] if x[<span class="hljs-number">-1</span>] == <span class="hljs-string">&#x27;s&#x27;</span> else x)<br><br># np.where<br>data[<span class="hljs-string">&#x27;var&#x27;</span>] = np.where(data[<span class="hljs-string">&#x27;var&#x27;</span>] == <span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-number">0</span>,data[<span class="hljs-string">&#x27;var&#x27;</span>])<br></code></pre></td></tr></table></figure><h2 id="字符串处理"><a href="#字符串处理" class="headerlink" title="字符串处理"></a>字符串处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我的名字是%s，今年%i岁，体重%.2f&#x27;</span>%(<span class="hljs-string">&#x27;小明&#x27;</span>,<span class="hljs-number">20</span>,<span class="hljs-number">70.1</span>))<br><span class="hljs-comment"># 去除空格，改变大小写</span><br>df[<span class="hljs-string">&#x27;term&#x27;</span>] = df[<span class="hljs-string">&#x27;term&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-built_in">str</span>.strip)  <span class="hljs-comment">#去除左右两边的空格，strip后面是否带括号</span><br>df[<span class="hljs-string">&#x27;term&#x27;</span>] = df[<span class="hljs-string">&#x27;term&#x27;</span>].<span class="hljs-built_in">str</span>.strip()<br>df[<span class="hljs-string">&#x27;term&#x27;</span>] = df[<span class="hljs-string">&#x27;term&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-built_in">str</span>.lstrip)  <span class="hljs-comment">#去除左边的空格</span><br>df[<span class="hljs-string">&#x27;term&#x27;</span>] = df[<span class="hljs-string">&#x27;term&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-built_in">str</span>.rstrip)  <span class="hljs-comment">#去除右边的空格</span><br>df[<span class="hljs-string">&#x27;term&#x27;</span>] = df[<span class="hljs-string">&#x27;term&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-built_in">str</span>.upper)  <span class="hljs-comment"># 改成全部大写</span><br>df[<span class="hljs-string">&#x27;term&#x27;</span>] = df[<span class="hljs-string">&#x27;term&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-built_in">str</span>.lower) <span class="hljs-comment"># 改成全部小写</span><br>df[<span class="hljs-string">&#x27;term&#x27;</span>] = df[<span class="hljs-string">&#x27;term&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-built_in">str</span>.title) <span class="hljs-comment"># 改成首字母大写</span><br><br><span class="hljs-comment"># 去掉所有空格</span><br>data = data.applymap(<span class="hljs-keyword">lambda</span> x: x.strip() <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(x,<span class="hljs-built_in">str</span>) <span class="hljs-keyword">else</span> x)<br></code></pre></td></tr></table></figure><h2 id="字符串判断"><a href="#字符串判断" class="headerlink" title="字符串判断"></a>字符串判断</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 判断是否为某一特定格式</span><br><span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;emp_length&#x27;</span>].apply(lambda x: x.isalpha())   <span class="hljs-comment">#是否为字母</span><br><span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;emp_length&#x27;</span>].apply(lambda x: x. isalnum ())  <span class="hljs-comment">#是否全未数字或者字母</span><br><span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;emp_length&#x27;</span>].apply(lambda x: x. isdigit ())  <span class="hljs-comment">#是否为数字</span><br></code></pre></td></tr></table></figure><h2 id="字符串拆分"><a href="#字符串拆分" class="headerlink" title="字符串拆分"></a>字符串拆分</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">s.str.split(<span class="hljs-string">&#x27;_&#x27;</span>)  # 拆分，结果是列表<br>s.str.split(<span class="hljs-string">&#x27;_&#x27;</span>).str.<span class="hljs-built_in">get</span>(1)   # 拆分后取第一个，可用于生成新变量<br>s.str.split(<span class="hljs-string">&#x27;_&#x27;</span>).str[1]       # 拆分后取第一个，可用于生成新变量<br>s.str.split(<span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-attribute">expand</span>=<span class="hljs-literal">True</span>) # 拆分，并展开成多列<br>s.str.split(<span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-attribute">expand</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">n</span>=1)   # 按第一个拆分<br>s.str.rsplit(<span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-attribute">expand</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">n</span>=1)  # 按最后一个拆分<br>data[<span class="hljs-string">&#x27;var&#x27;</span>] = data[<span class="hljs-string">&#x27;var&#x27;</span>].str.split(<span class="hljs-string">&#x27;?&#x27;</span>,<span class="hljs-attribute">expand</span>=<span class="hljs-literal">True</span>)[0]<br></code></pre></td></tr></table></figure><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 把列变成索引</span><br>df.set_index(<span class="hljs-string">&#x27;时间&#x27;</span>,<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>, drop = <span class="hljs-literal">False</span>)  <br>df.set_index([<span class="hljs-string">&#x27;r&#x27;</span>,<span class="hljs-string">&#x27;s&#x27;</span>], inplace = <span class="hljs-literal">True</span>) # r为一级，s为二级<br><br><span class="hljs-comment"># 取消层次化索引，将索引变回列</span><br>df.reset_index(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br>df.reset_index(<span class="hljs-attribute">drop</span>=<span class="hljs-literal">True</span>,inplace=True,level = None)<br><br><span class="hljs-comment"># 按索引排序</span><br>df.sort_index(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>,ascending=False)<br><br><span class="hljs-comment"># 更新索引（行、列都可以修改）</span><br>df2 = df1.reindex([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>]) # 返回一个新的DataFrame，按新的索引进行排序<br>df2 = df1.reindex([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>],  <span class="hljs-attribute">fill_value</span>=0) <br><br><span class="hljs-comment"># 多级索引问题（索引可以在行，也可以在列）</span><br>df.index.names=[<span class="hljs-string">&#x27;类型&#x27;</span>,<span class="hljs-string">&#x27;供应商&#x27;</span>]  # 取名称<br>df.columns.names=[<span class="hljs-string">&#x27;类型&#x27;</span>,<span class="hljs-string">&#x27;日期&#x27;</span>]  # 取名称<br>df1 = df.swaplevel(<span class="hljs-string">&#x27;类型&#x27;</span>,<span class="hljs-string">&#x27;日期&#x27;</span>,<span class="hljs-attribute">axis</span>=1)  #调整多级索引次序<br>df1 = df.sort_index(<span class="hljs-attribute">level</span>=0, <span class="hljs-attribute">axis</span>=1, <span class="hljs-attribute">ascending</span>=<span class="hljs-literal">True</span>,sort_remaining=False)   # sort_remaining 默认是<span class="hljs-literal">True</span><br>df1 = df.sum(<span class="hljs-attribute">axis</span>=1,level=&#x27;类型&#x27;)  # 通过索引对多列进行求和<br></code></pre></td></tr></table></figure><h2 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h2><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stan"><span class="hljs-comment">#行遍历</span><br><span class="hljs-keyword">for</span> index, <span class="hljs-built_in">row</span> <span class="hljs-keyword">in</span> df.iterrows():<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">row</span>.s0)<br><span class="hljs-comment">#行遍历</span><br><span class="hljs-keyword">for</span> <span class="hljs-built_in">row</span> <span class="hljs-keyword">in</span> df.itertuples():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">row</span>.s0)<br><br><span class="hljs-comment">#列遍历</span><br><span class="hljs-keyword">for</span> index, <span class="hljs-built_in">row</span> <span class="hljs-keyword">in</span> df.iteritems():<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">row</span>[<span class="hljs-number">0</span>])<br><br></code></pre></td></tr></table></figure><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># na_position对NaN值的处理方式，可以选择first和last两种方式</span><br>df.sort()   <br><span class="hljs-comment"># 单列排序</span><br>df.sort([<span class="hljs-string">&quot;loan_amnt&quot;</span>],<span class="hljs-attribute">ascending</span>=<span class="hljs-literal">False</span>)  # 降序<br><span class="hljs-comment"># 多列进行排序,加入一个列表</span><br>df.sort([<span class="hljs-string">&quot;loan_amnt&quot;</span>,<span class="hljs-string">&quot;int_rate&quot;</span>],<span class="hljs-attribute">ascending</span>=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 升序排列后选择前10</span><br>df.sort([<span class="hljs-string">&quot;loan_amnt&quot;</span>],<span class="hljs-attribute">ascending</span>=<span class="hljs-literal">True</span>).head(10)<br><span class="hljs-comment"># 自定义排序</span><br>df.sort_values(by=[<span class="hljs-string">&#x27;日期&#x27;</span>,<span class="hljs-string">&#x27;类型&#x27;</span>,<span class="hljs-string">&#x27;距离&#x27;</span>],<span class="hljs-attribute">ascending</span>=<span class="hljs-literal">True</span>,inplace=True,na_position = <span class="hljs-string">&#x27;last&#x27;</span>)   <br><span class="hljs-comment"># 索引排序</span><br>df.sort_index(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h2 id="日期"><a href="#日期" class="headerlink" title="日期"></a>日期</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置成日期格式</span><br>df[<span class="hljs-string">&#x27;创建日期&#x27;</span>] = pd.to_datetime(data_final[<span class="hljs-string">&#x27;创建日期&#x27;</span>],<span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%Y/%m/%d&#x27;</span>)<br><span class="hljs-comment"># 设置为日期索引</span><br>df.set_index(<span class="hljs-string">&#x27;订单提交时间&#x27;</span>,inplace=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 日期索引排序</span><br>df.sort_index(inplace=<span class="hljs-literal">True</span>,ascending=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 日期索引的筛选</span><br>df_2019 = df[<span class="hljs-string">&#x27;2019&#x27;</span>]  <span class="hljs-comment"># 日期索引可以直接这样筛选  </span><br>df[<span class="hljs-string">&#x27;2019-01&#x27;</span>]  <span class="hljs-comment">#按月筛选</span><br>df[<span class="hljs-string">&#x27;2019-03-12&#x27;</span>:<span class="hljs-string">&#x27;2019-03-12&#x27;</span>] <span class="hljs-comment">#筛选某一天数据</span><br><br><span class="hljs-comment"># 改变显示频率（只针对日期索引）</span><br>df = df.to_period(<span class="hljs-string">&#x27;M&#x27;</span>)  <span class="hljs-comment"># 针对索引，关键步骤  Q 季度 Y年 D天</span><br><br><span class="hljs-comment"># 计算时间间隔</span><br>data[<span class="hljs-string">&#x27;间隔天数&#x27;</span>] = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x.days, pd.to_datetime(<span class="hljs-string">&#x27;today&#x27;</span>) - data[<span class="hljs-string">&#x27;生产时间&#x27;</span>]))<br>df[<span class="hljs-string">&#x27;天数&#x27;</span>] = df[<span class="hljs-string">&#x27;天数&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: x.days)<br><br><span class="hljs-comment"># 单独表示间隔天数</span><br><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> timedelta<br>aDay = timedelta(days=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 时间对象格式化</span><br>df[<span class="hljs-string">&#x27;DATE&#x27;</span>] = [datetime.strftime(x,<span class="hljs-string">&#x27;%Y-%m-%d&#x27;</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;DATE&#x27;</span>]]<br><br><span class="hljs-comment"># 时间分组后计算</span><br>final = data.groupby(data[<span class="hljs-string">&#x27;订单时间&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: x.strftime(<span class="hljs-string">&#x27;%Y%m&#x27;</span>)))[<span class="hljs-string">&#x27;总价&#x27;</span>].<span class="hljs-built_in">sum</span>()/<span class="hljs-number">10000</span><br>data.groupby(data[<span class="hljs-string">&#x27;日期&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: datetime.strftime(x,<span class="hljs-string">&#x27;%Y-%m-%d&#x27;</span>)))[<span class="hljs-string">&#x27;var&#x27;</span>].count()<br>data.set_index(<span class="hljs-string">&#x27;日期&#x27;</span>).groupby(<span class="hljs-string">&#x27;供应商&#x27;</span>)[<span class="hljs-string">&#x27;数量&#x27;</span>].rolling(<span class="hljs-string">&#x27;7D&#x27;</span>).<span class="hljs-built_in">sum</span>().reset_index()<br>final1 = data.groupby([data.to_period(<span class="hljs-string">&#x27;Q&#x27;</span>).index,<span class="hljs-string">&#x27;var&#x27;</span>]).apply(<span class="hljs-keyword">lambda</span> g: np.average(g[<span class="hljs-string">&#x27;var2&#x27;</span>], weights=g[<span class="hljs-string">&#x27;模次&#x27;</span>])).reset_index()<br><br><span class="hljs-comment"># 当前时间</span><br><span class="hljs-keyword">import</span> time<br>starttime = time.time()<br><br><span class="hljs-comment"># 当前日期</span><br>tim = time.strftime(<span class="hljs-string">&quot;%Y-%m-%d%H%M%S&quot;</span>, time.localtime())<br><br><span class="hljs-comment"># pd.date_range  时间戳  pd.period_range 时间周期  pd.timedelta_range  时间间隔</span><br>datelist = pd.date_range(<span class="hljs-string">&#x27;2020/11/21&#x27;</span>, periods=<span class="hljs-number">5</span>)<br>datelist = pd.date_range(<span class="hljs-string">&#x27;2020/11/21&#x27;</span>, periods=<span class="hljs-number">5</span>,freq=<span class="hljs-string">&#x27;M&#x27;</span>)<br><span class="hljs-comment"># 生成的数据是每月月初</span><br>index= pd.date_range(<span class="hljs-string">&#x27;2019/02/01&#x27;</span>, periods=<span class="hljs-number">23</span>,freq=<span class="hljs-string">&#x27;MS&#x27;</span>)<br>pd.date_range(<span class="hljs-string">&#x27;2017-01-01 01:00:00&#x27;</span>, <span class="hljs-string">&#x27;2017-01-01 02:00:00&#x27;</span>, freq= <span class="hljs-string">&#x27;5min&#x27;</span>)<br><br><span class="hljs-comment"># 当前日期前3天</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">import</span> time<br>lis = pd.date_range(end=<span class="hljs-string">&#x27;2021-4-21&#x27;</span>,periods=<span class="hljs-number">3</span>)<br>str_lis = [datetime.strftime(x,<span class="hljs-string">&#x27;%Y-%m-%d&#x27;</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> lis]<br>lis = pd.date_range(end=time.strftime(<span class="hljs-string">&quot;%Y/%m/%d&quot;</span>),periods=<span class="hljs-number">3</span>)<br>str_lis = [datetime.strftime(x,<span class="hljs-string">&#x27;%Y-%m-%d&#x27;</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> lis]<br></code></pre></td></tr></table></figure><h2 id="判断是否为假期"><a href="#判断是否为假期" class="headerlink" title="判断是否为假期"></a>判断是否为假期</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">import</span> chinese_calendar<br>demo_time = datetime.date(<span class="hljs-number">2018</span>, <span class="hljs-number">10</span>, <span class="hljs-number">2</span>)<br># 判断是否是节假日<br>data_is_holiday = chinese_calendar.is_holiday(demo_time)  # <span class="hljs-literal">True</span><br># 判断某日是否工作日<br>data_is_workday = chinese_calendar.is_workday(demo_time)  # <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><h2 id="数据分组"><a href="#数据分组" class="headerlink" title="数据分组"></a>数据分组</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 方法1</span><br>data[<span class="hljs-string">&#x27;for_m&#x27;</span>] = pd.cut(data[<span class="hljs-string">&#x27;fortune_x&#x27;</span>],[0,50,70,500],labels = [<span class="hljs-string">&#x27;财低&#x27;</span>,<span class="hljs-string">&#x27;财中&#x27;</span>,<span class="hljs-string">&#x27;财高&#x27;</span>])<br><span class="hljs-comment"># 方法2</span><br>df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;value&#x27;</span>: np.random.randint(0, 100, 20)&#125;)<br>labels = [<span class="hljs-string">&quot;&#123;0&#125; - &#123;1&#125;&quot;</span>.format(i, i + 9) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(0, 100, 10)]<br>df[<span class="hljs-string">&#x27;group&#x27;</span>] = pd.cut(df.value, range(0, 105, 10), <span class="hljs-attribute">right</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">labels</span>=labels)<br></code></pre></td></tr></table></figure><h2 id="多表合并"><a href="#多表合并" class="headerlink" title="多表合并"></a>多表合并</h2><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-comment"># 默认纵向连接，产生新的坐标轴   </span><br>df = pd.concat([df1,df2],ignore_index=<span class="hljs-literal">True</span>)  <br><br><span class="hljs-comment"># merge 合并</span><br>pd.merge(<span class="hljs-literal">left</span>,  <span class="hljs-literal">right</span>,  left_on=<span class="hljs-string">&quot;lkey&quot;</span>,  right_on=<span class="hljs-string">&quot;rkey&quot;</span>,suffixes=(<span class="hljs-string">&quot;_left&quot;</span>,  <span class="hljs-string">&quot;_right&quot;</span>))<br>pd.merge(df, df, left_on=[<span class="hljs-string">&#x27;&#x27;</span>,<span class="hljs-string">&#x27;&#x27;</span>,<span class="hljs-string">&#x27;&#x27;</span>], right_on=[<span class="hljs-string">&#x27;&#x27;</span>,<span class="hljs-string">&#x27;&#x27;</span>,<span class="hljs-string">&#x27;&#x27;</span>], suffixes=(<span class="hljs-string">&#x27;&#x27;</span>,<span class="hljs-string">&#x27;&#x27;</span>))<br><br><span class="hljs-comment"># append 表头一致的多张表，进行连接（上下连接）</span><br>df1.append(df2).append(df3)<br><br><span class="hljs-comment"># combine_first</span><br>数据填补.有两张表<span class="hljs-literal">left</span>和<span class="hljs-literal">right</span>，一般要求它们的表格结构一致，数据量也一致，使用<span class="hljs-literal">right</span>的数据去填补<span class="hljs-literal">left</span>的数据缺漏, 如果在同一位置<span class="hljs-literal">left</span>与<span class="hljs-literal">right</span>数据不一致，保留<span class="hljs-literal">left</span>的数据<br>df1.combine_first(df2)<br></code></pre></td></tr></table></figure><h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">count</span><span class="hljs-params">()</span></span>非空观测数量  <br><span class="hljs-function"><span class="hljs-title">sum</span><span class="hljs-params">()</span></span>所有值之和      <br><span class="hljs-function"><span class="hljs-title">mean</span><span class="hljs-params">()</span></span>所有值的平均值   <br><span class="hljs-function"><span class="hljs-title">median</span><span class="hljs-params">()</span></span>所有值的中位数<br><span class="hljs-function"><span class="hljs-title">mode</span><span class="hljs-params">()</span></span>值的模值       <br><span class="hljs-function"><span class="hljs-title">std</span><span class="hljs-params">()</span></span>值的标准偏差    <br><span class="hljs-selector-tag">var</span>()方差             <br><span class="hljs-function"><span class="hljs-title">min</span><span class="hljs-params">()</span></span>所有值中的最小值  <br><span class="hljs-function"><span class="hljs-title">max</span><span class="hljs-params">()</span></span>所有值中的最大值 <br><span class="hljs-function"><span class="hljs-title">abs</span><span class="hljs-params">()</span></span>绝对值          <br><span class="hljs-function"><span class="hljs-title">prod</span><span class="hljs-params">()</span></span>数组元素的乘积  <br><span class="hljs-function"><span class="hljs-title">cumsum</span><span class="hljs-params">()</span></span>累计总和     <br><span class="hljs-function"><span class="hljs-title">cumprod</span><span class="hljs-params">()</span></span>累计乘积     <br><span class="hljs-function"><span class="hljs-title">skew</span><span class="hljs-params">()</span></span>偏斜          <br><span class="hljs-function"><span class="hljs-title">kurt</span><span class="hljs-params">()</span></span>峰度           <br><span class="hljs-function"><span class="hljs-title">quantile</span><span class="hljs-params">()</span></span>分位数<br><span class="hljs-function"><span class="hljs-title">apply</span><span class="hljs-params">()</span></span>通用申请       <br><span class="hljs-function"><span class="hljs-title">cov</span><span class="hljs-params">()</span></span>协方差         <br><span class="hljs-function"><span class="hljs-title">corr</span><span class="hljs-params">()</span></span> 相关系数<br></code></pre></td></tr></table></figure><h2 id="描述性统计"><a href="#描述性统计" class="headerlink" title="描述性统计"></a>描述性统计</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">df.describe(include=[<span class="hljs-string">&#x27;object&#x27;</span>])    <span class="hljs-comment">#object - 汇总字符串列  number - 汇总数字列 all - 将所有列汇总在一起(不应将其作为列表值传递)</span><br>df.describe().<span class="hljs-built_in">round</span>(<span class="hljs-number">2</span>)   <span class="hljs-comment"># 只保留两位小数</span><br>df.describe().<span class="hljs-built_in">round</span>(<span class="hljs-number">2</span>).T   <span class="hljs-comment">#只保留两位小数并转置</span><br>df.groupby(<span class="hljs-string">&#x27;性别&#x27;</span>).describe().unstack()<br>df.groupby(<span class="hljs-string">&#x27;性别&#x27;</span>)[<span class="hljs-string">&#x27;身高&#x27;</span>].describe().unstack()<br>df.<span class="hljs-built_in">round</span>(<span class="hljs-number">3</span>)  <span class="hljs-comment"># 数据可以取消科学计数法？</span><br>df = df.<span class="hljs-built_in">round</span>(<span class="hljs-number">0</span>)   <span class="hljs-comment"># 都改为整数</span><br><span class="hljs-comment"># 保留2位小数</span><br>df[<span class="hljs-string">&#x27;a&#x27;</span>]=df[<span class="hljs-string">&#x27;mean&#x27;</span>].<span class="hljs-built_in">round</span>(decimals=<span class="hljs-number">2</span>) <br>df[<span class="hljs-string">&#x27;b&#x27;</span>]=df[<span class="hljs-string">&#x27;mean&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:(<span class="hljs-string">&quot;%.2f&quot;</span>)%x) <br>df[<span class="hljs-string">&#x27;c&#x27;</span>]=df[<span class="hljs-string">&#x27;mean&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">format</span>(x,<span class="hljs-string">&quot;.2%&quot;</span>))<br>description = [data.<span class="hljs-built_in">min</span>(), data.<span class="hljs-built_in">max</span>(), data.mean(), data.std()]  <span class="hljs-comment"># 依次计算最小值、最大值、均值、标准差</span><br>description = pd.DataFrame(description, index = [<span class="hljs-string">&#x27;Min&#x27;</span>, <span class="hljs-string">&#x27;Max&#x27;</span>, <span class="hljs-string">&#x27;Mean&#x27;</span>, <span class="hljs-string">&#x27;STD&#x27;</span>]).T  <span class="hljs-comment"># 将结果存入数据框</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;描述性统计结果：\n&#x27;</span>,np.<span class="hljs-built_in">round</span>(description, <span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><h2 id="数据预览"><a href="#数据预览" class="headerlink" title="数据预览"></a>数据预览</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 在cmd中安装<br>pip install pandas-profiling   <br>import  pandas_profiling<br>pro = pandas_profiling.<span class="hljs-constructor">ProfileReport(<span class="hljs-params">data1</span>)</span><br>pro.<span class="hljs-keyword">to</span><span class="hljs-constructor">_file(&#x27;<span class="hljs-params">output_file</span>.<span class="hljs-params">html</span>&#x27;)</span><br></code></pre></td></tr></table></figure><h2 id="变化百分比"><a href="#变化百分比" class="headerlink" title="变化百分比"></a>变化百分比</h2><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta"># 元素变化百分比 每个元素与其前一个元素进行比较，并计算变化百分比</span><br>df.pct_change() <br></code></pre></td></tr></table></figure><h2 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 协方差，cov用来计算序列对象之间的协方差</span><br>s1.cov(s2)<br><span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;a&#x27;</span>].cov(<span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;b&#x27;</span>])<br></code></pre></td></tr></table></figure><h2 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h2><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"># 相关性，pearson(默认)，spearman和kendall之间的相关性<br>df[<span class="hljs-string">&#x27;a&#x27;</span>].corr(df[<span class="hljs-string">&#x27;b&#x27;</span>]，<span class="hljs-keyword">method</span> =&#x27;<span class="hljs-title function_">spearman</span>&#x27;)<br><span class="hljs-title function_">print</span> <span class="hljs-params">(frame.corr()</span>)<br></code></pre></td></tr></table></figure><h2 id="排名"><a href="#排名" class="headerlink" title="排名"></a>排名</h2><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">s.rank() 或者 df.rank()  <br># （axis=<span class="hljs-number">0</span>）或列（axis=<span class="hljs-number">1</span>）  <br>#  ascending=<span class="hljs-keyword">True</span> 正向排名或者反向排名<br>#  <span class="hljs-keyword">method</span> （<span class="hljs-title function_">average</span> ：并列组平均排名，<span class="hljs-title function_">min</span> ：组中最低排名，<span class="hljs-title function_">max</span> ：组中最高等级，<span class="hljs-title function_">first</span> ：按在数组中出现的顺序分配等级）<br></code></pre></td></tr></table></figure><h2 id="分组计算"><a href="#分组计算" class="headerlink" title="分组计算"></a>分组计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 多组运算</span><br>df.groupby([<span class="hljs-string">&#x27;班级&#x27;</span>,<span class="hljs-string">&#x27;性别&#x27;</span>])[<span class="hljs-string">&#x27;身高&#x27;</span>].agg([np.<span class="hljs-built_in">sum</span>,np.mean,np.std])<br>df.groupby([<span class="hljs-string">&#x27;班级&#x27;</span>,<span class="hljs-string">&#x27;性别&#x27;</span>]).agg(&#123;<span class="hljs-string">&#x27;身高&#x27;</span>:[<span class="hljs-string">&#x27;min&#x27;</span>],<span class="hljs-string">&#x27;体重&#x27;</span>:[<span class="hljs-string">&#x27;max&#x27;</span>]&#125;)<br>df.groupby(<span class="hljs-string">&#x27;flee&#x27;</span>).agg(&#123;<span class="hljs-string">&#x27;身高&#x27;</span>: [np.median, np.mean], <span class="hljs-string">&#x27;signs&#x27;</span>: np.mean&#125;)<br>df.agg(&#123;<span class="hljs-string">&#x27;A&#x27;</span>:np.<span class="hljs-built_in">sum</span>,<span class="hljs-string">&#x27;B&#x27;</span>:np.mean&#125;)  <span class="hljs-comment"># 对不同列进行不同的计算</span><br>df[[<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-string">&#x27;B&#x27;</span>]].agg([np.<span class="hljs-built_in">sum</span>,np.mean,np.<span class="hljs-built_in">min</span>])  <span class="hljs-comment"># 对多个变量进行多种计算</span><br><br><span class="hljs-comment"># 时间分组 ，先用pd.to_datetime(字段,格式)将某一列转成日期格式</span><br>df.groupby(df[<span class="hljs-string">&#x27;生日&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x:x.year)).count()<br><span class="hljs-comment"># 分组后选第一个,一般数据先排序</span><br>df.groupby(df[<span class="hljs-string">&#x27;生日&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x:x.year),as_index=<span class="hljs-literal">False</span>).first()       <span class="hljs-comment"># Tail(n=1) head()</span><br><span class="hljs-comment"># 找到每组中只有一个数据的</span><br>df.groupby(df[<span class="hljs-string">&#x27;生日&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x:x.month),as_index=<span class="hljs-literal">False</span>).<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x)==<span class="hljs-number">1</span>)<br>data2.groupby(<span class="hljs-string">&#x27;var&#x27;</span>).<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">len</span>(x)&gt;=<span class="hljs-number">10</span>)<br>data.groupby(data.index.year)[<span class="hljs-string">&#x27;年龄&#x27;</span>].mean()<br><br><span class="hljs-comment"># 加权平均</span><br>final3_1 = data_jiep.groupby([<span class="hljs-string">&#x27;产业线&#x27;</span>,<span class="hljs-string">&#x27;模号&#x27;</span>]).apply(<span class="hljs-keyword">lambda</span> g: np.average(g[<span class="hljs-string">&#x27;平均节拍&#x27;</span>], weights=g[<span class="hljs-string">&#x27;模次&#x27;</span>])).reset_index()<br><br><span class="hljs-comment"># groupby作图</span><br>data.groupby(<span class="hljs-string">&#x27;race&#x27;</span>)[<span class="hljs-string">&#x27;flee&#x27;</span>].value_counts().unstack().plot(kind=<span class="hljs-string">&#x27;bar&#x27;</span>, figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">4</span>))<br>data.groupby(<span class="hljs-string">&#x27;flee&#x27;</span>)[<span class="hljs-string">&#x27;age&#x27;</span>].plot(kind=<span class="hljs-string">&#x27;kde&#x27;</span>, legend=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">5</span>))<br><br><span class="hljs-comment"># groupby中的几个函数</span><br><span class="hljs-comment"># 累计  </span><br>df.groupby(<span class="hljs-string">&#x27;key&#x27;</span>).aggregate(<span class="hljs-string">&#x27;min&#x27;</span>, np.median, <span class="hljs-built_in">max</span>)<br><span class="hljs-comment"># 过滤   </span><br>df.groupby(<span class="hljs-string">&#x27;key&#x27;</span>).<span class="hljs-built_in">filter</span>(某个函数)<br><span class="hljs-comment"># 转换    </span><br>df.groupby(<span class="hljs-string">&#x27;key&#x27;</span>).transform(<span class="hljs-keyword">lambda</span> x: x- x.mean())<br><br><span class="hljs-comment">#通过某一个字段分组后，选另一个字段的最小值，构成的数据</span><br>df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;AAA&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>],<span class="hljs-string">&#x27;BBB&#x27;</span>: [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]&#125;)<br>df.loc[df.groupby(<span class="hljs-string">&quot;AAA&quot;</span>)[<span class="hljs-string">&quot;BBB&quot;</span>].idxmin()]<br><span class="hljs-comment"># 按照一个字段排序，另一个字段分组，选取第一个</span><br>df.sort_values(by=<span class="hljs-string">&quot;BBB&quot;</span>).groupby(<span class="hljs-string">&quot;AAA&quot;</span>, as_index=<span class="hljs-literal">False</span>).first()  <span class="hljs-comment">#重新设置索引</span><br><br><span class="hljs-comment"># transform后数据大小不变</span><br>df[<span class="hljs-string">&quot;Order_Total&quot;</span>] = df.groupby(<span class="hljs-string">&#x27;order&#x27;</span>)[<span class="hljs-string">&quot;ext price&quot;</span>].transform(<span class="hljs-string">&#x27;sum&#x27;</span>)<br><br>result0 = data1.to_period(<span class="hljs-string">&#x27;Q&#x27;</span>).groupby(level=<span class="hljs-number">0</span>).apply(<span class="hljs-keyword">lambda</span> x :<span class="hljs-built_in">len</span>(x[<span class="hljs-string">&#x27;var&#x27;</span>].unique().tolist()))<br></code></pre></td></tr></table></figure><h2 id="交叉表"><a href="#交叉表" class="headerlink" title="交叉表"></a>交叉表</h2><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">result1</span> = pd.crosstab(data.index,data[<span class="hljs-string">&#x27;产业线&#x27;</span>],margins=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h2 id="数据透视表"><a href="#数据透视表" class="headerlink" title="数据透视表"></a>数据透视表</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">df.pivot_table(<span class="hljs-string">&#x27;价格&#x27;</span>,<span class="hljs-keyword">index</span>=<span class="hljs-string">&#x27;产地&#x27;</span>,<span class="hljs-keyword">columns</span>=<span class="hljs-string">&#x27;类别&#x27;</span>,aggfunc=<span class="hljs-string">&#x27;max&#x27;</span>,margins=<span class="hljs-keyword">True</span>,fill_value=<span class="hljs-number">0</span>,margins_name=<span class="hljs-string">&#x27;合计&#x27;</span>)<br><br># 用字典形式，可不用<span class="hljs-keyword">values</span>参数<br>df.pivot_table(<span class="hljs-keyword">index</span>=<span class="hljs-string">&#x27;sex&#x27;</span>, <span class="hljs-keyword">columns</span>=<span class="hljs-string">&#x27;class&#x27;</span>, aggfunc=&#123;<span class="hljs-string">&#x27;surviced&#x27;</span>:<span class="hljs-string">&#x27;sum&#x27;</span>, <span class="hljs-string">&#x27;fare&#x27;</span>:<span class="hljs-string">&#x27;mean&#x27;</span>&#125;)<br><br>result  = data.pivot_table(<span class="hljs-keyword">index</span>=data3.to_period(<span class="hljs-string">&#x27;M&#x27;</span>).<span class="hljs-keyword">index</span>,<span class="hljs-keyword">columns</span>= <span class="hljs-string">&#x27;是否异常&#x27;</span>,<span class="hljs-keyword">values</span>=<span class="hljs-string">&#x27;模号&#x27;</span>, aggfunc=<span class="hljs-string">&#x27;count&#x27;</span>)<br><br>result1 = data.pivot_table(<span class="hljs-keyword">index</span>= <span class="hljs-string">&#x27;var1&#x27;</span>,<span class="hljs-keyword">columns</span>=data[<span class="hljs-string">&#x27;var3&#x27;</span>].apply(lambda x: x.strftime(<span class="hljs-string">&#x27;%Y&#x27;</span>)),<br>                      aggfunc=<span class="hljs-string">&#x27;count&#x27;</span>,<span class="hljs-keyword">values</span>=<span class="hljs-string">&#x27;var2&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs sas">#索引需要为日期 对于明细数据，计算固定大小区域的指标<br>s = pd.Series(np.random.rand<span class="hljs-meta">n</span>(1000),<span class="hljs-keyword">index</span>=pd.date_<span class="hljs-meta">range</span>(<span class="hljs-string">&#x27;1/1/2000&#x27;</span>, periods=1000))<br>s = s.cum<span class="hljs-meta">sum</span>()<br>r = s.rolling(<span class="hljs-keyword">window</span>=60)   <br># <span class="hljs-keyword">window</span>：移动窗口的大小<br># min_periods：需要的非空数据点的阈值（否则结果为NA）<br># center：布尔值，是否将标签设置在中间（默认为False）<br><br>df[<span class="hljs-string">&#x27;数量_re&#x27;</span>] = df[<span class="hljs-string">&#x27;数量&#x27;</span>].rolling(<span class="hljs-string">&#x27;7D&#x27;</span>).<span class="hljs-meta">sum</span>()<br><br>data1 = data.set_<span class="hljs-meta">index</span>(<span class="hljs-string">&#x27;入库日期&#x27;</span>).groupby(<span class="hljs-string">&#x27;供应商&#x27;</span>)[<span class="hljs-string">&#x27;入库量&#x27;</span>].rolling(<span class="hljs-string">&#x27;7D&#x27;</span>).<span class="hljs-meta">sum</span>().reset_<span class="hljs-meta">index</span>()<br></code></pre></td></tr></table></figure><h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs prolog"># 当越小越好时<br>df[<span class="hljs-string">&#x27;var_nor&#x27;</span>] = (df[<span class="hljs-string">&#x27;var&#x27;</span>].max() - df[<span class="hljs-string">&#x27;var&#x27;</span>]) / (df[<span class="hljs-string">&#x27;var&#x27;</span>].max() - df[<span class="hljs-string">&#x27;var&#x27;</span>].min())<br># 当越大越好时<br>df[<span class="hljs-string">&#x27;var_nor&#x27;</span>] = (df[<span class="hljs-string">&#x27;var&#x27;</span>] - df[<span class="hljs-string">&#x27;var&#x27;</span>].min()) / (df[<span class="hljs-string">&#x27;var&#x27;</span>].max() - df[<span class="hljs-string">&#x27;var&#x27;</span>].min())<br># 当中值为好是<br>df[<span class="hljs-string">&#x27;var&#x27;</span>] = np.abs(df[<span class="hljs-string">&#x27;var&#x27;</span>]-标准值)<br>df[<span class="hljs-string">&#x27;var_nor&#x27;</span>] = (df[<span class="hljs-string">&#x27;var&#x27;</span>].max() - df[<span class="hljs-string">&#x27;var&#x27;</span>]) / (df[<span class="hljs-string">&#x27;var&#x27;</span>].max() - df[<span class="hljs-string">&#x27;var&#x27;</span>].min())<br><br># 可以写成通用函数<br>def f2(data,col):<br>    col_name = col + <span class="hljs-string">&#x27;_nor&#x27;</span><br>    data_gp = data.groupby(<span class="hljs-string">&#x27;类别&#x27;</span>).mean()<br>    data_gp[col_name] = (data_gp[col] - data_gp[col].min() ) / (data_gp[col].max() - data_gp[col].min() )<br>    return data_gp<br></code></pre></td></tr></table></figure><h2 id="去掉异常值"><a href="#去掉异常值" class="headerlink" title="去掉异常值"></a>去掉异常值</h2><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stan">def f2(<span class="hljs-title">data</span>,<span class="hljs-built_in">col</span>):<br>    q1 = <span class="hljs-title">data</span>[<span class="hljs-built_in">col</span>].<span class="hljs-built_in">quantile</span>(q=<span class="hljs-number">0.25</span>)     <br>    q3 = <span class="hljs-title">data</span>[<span class="hljs-built_in">col</span>].<span class="hljs-built_in">quantile</span>(q=<span class="hljs-number">0.75</span>)<br>    iqr = q3 - q1<br>    t1 = q1 - <span class="hljs-number">3</span>*iqr<br>    t2 = q3 + <span class="hljs-number">3</span>*iqr<br>    <span class="hljs-keyword">return</span> <span class="hljs-title">data</span>[(<span class="hljs-title">data</span>[<span class="hljs-built_in">col</span>]&gt;t1)&amp;(<span class="hljs-title">data</span>[<span class="hljs-built_in">col</span>]&lt;t2)][[&#x27;类别&#x27;,<span class="hljs-built_in">col</span>]]<br></code></pre></td></tr></table></figure><h2 id="正太分布和指数分布数据"><a href="#正太分布和指数分布数据" class="headerlink" title="正太分布和指数分布数据"></a>正太分布和指数分布数据</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">data_norm = pd.<span class="hljs-constructor">DataFrame(&#123;&#x27;正太分布&#x27;:<span class="hljs-params">np</span>.<span class="hljs-params">random</span>.<span class="hljs-params">normal</span>(<span class="hljs-params">loc</span>=60,<span class="hljs-params">scale</span>=15,<span class="hljs-params">size</span>=10000)</span>&#125;)<br>data_exp = pd.<span class="hljs-constructor">DataFrame(&#123;&#x27;指数分布&#x27;:<span class="hljs-params">np</span>.<span class="hljs-params">random</span>.<span class="hljs-params">exponential</span>(<span class="hljs-params">scale</span>=15,<span class="hljs-params">size</span>=10000)</span>+<span class="hljs-number">45</span>&#125;)<br></code></pre></td></tr></table></figure><h2 id="随机选择"><a href="#随机选择" class="headerlink" title="随机选择"></a>随机选择</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">df<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;strategy&#x27;</span>]</span> = np<span class="hljs-selector-class">.random</span><span class="hljs-selector-class">.choice</span>(<span class="hljs-selector-attr">[1,2,3]</span>,<span class="hljs-number">99</span>)<br></code></pre></td></tr></table></figure><h2 id="行列求和"><a href="#行列求和" class="headerlink" title="行列求和"></a>行列求和</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 增加列合计</span><br>df[<span class="hljs-string">&#x27;合计&#x27;</span>]  = df.sum(<span class="hljs-attribute">axis</span>=1)<br><span class="hljs-comment"># 增加行合计</span><br>df.loc[<span class="hljs-string">&#x27;合计&#x27;</span>]  = df.sum(<span class="hljs-attribute">axis</span>=0)<br></code></pre></td></tr></table></figure><h2 id="数据平移"><a href="#数据平移" class="headerlink" title="数据平移"></a>数据平移</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">data<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;经度_前1天&#x27;</span>]</span> = data<span class="hljs-selector-class">.groupby</span>(<span class="hljs-string">&#x27;var&#x27;</span>)<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;经度&#x27;</span>]</span><span class="hljs-selector-class">.shift</span>(<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h2 id="宽表转窄表"><a href="#宽表转窄表" class="headerlink" title="宽表转窄表"></a>宽表转窄表</h2><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs subunit"><span class="hljs-keyword">test </span>= pd.DataFrame(fake_data, columns=[&#x27;subject&#x27;, &#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;])<br><span class="hljs-keyword">test</span><br><span class="hljs-keyword">    </span>subject    A    B    C<br>0    math      88   70   60<br>1    english   90   80   78<br><br># 转换为窄表<br>pd.melt(test, id_vars=[&#x27;subject&#x27;])<br>data3 = pd.melt(data2, id_vars=[&#x27;var1&#x27;,&#x27;var2&#x27;])<br>     subject    variable    value<br>0    math       A           88<br>1    english    A           90<br>2    math       B           70<br>3    english    B           80<br>4    math       C           60<br>5    english    C           78<br></code></pre></td></tr></table></figure><h2 id="二维表"><a href="#二维表" class="headerlink" title="二维表"></a>二维表</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">df.<span class="hljs-keyword">to</span><span class="hljs-constructor">_numpy()</span><br></code></pre></td></tr></table></figure><h2 id="改字典列表"><a href="#改字典列表" class="headerlink" title="改字典列表"></a>改字典列表</h2><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-title">datajs</span> = <span class="hljs-class"><span class="hljs-keyword">data</span>.to_json(<span class="hljs-title">orient</span>=&#x27;<span class="hljs-title">records&#x27;</span>,<span class="hljs-title">force_ascii</span>=<span class="hljs-type">False</span>)</span><br><span class="hljs-meta"># 名称，经度，维度，数值</span><br>[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;虹桥火车站&quot;</span>,<span class="hljs-string">&quot;lng&quot;</span>:<span class="hljs-number">121.327908</span>,<span class="hljs-string">&quot;lat&quot;</span>:<span class="hljs-number">31.20033</span>,<span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-number">3.5225437574</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;上海火车站&quot;</span>,<span class="hljs-string">&quot;lng&quot;</span>:<span class="hljs-number">121.46396</span>,<span class="hljs-string">&quot;lat&quot;</span>:<span class="hljs-number">31.255155</span>,<span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-number">7.0937954904</span>&#125;]<br></code></pre></td></tr></table></figure><h2 id="转字典列表"><a href="#转字典列表" class="headerlink" title="转字典列表"></a>转字典列表</h2><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-title">datajs</span> = <span class="hljs-class"><span class="hljs-keyword">data</span>.to_json(<span class="hljs-title">orient</span>=&#x27;<span class="hljs-title">records&#x27;</span>,<span class="hljs-title">force_ascii</span>=<span class="hljs-type">False</span>)</span><br><span class="hljs-meta"># 名称，经度，维度，数值</span><br>[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;虹桥火车站&quot;</span>,<span class="hljs-string">&quot;lng&quot;</span>:<span class="hljs-number">121.327908</span>,<span class="hljs-string">&quot;lat&quot;</span>:<span class="hljs-number">31.20033</span>,<span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-number">3.5225437574</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;上海火车站&quot;</span>,<span class="hljs-string">&quot;lng&quot;</span>:<span class="hljs-number">121.46396</span>,<span class="hljs-string">&quot;lat&quot;</span>:<span class="hljs-number">31.255155</span>,<span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-number">7.0937954904</span>&#125;]<br></code></pre></td></tr></table></figure><h2 id="两列较大值"><a href="#两列较大值" class="headerlink" title="两列较大值"></a>两列较大值</h2><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lua">df[<span class="hljs-string">&#x27;z&#x27;</span>]=df<span class="hljs-string">[[&#x27;x&#x27;,&#x27;y&#x27;]]</span>.<span class="hljs-built_in">max</span>(axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pandas Notes</title>
    <link href="/2022/08/01/Python/PandasNote/"/>
    <url>/2022/08/01/Python/PandasNote/</url>
    
    <content type="html"><![CDATA[<h1 id="pandas常用速查"><a href="#pandas常用速查" class="headerlink" title="pandas常用速查"></a>pandas常用速查</h1><h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入模块</span><br><span class="hljs-keyword">import</span> pymysql<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># 数据库</span><br><span class="hljs-keyword">from</span> sqlalchemy <span class="hljs-keyword">import</span> create_engine<br><br><span class="hljs-comment"># 可视化</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 如果你的设备是配备Retina屏幕的mac，可以在jupyter notebook中，使用下面一行代码有效提高图像画质</span><br>%config InlineBackend.figure_format = <span class="hljs-string">&#x27;retina&#x27;</span><br><span class="hljs-comment"># 解决 plt 中文显示的问题 mymac</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;Arial Unicode MS&#x27;</span>]<br><span class="hljs-comment"># 设置显示中文 需要先安装字体 aistudio</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>] <span class="hljs-comment"># 指定默认字体</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span>  <span class="hljs-comment"># 用来正常显示负号</span><br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-comment"># notebook渲染图片</span><br>%matplotlib inline<br><span class="hljs-keyword">import</span> pyecharts<br><br><span class="hljs-comment"># 忽略版本问题</span><br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&quot;ignore&quot;</span>)  <br><span class="hljs-comment"># 下载中文字体</span><br>!wget https://mydueros.cdn.bcebos.com/font/simhei.ttf <br><span class="hljs-comment"># 将字体文件复制到 matplotlib&#x27;字体路径</span><br>!cp simhei.ttf /opt/conda/envs/python35-paddle120-env/Lib/python3,<span class="hljs-number">7</span>/site-packages/matplotib/mpl-data/fonts.<br><br><span class="hljs-comment"># 一般只需要将字体文件复制到系统字体田录下即可,但是在 studio上该路径没有写权限,所以此方法不能用 </span><br><span class="hljs-comment"># !cp simhei. ttf /usr/share/fonts/</span><br><br><span class="hljs-comment"># 创建系统字体文件路径</span><br>!mkdir .fonts<br><span class="hljs-comment"># 复制文件到该路径</span><br>!cp simhei.ttf .fonts/<br>!rm -rf .cache/matplotlib<br></code></pre></td></tr></table></figure><p><img src="F:\notes\随笔\src\640-1640748153093.webp" alt="图片"></p><h3 id="算法相关依赖"><a href="#算法相关依赖" class="headerlink" title="算法相关依赖"></a>算法相关依赖</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据归一化</span><br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler<br><br><span class="hljs-comment"># kmeans聚类</span><br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-comment"># DBSCAN聚类</span><br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN<br><span class="hljs-comment"># 线性回归算法</span><br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><span class="hljs-comment"># 逻辑回归算法</span><br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-comment"># 高斯贝叶斯</span><br><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> GaussianNB<br><span class="hljs-comment"># 划分训练/测试集</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-comment"># 准确度报告</span><br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><span class="hljs-comment"># 矩阵报告和均方误差</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report, mean_squared_error<br></code></pre></td></tr></table></figure><h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sqlalchemy <span class="hljs-keyword">import</span> create_engine<br>engine = create_engine(<span class="hljs-string">&#x27;mysql+pymysql://root:root@127.0.0.1:3306/ry?charset=utf8&#x27;</span>)<br><br><span class="hljs-comment"># 查询插入后相关表名及行数</span><br>result_query_sql = <span class="hljs-string">&quot;use information_schema;&quot;</span><br>engine.execute(result_query_sql)<br>result_query_sql = <span class="hljs-string">&quot;SELECT table_name,table_rows FROM tables WHERE TABLE_NAME LIKE &#x27;log%%&#x27; order by table_rows desc;&quot;</span><br>df_result = pd.read_sql(result_query_sql, engine)<br></code></pre></td></tr></table></figure><p><img src="F:\notes\随笔\src\640-1640748153099.webp" alt="图片"></p><h3 id="生成df"><a href="#生成df" class="headerlink" title="生成df"></a>生成df</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># list转df</span><br>df_result = pd.DataFrame(pred,columns=[<span class="hljs-string">&#x27;pred&#x27;</span>])<br>df_result[<span class="hljs-string">&#x27;actual&#x27;</span>] = test_target<br>df_result<br><br><span class="hljs-comment"># df取子df</span><br>df_new = df_old[[<span class="hljs-string">&#x27;col1&#x27;</span>,<span class="hljs-string">&#x27;col2&#x27;</span>]]<br><br><span class="hljs-comment"># dict生成df</span><br>df_test = pd.DataFrame(&#123;&lt;!-- --&gt;<span class="hljs-string">&#x27;A&#x27;</span>:[<span class="hljs-number">0.587221</span>, <span class="hljs-number">0.135673</span>, <span class="hljs-number">0.135673</span>, <span class="hljs-number">0.135673</span>, <span class="hljs-number">0.135673</span>], <br>                        <span class="hljs-string">&#x27;B&#x27;</span>:[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>],<br>                        <span class="hljs-string">&#x27;C&#x27;</span>:[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]&#125;)<br><br><span class="hljs-comment"># 指定列名</span><br>data = pd.DataFrame(dataset.data, columns=dataset.feature_names)<br><br><span class="hljs-comment"># 使用numpy生成20个指定分布(如标准正态分布)的数</span><br>tem = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">20</span>)<br>df3 = pd.DataFrame(tem)<br><br><span class="hljs-comment"># 生成一个和df长度相同的随机数dataframe</span><br>df1 = pd.DataFrame(pd.Series(np.random.randint(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">135</span>)))<br></code></pre></td></tr></table></figure><h3 id="重命名列"><a href="#重命名列" class="headerlink" title="重命名列"></a>重命名列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 重命名列</span><br>data_scaled = data_scaled.rename(columns=&#123;&lt;!-- --&gt;<span class="hljs-string">&#x27;本体油位&#x27;</span>: <span class="hljs-string">&#x27;OILLV&#x27;</span>&#125;)<br></code></pre></td></tr></table></figure><h3 id="增加列"><a href="#增加列" class="headerlink" title="增加列"></a>增加列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># df2df</span><br>df_jj2yyb[<span class="hljs-string">&#x27;r_time&#x27;</span>] = pd.to_datetime(df_jj2yyb[<span class="hljs-string">&#x27;cTime&#x27;</span>])<br><br><span class="hljs-comment"># 新增一列根据salary将数据分为3组</span><br>bins = [<span class="hljs-number">0</span>,<span class="hljs-number">5000</span>, <span class="hljs-number">20000</span>, <span class="hljs-number">50000</span>]<br>group_names = [<span class="hljs-string">&#x27;低&#x27;</span>, <span class="hljs-string">&#x27;中&#x27;</span>, <span class="hljs-string">&#x27;高&#x27;</span>]<br>df[<span class="hljs-string">&#x27;categories&#x27;</span>] = pd.cut(df[<span class="hljs-string">&#x27;salary&#x27;</span>], bins, labels=group_names)<br></code></pre></td></tr></table></figure><h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 检查数据中是否含有任何缺失值</span><br>df.isnull().values.<span class="hljs-built_in">any</span>()<br><br><span class="hljs-comment"># 查看每列数据缺失值情况</span><br>df.isnull().<span class="hljs-built_in">sum</span>()<br><br><span class="hljs-comment"># 提取某列含有空值的行</span><br>df[df[<span class="hljs-string">&#x27;日期&#x27;</span>].isnull()]<br><br><span class="hljs-comment"># 输出每列缺失值具体行数</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> df.columns:<br>    <span class="hljs-keyword">if</span> df[i].count() != <span class="hljs-built_in">len</span>(df):<br>        row = df[i][df[i].isnull().values].index.tolist()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;列名：&quot;&#123;&#125;&quot;, 第&#123;&#125;行位置有缺失值&#x27;</span>.<span class="hljs-built_in">format</span>(i,row))<br><br><span class="hljs-comment"># 众数填充</span><br>heart_df[<span class="hljs-string">&#x27;Thal&#x27;</span>].fillna(heart_df[<span class="hljs-string">&#x27;Thal&#x27;</span>].mode(dropna=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>], inplace=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 连续值列的空值用平均值填充</span><br>dfcolumns = heart_df_encoded.columns.values.tolist()<br><span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> dfcolumns:<br>    <span class="hljs-keyword">if</span> heart_df_encoded[item].dtype == <span class="hljs-string">&#x27;float&#x27;</span>:<br>       heart_df_encoded[item].fillna(heart_df_encoded[item].median(), inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h3 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df_encoded = pd.get_dummies(df_data)<br><span class="hljs-comment">#添加参数Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征</span><br></code></pre></td></tr></table></figure><h3 id="替换值"><a href="#替换值" class="headerlink" title="替换值"></a>替换值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 按列值替换</span><br>num_encode = &#123;&lt;!-- --&gt;<br>    <span class="hljs-string">&#x27;AHD&#x27;</span>: &#123;&lt;!-- --&gt;<span class="hljs-string">&#x27;No&#x27;</span>:<span class="hljs-number">0</span>, <span class="hljs-string">&quot;Yes&quot;</span>:<span class="hljs-number">1</span>&#125;,<br>&#125;<br>heart_df.replace(num_encode,inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h3 id="删除列"><a href="#删除列" class="headerlink" title="删除列"></a>删除列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df_jj2.drop([<span class="hljs-string">&#x27;coll_time&#x27;</span>, <span class="hljs-string">&#x27;polar&#x27;</span>, <span class="hljs-string">&#x27;conn_type&#x27;</span>, <span class="hljs-string">&#x27;phase&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;Unnamed: 0&#x27;</span>],axis=<span class="hljs-number">1</span>,inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h3 id="数据筛选"><a href="#数据筛选" class="headerlink" title="数据筛选"></a>数据筛选</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 取第33行数据</span><br>df.iloc[<span class="hljs-number">32</span>]<br><br><span class="hljs-comment"># 某列以xxx字符串开头</span><br>df_jj2 = df_512.loc[df_512[<span class="hljs-string">&quot;transformer&quot;</span>].<span class="hljs-built_in">str</span>.startswith(<span class="hljs-string">&#x27;JJ2&#x27;</span>)]<br><br>df_jj2yya = df_jj2.loc[df_jj2[<span class="hljs-string">&quot;变压器编号&quot;</span>]==<span class="hljs-string">&#x27;JJ2YYA&#x27;</span>]<br><br><span class="hljs-comment"># 提取第一列中不在第二列出现的数字</span><br>df[<span class="hljs-string">&#x27;col1&#x27;</span>][~df[<span class="hljs-string">&#x27;col1&#x27;</span>].isin(df[<span class="hljs-string">&#x27;col2&#x27;</span>])]<br><br><span class="hljs-comment"># 查找两列值相等的行号</span><br>np.where(df.secondType == df.thirdType)<br><br><span class="hljs-comment"># 包含字符串</span><br>results = df[<span class="hljs-string">&#x27;grammer&#x27;</span>].<span class="hljs-built_in">str</span>.contains(<span class="hljs-string">&quot;Python&quot;</span>)<br><br><span class="hljs-comment"># 提取列名</span><br>df.columns<br><br><span class="hljs-comment"># 查看某列唯一值（种类）</span><br>df[<span class="hljs-string">&#x27;education&#x27;</span>].nunique()<br><br><span class="hljs-comment"># 删除重复数据</span><br>df.drop_duplicates(inplace=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 某列等于某值</span><br>df[df.col_name==<span class="hljs-number">0.587221</span>]<br><span class="hljs-comment"># df.col_name==0.587221 各行判断结果返回值(True/False)</span><br><br><span class="hljs-comment"># 查看某列唯一值及计数</span><br>df_jj2[<span class="hljs-string">&quot;变压器编号&quot;</span>].value_counts()<br><br><span class="hljs-comment"># 时间段筛选</span><br>df_jj2yyb_0501_0701 = df_jj2yyb[(df_jj2yyb[<span class="hljs-string">&#x27;r_time&#x27;</span>] &gt;=pd.to_datetime(<span class="hljs-string">&#x27;20200501&#x27;</span>)) &amp; (df_jj2yyb[<span class="hljs-string">&#x27;r_time&#x27;</span>] &lt;= pd.to_datetime(<span class="hljs-string">&#x27;20200701&#x27;</span>))]<br><br><span class="hljs-comment"># 数值筛选</span><br>df[(df[<span class="hljs-string">&#x27;popularity&#x27;</span>] &gt; <span class="hljs-number">3</span>) &amp; (df[<span class="hljs-string">&#x27;popularity&#x27;</span>] &lt; <span class="hljs-number">7</span>)]<br><br><span class="hljs-comment"># 某列字符串截取</span><br>df[<span class="hljs-string">&#x27;Time&#x27;</span>].<span class="hljs-built_in">str</span>[<span class="hljs-number">0</span>:<span class="hljs-number">8</span>]<br><br><span class="hljs-comment"># 随机取num行</span><br>ins_1 = df.sample(n=num)<br><br><span class="hljs-comment"># 数据去重</span><br>df.drop_duplicates([<span class="hljs-string">&#x27;grammer&#x27;</span>])<br><br><span class="hljs-comment"># 按某列排序(降序)</span><br>df.sort_values(<span class="hljs-string">&quot;popularity&quot;</span>,inplace=<span class="hljs-literal">True</span>, ascending=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 取某列最大值所在行</span><br>df[df[<span class="hljs-string">&#x27;popularity&#x27;</span>] == df[<span class="hljs-string">&#x27;popularity&#x27;</span>].<span class="hljs-built_in">max</span>()]<br><br><span class="hljs-comment"># 取某列最大num行</span><br>df.nlargest(num,<span class="hljs-string">&#x27;col_name&#x27;</span>)<br><span class="hljs-comment"># 最大num列画横向柱形图</span><br>df.nlargest(<span class="hljs-number">10</span>).plot(kind=<span class="hljs-string">&#x27;barh&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="F:\notes\随笔\src\640-1640748153106.webp" alt="图片"></p><h3 id="差值计算"><a href="#差值计算" class="headerlink" title="差值计算"></a>差值计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># axis=0或index表示上下移动， periods表示移动的次数，为正时向下移，为负时向上移动。</span><br><span class="hljs-built_in">print</span>(df.diff( periods=<span class="hljs-number">1</span>, axis=‘index‘))<br><span class="hljs-built_in">print</span>(df.diff( periods=-<span class="hljs-number">1</span>, axis=<span class="hljs-number">0</span>))<br><span class="hljs-comment"># axis=1或columns表示左右移动，periods表示移动的次数，为正时向右移，为负时向左移动。</span><br><span class="hljs-built_in">print</span>(df.diff( periods=<span class="hljs-number">1</span>, axis=‘columns‘))<br><span class="hljs-built_in">print</span>(df.diff( periods=-<span class="hljs-number">1</span>, axis=<span class="hljs-number">1</span>))<br><br><span class="hljs-comment"># 变化率计算</span><br>data[<span class="hljs-string">&#x27;收盘价(元)&#x27;</span>].pct_change()<br><br><span class="hljs-comment"># 以5个数据作为一个数据滑动窗口，在这个5个数据上取均值</span><br>df[<span class="hljs-string">&#x27;收盘价(元)&#x27;</span>].rolling(<span class="hljs-number">5</span>).mean()<br></code></pre></td></tr></table></figure><h3 id="数据修改"><a href="#数据修改" class="headerlink" title="数据修改"></a>数据修改</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 删除最后一行</span><br>df = df.drop(labels=df.shape[<span class="hljs-number">0</span>]-<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 添加一行数据[&#x27;Perl&#x27;,6.6]</span><br>row = &#123;&lt;!-- --&gt;<span class="hljs-string">&#x27;grammer&#x27;</span>:<span class="hljs-string">&#x27;Perl&#x27;</span>,<span class="hljs-string">&#x27;popularity&#x27;</span>:<span class="hljs-number">6.6</span>&#125;<br>df = df.append(row,ignore_index=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 某列小数转百分数</span><br>df.style.<span class="hljs-built_in">format</span>(&#123;&lt;!-- --&gt;<span class="hljs-string">&#x27;data&#x27;</span>: <span class="hljs-string">&#x27;&#123;0:.2%&#125;&#x27;</span>.<span class="hljs-built_in">format</span>&#125;)<br><br><span class="hljs-comment"># 反转行</span><br>df.iloc[::-<span class="hljs-number">1</span>, :]<br><br><span class="hljs-comment"># 以两列制作数据透视</span><br>pd.pivot_table(df,values=[<span class="hljs-string">&quot;salary&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>],index=<span class="hljs-string">&quot;positionId&quot;</span>)<br><br><span class="hljs-comment"># 同时对两列进行计算</span><br>df[[<span class="hljs-string">&quot;salary&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>]].agg([np.<span class="hljs-built_in">sum</span>,np.mean,np.<span class="hljs-built_in">min</span>])<br><br><span class="hljs-comment"># 对不同列执行不同的计算</span><br>df.agg(&#123;&lt;!-- --&gt;<span class="hljs-string">&quot;salary&quot;</span>:np.<span class="hljs-built_in">sum</span>,<span class="hljs-string">&quot;score&quot;</span>:np.mean&#125;)<br></code></pre></td></tr></table></figure><h3 id="时间格式转换"><a href="#时间格式转换" class="headerlink" title="时间格式转换"></a>时间格式转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 时间戳转时间字符串</span><br>df_jj2[<span class="hljs-string">&#x27;cTime&#x27;</span>] =df_jj2[<span class="hljs-string">&#x27;coll_time&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: time.strftime(<span class="hljs-string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>, time.localtime(x)))<br><br><span class="hljs-comment"># 时间字符串转时间格式</span><br>df_jj2yyb[<span class="hljs-string">&#x27;r_time&#x27;</span>] = pd.to_datetime(df_jj2yyb[<span class="hljs-string">&#x27;cTime&#x27;</span>])<br><br><span class="hljs-comment"># 时间格式转时间戳</span><br>dtime = pd.to_datetime(df_jj2yyb[<span class="hljs-string">&#x27;r_time&#x27;</span>])<br>v = (dtime.values - np.datetime64(<span class="hljs-string">&#x27;1970-01-01T08:00:00Z&#x27;</span>)) / np.timedelta64(<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;ms&#x27;</span>)<br>df_jj2yyb[<span class="hljs-string">&#x27;timestamp&#x27;</span>] = v<br></code></pre></td></tr></table></figure><h3 id="设置索引列"><a href="#设置索引列" class="headerlink" title="设置索引列"></a>设置索引列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df_jj2yyb_small_noise = df_jj2yyb_small_noise.set_index(<span class="hljs-string">&#x27;timestamp&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">fig, ax = plt.subplots()<br>df.plot(legend=<span class="hljs-literal">True</span>, ax=ax)<br>plt.legend(loc=<span class="hljs-number">1</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="F:\notes\随笔\src\640-1640748153114.webp" alt="图片"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">6</span>))<br>plt.plot(max_iter_list, accuracy, color=<span class="hljs-string">&#x27;red&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>,<br>         markersize=<span class="hljs-number">10</span>)<br>plt.title(<span class="hljs-string">&#x27;Accuracy Vs max_iter Value&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;max_iter Value&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Accuracy&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="F:\notes\随笔\src\640-1640748153122.webp" alt="图片"></p><h3 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(df[:, <span class="hljs-number">0</span>], df[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&quot;red&quot;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;lable0&#x27;</span>)   <br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)  <br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)  <br>plt.legend(loc=<span class="hljs-number">2</span>)  <br>plt.show()  <br></code></pre></td></tr></table></figure><p><img src="F:\notes\随笔\src\640-1640748153129.webp" alt="图片"></p><h3 id="柱状图"><a href="#柱状图" class="headerlink" title="柱状图"></a>柱状图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">df = pd.Series(tree.feature_importances_, index=data.columns)<br><span class="hljs-comment"># 取某列最大Num行画横向柱形图</span><br>df.nlargest(<span class="hljs-number">10</span>).plot(kind=<span class="hljs-string">&#x27;barh&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="F:\notes\随笔\src\640-1640748153140.webp" alt="图片"><img src="F:\notes\随笔\src\640-1640748153147.webp" alt="图片"></p><h3 id="热力图"><a href="#热力图" class="headerlink" title="热力图"></a>热力图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">df_corr = combine.corr()<br>plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>))<br>g=sns.heatmap(df_corr,annot=<span class="hljs-literal">True</span>,cmap=<span class="hljs-string">&quot;RdYlGn&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="F:\notes\随笔\src\640-1640748153156.webp" alt="图片"></p><h1 id="66个最常用的pandas数据分析函数"><a href="#66个最常用的pandas数据分析函数" class="headerlink" title="66个最常用的pandas数据分析函数"></a>66个最常用的pandas数据分析函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df <span class="hljs-comment">#任何pandas DataFrame对象 </span><br>s <span class="hljs-comment">#任何pandas series对象</span><br></code></pre></td></tr></table></figure><h4 id="从各种不同的来源和格式导入数据"><a href="#从各种不同的来源和格式导入数据" class="headerlink" title="从各种不同的来源和格式导入数据"></a>从各种不同的来源和格式导入数据</h4><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">pd.read<span class="hljs-constructor">_csv(<span class="hljs-params">filename</span>)</span> # 从CSV文件 <br>pd.read<span class="hljs-constructor">_table(<span class="hljs-params">filename</span>)</span> # 从分隔的文本文件（例如CSV）中 <br>pd.read<span class="hljs-constructor">_excel(<span class="hljs-params">filename</span>)</span> # 从Excel文件 <br>pd.read<span class="hljs-constructor">_sql(<span class="hljs-params">query</span>, <span class="hljs-params">connection_object</span>)</span> # 从SQL表/数据库中读取 <br>pd.read<span class="hljs-constructor">_json(<span class="hljs-params">json_string</span>)</span> # 从JSON格式的字符串，URL或文件中读取。<br>pd.read<span class="hljs-constructor">_html(<span class="hljs-params">url</span>)</span> # 解析html URL，字符串或文件，并将表提取到数据帧列表 <br>pd.read<span class="hljs-constructor">_clipboard()</span> # 获取剪贴板的内容并将其传递给 read<span class="hljs-constructor">_table()</span> <br>pd.<span class="hljs-constructor">DataFrame(<span class="hljs-params">dict</span>)</span> # 从字典中，列名称的键，列表中的数据的值<br></code></pre></td></tr></table></figure><h4 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h4><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">df.<span class="hljs-keyword">to</span><span class="hljs-constructor">_csv(<span class="hljs-params">filename</span>)</span> # 写入CSV文件 <br>df.<span class="hljs-keyword">to</span><span class="hljs-constructor">_excel(<span class="hljs-params">filename</span>)</span> # 写入Excel文件 <br>df.<span class="hljs-keyword">to</span><span class="hljs-constructor">_sql(<span class="hljs-params">table_name</span>, <span class="hljs-params">connection_object</span>)</span> # 写入SQL表 <br>df.<span class="hljs-keyword">to</span><span class="hljs-constructor">_json(<span class="hljs-params">filename</span>)</span> # 以JSON格式写入文件<br></code></pre></td></tr></table></figure><h4 id="创建测试对象"><a href="#创建测试对象" class="headerlink" title="创建测试对象"></a>创建测试对象</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">pd.DataFrame(np.random.rand(<span class="hljs-number">20</span>,<span class="hljs-number">5</span>))  <span class="hljs-comment"># 5列20行随机浮点数 pd.Series(my_list)      # 从一个可迭代的序列创建一个序列my_list </span><br>df.index = pd.date_range(<span class="hljs-string">&#x27;1900/1/30&#x27;</span>, periods=df.shape[<span class="hljs-number">0</span>]) <span class="hljs-comment"># 添加日期索引</span><br></code></pre></td></tr></table></figure><h4 id="查看、检查数据"><a href="#查看、检查数据" class="headerlink" title="查看、检查数据"></a>查看、检查数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">df.head(n)                       <span class="hljs-comment"># DataFrame的前n行 </span><br>df.tail(n)                       <span class="hljs-comment"># DataFrame的最后n行 </span><br>df.shape                         <span class="hljs-comment"># 行数和列数 </span><br>df.info()                        <span class="hljs-comment"># 索引，数据类型和内存信息 </span><br>df.describe()                    <span class="hljs-comment"># 数值列的摘要统计信息 </span><br>s.value_counts(dropna=<span class="hljs-literal">False</span>)     <span class="hljs-comment"># 查看唯一值和计数 </span><br>df.apply(pd.Series.value_counts) <span class="hljs-comment"># 所有列的唯一值和计数</span><br></code></pre></td></tr></table></figure><h4 id="数据选取"><a href="#数据选取" class="headerlink" title="数据选取"></a>数据选取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">使用这些命令选择数据的特定子集。<br>df[col]               <span class="hljs-comment"># 返回带有标签col的列 </span><br>df[[col1, col2]]      <span class="hljs-comment"># 返回列作为新的DataFrame </span><br>s.iloc[<span class="hljs-number">0</span>]             <span class="hljs-comment"># 按位置选择 </span><br>s.loc[<span class="hljs-string">&#x27;index_one&#x27;</span>]    <span class="hljs-comment"># 按索引选择 </span><br>df.iloc[<span class="hljs-number">0</span>,:]          <span class="hljs-comment"># 第一行 </span><br>df.iloc[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]          <span class="hljs-comment"># 第一栏的第一元素</span><br></code></pre></td></tr></table></figure><h4 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">df.columns = [<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]      <span class="hljs-comment"># 重命名列 </span><br>pd.isnull()                     <span class="hljs-comment"># 空值检查，返回Boolean Arrray </span><br>pd.notnull()                    <span class="hljs-comment"># 与pd.isnull() 相反 </span><br>df.dropna()                     <span class="hljs-comment"># 删除所有包含空值的行 </span><br>df.dropna(axis=<span class="hljs-number">1</span>)               <span class="hljs-comment"># 删除所有包含空值的列 </span><br>df.dropna(axis=<span class="hljs-number">1</span>,thresh=n)      <span class="hljs-comment"># 删除所有具有少于n个非null值的行 </span><br>df.fillna(x)                    <span class="hljs-comment"># 将所有空值替换为x </span><br>s.fillna(s.mean())              <span class="hljs-comment"># 用均值替换所有空值（均值可以用统计模块中的几乎所有函数替换 ） </span><br>s.astype(<span class="hljs-built_in">float</span>)                 <span class="hljs-comment"># 将系列的数据类型转换为float </span><br>s.replace(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;one&#x27;</span>)              <span class="hljs-comment"># 1 用 &#x27;one&#x27; </span><br>s.replace([<span class="hljs-number">1</span>,<span class="hljs-number">3</span>],[<span class="hljs-string">&#x27;one&#x27;</span>,<span class="hljs-string">&#x27;three&#x27;</span>]) <span class="hljs-comment"># 替换所有等于的值 替换为所有1 &#x27;one&#x27; ，并 3 用 &#x27;three&#x27; </span><br>df.rename(columns=<span class="hljs-keyword">lambda</span> x: x + <span class="hljs-number">1</span>)   <span class="hljs-comment"># 列的重命名 </span><br>df.rename(columns=&#123;&lt;!-- --&gt;<span class="hljs-string">&#x27;old_name&#x27;</span>: <span class="hljs-string">&#x27;new_ name&#x27;</span>&#125;)<span class="hljs-comment"># 选择性重命名 </span><br>df.set_index(<span class="hljs-string">&#x27;column_one&#x27;</span>)           <span class="hljs-comment"># 更改索引 </span><br>df.rename(index=<span class="hljs-keyword">lambda</span> x: x + <span class="hljs-number">1</span>)     <span class="hljs-comment"># 大规模重命名索引</span><br></code></pre></td></tr></table></figure><h4 id="筛选，排序和分组依据"><a href="#筛选，排序和分组依据" class="headerlink" title="筛选，排序和分组依据"></a>筛选，排序和分组依据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">df[df[col] &gt; <span class="hljs-number">0.5</span>]                      <span class="hljs-comment"># 列 col 大于 0.5 df[(df[col] &gt; 0.5) &amp; (df[col] &lt; 0.7)]  # 小于 0.7 大于0.5的行 </span><br>df.sort_values(col1)                   <span class="hljs-comment"># 按col1升序对值进行排序 </span><br>df.sort_values(col2,ascending=<span class="hljs-literal">False</span>)   <span class="hljs-comment"># 按col2 降序对值进行 排序 </span><br>df.sort_values([col1,col2],ascending=[<span class="hljs-literal">True</span>,<span class="hljs-literal">False</span>]) <span class="hljs-comment">#按 col1 升序排序，然后 col2 按降序排序 </span><br>df.groupby(col)                        <span class="hljs-comment">#从一个栏返回GROUPBY对象 </span><br>df.groupby([col1,col2]) <span class="hljs-comment"># 返回来自多个列的groupby对象 </span><br>df.groupby(col1)[col2]                 <span class="hljs-comment"># 返回中的值的平均值 col2，按中的值分组 col1 （平均值可以用统计模块中的几乎所有函数替换 ） </span><br>df.pivot_table(index=col1,values=[col2,col3],aggfunc=mean) <span class="hljs-comment"># 创建一个数据透视表组通过 col1 ，并计算平均值的 col2 和 col3 </span><br>df.groupby(col1).agg(np.mean)          <span class="hljs-comment"># 在所有列中找到每个唯一col1 组的平均值 </span><br>df.apply(np.mean)                      <span class="hljs-comment">#np.mean() 在每列上应用该函数 </span><br>df.apply(np.<span class="hljs-built_in">max</span>,axis=<span class="hljs-number">1</span>)                <span class="hljs-comment"># np.max() 在每行上应用功能</span><br></code></pre></td></tr></table></figure><h4 id="数据合并"><a href="#数据合并" class="headerlink" title="数据合并"></a>数据合并</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">df1.append(df2)                   <span class="hljs-comment"># 将df2添加 df1的末尾 （各列应相同） </span><br>pd.concat([df1, df2],axis=<span class="hljs-number">1</span>)      <span class="hljs-comment"># 将 df1的列添加到df2的末尾 （行应相同） </span><br>df1.join(df2,on=col1,how=<span class="hljs-string">&#x27;inner&#x27;</span>) <span class="hljs-comment"># SQL样式将列 df1 与 df2 行所在的列col 具有相同值的列连接起来。&#x27;how&#x27;可以是一个 &#x27;left&#x27;， &#x27;right&#x27;， &#x27;outer&#x27;， &#x27;inner&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">df.describe()    <span class="hljs-comment"># 数值列的摘要统计信息 </span><br>df.mean()        <span class="hljs-comment"># 返回均值的所有列 </span><br>df.corr()        <span class="hljs-comment"># 返回DataFrame中各列之间的相关性 </span><br>df.count()       <span class="hljs-comment"># 返回非空值的每个数据帧列中的数字 </span><br>df.<span class="hljs-built_in">max</span>()         <span class="hljs-comment"># 返回每列中的最高值 </span><br>df.<span class="hljs-built_in">min</span>()         <span class="hljs-comment"># 返回每一列中的最小值 </span><br>df.median()      <span class="hljs-comment"># 返回每列的中位数 </span><br>df.std()         <span class="hljs-comment"># 返回每列的标准偏差</span><br></code></pre></td></tr></table></figure><h1 id="16个函数，用于数据清洗"><a href="#16个函数，用于数据清洗" class="headerlink" title="16个函数，用于数据清洗"></a>16个函数，用于数据清洗</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入数据集</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>df =&#123;&lt;!-- --&gt;<span class="hljs-string">&#x27;姓名&#x27;</span>:[<span class="hljs-string">&#x27; 黄同学&#x27;</span>,<span class="hljs-string">&#x27;黄至尊&#x27;</span>,<span class="hljs-string">&#x27;黄老邪 &#x27;</span>,<span class="hljs-string">&#x27;陈大美&#x27;</span>,<span class="hljs-string">&#x27;孙尚香&#x27;</span>],<br>     <span class="hljs-string">&#x27;英文名&#x27;</span>:[<span class="hljs-string">&#x27;Huang tong_xue&#x27;</span>,<span class="hljs-string">&#x27;huang zhi_zun&#x27;</span>,<span class="hljs-string">&#x27;Huang Lao_xie&#x27;</span>,<span class="hljs-string">&#x27;Chen Da_mei&#x27;</span>,<span class="hljs-string">&#x27;sun shang_xiang&#x27;</span>],<br>     <span class="hljs-string">&#x27;性别&#x27;</span>:[<span class="hljs-string">&#x27;男&#x27;</span>,<span class="hljs-string">&#x27;women&#x27;</span>,<span class="hljs-string">&#x27;men&#x27;</span>,<span class="hljs-string">&#x27;女&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>],<br>     <span class="hljs-string">&#x27;身份证&#x27;</span>:[<span class="hljs-string">&#x27;463895200003128433&#x27;</span>,<span class="hljs-string">&#x27;429475199912122345&#x27;</span>,<span class="hljs-string">&#x27;420934199110102311&#x27;</span>,<span class="hljs-string">&#x27;431085200005230122&#x27;</span>,<span class="hljs-string">&#x27;420953199509082345&#x27;</span>],<br>     <span class="hljs-string">&#x27;身高&#x27;</span>:[<span class="hljs-string">&#x27;mid:175_good&#x27;</span>,<span class="hljs-string">&#x27;low:165_bad&#x27;</span>,<span class="hljs-string">&#x27;low:159_bad&#x27;</span>,<span class="hljs-string">&#x27;high:180_verygood&#x27;</span>,<span class="hljs-string">&#x27;low:172_bad&#x27;</span>],<br>     <span class="hljs-string">&#x27;家庭住址&#x27;</span>:[<span class="hljs-string">&#x27;湖北广水&#x27;</span>,<span class="hljs-string">&#x27;河南信阳&#x27;</span>,<span class="hljs-string">&#x27;广西桂林&#x27;</span>,<span class="hljs-string">&#x27;湖北孝感&#x27;</span>,<span class="hljs-string">&#x27;广东广州&#x27;</span>],<br>     <span class="hljs-string">&#x27;电话号码&#x27;</span>:[<span class="hljs-string">&#x27;13434813546&#x27;</span>,<span class="hljs-string">&#x27;19748672895&#x27;</span>,<span class="hljs-string">&#x27;16728613064&#x27;</span>,<span class="hljs-string">&#x27;14561586431&#x27;</span>,<span class="hljs-string">&#x27;19384683910&#x27;</span>],<br>     <span class="hljs-string">&#x27;收入&#x27;</span>:[<span class="hljs-string">&#x27;1.1万&#x27;</span>,<span class="hljs-string">&#x27;8.5千&#x27;</span>,<span class="hljs-string">&#x27;0.9万&#x27;</span>,<span class="hljs-string">&#x27;6.5千&#x27;</span>,<span class="hljs-string">&#x27;2.0万&#x27;</span>]&#125;<br>df = pd.DataFrame(df)<br>df<br></code></pre></td></tr></table></figure><h4 id="1-cat函数"><a href="#1-cat函数" class="headerlink" title="1.cat函数"></a>1.cat函数</h4><p>用于字符串的拼接</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;姓名&quot;</span>].<span class="hljs-built_in">str</span>.cat(df[<span class="hljs-string">&quot;家庭住址&quot;</span>],sep=<span class="hljs-string">&#x27;-&#x27;</span>*<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><h4 id="2-contains"><a href="#2-contains" class="headerlink" title="2.contains"></a>2.contains</h4><p>判断某个字符串是否包含给定字符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;家庭住址&quot;</span>].<span class="hljs-built_in">str</span>.contains(<span class="hljs-string">&quot;广&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="3-startswith-x2F-endswith"><a href="#3-startswith-x2F-endswith" class="headerlink" title="3.startswith&#x2F;endswith"></a>3.startswith&#x2F;endswith</h4><p>判断某个字符串是否以…开头&#x2F;结尾</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第一个行的“ 黄伟”是以空格开头的</span><br>df[<span class="hljs-string">&quot;姓名&quot;</span>].<span class="hljs-built_in">str</span>.startswith(<span class="hljs-string">&quot;黄&quot;</span>) <br>df[<span class="hljs-string">&quot;英文名&quot;</span>].<span class="hljs-built_in">str</span>.endswith(<span class="hljs-string">&quot;e&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="4-count"><a href="#4-count" class="headerlink" title="4.count"></a>4.count</h4><p>计算给定字符在字符串中出现的次数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;电话号码&quot;</span>].<span class="hljs-built_in">str</span>.count(<span class="hljs-string">&quot;3&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="5-get"><a href="#5-get" class="headerlink" title="5.get"></a>5.get</h4><p>获取指定位置的字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;姓名&quot;</span>].<span class="hljs-built_in">str</span>.get(-<span class="hljs-number">1</span>)<br>df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;:&quot;</span>)<br>df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;:&quot;</span>).<span class="hljs-built_in">str</span>.get(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h4 id="6-len"><a href="#6-len" class="headerlink" title="6.len"></a>6.len</h4><p>计算字符串长度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;性别&quot;</span>].<span class="hljs-built_in">str</span>.<span class="hljs-built_in">len</span>()<br></code></pre></td></tr></table></figure><h4 id="7-upper-x2F-lower"><a href="#7-upper-x2F-lower" class="headerlink" title="7.upper&#x2F;lower"></a>7.upper&#x2F;lower</h4><p>英文大小写转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;英文名&quot;</span>].<span class="hljs-built_in">str</span>.upper()<br>df[<span class="hljs-string">&quot;英文名&quot;</span>].<span class="hljs-built_in">str</span>.lower()<br></code></pre></td></tr></table></figure><h4 id="8-pad-side参数-x2F-center"><a href="#8-pad-side参数-x2F-center" class="headerlink" title="8.pad+side参数&#x2F;center"></a>8.pad+side参数&#x2F;center</h4><p>在字符串的左边、右边或左右两边添加给定字符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;家庭住址&quot;</span>].<span class="hljs-built_in">str</span>.pad(<span class="hljs-number">10</span>,fillchar=<span class="hljs-string">&quot;*&quot;</span>)      <span class="hljs-comment"># 相当于ljust()</span><br>df[<span class="hljs-string">&quot;家庭住址&quot;</span>].<span class="hljs-built_in">str</span>.pad(<span class="hljs-number">10</span>,side=<span class="hljs-string">&quot;right&quot;</span>,fillchar=<span class="hljs-string">&quot;*&quot;</span>)    <span class="hljs-comment"># 相当于rjust()</span><br>df[<span class="hljs-string">&quot;家庭住址&quot;</span>].<span class="hljs-built_in">str</span>.center(<span class="hljs-number">10</span>,fillchar=<span class="hljs-string">&quot;*&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="9-repeat"><a href="#9-repeat" class="headerlink" title="9.repeat"></a>9.repeat</h4><p>重复字符串几次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;性别&quot;</span>].<span class="hljs-built_in">str</span>.repeat(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><h4 id="10-slice-replace"><a href="#10-slice-replace" class="headerlink" title="10.slice_replace"></a>10.slice_replace</h4><p>使用给定的字符串，替换指定的位置的字符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;电话号码&quot;</span>].<span class="hljs-built_in">str</span>.slice_replace(<span class="hljs-number">4</span>,<span class="hljs-number">8</span>,<span class="hljs-string">&quot;*&quot;</span>*<span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure><h4 id="11-replace"><a href="#11-replace" class="headerlink" title="11.replace"></a>11.replace</h4><p>将指定位置的字符，替换为给定的字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.replace(<span class="hljs-string">&quot;:&quot;</span>,<span class="hljs-string">&quot;-&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="12-replace"><a href="#12-replace" class="headerlink" title="12.replace"></a>12.replace</h4><p>将指定位置的字符，替换为给定的字符串(接受正则表达式)</p><ul><li>replace中传入正则表达式，才叫好用；- 先不要管下面这个案例有没有用，你只需要知道，使用正则做数据清洗多好用；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;收入&quot;</span>].<span class="hljs-built_in">str</span>.replace(<span class="hljs-string">&quot;\d+\.\d+&quot;</span>,<span class="hljs-string">&quot;正则&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="13-split方法-expand参数"><a href="#13-split方法-expand参数" class="headerlink" title="13.split方法+expand参数"></a>13.split方法+expand参数</h4><p>搭配join方法功能很强大</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 普通用法</span><br>df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;:&quot;</span>)<br><span class="hljs-comment"># split方法，搭配expand参数</span><br>df[[<span class="hljs-string">&quot;身高描述&quot;</span>,<span class="hljs-string">&quot;final身高&quot;</span>]] = df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;:&quot;</span>,expand=<span class="hljs-literal">True</span>)<br>df<br><span class="hljs-comment"># split方法搭配join方法</span><br>df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;:&quot;</span>).<span class="hljs-built_in">str</span>.join(<span class="hljs-string">&quot;?&quot;</span>*<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><h4 id="14-strip-x2F-rstrip-x2F-lstrip"><a href="#14-strip-x2F-rstrip-x2F-lstrip" class="headerlink" title="14.strip&#x2F;rstrip&#x2F;lstrip"></a>14.strip&#x2F;rstrip&#x2F;lstrip</h4><p>去除空白符、换行符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;姓名&quot;</span>].<span class="hljs-built_in">str</span>.<span class="hljs-built_in">len</span>()<br>df[<span class="hljs-string">&quot;姓名&quot;</span>] = df[<span class="hljs-string">&quot;姓名&quot;</span>].<span class="hljs-built_in">str</span>.strip()<br>df[<span class="hljs-string">&quot;姓名&quot;</span>].<span class="hljs-built_in">str</span>.<span class="hljs-built_in">len</span>()<br></code></pre></td></tr></table></figure><h4 id="15-findall"><a href="#15-findall" class="headerlink" title="15.findall"></a>15.findall</h4><p>利用正则表达式，去字符串中匹配，返回查找结果的列表</p><ul><li>findall使用正则表达式，做数据清洗，真的很香！</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;身高&quot;</span>]<br>df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.findall(<span class="hljs-string">&quot;[a-zA-Z]+&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="16-extract-x2F-extractall"><a href="#16-extract-x2F-extractall" class="headerlink" title="16.extract&#x2F;extractall"></a>16.extract&#x2F;extractall</h4><p>接受正则表达式，抽取匹配的字符串(一定要加上括号)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.extract(<span class="hljs-string">&quot;([a-zA-Z]+)&quot;</span>)<br><span class="hljs-comment"># extractall提取得到复合索引</span><br>df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.extractall(<span class="hljs-string">&quot;([a-zA-Z]+)&quot;</span>)<br><span class="hljs-comment"># extract搭配expand参数</span><br>df[<span class="hljs-string">&quot;身高&quot;</span>].<span class="hljs-built_in">str</span>.extract(<span class="hljs-string">&quot;([a-zA-Z]+).*?([a-zA-Z]+)&quot;</span>,expand=<span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Numpy trick</title>
    <link href="/2022/08/01/Python/numpy%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C/"/>
    <url>/2022/08/01/Python/numpy%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="01数组上的迭代"><a href="#01数组上的迭代" class="headerlink" title="01数组上的迭代"></a>01数组上的迭代</h2><p>NumPy 包含一个迭代器对象numpy.nditer。它是一个有效的多维迭代器对象，可以用于在数组上进行迭代。数组的每个元素可使用 Python 的标准Iterator接口来访问。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">0</span>, <span class="hljs-number">60</span>, <span class="hljs-number">5</span>)<br><span class="hljs-selector-tag">a</span> = <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np<span class="hljs-selector-class">.nditer</span>(a):<br>    <span class="hljs-built_in">print</span>(x)<br><span class="hljs-selector-attr">[[ 0  5 10 15]</span><br> <span class="hljs-selector-attr">[20 25 30 35]</span><br> <span class="hljs-selector-attr">[40 45 50 55]</span>]<br><span class="hljs-number">0</span><br><span class="hljs-number">5</span><br><span class="hljs-number">10</span><br><span class="hljs-number">15</span><br><span class="hljs-number">20</span><br><span class="hljs-number">25</span><br><span class="hljs-number">30</span><br><span class="hljs-number">35</span><br><span class="hljs-number">40</span><br><span class="hljs-number">45</span><br><span class="hljs-number">50</span><br><span class="hljs-number">55</span><br></code></pre></td></tr></table></figure><p>如果两个数组是可广播的，nditer组合对象能够同时迭代它们。假设数 组a具有维度 3X4，并且存在维度为 1X4 的另一个数组b，则使用以下类型的迭代器(数组b被广播到a的大小)。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">0</span>, <span class="hljs-number">60</span>, <span class="hljs-number">5</span>)<br><span class="hljs-selector-tag">a</span> = <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[1, 2, 3, 4]</span>, dtype=int)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> np<span class="hljs-selector-class">.nditer</span>(<span class="hljs-selector-attr">[a, b]</span>):<br>    <span class="hljs-built_in">print</span>(x, y)<br><span class="hljs-selector-attr">[[ 0  5 10 15]</span><br> <span class="hljs-selector-attr">[20 25 30 35]</span><br> <span class="hljs-selector-attr">[40 45 50 55]</span>]<br><span class="hljs-selector-attr">[1 2 3 4]</span><br><span class="hljs-number">0</span> <span class="hljs-number">1</span><br><span class="hljs-number">5</span> <span class="hljs-number">2</span><br><span class="hljs-number">10</span> <span class="hljs-number">3</span><br><span class="hljs-number">15</span> <span class="hljs-number">4</span><br><span class="hljs-number">20</span> <span class="hljs-number">1</span><br><span class="hljs-number">25</span> <span class="hljs-number">2</span><br><span class="hljs-number">30</span> <span class="hljs-number">3</span><br><span class="hljs-number">35</span> <span class="hljs-number">4</span><br><span class="hljs-number">40</span> <span class="hljs-number">1</span><br><span class="hljs-number">45</span> <span class="hljs-number">2</span><br><span class="hljs-number">50</span> <span class="hljs-number">3</span><br><span class="hljs-number">55</span> <span class="hljs-number">4</span><br></code></pre></td></tr></table></figure><h2 id="02-数组形状修改函数"><a href="#02-数组形状修改函数" class="headerlink" title="02 数组形状修改函数"></a>02 数组形状修改函数</h2><h3 id="1-ndarray-reshape"><a href="#1-ndarray-reshape" class="headerlink" title="1.ndarray.reshape"></a><strong>1.ndarray.reshape</strong></h3><p>函数在不改变数据的条件下修改形状，参数如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">ndarray<span class="hljs-selector-class">.reshape</span>(arr, newshape, <span class="hljs-attribute">order</span>)<br>import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">8</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-selector-tag">b</span> = <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-selector-attr">[0 1 2 3 4 5 6 7]</span><br><span class="hljs-selector-attr">[[0 1]</span><br> <span class="hljs-selector-attr">[2 3]</span><br> <span class="hljs-selector-attr">[4 5]</span><br> <span class="hljs-selector-attr">[6 7]</span>]<br></code></pre></td></tr></table></figure><h3 id="2-ndarray-flat"><a href="#2-ndarray-flat" class="headerlink" title="2.ndarray.flat"></a><strong>2.ndarray.flat</strong></h3><p>函数返回数组上的一维迭代器，行为类似 Python 内建的迭代器。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">16</span>, <span class="hljs-number">2</span>).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(a)<br># 返回展开数组中的下标的对应元素<br><span class="hljs-built_in">print</span>(list(a.flat))<br><span class="hljs-string">[[ 0  2  4  6]</span><br><span class="hljs-string"> [ 8 10 12 14]]</span><br>[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">14</span>]<br></code></pre></td></tr></table></figure><h3 id="3-ndarray-flatten"><a href="#3-ndarray-flatten" class="headerlink" title="3.ndarray.flatten"></a><strong>3.ndarray.flatten</strong></h3><p>函数返回折叠为一维的数组副本，函数接受下列参数：</p><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sas">ndarray.flatte<span class="hljs-meta">n</span>(<span class="hljs-keyword">order</span>)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>order</code>：‘C’ — 按行，‘F’ — 按列，‘A’ — 原顺序，‘k’ —元素在内存中的出现顺序。</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>a = np.arange(8).reshape(2, 4)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-comment"># default is column-major</span><br><span class="hljs-built_in">print</span>(a.flatten())<br><span class="hljs-built_in">print</span>(a.flatten(<span class="hljs-attribute">order</span>=<span class="hljs-string">&#x27;F&#x27;</span>))<br>[[0 1 2 3]<br> [4 5 6 7]]<br>[0 1 2 3 4 5 6 7]<br>[0 4 1 5 2 6 3 7]<br></code></pre></td></tr></table></figure><h2 id="03-数组翻转操作函数"><a href="#03-数组翻转操作函数" class="headerlink" title="03 数组翻转操作函数"></a>03 数组翻转操作函数</h2><h3 id="1-numpy-transpose"><a href="#1-numpy-transpose" class="headerlink" title="1.numpy.transpose"></a><strong>1.numpy.transpose</strong></h3><p>函数翻转给定数组的维度。如果可能的话它会返回一个视图。函数接受下列参数：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">numpy.<span class="hljs-built_in">transpose</span>(arr, <span class="hljs-built_in">axes</span>)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：要转置的数组</li><li>a<code>x</code>es：整数的列表，对应维度，通常所有维度都会翻转。</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.arange(<span class="hljs-number">24</span>).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(a)<br>b = np.array(np.transpose(a))<br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-built_in">print</span>(b.shape)<br><span class="hljs-string">[[[ 0  1  2  3]</span><br><span class="hljs-string">  [ 4  5  6  7]</span><br><span class="hljs-string">  [ 8  9 10 11]]</span><br><br> <span class="hljs-string">[[12 13 14 15]</span><br><span class="hljs-string">  [16 17 18 19]</span><br><span class="hljs-string">  [20 21 22 23]]</span>]<br><span class="hljs-string">[[[ 0 12]</span><br><span class="hljs-string">  [ 4 16]</span><br><span class="hljs-string">  [ 8 20]]</span><br><br> <span class="hljs-string">[[ 1 13]</span><br><span class="hljs-string">  [ 5 17]</span><br><span class="hljs-string">  [ 9 21]]</span><br><br> <span class="hljs-string">[[ 2 14]</span><br><span class="hljs-string">  [ 6 18]</span><br><span class="hljs-string">  [10 22]]</span><br><br> <span class="hljs-string">[[ 3 15]</span><br><span class="hljs-string">  [ 7 19]</span><br><span class="hljs-string">  [11 23]]</span>]<br>(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>b = np.array(np.transpose(a, (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)))<br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-built_in">print</span>(b.shape<br><span class="hljs-string">[[[ 0  1  2  3]</span><br><span class="hljs-string">  [12 13 14 15]]</span><br><br> <span class="hljs-string">[[ 4  5  6  7]</span><br><span class="hljs-string">  [16 17 18 19]]</span><br><br> <span class="hljs-string">[[ 8  9 10 11]</span><br><span class="hljs-string">  [20 21 22 23]]</span>]<br>(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure><h3 id="2-numpy-ndarray-T"><a href="#2-numpy-ndarray-T" class="headerlink" title="2. numpy.ndarray.T"></a><strong>2. numpy.ndarray.T</strong></h3><p>该函数属于ndarray类，行为类似于<code>numpy.transpose.</code></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">12</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a.T)</span></span><br><span class="hljs-selector-attr">[[ 0  1  2  3]</span><br> <span class="hljs-selector-attr">[ 4  5  6  7]</span><br> <span class="hljs-selector-attr">[ 8  9 10 11]</span>]<br><span class="hljs-selector-attr">[[ 0  4  8]</span><br> <span class="hljs-selector-attr">[ 1  5  9]</span><br> <span class="hljs-selector-attr">[ 2  6 10]</span><br> <span class="hljs-selector-attr">[ 3  7 11]</span>]<br></code></pre></td></tr></table></figure><h3 id="3-numpy-swapaxes"><a href="#3-numpy-swapaxes" class="headerlink" title="3.numpy.swapaxes"></a><strong>3.numpy.swapaxes</strong></h3><p>函数交换数组的两个轴。这个函数接受下列参数：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">numpy.<span class="hljs-keyword">swapaxes(arr, </span>axis1, axis2)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：要交换其轴的输入数组</li><li><code>axis1</code>：对应第一个轴的整数</li><li><code>axis2</code>：对应第二个轴的整数</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.arange(<span class="hljs-number">8</span>).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(np.swapaxes(a, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))<br><span class="hljs-string">[[[0 1]</span><br><span class="hljs-string">  [2 3]]</span><br><br> <span class="hljs-string">[[4 5]</span><br><span class="hljs-string">  [6 7]]</span>]<br><span class="hljs-string">[[[0 4]</span><br><span class="hljs-string">  [2 6]]</span><br><br> <span class="hljs-string">[[1 5]</span><br><span class="hljs-string">  [3 7]]</span>]<br></code></pre></td></tr></table></figure><h3 id="4-numpy-rollaxis"><a href="#4-numpy-rollaxis" class="headerlink" title="4.numpy.rollaxis"></a><strong>4.numpy.rollaxis</strong></h3><p><code>numpy.rollaxis()</code> 函数向后滚动特定的轴，直到一个特定位置。这个函数接受三个参数：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">numpy.rollaxis(arr, axis, <span class="hljs-literal">start</span>)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>axis</code>：要向后滚动的轴，其它轴的相对位置不会改变</li><li><code>start</code>：默认为零，表示完整的滚动。会滚动到特定位置。</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.arange(<span class="hljs-number">8</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(np.rollaxis(a,<span class="hljs-number">2</span>))<br><span class="hljs-built_in">print</span>(np.rollaxis(a,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><span class="hljs-string">[[[0 1]</span><br><span class="hljs-string">  [2 3]]</span><br><br> <span class="hljs-string">[[4 5]</span><br><span class="hljs-string">  [6 7]]</span>]<br><span class="hljs-string">[[[0 2]</span><br><span class="hljs-string">  [4 6]]</span><br><br> <span class="hljs-string">[[1 3]</span><br><span class="hljs-string">  [5 7]]</span>]<br><span class="hljs-string">[[[0 2]</span><br><span class="hljs-string">  [1 3]]</span><br><br> <span class="hljs-string">[[4 6]</span><br><span class="hljs-string">  [5 7]]</span>]<br></code></pre></td></tr></table></figure><h2 id="04-数组修改维度函数"><a href="#04-数组修改维度函数" class="headerlink" title="04 数组修改维度函数"></a>04 数组修改维度函数</h2><h3 id="1-numpy-broadcast-to"><a href="#1-numpy-broadcast-to" class="headerlink" title="1.numpy.broadcast_to"></a><strong>1.numpy.broadcast_to</strong></h3><p>函数将数组广播到新形状。它在原始数组上返回只 读视图。它通常不连续。如果新形状不符合 NumPy 的广播规则，该函数可能会抛出ValueError。该函数接受以下参数：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">numpy<span class="hljs-selector-class">.broadcast_to</span>(array, shape, subok)<br>import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">4</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.broadcast_to(a,(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)</span></span>))<br><span class="hljs-selector-attr">[[0 1 2 3]</span>]<br><span class="hljs-selector-attr">[[0 1 2 3]</span><br> <span class="hljs-selector-attr">[0 1 2 3]</span><br> <span class="hljs-selector-attr">[0 1 2 3]</span><br> <span class="hljs-selector-attr">[0 1 2 3]</span>]<br></code></pre></td></tr></table></figure><h3 id="2-numpy-expand-dims"><a href="#2-numpy-expand-dims" class="headerlink" title="2.numpy.expand_dims"></a><strong>2.numpy.expand_dims</strong></h3><p>函数通过在指定位置插入新的轴来扩展数组形状。该函数需要两个参数：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">numpy.expand<span class="hljs-constructor">_dims(<span class="hljs-params">arr</span>, <span class="hljs-params">axis</span>)</span><br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>axis</code>：新轴插入的位置</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>x = np<span class="hljs-selector-class">.array</span>((<span class="hljs-selector-attr">[1, 2]</span>, <span class="hljs-selector-attr">[3, 4]</span>))<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x)</span></span><br>y = np<span class="hljs-selector-class">.expand_dims</span>(x, axis=<span class="hljs-number">0</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(y)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x.shape, y.shape)</span></span><br>y = np<span class="hljs-selector-class">.expand_dims</span>(x, axis=<span class="hljs-number">1</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(y)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x.ndim, y.ndim)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x.shape, y.shape)</span></span><br><span class="hljs-selector-attr">[[1 2]</span><br> <span class="hljs-selector-attr">[3 4]</span>]<br><span class="hljs-selector-attr">[[[1 2]</span><br>  <span class="hljs-selector-attr">[3 4]</span>]]<br>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><span class="hljs-selector-attr">[[[1 2]</span>]<br><br> <span class="hljs-selector-attr">[[3 4]</span>]]<br><span class="hljs-number">2</span> <span class="hljs-number">3</span><br>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) (<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h3 id="3-numpy-squeeze"><a href="#3-numpy-squeeze" class="headerlink" title="3.numpy.squeeze"></a><strong>3.numpy.squeeze</strong></h3><p>函数从给定数组的形状中删除一维条目。此函数需要两 个参数。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs matlab">numpy.<span class="hljs-built_in">squeeze</span>(arr, axis)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>axis</code>：整数或整数元组，用于选择形状中单一维度条目的子集</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>x = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">9</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x)</span></span><br>y = np<span class="hljs-selector-class">.squeeze</span>(x)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(y)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x.shape, y.shape)</span></span><br><span class="hljs-selector-attr">[[[0 1 2]</span><br>  <span class="hljs-selector-attr">[3 4 5]</span><br>  <span class="hljs-selector-attr">[6 7 8]</span>]]<br><span class="hljs-selector-attr">[[0 1 2]</span><br> <span class="hljs-selector-attr">[3 4 5]</span><br> <span class="hljs-selector-attr">[6 7 8]</span>]<br>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>) (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><h2 id="05-数组的连接操作"><a href="#05-数组的连接操作" class="headerlink" title="05 数组的连接操作"></a>05 数组的连接操作</h2><p>NumPy中数组的连接函数主要有如下四个：</p><ul><li><code>concatenate</code> 沿着现存的轴连接数据序列</li><li><code>stack</code> 沿着新轴连接数组序列</li><li><code>hstack</code> 水平堆叠序列中的数组(列方向)</li><li><code>vstack</code> 竖直堆叠序列中的数组(行方向)</li></ul><h3 id="1-numpy-stack"><a href="#1-numpy-stack" class="headerlink" title="1.numpy.stack"></a><strong>1.numpy.stack</strong></h3><p>函数沿新轴连接数组序列，需要提供以下参数：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">numpy.stack(<span class="hljs-built_in">array</span>s, axis)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arrays</code>：相同形状的数组序列</li><li><code>axis</code>：返回数组中的轴，输入数组沿着它来堆叠</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.array(<span class="hljs-string">[[1,2],[3,4]]</span>)<br><span class="hljs-built_in">print</span>(a)<br>b = np.array(<span class="hljs-string">[[5,6],[7,8]]</span>)<br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-built_in">print</span>(np.stack((a,b),<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(np.stack((a,b),<span class="hljs-number">1</span>))<br><span class="hljs-string">[[1 2]</span><br><span class="hljs-string"> [3 4]]</span><br><span class="hljs-string">[[5 6]</span><br><span class="hljs-string"> [7 8]]</span><br><span class="hljs-string">[[[1 2]</span><br><span class="hljs-string">  [3 4]]</span><br><br> <span class="hljs-string">[[5 6]</span><br><span class="hljs-string">  [7 8]]</span>]<br><span class="hljs-string">[[[1 2]</span><br><span class="hljs-string">  [5 6]]</span><br><br> <span class="hljs-string">[[3 4]</span><br><span class="hljs-string">  [7 8]]</span>]<br></code></pre></td></tr></table></figure><h3 id="2-numpy-hstack"><a href="#2-numpy-hstack" class="headerlink" title="2.numpy.hstack"></a><strong>2.numpy.hstack</strong></h3><p>是<code>numpy.stack()</code>函数的变体，通过堆叠来生成水平的单个数组。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.array(<span class="hljs-string">[[1, 2], [3, 4]]</span>)<br><span class="hljs-built_in">print</span>(a)<br>b = np.array(<span class="hljs-string">[[5, 6], [7, 8]]</span>)<br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;水平堆叠：&#x27;</span>)<br>c = np.hstack((a, b))<br><span class="hljs-built_in">print</span>(c)<br><span class="hljs-string">[[1 2]</span><br><span class="hljs-string"> [3 4]]</span><br><span class="hljs-string">[[5 6]</span><br><span class="hljs-string"> [7 8]]</span><br>水平堆叠：<br><span class="hljs-string">[[1 2 5 6]</span><br><span class="hljs-string"> [3 4 7 8]]</span><br></code></pre></td></tr></table></figure><h3 id="3-numpy-vstack"><a href="#3-numpy-vstack" class="headerlink" title="3.numpy.vstack"></a><strong>3.numpy.vstack</strong></h3><p>是<code>numpy.stack()</code>函数的变体，通过堆叠来生成竖直的单个数组。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[1, 2]</span>, <span class="hljs-selector-attr">[3, 4]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[5, 6]</span>, <span class="hljs-selector-attr">[7, 8]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;竖直堆叠：&#x27;</span>)</span></span><br>c = np<span class="hljs-selector-class">.vstack</span>((<span class="hljs-selector-tag">a</span>, b))<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(c)</span></span><br><span class="hljs-selector-attr">[[1 2]</span><br> <span class="hljs-selector-attr">[3 4]</span>]<br><span class="hljs-selector-attr">[[5 6]</span><br> <span class="hljs-selector-attr">[7 8]</span>]<br>竖直堆叠：<br><span class="hljs-selector-attr">[[1 2]</span><br> <span class="hljs-selector-attr">[3 4]</span><br> <span class="hljs-selector-attr">[5 6]</span><br> <span class="hljs-selector-attr">[7 8]</span>]<br></code></pre></td></tr></table></figure><h3 id="4-numpy-concatenate"><a href="#4-numpy-concatenate" class="headerlink" title="4.numpy.concatenate"></a><strong>4.numpy.concatenate</strong></h3><p>函数用于沿指定轴连接相同形状的两个或多个数组。该函数接受以下参数。</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">numpy.concatenate</span>((<span class="hljs-built_in">a1</span>, <span class="hljs-built_in">a2</span>, …), axis)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>a1, a2, .</code>：相同类型的数组序列</li><li><code>axis</code>：沿着它连接数组的轴，默认为 0</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.array(<span class="hljs-string">[[1,2],[3,4]]</span>)<br><span class="hljs-built_in">print</span>(a)<br>b = np.array(<span class="hljs-string">[[5,6],[7,8]]</span>)<br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-built_in">print</span>(np.concatenate((a,b)))<br><span class="hljs-built_in">print</span>(np.concatenate((a,b),axis = <span class="hljs-number">1</span>))<br><span class="hljs-string">[[1 2]</span><br><span class="hljs-string"> [3 4]]</span><br><span class="hljs-string">[[5 6]</span><br><span class="hljs-string"> [7 8]]</span><br><span class="hljs-string">[[1 2]</span><br><span class="hljs-string"> [3 4]</span><br><span class="hljs-string"> [5 6]</span><br><span class="hljs-string"> [7 8]]</span><br><span class="hljs-string">[[1 2 5 6]</span><br><span class="hljs-string"> [3 4 7 8]]</span><br></code></pre></td></tr></table></figure><h2 id="06-数组的分割操作"><a href="#06-数组的分割操作" class="headerlink" title="06 数组的分割操作"></a>06 数组的分割操作</h2><p>NumPy中数组的数组分割函数主要如下：</p><ul><li><code>split</code> 将一个数组分割为多个子数组</li><li><code>hsplit</code> 将一个数组水平分割为多个子数组(按列)</li><li><code>vsplit</code> 将一个数组竖直分割为多个子数组(按行)</li></ul><h3 id="1-numpy-split"><a href="#1-numpy-split" class="headerlink" title="1.numpy.split"></a><strong>1.numpy.split</strong></h3><p>该函数沿特定的轴将数组分割为子数组。函数接受三个参数：</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">numpy.<span class="hljs-built_in">split</span>(ary, indices_or_sections, axis)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>ary</code>：被分割的输入数组</li><li><code>indices_or_sections</code>：可以是整数，表明要从输入数组创建的，等大小的子数组的数量。如果此参数是一维数组，则其元素表明要创建新子数组的点。</li><li><code>axis</code>：默认为 0</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">9</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;将数组分为三个大小相等的子数组：&#x27;</span>)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.split</span>(<span class="hljs-selector-tag">a</span>,<span class="hljs-number">3</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;将数组在一维数组中表明的位置分割：&#x27;</span>)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.split</span>(<span class="hljs-selector-tag">a</span>,<span class="hljs-selector-attr">[4,7]</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-selector-attr">[0 1 2 3 4 5 6 7 8]</span><br>将数组分为三个大小相等的子数组：<br><span class="hljs-selector-attr">[array([0, 1, 2]</span>), <br><span class="hljs-function"><span class="hljs-title">array</span><span class="hljs-params">([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])</span></span>, <br><span class="hljs-function"><span class="hljs-title">array</span><span class="hljs-params">([<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])</span></span>]<br>将数组在一维数组中表明的位置分割：<br><span class="hljs-selector-attr">[array([0, 1, 2, 3]</span>), <br><span class="hljs-function"><span class="hljs-title">array</span><span class="hljs-params">([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])</span></span>, <br><span class="hljs-function"><span class="hljs-title">array</span><span class="hljs-params">([<span class="hljs-number">7</span>, <span class="hljs-number">8</span>])</span></span>]<br></code></pre></td></tr></table></figure><h3 id="2-numpy-hsplit"><a href="#2-numpy-hsplit" class="headerlink" title="2.numpy.hsplit"></a><strong>2.numpy.hsplit</strong></h3><p><code>split()</code>函数的特例，其中轴为 1 表示水平分割。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">16</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;水平分割：&#x27;</span>)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.hsplit</span>(<span class="hljs-selector-tag">a</span>,<span class="hljs-number">2</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-selector-attr">[[ 0  1  2  3]</span><br> <span class="hljs-selector-attr">[ 4  5  6  7]</span><br> <span class="hljs-selector-attr">[ 8  9 10 11]</span><br> <span class="hljs-selector-attr">[12 13 14 15]</span>]<br>水平分割：<br><span class="hljs-selector-attr">[array([[ 0,  1]</span>,<br>       <span class="hljs-selector-attr">[ 4,  5]</span>,<br>       <span class="hljs-selector-attr">[ 8,  9]</span>,<br>       <span class="hljs-selector-attr">[12, 13]</span>]), <br> <span class="hljs-built_in">array</span>(<span class="hljs-selector-attr">[[ 2,  3]</span>,<br>       <span class="hljs-selector-attr">[ 6,  7]</span>,<br>       <span class="hljs-selector-attr">[10, 11]</span>,<br>       <span class="hljs-selector-attr">[14, 15]</span>])]<br></code></pre></td></tr></table></figure><h3 id="3-numpy-vsplit"><a href="#3-numpy-vsplit" class="headerlink" title="3.numpy.vsplit"></a><strong>3.numpy.vsplit</strong></h3><p><code>split()</code>函数的特例，其中轴为 0 表示竖直分割，无论输入数组的维度是什么。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">16</span>)<span class="hljs-selector-class">.reshape</span>(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;竖直分割：&#x27;</span>)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.vsplit</span>(<span class="hljs-selector-tag">a</span>,<span class="hljs-number">2</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-selector-attr">[[ 0  1  2  3]</span><br> <span class="hljs-selector-attr">[ 4  5  6  7]</span><br> <span class="hljs-selector-attr">[ 8  9 10 11]</span><br> <span class="hljs-selector-attr">[12 13 14 15]</span>]<br>竖直分割：<br><span class="hljs-selector-attr">[array([[0, 1, 2, 3]</span>,<br>       <span class="hljs-selector-attr">[4, 5, 6, 7]</span>]),<br> <span class="hljs-built_in">array</span>(<span class="hljs-selector-attr">[[ 8,  9, 10, 11]</span>,<br>       <span class="hljs-selector-attr">[12, 13, 14, 15]</span>])]<br></code></pre></td></tr></table></figure><h2 id="07-数组元素操作"><a href="#07-数组元素操作" class="headerlink" title="07 数组元素操作"></a>07 数组元素操作</h2><p>NumPy中数组操作函数主要如下：</p><ul><li><code>resize</code> 返回指定形状的新数组</li><li><code>append</code> 将值添加到数组末尾</li><li><code>insert</code> 沿指定轴将值插入到指定下标之前</li><li><code>delete</code> 返回删掉某个轴的子数组的新数组</li><li><code>unique</code> 寻找数组内的唯一元素</li></ul><h3 id="1-numpy-resize"><a href="#1-numpy-resize" class="headerlink" title="1.numpy.resize"></a><strong>1.numpy.resize</strong></h3><p>函数返回指定大小的新数组。如果新大小大于原始大小，则包含原始数组中的元素的重复副本。如果小于则去掉原始数组的部分数据。该函数接受以下参数：</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">numpy.<span class="hljs-built_in">resize</span>(arr, shape)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：要修改大小的输入数组</li><li><code>shape</code>：返回数组的新形状</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[1,2,3]</span>,<span class="hljs-selector-attr">[4,5,6]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a.shape)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.resize</span>(<span class="hljs-selector-tag">a</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">2</span>))<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b.shape)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;修改第二个数组的大小：&#x27;</span>)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.resize</span>(<span class="hljs-selector-tag">a</span>,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;修改第三个数组的大小：&#x27;</span>)</span></span><br><span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.resize</span>(<span class="hljs-selector-tag">a</span>,(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-selector-attr">[[1 2 3]</span><br> <span class="hljs-selector-attr">[4 5 6]</span>]<br>(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-selector-attr">[[1 2]</span><br> <span class="hljs-selector-attr">[3 4]</span><br> <span class="hljs-selector-attr">[5 6]</span>]<br>(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>修改第二个数组的大小：<br><span class="hljs-selector-attr">[[1 2 3]</span><br> <span class="hljs-selector-attr">[4 5 6]</span><br> <span class="hljs-selector-attr">[1 2 3]</span>]<br>修改第三个数组的大小：<br><span class="hljs-selector-attr">[[1 2]</span><br> <span class="hljs-selector-attr">[3 4]</span>]<br></code></pre></td></tr></table></figure><h3 id="2-numpy-append"><a href="#2-numpy-append" class="headerlink" title="2.numpy.append"></a><strong>2.numpy.append</strong></h3><p>函数在输入数组的末尾添加值。附加操作不是原地的，而是分配新的数组。此外，输入数组的维度必须匹配否则将生成ValueError。函数接受下列函数：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">numpy.<span class="hljs-built_in">append</span>(arr, <span class="hljs-built_in">values</span>, axis)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>values</code>：要向arr添加的值，比如和arr形状相同(除了要添加的轴)</li><li><code>axis</code>：沿着它完成操作的轴。如果没有提供，两个参数都会被展开。</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs lua">import numpy as np<br>a = np.array(<span class="hljs-string">[[1,2,3],[4,5,6]]</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(np.append(a, <span class="hljs-string">[[7,8,9]]</span>,axis = <span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(np.append(a, <span class="hljs-string">[[5,5,5],[7,8,9]]</span>,axis = <span class="hljs-number">1</span>))<br><span class="hljs-string">[[1 2 3]</span><br><span class="hljs-string"> [4 5 6]]</span><br><span class="hljs-string">[[1 2 3]</span><br><span class="hljs-string"> [4 5 6]</span><br><span class="hljs-string"> [7 8 9]]</span><br><span class="hljs-string">[[1 2 3 5 5 5]</span><br><span class="hljs-string"> [4 5 6 7 8 9]]</span><br></code></pre></td></tr></table></figure><h3 id="3-numpy-insert"><a href="#3-numpy-insert" class="headerlink" title="3.numpy.insert"></a><strong>3.numpy.insert</strong></h3><p>函数在给定索引之前，沿给定轴在输入数组中插入值。如果值的类型转换为要插入，则它与输入数组不同。插入没有原地的，函数会返回一个新数组。此外，如果未提供轴，则输入数组会被展开。</p><p>insert()函数接受以下参数：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">numpy.<span class="hljs-keyword">insert</span>(arr, obj, <span class="hljs-keyword">values</span>, axis)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>obj</code>：在其之前插入值的索引</li><li><code>values</code>：要插入的值</li><li><code>axis</code>：沿着它插入的轴</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[1,2]</span>,<span class="hljs-selector-attr">[3,4]</span>,<span class="hljs-selector-attr">[5,6]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.insert(a,<span class="hljs-number">3</span>,[<span class="hljs-number">11</span>,<span class="hljs-number">12</span>])</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.insert(a,<span class="hljs-number">1</span>,[<span class="hljs-number">11</span>],axis = <span class="hljs-number">0</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.insert(a,<span class="hljs-number">1</span>,[<span class="hljs-number">11</span>],axis = <span class="hljs-number">1</span>)</span></span>)<br><span class="hljs-selector-attr">[[1 2]</span><br> <span class="hljs-selector-attr">[3 4]</span><br> <span class="hljs-selector-attr">[5 6]</span>]<br><span class="hljs-selector-attr">[ 1  2  3 11 12  4  5  6]</span><br><span class="hljs-selector-attr">[[ 1  2]</span><br> <span class="hljs-selector-attr">[11 11]</span><br> <span class="hljs-selector-attr">[ 3  4]</span><br> <span class="hljs-selector-attr">[ 5  6]</span>]<br><span class="hljs-selector-attr">[[ 1 11  2]</span><br> <span class="hljs-selector-attr">[ 3 11  4]</span><br> <span class="hljs-selector-attr">[ 5 11  6]</span>]<br></code></pre></td></tr></table></figure><h3 id="4-numpy-delete"><a href="#4-numpy-delete" class="headerlink" title="4.numpy.delete"></a><strong>4.numpy.delete</strong></h3><p>函数返回从输入数组中删除指定子数组的新数组。与insert()函数的情况一样，如果未提供轴参数，则输入数组将展开。该函 数接受以下参数：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">Numpy.<span class="hljs-keyword">delete</span>(arr, obj, axis)<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>obj</code>：可以被切片，整数或者整数数组，表明要从输入数组删除的子数组</li><li><code>axis</code>：沿着它删除给定子数组的轴</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[1,2]</span>,<span class="hljs-selector-attr">[3,4]</span>,<span class="hljs-selector-attr">[5,6]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.delete(a,<span class="hljs-number">5</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.delete(a,<span class="hljs-number">1</span>,axis = <span class="hljs-number">1</span>)</span></span>)<br><span class="hljs-selector-attr">[[1 2]</span><br> <span class="hljs-selector-attr">[3 4]</span><br> <span class="hljs-selector-attr">[5 6]</span>]<br><span class="hljs-selector-attr">[1 2 3 4 5]</span><br><span class="hljs-selector-attr">[[1]</span><br> <span class="hljs-selector-attr">[3]</span><br> <span class="hljs-selector-attr">[5]</span>]<br></code></pre></td></tr></table></figure><h3 id="5-numpy-unique"><a href="#5-numpy-unique" class="headerlink" title="5.numpy.unique"></a><strong>5.numpy.unique</strong></h3><p>函数返回输入数组中的去重元素数组。该函数能够返回一个元组，包含去重数组和相关索引的数组。索引的性质取决于函数调用中返回参数的类型。</p><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ceylon">numpy.unique(arr, <span class="hljs-keyword">return</span><span class="hljs-number">_</span>index, <span class="hljs-keyword">return</span><span class="hljs-number">_</span>inverse, <span class="hljs-keyword">return</span><span class="hljs-number">_</span>counts)<br></code></pre></td></tr></table></figure><p>其中：</p><p>• <code>arr</code>：输入数组，如果不是一维数组则会展开<br>• <code>return_index</code>：如果为true，返回输入数组中的元素下标<br>• <code>return_inverse</code>：如果为true，返回去重数组的下标，它可以用于重构输入数组<br>• <code>return_counts</code>：如果为true，返回去重数组中的元素在原数组中的出现次数</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[5,2,6,2,7,5,6,8,2,9]</span>)<br>u = np<span class="hljs-selector-class">.unique</span>(a)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(u)</span></span><br>u,indices = np<span class="hljs-selector-class">.unique</span>(<span class="hljs-selector-tag">a</span>, return_index = True)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(u, indices)</span></span><br>u,indices = np<span class="hljs-selector-class">.unique</span>(<span class="hljs-selector-tag">a</span>,return_inverse = True)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(u, indices)</span></span><br>u,indices = np<span class="hljs-selector-class">.unique</span>(<span class="hljs-selector-tag">a</span>,return_counts = True)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(u, indices)</span></span><br><span class="hljs-selector-attr">[2 5 6 7 8 9]</span><span class="hljs-selector-attr">[2 5 6 7 8 9]</span> <br><span class="hljs-selector-attr">[1 0 2 4 7 9]</span><span class="hljs-selector-attr">[2 5 6 7 8 9]</span> <br><span class="hljs-selector-attr">[1 0 2 0 3 1 2 4 0 5]</span><span class="hljs-selector-attr">[2 5 6 7 8 9]</span> <br><span class="hljs-selector-attr">[3 2 2 1 1 1]</span><br></code></pre></td></tr></table></figure><h2 id="08-NumPy-字符串函数"><a href="#08-NumPy-字符串函数" class="headerlink" title="08 NumPy - 字符串函数"></a>08 NumPy - 字符串函数</h2><p>以下函数用于对dtype为numpy.string_或numpy.unicode_的数组执行向量 化字符串操作。它们基于 Python 内置库中的标准字符串函数。字符数组类(numpy.char)中定义</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640" alt="图片"></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br><span class="hljs-built_in">print</span>(np.char.<span class="hljs-built_in">add</span>([<span class="hljs-string">&#x27;hello&#x27;</span>],[<span class="hljs-string">&#x27; xyz&#x27;</span>]))<br><span class="hljs-built_in">print</span>(np.char.<span class="hljs-built_in">add</span>([<span class="hljs-string">&#x27;hello&#x27;</span>, <span class="hljs-string">&#x27;hi&#x27;</span>],[<span class="hljs-string">&#x27; abc&#x27;</span>, <span class="hljs-string">&#x27; xyz&#x27;</span>]))<br><span class="hljs-built_in">print</span>(np.char.multiply(<span class="hljs-string">&#x27;Hello &#x27;</span>,3))<br><span class="hljs-built_in">print</span>(np.char.center(<span class="hljs-string">&#x27;hello&#x27;</span>, 20,fillchar = <span class="hljs-string">&#x27;*&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.capitalize(<span class="hljs-string">&#x27;hello world&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.title(<span class="hljs-string">&#x27;hello how are you?&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.lower([<span class="hljs-string">&#x27;HELLO&#x27;</span>,<span class="hljs-string">&#x27;WORLD&#x27;</span>]))<br><span class="hljs-built_in">print</span>(np.char.lower(<span class="hljs-string">&#x27;HELLO&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.upper(<span class="hljs-string">&#x27;hello&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.upper([<span class="hljs-string">&#x27;hello&#x27;</span>,<span class="hljs-string">&#x27;world&#x27;</span>]))<br><span class="hljs-built_in">print</span>(np.char.split (<span class="hljs-string">&#x27;hello how are you?&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.split (<span class="hljs-string">&#x27;YiibaiPoint,Hyderabad,Telangana&#x27;</span>, sep = <span class="hljs-string">&#x27;,&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.splitlines(<span class="hljs-string">&#x27;hello\nhow are you?&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.splitlines(<span class="hljs-string">&#x27;hello\rhow are you?&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.strip(<span class="hljs-string">&#x27;ashok arora&#x27;</span>,<span class="hljs-string">&#x27;a&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.strip([<span class="hljs-string">&#x27;arora&#x27;</span>,<span class="hljs-string">&#x27;admin&#x27;</span>,<span class="hljs-string">&#x27;java&#x27;</span>],<span class="hljs-string">&#x27;a&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.join(<span class="hljs-string">&#x27;:&#x27;</span>,<span class="hljs-string">&#x27;dmy&#x27;</span>))<br><span class="hljs-built_in">print</span>(np.char.join([<span class="hljs-string">&#x27;:&#x27;</span>,<span class="hljs-string">&#x27;-&#x27;</span>],[<span class="hljs-string">&#x27;dmy&#x27;</span>,<span class="hljs-string">&#x27;ymd&#x27;</span>]))<br><span class="hljs-built_in">print</span>(np.char.replace (<span class="hljs-string">&#x27;He is a good boy&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;was&#x27;</span>))<br>a = np.char.encode(<span class="hljs-string">&#x27;hello&#x27;</span>, <span class="hljs-string">&#x27;cp500&#x27;</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(np.char.decode(a,<span class="hljs-string">&#x27;cp500&#x27;</span>))<br>[<span class="hljs-string">&#x27;hello xyz&#x27;</span>]<br>[<span class="hljs-string">&#x27;hello abc&#x27;</span> <span class="hljs-string">&#x27;hi xyz&#x27;</span>]<br>Hello Hello Hello <br>*******hello********<br>Hello world<br>Hello How Are You?<br>[<span class="hljs-string">&#x27;hello&#x27;</span> <span class="hljs-string">&#x27;world&#x27;</span>]<br>hello<br>HELLO<br>[<span class="hljs-string">&#x27;HELLO&#x27;</span> <span class="hljs-string">&#x27;WORLD&#x27;</span>]<br>[<span class="hljs-string">&#x27;hello&#x27;</span>, <span class="hljs-string">&#x27;how&#x27;</span>, <span class="hljs-string">&#x27;are&#x27;</span>, <span class="hljs-string">&#x27;you?&#x27;</span>]<br>[<span class="hljs-string">&#x27;YiibaiPoint&#x27;</span>, <span class="hljs-string">&#x27;Hyderabad&#x27;</span>, <span class="hljs-string">&#x27;Telangana&#x27;</span>]<br>[<span class="hljs-string">&#x27;hello&#x27;</span>, <span class="hljs-string">&#x27;how are you?&#x27;</span>]<br>[<span class="hljs-string">&#x27;hello&#x27;</span>, <span class="hljs-string">&#x27;how are you?&#x27;</span>]<br>shok aror<br>[<span class="hljs-string">&#x27;ror&#x27;</span> <span class="hljs-string">&#x27;dmin&#x27;</span> <span class="hljs-string">&#x27;jav&#x27;</span>]<br>d:m:y<br>[<span class="hljs-string">&#x27;d:m:y&#x27;</span> <span class="hljs-string">&#x27;y-m-d&#x27;</span>]<br>He was a good boy<br>b<span class="hljs-string">&#x27;\x88\x85\x93\x93\x96&#x27;</span><br>hello<br></code></pre></td></tr></table></figure><h2 id="09-NumPy-算数函数"><a href="#09-NumPy-算数函数" class="headerlink" title="09 NumPy - 算数函数"></a>09 NumPy - 算数函数</h2><p>NumPy 包含大量的各种数学运算功能。NumPy 提供标准的三角函数，算术运算的函数，复数处理函数等。</p><ul><li>三角函数</li><li>舍入函数</li><li>算数函数</li></ul><h3 id="1-NumPy-三角函数"><a href="#1-NumPy-三角函数" class="headerlink" title="1. NumPy -三角函数"></a><strong>1. NumPy -三角函数</strong></h3><p>NumPy 拥有标准的三角函数，它为弧度制单位的给定角度返回三角函 数比值。arcsin，arccos，和arctan函数返回给定角度的sin，cos和tan的反三角函数。这些函数的结果可以通过 <code>numpy.degrees()</code>函数通过将弧度制 转换为角度制来验证。</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import numpy as <span class="hljs-built_in">np</span><br>a = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([<span class="hljs-number">0</span>,<span class="hljs-number">30</span>,<span class="hljs-number">45</span>,<span class="hljs-number">60</span>,<span class="hljs-number">90</span>])<br># 通过乘 pi/<span class="hljs-number">180</span> 转化为弧度<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">np</span>.<span class="hljs-built_in">sin</span>(a*<span class="hljs-built_in">np</span>.pi/<span class="hljs-number">180</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">np</span>.<span class="hljs-built_in">cos</span>(a*<span class="hljs-built_in">np</span>.pi/<span class="hljs-number">180</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">np</span>.<span class="hljs-built_in">tan</span>(a*<span class="hljs-built_in">np</span>.pi/<span class="hljs-number">180</span>))<br>[ <span class="hljs-number">0</span>.          <span class="hljs-number">0.5</span>         <span class="hljs-number">0.70710678</span>  <span class="hljs-number">0.8660254</span>   <span class="hljs-number">1</span>.        ]<br>[  <span class="hljs-number">1.00000000e+00</span>   <span class="hljs-number">8.66025404e-01</span>   <span class="hljs-number">7.07106781e-01</span>   <span class="hljs-number">5.00000000e-01</span><br>   <span class="hljs-number">6.12323400e-17</span>]<br>[  <span class="hljs-number">0.00000000e+00</span>   <span class="hljs-number">5.77350269e-01</span>   <span class="hljs-number">1.00000000e+00</span>   <span class="hljs-number">1.73205081e+00</span><br>   <span class="hljs-number">1.63312394e+16</span>]<br></code></pre></td></tr></table></figure><h3 id="2-NumPy-舍入函数"><a href="#2-NumPy-舍入函数" class="headerlink" title="2.NumPy -舍入函数"></a><strong>2.NumPy -舍入函数</strong></h3><ul><li><p><code>numpy.around()</code>这个函数返回四舍五入到所需精度的值</p></li><li><ul><li>numpy.around(a,decimals) – a 输入数组</li><li>decimals 要舍入的小数位数。默认值为0。如果为负，整数将四舍五入到小数点左侧的位置</li></ul></li><li><p><code>numpy.floor()</code> 函数返回不大于输入参数的最大整数。</p></li><li><p><code>numpy.ceil()</code> 函数返回输入值的上限，大于输入参数的最小整数</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[1.0, 5.55, 123, 0.567, 25.532]</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.around(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.around(a, decimals=<span class="hljs-number">1</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.floor(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.ceil(a)</span></span>)<br><span class="hljs-selector-attr">[   1.    6.  123.    1.   26.]</span><br><span class="hljs-selector-attr">[   1.     5.6  123.     0.6   25.5]</span><br><span class="hljs-selector-attr">[   1.    5.  123.    0.   25.]</span><br><span class="hljs-selector-attr">[   1.    6.  123.    1.   26.]</span><br></code></pre></td></tr></table></figure><h3 id="3-NumPy-算数运算"><a href="#3-NumPy-算数运算" class="headerlink" title="3.NumPy - 算数运算"></a><strong>3.NumPy - 算数运算</strong></h3><p>用于执行算术运算(如add()，subtract()，multiply()和divide())的输入数组必须具有相同的形状或符合数组广播规则。</p><ul><li><code>numpy.reciprocal()</code> 函数返回参数逐元素的倒数。</li><li><code>numpy.power()</code> 函数将第一个输入数组中的元素作为底数，计算它与第二个输入数组中相应元素的幂。</li><li><code>numpy.mod()</code> 函数返回输入数组中相应元素的除法余数</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">a</span> = np.array([<span class="hljs-number">0</span>.<span class="hljs-number">25</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>.<span class="hljs-number">2</span>, <span class="hljs-number">100</span>])<br><span class="hljs-attribute">print</span>(np.reciprocal(a))<br><span class="hljs-attribute">print</span>(np.power(a,<span class="hljs-number">2</span>))<br><span class="hljs-attribute">a</span> = np.array([<span class="hljs-number">10</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>])<br><span class="hljs-attribute">b</span> = np.array([<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,<span class="hljs-number">7</span>])<br><span class="hljs-attribute">print</span>(np.mod(a,b))<span class="hljs-meta"></span><br><span class="hljs-meta">[ 4.    0.5   1.    5.    0.01]</span><span class="hljs-meta"></span><br><span class="hljs-meta">[  6.25000000e-02   4.00000000e+00   1.00000000e+00</span><br><span class="hljs-meta">   4.00000000e-02.  1.00000000e+04]</span><span class="hljs-meta"></span><br><span class="hljs-meta">[1 0 2]</span><br></code></pre></td></tr></table></figure><h3 id="4-NumPy-统计函数"><a href="#4-NumPy-统计函数" class="headerlink" title="4.NumPy - 统计函数"></a><strong>4.NumPy - 统计函数</strong></h3><p>NumPy 有很多有用的统计函数，用于从数组中给定的元素中查找最小，最大，百分标准差和方差等。</p><ul><li><p><code>numpy.amin() , numpy.amax()</code> 从给定数组中的元素沿指定轴返回最小值和最大值。</p></li><li><p><code>numpy.ptp()</code> 函数返回沿轴的值的范围(最大值 - 最小值)。</p></li><li><p><code>numpy.percentile()</code> 表示小于这个值得观察值占某个百分比</p></li><li><p><code>numpy.percentile(a, q, axis)</code></p></li><li><ul><li>a 输入数组;</li><li>q 要计算的百分位数，在 0 ~ 100 之间;</li><li>axis 沿着它计算百分位数的轴</li></ul></li><li><p><code>numpy.median() </code>返回数据样本的中位数。</p></li><li><p><code>numpy.mean() </code>沿轴返回数组中元素的算术平均值。</p></li><li><p><code>numpy.average()</code> 返回由每个分量乘以反映其重要性的因子得到的加权平均值</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[3,7,5]</span>,<span class="hljs-selector-attr">[8,4,3]</span>,<span class="hljs-selector-attr">[2,4,9]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.amin(a,<span class="hljs-number">1</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.amax(a,<span class="hljs-number">1</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.ptp(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.percentile(a,<span class="hljs-number">50</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.median(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.mean(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.average(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.std([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])</span></span>) #返回数组标准差<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.var([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])</span></span>) #返回数组方差<br><span class="hljs-selector-attr">[3 3 2]</span><br><span class="hljs-selector-attr">[7 8 9]</span><br><span class="hljs-number">7</span><br><span class="hljs-number">4.0</span><br><span class="hljs-number">4.0</span><br><span class="hljs-number">5.0</span><br><span class="hljs-number">5.0</span><br><span class="hljs-number">1.11803398875</span><br><span class="hljs-number">1.25</span><br></code></pre></td></tr></table></figure><h2 id="10-排序、搜索和计数函数"><a href="#10-排序、搜索和计数函数" class="headerlink" title="10 排序、搜索和计数函数"></a>10 排序、搜索和计数函数</h2><p>NumPy中提供了各种排序相关功能。</p><ul><li><p><code>numpy.sort()</code> 函数返回输入数组的排序副本。numpy.sort(a, axis, kind, order)</p></li><li><ul><li>a 要排序的数组;</li><li>axis 沿着它排序数组的轴，如果没有数组会被展开，沿着最后的轴排序;</li><li>kind 默认为’quicksort’(快速排序);</li><li>order 如果数组包含字段，则是要排序的字段</li></ul></li><li><p><code>numpy.argsort()</code> 函数对输入数组沿给定轴执行间接排序，并使用指定排序类型返回数据的索引数组。这个索引数组用于构造排序后的数组。</p></li><li><p><code>numpy.lexsort()</code> 函数使用键序列执行间接排序。键可以看作是电子表格中的一列。该函数返回一个索引数组，使用它可以获得排序数据。注意，最后一个键恰好是 sort 的主键。</p></li><li><p><code>numpy.argmax() 和 numpy.argmin()</code> 这两个函数分别沿给定轴返回最大和最小元素的索引。</p></li><li><p><code>numpy.nonzero()</code> 函数返回输入数组中非零元素的索引。</p></li><li><p><code>numpy.where()</code> 函数返回输入数组中满足给定条件的元素的索引。</p></li><li><p><code>numpy.extract()</code> 函数返回满足任何条件的元素。</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[3, 7, 3, 1]</span>, <span class="hljs-selector-attr">[9, 7, 8, 7]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.sort(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.argsort(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.argmax(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.argmin(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.nonzero(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.where(a &gt; <span class="hljs-number">3</span>)</span></span>)<br>nm = (<span class="hljs-string">&#x27;raju&#x27;</span>, <span class="hljs-string">&#x27;anil&#x27;</span>, <span class="hljs-string">&#x27;ravi&#x27;</span>, <span class="hljs-string">&#x27;amar&#x27;</span>)<br>dv = (<span class="hljs-string">&#x27;f.y.&#x27;</span>, <span class="hljs-string">&#x27;s.y.&#x27;</span>, <span class="hljs-string">&#x27;s.y.&#x27;</span>, <span class="hljs-string">&#x27;f.y.&#x27;</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.lexsort((dv, nm)</span></span>))<br><span class="hljs-selector-attr">[[1 3 3 7]</span><br> <span class="hljs-selector-attr">[7 7 8 9]</span>]<br><span class="hljs-selector-attr">[[3 0 2 1]</span><br> <span class="hljs-selector-attr">[1 3 2 0]</span>]<br><span class="hljs-number">4</span><br><span class="hljs-number">3</span><br>(<span class="hljs-built_in">array</span>(<span class="hljs-selector-attr">[0, 0, 0, 0, 1, 1, 1, 1]</span>, dtype=int64), <br><span class="hljs-function"><span class="hljs-title">array</span><span class="hljs-params">([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=int64)</span></span>)<br>(<span class="hljs-built_in">array</span>(<span class="hljs-selector-attr">[0, 1, 1, 1, 1]</span>, dtype=int64), <br><span class="hljs-function"><span class="hljs-title">array</span><span class="hljs-params">([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=int64)</span></span>)<br><span class="hljs-selector-attr">[3 1 0 2]</span><br></code></pre></td></tr></table></figure><h2 id="11-IO文件操作"><a href="#11-IO文件操作" class="headerlink" title="11 IO文件操作"></a>11 IO文件操作</h2><p>ndarray对象可以保存到磁盘文件并从磁盘文件加载。可用的 IO 功能有：</p><ul><li><code>numpy.save()</code> 文件将输入数组存储在具有npy扩展名的磁盘文件中。</li><li><code>numpy.load()</code> 从npy文件中重建数组。</li><li><code>numpy.savetxt()和numpy.loadtxt()</code> 函数以简单文本文件格式存储和获取数组数据。</li></ul><pre><code class="hljs">import numpy as npa = np.array([1,2,3,4,5])np.save(&#39;outfile&#39;,a)b = np.load(&#39;outfile.npy&#39;)print(b)a = np.array([1,2,3,4,5])np.savetxt(&#39;out.txt&#39;,a)b = np.loadtxt(&#39;out.txt&#39;)print(b)[1 2 3 4 5][ 1.  2.  3.  4.  5.]</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Numpy function</title>
    <link href="/2022/08/01/Python/Numpy%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%A4%E6%96%AD/"/>
    <url>/2022/08/01/Python/Numpy%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%A4%E6%96%AD/</url>
    
    <content type="html"><![CDATA[<h1 id="数据计算"><a href="#数据计算" class="headerlink" title="数据计算"></a>数据计算</h1><p>Numpy对于相同维度的数组可以直接运算。对于不同shape大小的数据，利用广播机制（Broadcast）进行操作。但是要求最后一个维度的大小为1，这样才能对数据进行加减可以使用进行数据计</p><blockquote><p>广播的规则：</p><ul><li>让所有输入数组都向其中形状最长的数组看齐，形状中不足的部分都通过在前面加 1 补齐。</li><li>输出数组的形状是输入数组形状的各个维度上的最大值。</li><li>如果输入数组的某个维度和输出数组的对应维度的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错。</li><li>当输入数组的某个维度的长度为 1 时，沿着此维度运算时都用此维度上的第一组值。</li></ul></blockquote><h1 id="数学函数"><a href="#数学函数" class="headerlink" title="数学函数"></a>数学函数</h1><p>三角函数：sin() cos() tan()  arcsin()  arccos() arctan()等<br>角度与弧度的转化主要利用的是 np.degrees() 和np.radians() </p><h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><p><strong>linalg</strong>是关于线性代数的函数库。</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>numpy.dot(a, b, out=None) </code></td><td align="left">两个数组的点积，即元素对应相乘。</td></tr><tr><td align="left"><code>numpy.vdot() </code></td><td align="left">两个向量的点积</td></tr><tr><td align="left"><code>numpy.inner()</code></td><td align="left">两个数组的内积</td></tr><tr><td align="left"><code>numpy.matmul()</code></td><td align="left">两个数组的矩阵积</td></tr><tr><td align="left"><code>numpy.linalg.det()</code></td><td align="left">矩阵的行列式</td></tr><tr><td align="left"><code>numpy.linalg.solve()</code></td><td align="left">求解线性矩阵方程</td></tr><tr><td align="left"><code>numpy.linalg.inv()</code></td><td align="left">计算矩阵的乘法逆矩阵</td></tr></tbody></table><h1 id="舍入函数"><a href="#舍入函数" class="headerlink" title="舍入函数"></a>舍入函数</h1><ul><li><code>numpy.around(a,decimals)</code> 返回指定数字的四舍五入值<br> <strong>decimals</strong>: 舍入的小数位数。 默认值为0。 如果为负，整数将四舍五入到小数点左侧的位置</li><li><code>numpy.floor()</code> 向下取整</li><li><code>numpy.ceil()</code> 向上取整</li><li><code>numpy.trunc()</code> 返回数据最接近的整数</li></ul><h1 id="算术函数"><a href="#算术函数" class="headerlink" title="算术函数"></a>算术函数</h1><ul><li><p><code>numpy.add()，numpy.subtract()，numpy.multiply() 和 numpy.divide()</code> 数组的加减乘除</p></li><li><p><code>numpy.reciprocal()</code> 返回参数逐元素的倒数</p></li><li><p><code>numpy.power()</code>将第一个输入数组中的元素作为底数，计算它与第二个输入数组中相应元素的幂。 </p></li><li><p><code>numpy.mod()</code>计算输入数组中相应元素的相除后的余数</p></li><li><p><code>numpy.ufunc.outer</code></p><blockquote><p>他的原理是针对两个数组或者矩阵的每个元素做运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">r = empty(<span class="hljs-built_in">len</span>(A), <span class="hljs-built_in">len</span>(B))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(A)):<br> <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(B)):<br>     r[i,j]=op(A[i], B[j])<br>//op指的是ufunc的操作是什么可能是加 减 乘 除<br></code></pre></td></tr></table></figure></blockquote></li></ul><h1 id="统计函数"><a href="#统计函数" class="headerlink" title="统计函数"></a>统计函数</h1><ul><li><p><code>numpy.amin() 和 numpy.amax()</code> 计算数组中的元素沿指定轴的最小值或最大值。</p></li><li><p><code>numpy.ptp()</code> 计算数组中元素最大值与最小值的差</p></li><li><p><code>numpy.percentile(a, q, axis)</code> 百分位数是统计中使用的度量，表示小于这个值的观察值的百分比</p><p><strong>a</strong>: 输入数组; <strong>q</strong>: 要计算的百分位数，在 0 ~ 100 之间; <strong>axis</strong>: 沿着它计算百分位数的轴</p></li><li><p><code>numpy.median() </code> 中位数</p></li><li><p><code>numpy.mean()</code> 平均值 </p></li><li><p><code>numpy.average()</code> 平均值 更多的用于计算加权平均值</p></li><li><p><code>numpy.std()</code> 标准差</p></li><li><p><code>numpy.var()</code> 方差</p></li></ul><h1 id="排序函数与条件函数"><a href="#排序函数与条件函数" class="headerlink" title="排序函数与条件函数"></a>排序函数与条件函数</h1><table><thead><tr><th align="left">种类</th><th align="left">速度</th><th align="left">最坏情况</th><th align="left">工作空间</th><th align="left">稳定性</th></tr></thead><tbody><tr><td align="left"><code>&#39;quicksort&#39;</code>（快速排序）</td><td align="left">1</td><td align="left"><code>O(n^2)</code></td><td align="left">0</td><td align="left">否</td></tr><tr><td align="left"><code>&#39;mergesort&#39;</code>（归并排序）</td><td align="left">2</td><td align="left"><code>O(n*log(n))</code></td><td align="left">~n&#x2F;2</td><td align="left">是</td></tr><tr><td align="left"><code>&#39;heapsort&#39;</code>（堆排序）</td><td align="left">3</td><td align="left"><code>O(n*log(n))</code></td><td align="left">0</td><td align="left">否</td></tr></tbody></table><ul><li><p><code>numpy.sort(a, axis, kind, order)</code> </p><p><strong>a</strong>: 要排序的数组; <strong>axis</strong>: 沿着它排序数组的轴，如果没有数组会被展开，沿着最后的轴排序， axis&#x3D;0 按列排序，axis&#x3D;1 按行排序; <strong>kind</strong>: 默认为’quicksort’（快速排序） <strong>order</strong>: 如果数组包含字段，则是要排序的字段</p></li><li><p><code>numpy.argsort()</code> 返回的是数组值从小到大的索引值</p></li><li><p><code>numpy.argmax() 和 numpy.argmin()</code>函数分别沿给定轴返回最大和最小元素的索引</p></li><li><p><code>numpy.argpartition(a, kth, axis=- 1, kind=&#39;introselect&#39;, order=None)</code>可以找出 k 个最大数值的索引 </p></li><li><p><code>numpy.nonzero() </code>函数返回输入数组中非零元素的索引</p></li><li><p><code>numpy.where(condition, ture, false)</code> 函数返回输入数组中满足给定条件的元素的索引</p></li><li><p><code>numpy.extract(condition, array)</code> 函数根据某个条件从数组中抽取元素，返回满条件的元素。</p></li><li><p><code>numpy.allclose()</code> 匹配两个数组，检查两个数组是否相似，并得到布尔值表示的输出</p></li></ul><h1 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h1><p><code>numpy.save(file, arr, allow_pickle=True, fix_imports=True)</code> 将数组保存到以 .npy 为扩展名的文件中。</p><ul><li><strong>file</strong>：要保存的文件，扩展名为 .npy，如果文件路径末尾没有扩展名 .npy，该扩展名会被自动加上。</li><li><strong>arr</strong>: 要保存的数组</li><li><strong>allow_pickle</strong>: 可选，布尔值，允许使用 Python pickles 保存对象数组，Python 中的 pickle 用于在保存到磁盘文件或从磁盘文件读取之前，对对象进行序列化和反序列化。</li><li><strong>fix_imports</strong>: 可选，为了方便 Pyhton2 中读取 Python3 保存的数据。</li></ul><h1 id="输出格式设置"><a href="#输出格式设置" class="headerlink" title="输出格式设置"></a>输出格式设置</h1><p><strong>np.set_printopoints()可以控制输出格式 这些选项确定浮点数、数组和其他 NumPy 对象的显示方式。</strong></p><p>precision：int or None浮点输出的精度位数(默认8)# 如floatmode不是fixed，可能是None</p><p>threshold：int触发汇总的数组元素总数而不是完整的repr(默认1000)<br>edgeitems：int在开头和结尾的摘要中的数组项数 每个维度(默认为3)<br>linewidth：int每行用于插入的字符数# 换行符(默认为75)<br>suppress : bool,科学记数法启用</p><p>True用固定点打印浮点数符号，当前精度中的数字等于零将打印为零。</p><p>False用科学记数法；最小数绝对值是&lt;1e-4或比率最大绝对值&gt; 1e3。默认值False</p><p>nanstr：str浮点非字母数字的字符串表示形式(默认为nan)<br>infstr：str浮点无穷大字符串表示形式(默认inf)<br>sign：string，’ - ‘，’+’或’’，控制浮点类型符号的打印。<br>        ‘+’打印正值标志。’’打印空格。’ - ‘省略正值符号，默认</p><p>formatter：可调用字典，格式化功能<br>格式化设置类型：</p><ul><li>‘bool’</li><li>‘int’</li><li>‘timedelta’：’numpy.timedelta64’</li><li>‘datetime’：numpy.datetime64</li><li>‘float’</li><li>‘longfloat’：128位浮点数</li><li>‘complexfloat’</li><li>‘longcomplexfloat’：由两个128位浮点组成</li><li>‘numpystr’ : types numpy.string_ and numpy.unicode</li><li><em>‘object’ : np.object</em> arrays</li><li>‘str’：所有其他字符串</li></ul><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#用于一次设置一组类型的其他键：</span><br> - &#x27;all&#x27;：设置所有类型<br> - &#x27;int_kind&#x27;：设置&#x27;int&#x27;<br> - &#x27;float_kind&#x27;：设置&#x27;float&#x27;和&#x27;longfloat&#x27;<br> - &#x27;complex_kind&#x27;：设置&#x27;complexfloat&#x27;和&#x27;longcomplexfloat&#x27;<br> - &#x27;str_kind&#x27;：设置&#x27;str&#x27;和&#x27;numpystr&#x27;<br></code></pre></td></tr></table></figure><p>floatmode：str控制precision选项的解释<br>   浮点类型值：<br>    -‘fixed’：始终打印精确的’precision精度’小数位<br>    -‘unique’：打印最小小数位数，precision选项被忽略。<br>    -‘maxprec’：打印最多precision小数位数<br>    -‘maxprec_equal’：最多打印precision小数位数</p><p>legacy：string或False<br>     如为字符串“1.13”，则启用1.13传统打印模式。<br>    如设置“False”，禁用传统模式。无法识别的字符串将被忽略</p><h1 id="tips"><a href="#tips" class="headerlink" title="tips:"></a>tips:</h1><p><strong>np.sum和np.add.reduce有什么区别？</strong>两者的性能似乎是完全不同的：对于相对较小的数组大小而言。<code>add.reduce</code>大约快两倍。对于较大的数组大小，差别似乎消失了。较长的答案：  <code>np.sum</code>在中定义numpy&#x2F;core&#x2F;fromnumeric.py。在定义中<code>np.sum</code>，你会看到工作被传递给<code>_methods._sum</code>。该功能是_methods.py简单地说就是：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs javascript">def <span class="hljs-title function_">_sum</span>(a, axis=<span class="hljs-title class_">None</span>, dtype=<span class="hljs-title class_">None</span>, out=<span class="hljs-title class_">None</span>, keepdims=<span class="hljs-title class_">False</span>):<br>    <span class="hljs-keyword">return</span> um.<span class="hljs-property">add</span>.<span class="hljs-title function_">reduce</span>(a, axis=axis, dtype=dtype,<br>                            out=out, keepdims=keepdims)<br></code></pre></td></tr></table></figure><p><strong>怎么对矩阵上三角或者下三角处理数据？</strong></p><p><img src="/%5Csrc%5Cimage-20220222154739735.png" alt="image-20220222154739735"></p><p><strong>numpy.tile(array , size)</strong>  拼接数组 自身多个��</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Numpy array</title>
    <link href="/2022/08/01/Python/Numpy%E6%95%B0%E7%BB%84/"/>
    <url>/2022/08/01/Python/Numpy%E6%95%B0%E7%BB%84/</url>
    
    <content type="html"><![CDATA[<h1 id="数组的定义"><a href="#数组的定义" class="headerlink" title="数组的定义"></a>数组的定义</h1><p>NumPy 最重要的一个特点是其 N 维数组对象 ndarray，它是一系列同类型数据的集合，以 0 下标为开始进行集合中元素的索引。</p><p>ndarray 对象是用于存放同类型元素的多维数组。</p><p>ndarray 中的每个元素在内存中都有相同存储大小的区域。</p><blockquote><p>创建一个 ndarray 只需调用 NumPy 的 array 函数即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.array(<span class="hljs-built_in">object</span>, dtype = <span class="hljs-literal">None</span>, copy = <span class="hljs-literal">True</span>, order = <span class="hljs-literal">None</span>, subok = <span class="hljs-literal">False</span>, ndmin = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><table><thead><tr><th align="left">名称</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">object</td><td align="left">数组或嵌套的数列</td></tr><tr><td align="left">dtype</td><td align="left">数组元素的数据类型，可选</td></tr><tr><td align="left">copy</td><td align="left">对象是否需要复制，可选</td></tr><tr><td align="left">order</td><td align="left">创建数组的样式，C为行方向，F为列方向，A为任意方向（默认）</td></tr><tr><td align="left">subok</td><td align="left">默认返回一个与基类类型一致的数组</td></tr><tr><td align="left">ndmin</td><td align="left">指定生成数组的最小维度</td></tr></tbody></table></blockquote><h1 id="数组的属性"><a href="#数组的属性" class="headerlink" title="数组的属性"></a>数组的属性</h1><table><thead><tr><th align="left">属性</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">ndarray.ndim</td><td align="left">秩，即轴的数量或维度的数量</td></tr><tr><td align="left">ndarray.shape</td><td align="left">数组的维度，对于矩阵，n 行 m 列</td></tr><tr><td align="left">ndarray.size</td><td align="left">数组元素的总个数，相当于 .shape 中 n*m 的值</td></tr><tr><td align="left">ndarray.dtype</td><td align="left">ndarray 对象的元素类型</td></tr><tr><td align="left">ndarray.itemsize</td><td align="left">ndarray 对象中每个元素的大小，以字节为单位</td></tr><tr><td align="left">ndarray.flags</td><td align="left">ndarray 对象的内存信息</td></tr><tr><td align="left">ndarray.real</td><td align="left">ndarray元素的实部</td></tr><tr><td align="left">ndarray.imag</td><td align="left">ndarray 元素的虚部</td></tr><tr><td align="left">ndarray.data</td><td align="left">包含实际数组元素的缓冲区，由于一般通过数组的索引获取元素，所以通常不需要使用这个属性。</td></tr></tbody></table><h1 id="数组的创建方式"><a href="#数组的创建方式" class="headerlink" title="数组的创建方式"></a>数组的创建方式</h1><ul><li><p><code>numpy.empty(shape, dtype = float, order = &#39;C&#39;) </code> 创建一个未初始化的数值<br><strong>order</strong>有两个选项：”C”和”F”；分别代表，行优先和列优先，在计算机内存中的存储元素的顺序。</p></li><li><p><code>numpy.zeros(shape, dtype = float, order = &#39;C&#39;)</code> 创建指定大小的数组，数组元素以 0 来填充<br><strong>order</strong>有两个选项：’C’ 用于 C 的行数组，或者 ‘F’ 用于 FORTRAN 的列数组</p></li><li><p><code>numpy.ones(shape, dtype = None, order = &#39;C&#39;)</code> 创建指定形状的数组，数组元素以 1 来填充</p></li><li><p><code>numpy.asarray(a, dtype = None, order = None)</code><br>numpy.asarray 类似 numpy.array，但 numpy.asarray 参数只有三个，比 numpy.array 少两个。<br>a可以是任意行事的输入，例如列表，元组，多维数组</p></li><li><p><code>numpy.frombuffer(buffer, dtype = float, count = -1, offset = 0)</code><br>numpy.frombuffer 用于实现动态数组。<br>numpy.frombuffer 接受 buffer 输入参数，以流的形式读入转化成 ndarray 对象。<br><em><strong>buffer</strong> 是字符串的时候，Python3 默认 str 是 Unicode 类型，所以要转成 bytestring 在原 str 前加上 b。</em></p><p><strong>count</strong>读取的数据数量，默认为-1，读取所有数据。<strong>offset</strong>读取的起始位置，默认为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br> <br>s =  <span class="hljs-string">b&#x27;Hello World&#x27;</span> <br>a = np.frombuffer(s, dtype =  <span class="hljs-string">&#x27;S1&#x27;</span>)  <br><span class="hljs-built_in">print</span> (a)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5Cimage-20220223120720155.png" alt="image-20220223120720155"></p></li><li><p><code>numpy.fromiter(iterable, dtype, count=-1)</code> numpy.fromiter 方法从可迭代对象中建立 ndarray 对象，返回一维数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br> <br><span class="hljs-comment"># 使用 range 函数创建列表对象  </span><br><span class="hljs-built_in">list</span>=<span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)<br>it=<span class="hljs-built_in">iter</span>(<span class="hljs-built_in">list</span>)<br> <br><span class="hljs-comment"># 使用迭代器创建 ndarray </span><br>x=np.fromiter(it, dtype=<span class="hljs-built_in">float</span>)<br><span class="hljs-built_in">print</span>(x)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5Cimage-20220223120836562.png" alt="image-20220223120836562"></p></li><li><p><code>numpy.arange(start, stop, step, dtype)</code> 根据 start 与 stop 指定的范围以及 step 设定的步长，生成一个 ndarray。<br><strong>start</strong>起始值，默认为0；<strong>stop</strong>终止值（不包含）；<strong>step</strong>步长，默认为1</p></li><li><p><code>np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)</code> 创建一个一维数组，数组是一个<em><strong>等差数列</strong></em>构成的<br><strong>num</strong>要生成的等步长的样本数量，默认为50；<strong>endpoint</strong>该值为 true 时，数列中包含stop值，反之不包含，默认是True。；<strong>retstep</strong>如果为 True 时，生成的数组中会显示间距，反之不显示。</p></li><li><p><code>np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None)</code> 创建一个等比数列<br><strong>base</strong>对数 log 的底数。</p></li></ul><h1 id="数组的索引方法"><a href="#数组的索引方法" class="headerlink" title="数组的索引方法"></a>数组的索引方法</h1><p>简单的方法就是利用下标，例如<code>array[1:5]</code>这类的方法</p><h1 id="常见的数据操作"><a href="#常见的数据操作" class="headerlink" title="常见的数据操作"></a>常见的数据操作</h1><ol><li><p><code>numpy.reshape(arr, newshape, order=&#39;C&#39;)</code> 在不改变数据的条件下 修改数组的形状</p></li><li><p><code>ndarray.flat</code> 数组元素迭代器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>a = np.arange(<span class="hljs-number">9</span>).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br><span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> a:<br><span class="hljs-built_in">print</span>(element)<br><span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> a.flat:<br><span class="hljs-built_in">print</span>(element)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5Cimage-20220223134015788.png" alt="image-20220223134015788"></p></li><li><p><code>ndarray.flatten(order=&#39;C&#39;)</code> 返回一份数组拷贝， 对拷贝所做的修改不会影响原始数据<br>order：’C’ – 按行，’F’ – 按列，’A’ – 原顺序，’K’ – 元素在内存中的出现顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>a = np.arange(<span class="hljs-number">9</span>).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(a.flatten())<br><span class="hljs-built_in">print</span>(a.flatten(order=<span class="hljs-string">&#x27;F&#x27;</span>))<br><span class="hljs-built_in">print</span>(a.flatten(order=<span class="hljs-string">&#x27;A&#x27;</span>))<br><span class="hljs-built_in">print</span>(a.flatten(order=<span class="hljs-string">&#x27;K&#x27;</span>))<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5Cimage-20220223134307151.png" alt="image-20220223134307151"></p></li><li><p><code>numpy.ravel(a, order=&#39;C&#39;)</code> 返回展开数组。返回的是一个视图，对其操作会对原始数据有影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>a = np.arange(<span class="hljs-number">8</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span> (a)<br><span class="hljs-built_in">print</span> (a.ravel())<br><span class="hljs-built_in">print</span> (a.ravel(order = <span class="hljs-string">&#x27;F&#x27;</span>))<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5Cimage-20220223141006315.png" alt="image-20220223141006315"></p></li><li><p><code>numpy.transpose(arr, axes)</code> 用于对换数组的维度<br><strong>arr</strong>：要操作的数组；<strong>axes</strong>：整数列表，对应维度，通常所有维度都会对换。</p></li><li><p><code>numpy.expand_dims(arr, axis)</code> 通过在指定位置插入新的轴来扩展数组形状<br><strong>arr</strong>：输入数组; <strong>axis</strong>：新轴插入的位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x = np.array(([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]))<br><span class="hljs-built_in">print</span>(x.shape)<br>x = np.expand_dims(x, <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(x.shape)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5Cimage-20220223141702884.png" alt="image-20220223141702884"></p></li><li><p><code>numpy.squeeze(arr, axis)</code>从给定数组的形状中删除一维的条目 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x = np.arange(<span class="hljs-number">20</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(x.shape)<br>x = np.squeeze(x)<br><span class="hljs-built_in">print</span>(x.shape)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5Cimage-20220223142030073.png" alt="image-20220223142030073"><br>注意 不表明维度的情况下，squeeze会自动压缩维度大小为1的维度。如果需要压缩的维度大小不为1，回报错的。</p></li><li><p><code>numpy.concatenate((a1, a2, .), axis)</code> 用于沿指定轴连接相同形状的两个或多个数组</p></li><li><p><code>numpy.stack(arrays, axis)</code> 用于沿新轴连接数组序列 </p></li><li><p><code>numpy.split(ary, indices_or_sections, axis)</code> 沿特定的轴将数组分割为子数组<br><strong>indices_or_sections</strong>：如果是一个整数，就用该数平均切分，如果是一个数组，为沿轴切分的位置（左开右闭）</p></li><li><p><code>numpy.array_split(ary, indices_or_sections, axis)</code> 可以不规则的切分数组。 split要求对数据均匀切分，而array_split可以切分成不同的形状</p></li><li><p><code>numpy.append(arr, values, axis=None)</code>在数组的末尾添加值。 追加操作会分配整个数组，并把原来的数组复制到新数组中。</p></li><li><p><code>numpy.insert(arr, obj, values, axis)</code>在给定索引之前，沿给定轴在输入数组中插入值。</p></li><li><p><code>numpy.delete(arr, obj, axis)</code> 返回从输入数组中删除指定子数组的新数组</p></li><li><p><code>numpy.unique(arr, return_index, return_inverse, return_counts)</code>用于去除数组中的重复元素<strong>return_index</strong>：如果为true，返回新列表元素在旧列表中的位置（下标），并以列表形式储; <strong>return_inverse</strong>：如果为true，返回旧列表元素在新列表中的位置（下标），并以列表形式储; <strong>return_counts</strong>：如果为true，返回去重数组中的元素在原数组中的出现次数</p></li><li><p><code>numpy.clip(array, min, max)</code> 使数组数值在上下限范围内</p></li><li><p><code>np.trim_zeros(array)</code> 除去数组中前面和后面的0</p></li><li><p><code>np.full(size, data, dtype)</code> 以data填入size形成一个array</p></li></ol><h1 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h1><ol><li><p><code>np.arange</code>：从范围内选取所有整数（左闭右开）</p></li><li><p><code>numpy.random.choice</code> 从一定范围内随机选取几个</p></li><li><p><code>numpy.random.uniform</code> 均匀分布</p></li><li><p><code>numpy.random.randn</code> 正态分布</p></li><li><p><code>numpy.random.random</code> 随机数  浮点数</p></li><li><p><code>numpy.random.rand</code>    0-1分布 随机数</p></li><li><p><code>numpy.random.choice(a, size=None, replace=True, p=None)</code><br>从a(只要是ndarray都可以，但必须是一维的)中随机抽取数字，并组成指定大小(size)的数组<br>replace:True表示可以取相同数字，False表示不可以取相同数字<br>数组p：与数组a相对应，表示取数组a中每个元素的概率，默认为选取每个元素的概率相同。</p></li><li><p>随机数，可以直接调用<code>np.random.rand()</code><br>但是设定一个随机种子<code>np.random.seed()</code>，你可以实现每次获取相同的随机数</p></li><li><p><code>np.random.permutation()</code>随机排列序列 效果同shuflle  内部调用了shuffle</p><p>shuffle 的参数只能是 array_like，而 permutation 除了 array_like 还可以是 int 类型，如果是 int 类型，那就随机打乱 numpy.arange(int)。</p><p>shuffle 返回 None，这点尤其要注意，也就是说没有返回值，而 permutation 则返回打乱后的 array。</p></li></ol><h1 id="检查元素-判断-及集合操作"><a href="#检查元素-判断-及集合操作" class="headerlink" title="检查元素 判断 及集合操作"></a>检查元素 判断 及集合操作</h1><ul><li><code>np.any()</code>  是否有假</li><li><code>np.all()</code>  是否全为真</li><li><code>np.inld(array1, array2)</code> 判断2是否是1的子集</li><li><code>np.intersection()</code> 输出的数据则是不重复的</li><li><code>np.setdiff1d()</code> 与in1d()相反，寻找与y不同的数值 </li><li><code>np.setxor1d()</code> 寻找只在x或者y中存在的数据 并输出</li><li><code>np.union1d()</code> 寻找共同子集</li><li><code>np.correlate()</code> 两个序列的相关性</li></ul><h1 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h1><ol><li>行向量或者一维向量没有办法直接转置，转置后一维向量应该是二维，但是直接使用transpose是没有办法达到的<br>所以我们需要对维度下刀<img src="/%5Csrc%5Cimage-20220214204451566.png" alt="image-20220214204451566"></li><li><code>np.bincount(array, weights, minlength)</code> 数组内数字统计<br><strong>weight</strong>是指数据的权重，默认权重为1，<strong>minlength</strong>指的是array的值域大小，当值域不满足minlength时，后面默认补0.</li><li><code>np.astype</code> 可以转换数组值 数据类型</li></ol><p>� 数据类型</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Numpy、Scipy、Pandas的关联</title>
    <link href="/2022/08/01/Python/numpy%E3%80%81scipy%E3%80%81pandas%E7%9A%84%E5%85%B3%E8%81%94/"/>
    <url>/2022/08/01/Python/numpy%E3%80%81scipy%E3%80%81pandas%E7%9A%84%E5%85%B3%E8%81%94/</url>
    
    <content type="html"><![CDATA[<p>打个比方，Pandas类似Excel软件，Scipy就像Excel里的函数算法包，Numpy则好比构建Excel逻辑的底层语句。所以说Pandas擅长数据处理，Scipy精通数学计算，Numpy是构建Pandas、Scipy的基础库。</p><p>Numpy通过N维数组来实现快速的数据计算和处理，它也是Python众多数据科学库的依赖，其中就包括Pandas、Scipy。而Numpy本身不依赖于任何其他Python软件包，只依赖于加速的线性代数库，通常为 Intel MKL或 OpenBLAS。</p><p>Pandas是python数据处理的核心库，它基于数组形式提供了极其丰富的数据操作，对比excel有过之而无不及。它可以对各种数据进行运算操作，比如归并、再成形、选择，还有数据清洗和数据加工特征，广泛应用在学术、金融、统计学等各个数据分析领域。一般来说，学python数据分析只需要学透Pandas就够了，辅助加一些Numpy、Scipy、Matplotlib等库。</p><p>Scipy是一个用于数学、科学、工程领域的常用软件包，可以处理最优化、线性代数、积分、插值、拟合、特殊函数、快速傅里叶变换、信号处理、图像处理、常微分方程求解器等。其包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python numpy char</title>
    <link href="/2022/08/01/Python/Numpy.char/"/>
    <url>/2022/08/01/Python/Numpy.char/</url>
    
    <content type="html"><![CDATA[<p>主要是针对ndarray中的值为字符串的数据，主要是利用numpy.char</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>add()</code></td><td align="left">对两个数组的逐个字符串元素进行连接</td></tr><tr><td align="left">multiply()</td><td align="left">返回按元素多重连接后的字符串</td></tr><tr><td align="left"><code>center()</code></td><td align="left">居中字符串</td></tr><tr><td align="left"><code>capitalize()</code></td><td align="left">将字符串第一个字母转换为大写</td></tr><tr><td align="left"><code>title()</code></td><td align="left">将字符串的每个单词的第一个字母转换为大写</td></tr><tr><td align="left"><code>lower()</code></td><td align="left">数组元素转换为小写</td></tr><tr><td align="left"><code>upper()</code></td><td align="left">数组元素转换为大写</td></tr><tr><td align="left"><code>split()</code></td><td align="left">指定分隔符对字符串进行分割，并返回数组列表</td></tr><tr><td align="left"><code>splitlines()</code></td><td align="left">返回元素中的行列表，以换行符分割</td></tr><tr><td align="left"><code>strip()</code></td><td align="left">移除元素开头或者结尾处的特定字符</td></tr><tr><td align="left"><code>join()</code></td><td align="left">通过指定分隔符来连接数组中的元素</td></tr><tr><td align="left"><code>replace()</code></td><td align="left">使用新字符串替换字符串中的所有子字符串</td></tr><tr><td align="left"><code>decode()</code></td><td align="left">数组元素依次调用<code>str.decode</code></td></tr><tr><td align="left"><code>encode()</code></td><td align="left">数组元素依次调用<code>str.encode</code></td></tr></tbody></table><hr>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>matplotlib example</title>
    <link href="/2022/08/01/Python/matplotlib/"/>
    <url>/2022/08/01/Python/matplotlib/</url>
    
    <content type="html"><![CDATA[<h2 id="1启用和检查交互模式"><a href="#1启用和检查交互模式" class="headerlink" title="1启用和检查交互模式"></a>1启用和检查交互模式</h2><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br> <br><span class="hljs-comment"># Set the interactive mode to ON</span><br>plt.ion()<br> <br><span class="hljs-comment"># Check the current status of interactive mode</span><br><span class="hljs-built_in">print</span>(mpl.is_interactive())<br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada"><span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><h2 id="2在-Matplotlib-中绘制折线图"><a href="#2在-Matplotlib-中绘制折线图" class="headerlink" title="2在 Matplotlib 中绘制折线图"></a>2在 Matplotlib 中绘制折线图</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import matplotlib.pyplot as plt<br><br>#Plot a line graph<br>plt.plot([<span class="hljs-number">5</span>, <span class="hljs-number">15</span>])<br><br># Add <span class="hljs-built_in">labels</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">title</span><br>plt.<span class="hljs-built_in">title</span>(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br>plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">&quot;X-axis&quot;</span>)<br>plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br>plt.<span class="hljs-built_in">show</span>()<br></code></pre></td></tr></table></figure><h2 id="3绘制带有标签和图例的多条线的折线图"><a href="#3绘制带有标签和图例的多条线的折线图" class="headerlink" title="3绘制带有标签和图例的多条线的折线图"></a>3绘制带有标签和图例的多条线的折线图</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment">#Plot a line graph</span><br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">5</span>, <span class="hljs-number">15</span>], label=&#x27;Rice&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">3</span>, <span class="hljs-number">6</span>], label=&#x27;Oil&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">8</span>.<span class="hljs-number">0010</span>, <span class="hljs-number">14</span>.<span class="hljs-number">2</span>], label=&#x27;Wheat&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">1</span>.<span class="hljs-number">95412</span>, <span class="hljs-number">6</span>.<span class="hljs-number">98547</span>, <span class="hljs-number">5</span>.<span class="hljs-number">41411</span>, <span class="hljs-number">5</span>.<span class="hljs-number">99</span>, <span class="hljs-number">7</span>.<span class="hljs-number">9999</span>], label=&#x27;Coffee&#x27;)<br><br><span class="hljs-comment"># Add labels and title</span><br><span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br><br><span class="hljs-attribute">plt</span>.legend()<br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287589024200.png" alt="图片"></p><h2 id="4在-Matplotlib-中绘制带有标记的折线图"><a href="#4在-Matplotlib-中绘制带有标记的折线图" class="headerlink" title="4在 Matplotlib 中绘制带有标记的折线图"></a>4在 Matplotlib 中绘制带有标记的折线图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br> <br><span class="hljs-comment"># Changing default values for parameters individually</span><br>plt.rc(<span class="hljs-string">&#x27;lines&#x27;</span>, <span class="hljs-attribute">linewidth</span>=2, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&#x27;-&#x27;</span>, <span class="hljs-attribute">marker</span>=<span class="hljs-string">&#x27;*&#x27;</span>)<br>plt.rcParams[<span class="hljs-string">&#x27;lines.markersize&#x27;</span>] = 25<br>plt.rcParams[<span class="hljs-string">&#x27;font.size&#x27;</span>] = <span class="hljs-string">&#x27;10.0&#x27;</span><br> <br><span class="hljs-comment">#Plot a line graph</span><br>plt.plot([10, 20, 30, 40, 50])<br><span class="hljs-comment"># Add labels and title</span><br>plt.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br> <br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287582384198.png" alt="图片"></p><h2 id="5改变-Matplotlib-中绘制的图形的大小"><a href="#5改变-Matplotlib-中绘制的图形的大小" class="headerlink" title="5改变 Matplotlib 中绘制的图形的大小"></a>5改变 Matplotlib 中绘制的图形的大小</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br> <br><span class="hljs-comment"># Changing default values for parameters individually</span><br><span class="hljs-attribute">plt</span>.rc(&#x27;lines&#x27;, linewidth=<span class="hljs-number">2</span>, linestyle=&#x27;-&#x27;, marker=&#x27;*&#x27;)<br> <br><span class="hljs-attribute">plt</span>.rcParams[<span class="hljs-string">&quot;figure.figsize&quot;</span>] = (<span class="hljs-number">4</span>, <span class="hljs-number">8</span>)<br> <br><span class="hljs-comment"># Plot a line graph</span><br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">50</span>, <span class="hljs-number">60</span>, <span class="hljs-number">70</span>, <span class="hljs-number">80</span>])<br><span class="hljs-comment"># Add labels and title</span><br><span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br> <br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141072.png" alt="图片"></p><h2 id="6在-Matplotlib-中设置轴限制"><a href="#6在-Matplotlib-中设置轴限制" class="headerlink" title="6在 Matplotlib 中设置轴限制"></a>6在 Matplotlib 中设置轴限制</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-attribute">data1</span> =<span class="hljs-meta"> [11, 12, 13, 14, 15, 16, 17]</span><br><span class="hljs-attribute">data2</span> =<span class="hljs-meta"> [15.5, 12.5, 11.7, 9.50, 12.50, 11.50, 14.75]</span><br><br><span class="hljs-comment"># Add labels and title</span><br><span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br><br><span class="hljs-comment"># Set the limit for each axis</span><br><span class="hljs-attribute">plt</span>.xlim(<span class="hljs-number">11</span>, <span class="hljs-number">17</span>)<br><span class="hljs-attribute">plt</span>.ylim(<span class="hljs-number">9</span>, <span class="hljs-number">16</span>)<br><br><span class="hljs-comment"># Plot a line graph</span><br><span class="hljs-attribute">plt</span>.plot(data1, data2)<br><br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287567644196.png" alt="图片"></p><h2 id="7使用-Python-Matplotlib-显示背景网格"><a href="#7使用-Python-Matplotlib-显示背景网格" class="headerlink" title="7使用 Python Matplotlib 显示背景网格"></a>7使用 Python Matplotlib 显示背景网格</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br><br>plt.grid(<span class="hljs-literal">True</span>, <span class="hljs-attribute">linewidth</span>=0.5, <span class="hljs-attribute">color</span>=<span class="hljs-string">&#x27;#ff0000&#x27;</span>, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&#x27;-&#x27;</span>)<br><br><span class="hljs-comment">#Plot a line graph</span><br>plt.plot([10, 20, 30, 40, 50])<br><span class="hljs-comment"># Add labels and title</span><br>plt.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><h4 id="Output"><a href="#Output" class="headerlink" title="Output:"></a>Output:</h4><p><img src="/%5Csrc%5C640-1646287557689194.png" alt="图片"></p><h2 id="8使用-Python-Matplotlib-将绘图保存到图像文件"><a href="#8使用-Python-Matplotlib-将绘图保存到图像文件" class="headerlink" title="8使用 Python Matplotlib 将绘图保存到图像文件"></a>8使用 Python Matplotlib 将绘图保存到图像文件</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br><br>plt.grid(<span class="hljs-literal">True</span>, <span class="hljs-attribute">linewidth</span>=0.5, <span class="hljs-attribute">color</span>=<span class="hljs-string">&#x27;#ff0000&#x27;</span>, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&#x27;-&#x27;</span>)<br><br><span class="hljs-comment">#Plot a line graph</span><br>plt.plot([10, 20, 30, 40, 50])<br><span class="hljs-comment"># Add labels and title</span><br>plt.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br><br>plt.savefig(<span class="hljs-string">&quot;foo.png&quot;</span>, <span class="hljs-attribute">bbox_inches</span>=<span class="hljs-string">&#x27;tight&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141073.png" alt="图片"></p><h2 id="9将图例放在-plot-的不同位置"><a href="#9将图例放在-plot-的不同位置" class="headerlink" title="9将图例放在 plot 的不同位置"></a>9将图例放在 plot 的不同位置</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br> <br><span class="hljs-comment">#Plot a line graph</span><br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">5</span>, <span class="hljs-number">15</span>], label=&#x27;Rice&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">3</span>, <span class="hljs-number">6</span>], label=&#x27;Oil&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">8</span>.<span class="hljs-number">0010</span>, <span class="hljs-number">14</span>.<span class="hljs-number">2</span>], label=&#x27;Wheat&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">1</span>.<span class="hljs-number">95412</span>, <span class="hljs-number">6</span>.<span class="hljs-number">98547</span>, <span class="hljs-number">5</span>.<span class="hljs-number">41411</span>, <span class="hljs-number">5</span>.<span class="hljs-number">99</span>, <span class="hljs-number">7</span>.<span class="hljs-number">9999</span>], label=&#x27;Coffee&#x27;)<br> <br><span class="hljs-comment"># Add labels and title</span><br><span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br> <br><span class="hljs-attribute">plt</span>.legend(bbox_to_anchor=(<span class="hljs-number">1</span>.<span class="hljs-number">1</span>, <span class="hljs-number">1</span>.<span class="hljs-number">05</span>))<br> <br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287542238192.png" alt="图片"></p><h2 id="10绘制具有不同标记大小的线条"><a href="#10绘制具有不同标记大小的线条" class="headerlink" title="10绘制具有不同标记大小的线条"></a>10绘制具有不同标记大小的线条</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br> <br>y1 = [12, 14, 15, 18, 19, 13, 15, 16]<br>y2 = [22, 24, 25, 28, 29, 23, 25, 26]<br>y3 = [32, 34, 35, 38, 39, 33, 35, 36]<br>y4 = [42, 44, 45, 48, 49, 43, 45, 46]<br>y5 = [52, 54, 55, 58, 59, 53, 55, 56]<br> <br> <br><span class="hljs-comment"># Plot lines with different marker sizes</span><br>plt.plot(y1, y2, label = <span class="hljs-string">&#x27;Y1-Y2&#x27;</span>, <span class="hljs-attribute">lw</span>=2, <span class="hljs-attribute">marker</span>=<span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-attribute">ms</span>=10) # square<br>plt.plot(y1, y3, label = <span class="hljs-string">&#x27;Y1-Y3&#x27;</span>, <span class="hljs-attribute">lw</span>=2, <span class="hljs-attribute">marker</span>=<span class="hljs-string">&#x27;^&#x27;</span>, <span class="hljs-attribute">ms</span>=10) # triangle<br>plt.plot(y1, y4, label = <span class="hljs-string">&#x27;Y1-Y4&#x27;</span>, <span class="hljs-attribute">lw</span>=2, <span class="hljs-attribute">marker</span>=<span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-attribute">ms</span>=10) # circle<br>plt.plot(y1, y5, label = <span class="hljs-string">&#x27;Y1-Y5&#x27;</span>, <span class="hljs-attribute">lw</span>=2, <span class="hljs-attribute">marker</span>=<span class="hljs-string">&#x27;D&#x27;</span>, <span class="hljs-attribute">ms</span>=10) # diamond<br>plt.plot(y2, y5, label = <span class="hljs-string">&#x27;Y2-Y5&#x27;</span>, <span class="hljs-attribute">lw</span>=2, <span class="hljs-attribute">marker</span>=<span class="hljs-string">&#x27;P&#x27;</span>, <span class="hljs-attribute">ms</span>=10) # filled plus sign<br> <br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287535664190.png" alt="图片"></p><h2 id="11用灰度线绘制折线图"><a href="#11用灰度线绘制折线图" class="headerlink" title="11用灰度线绘制折线图"></a>11用灰度线绘制折线图</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># Plot a line graph with grayscale lines </span><br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">5</span>, <span class="hljs-number">15</span>], label=&#x27;Rice&#x27;, c=&#x27;<span class="hljs-number">0</span>.<span class="hljs-number">15</span>&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">3</span>, <span class="hljs-number">6</span>], label=&#x27;Oil&#x27;, c=&#x27;<span class="hljs-number">0</span>.<span class="hljs-number">35</span>&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">8</span>.<span class="hljs-number">0010</span>, <span class="hljs-number">14</span>.<span class="hljs-number">2</span>], label=&#x27;Wheat&#x27;,  c=&#x27;<span class="hljs-number">0</span>.<span class="hljs-number">55</span>&#x27;)<br><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">1</span>.<span class="hljs-number">95412</span>, <span class="hljs-number">6</span>.<span class="hljs-number">98547</span>, <span class="hljs-number">5</span>.<span class="hljs-number">41411</span>, <span class="hljs-number">5</span>.<span class="hljs-number">99</span>, <span class="hljs-number">7</span>.<span class="hljs-number">9999</span>], label=&#x27;Coffee&#x27;,  c=&#x27;<span class="hljs-number">0</span>.<span class="hljs-number">85</span>&#x27;)<br><br><span class="hljs-comment"># Add labels and title</span><br><span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br><br><span class="hljs-attribute">plt</span>.legend()<br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287528650188.png" alt="图片"></p><h2 id="12以高-dpi-绘制-PDF-输出"><a href="#12以高-dpi-绘制-PDF-输出" class="headerlink" title="12以高 dpi 绘制 PDF 输出"></a>12以高 dpi 绘制 PDF 输出</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br><br><span class="hljs-comment">#Plot a line graph</span><br>plt.plot([5, 15], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Rice&#x27;</span>)<br>plt.plot([3, 6], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Oil&#x27;</span>)<br>plt.plot([8.0010, 14.2], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Wheat&#x27;</span>)<br>plt.plot([1.95412, 6.98547, 5.41411, 5.99, 7.9999], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Coffee&#x27;</span>)<br><br><span class="hljs-comment"># Add labels and title</span><br>plt.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br><br>plt.savefig(<span class="hljs-string">&#x27;output.pdf&#x27;</span>, <span class="hljs-attribute">dpi</span>=1200, <span class="hljs-attribute">format</span>=<span class="hljs-string">&#x27;pdf&#x27;</span>, <span class="hljs-attribute">bbox_inches</span>=<span class="hljs-string">&#x27;tight&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">生成带有图片的pdf文件<br></code></pre></td></tr></table></figure><h2 id="13绘制不同颜色的多线图"><a href="#13绘制不同颜色的多线图" class="headerlink" title="13绘制不同颜色的多线图"></a>13绘制不同颜色的多线图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br> <br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    plt.plot([i]*<span class="hljs-number">5</span>, c=<span class="hljs-string">&#x27;C&#x27;</span>+<span class="hljs-built_in">str</span>(i), label=<span class="hljs-string">&#x27;C&#x27;</span>+<span class="hljs-built_in">str</span>(i))<br> <br><span class="hljs-comment"># Plot a line graph</span><br>plt.xlim(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)<br> <br><span class="hljs-comment"># Add legend</span><br>plt.legend()<br> <br><span class="hljs-comment"># Display the graph on the screen</span><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141074.png" alt="图片"></p><h2 id="14语料库创建词云"><a href="#14语料库创建词云" class="headerlink" title="14语料库创建词云"></a>14语料库创建词云</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> webtext<br><span class="hljs-keyword">from</span> nltk.probability <span class="hljs-keyword">import</span> FreqDist<br><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>nltk.download(<span class="hljs-string">&#x27;webtext&#x27;</span>)<br>wt_words = webtext.words(<span class="hljs-string">&#x27;testing.txt&#x27;</span>)  <span class="hljs-comment"># Sample data</span><br>data_analysis = nltk.FreqDist(wt_words)<br><br>filter_words = <span class="hljs-built_in">dict</span>([(m, n) <span class="hljs-keyword">for</span> m, n <span class="hljs-keyword">in</span> data_analysis.items() <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(m) &gt; <span class="hljs-number">3</span>])<br><br>wcloud = WordCloud().generate_from_frequencies(filter_words)<br><br><span class="hljs-comment"># Plotting the wordcloud</span><br>plt.imshow(wcloud, interpolation=<span class="hljs-string">&quot;bilinear&quot;</span>)<br><br>plt.axis(<span class="hljs-string">&quot;off&quot;</span>)<br>(-<span class="hljs-number">0.5</span>, <span class="hljs-number">399.5</span>, <span class="hljs-number">199.5</span>, -<span class="hljs-number">0.5</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287504770186.png" alt="图片"></p><h2 id="15使用特定颜色在-Matplotlib-Python-中绘制图形"><a href="#15使用特定颜色在-Matplotlib-Python-中绘制图形" class="headerlink" title="15使用特定颜色在 Matplotlib Python 中绘制图形"></a>15使用特定颜色在 Matplotlib Python 中绘制图形</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br><br><span class="hljs-comment">#Plot a line graph with specific colors</span><br>plt.plot([5, 15], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Rice&#x27;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;C7&#x27;</span>)<br>plt.plot([3, 6], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Oil&#x27;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;C8&#x27;</span>)<br>plt.plot([8.0010, 14.2], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Wheat&#x27;</span>,  <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;C4&#x27;</span>)<br>plt.plot([1.95412, 6.98547, 5.41411, 5.99, 7.9999], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Coffee&#x27;</span>,  <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;C6&#x27;</span>)<br><br><span class="hljs-comment"># Add labels and title</span><br>plt.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br><br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output</p><p><img src="/%5Csrc%5C640-164628699141075.png" alt="图片"></p><h2 id="16NLTK-词汇色散图"><a href="#16NLTK-词汇色散图" class="headerlink" title="16NLTK 词汇色散图"></a>16NLTK 词汇色散图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> webtext<br><span class="hljs-keyword">from</span> nltk.probability <span class="hljs-keyword">import</span> FreqDist<br><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>words = [<span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-string">&#x27;science&#x27;</span>, <span class="hljs-string">&#x27;dataset&#x27;</span>]<br><br>nltk.download(<span class="hljs-string">&#x27;webtext&#x27;</span>)<br>wt_words = webtext.words(<span class="hljs-string">&#x27;testing.txt&#x27;</span>)  <span class="hljs-comment"># Sample data</span><br><br>points = [(x, y) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(wt_words))<br>          <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)) <span class="hljs-keyword">if</span> wt_words[x] == words[y]]<br><br><span class="hljs-keyword">if</span> points:<br>    x, y = <span class="hljs-built_in">zip</span>(*points)<br><span class="hljs-keyword">else</span>:<br>    x = y = ()<br><br>plt.plot(x, y, <span class="hljs-string">&quot;rx&quot;</span>, scalex=<span class="hljs-number">.1</span>)<br>plt.yticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)), words, color=<span class="hljs-string">&quot;b&quot;</span>)<br>plt.ylim(-<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(words))<br>plt.title(<span class="hljs-string">&quot;Lexical Dispersion Plot&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Word Offset&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141076.png" alt="图片"></p><h2 id="17绘制具有不同线条图案的折线图"><a href="#17绘制具有不同线条图案的折线图" class="headerlink" title="17绘制具有不同线条图案的折线图"></a>17绘制具有不同线条图案的折线图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br><br><span class="hljs-comment"># Plot a line graph with grayscale lines </span><br>plt.plot([5, 11], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Rice&#x27;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;C1&#x27;</span>, <span class="hljs-attribute">ls</span>=<span class="hljs-string">&#x27;--&#x27;</span>)<br>plt.plot([2, 16], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Oil&#x27;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;C4&#x27;</span>, <span class="hljs-attribute">ls</span>=<span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot([8, 14], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Wheat&#x27;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;C7&#x27;</span>, <span class="hljs-attribute">ls</span>=<span class="hljs-string">&#x27;:&#x27;</span>)<br><br><span class="hljs-comment"># Add labels and title</span><br>plt.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>)<br><br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287494076184.png" alt="图片"></p><h2 id="18更新-Matplotlib-折线图中的字体外观"><a href="#18更新-Matplotlib-折线图中的字体外观" class="headerlink" title="18更新 Matplotlib 折线图中的字体外观"></a>18更新 Matplotlib 折线图中的字体外观</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br><br>fontparams = &#123;<span class="hljs-string">&#x27;font.size&#x27;</span>: 12, <span class="hljs-string">&#x27;font.weight&#x27;</span>:<span class="hljs-string">&#x27;bold&#x27;</span>,<br>              <span class="hljs-string">&#x27;font.family&#x27;</span>:<span class="hljs-string">&#x27;arial&#x27;</span>, <span class="hljs-string">&#x27;font.style&#x27;</span>:<span class="hljs-string">&#x27;italic&#x27;</span>&#125;<br><br>plt.rcParams.update(fontparams)<br><br><span class="hljs-comment"># Plot a line graph with specific font style</span><br>plt.plot([5, 11], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Rice&#x27;</span>)<br>plt.plot([2, 16], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Oil&#x27;</span>)<br>plt.plot([8, 14], <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Wheat&#x27;</span>)<br><br>labelparams = &#123;<span class="hljs-string">&#x27;size&#x27;</span>: 20, <span class="hljs-string">&#x27;weight&#x27;</span>:<span class="hljs-string">&#x27;semibold&#x27;</span>,<br>              <span class="hljs-string">&#x27;family&#x27;</span>:<span class="hljs-string">&#x27;serif&#x27;</span>, <span class="hljs-string">&#x27;style&#x27;</span>:<span class="hljs-string">&#x27;italic&#x27;</span>&#125;<br><br><span class="hljs-comment"># Add labels and title</span><br>plt.title(<span class="hljs-string">&quot;Interactive Plot&quot;</span>, labelparams)<br>plt.xlabel(<span class="hljs-string">&quot;X-axis&quot;</span>, labelparams)<br>plt.ylabel(<span class="hljs-string">&quot;Y-axis&quot;</span>, labelparams)<br><br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141077.png" alt="图片"></p><h2 id="19用颜色名称绘制虚线和点状图"><a href="#19用颜色名称绘制虚线和点状图" class="headerlink" title="19用颜色名称绘制虚线和点状图"></a>19用颜色名称绘制虚线和点状图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br> <br>x = [2, 4, 5, 8, 9, 13, 15, 16]<br>y = [1, 3, 4, 7, 10, 11, 14, 17]<br> <br><span class="hljs-comment"># Plot a line graph with dashed and maroon color</span><br>plt.plot(x, y, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Price&#x27;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;maroon&#x27;</span>, ls=(<span class="hljs-string">&#x27;dashed&#x27;</span>), <span class="hljs-attribute">lw</span>=2)<br> <br><span class="hljs-comment"># Plot a line graph with dotted and teal color</span><br>plt.plot(y, x, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;Rank&#x27;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;teal&#x27;</span>, ls=(<span class="hljs-string">&#x27;dotted&#x27;</span>), <span class="hljs-attribute">lw</span>=2)<br> <br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287472453182.png" alt="图片"></p><h2 id="20以随机坐标绘制所有可用标记"><a href="#20以随机坐标绘制所有可用标记" class="headerlink" title="20以随机坐标绘制所有可用标记"></a>20以随机坐标绘制所有可用标记</h2><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import numpy <span class="hljs-keyword">as</span> np<br>import matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-built_in">from</span> matplotlib.<span class="hljs-keyword">lines</span> import Line2D<br> <br><span class="hljs-comment"># Prepare 50 random numbers to plot</span><br>n1 = np.<span class="hljs-built_in">random</span>.rand(<span class="hljs-number">50</span>)<br>n2 = np.<span class="hljs-built_in">random</span>.rand(<span class="hljs-number">50</span>)<br> <br>markerindex = np.<span class="hljs-built_in">random</span>.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(Line2D.markers), <span class="hljs-number">50</span>)<br> <br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> enumerate(Line2D.markers):<br>    i = (markerindex == x)<br>    plt.scatter(n1[i], n2[i], marker=y)<br> <br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141078.png" alt="图片"></p><h2 id="21绘制一个非常简单的条形图"><a href="#21绘制一个非常简单的条形图" class="headerlink" title="21绘制一个非常简单的条形图"></a>21绘制一个非常简单的条形图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br><br>year = [2001, 2002, 2003, 2004, 2005, 2006]<br>unit = [50, 60, 75, 45, 70, 105]<br><br><span class="hljs-comment"># Plot the bar graph</span><br>plot = plt.bar(year, unit)<br><br><span class="hljs-comment"># Add the data value on head of the bar</span><br><span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> plot:<br>    height = value.get_height()<br>    plt.text(value.get_x() + value.get_width()/2.,<br>             1.002*height,<span class="hljs-string">&#x27;%d&#x27;</span> % int(height), <span class="hljs-attribute">ha</span>=<span class="hljs-string">&#x27;center&#x27;</span>, <span class="hljs-attribute">va</span>=<span class="hljs-string">&#x27;bottom&#x27;</span>)<br><br><span class="hljs-comment"># Add labels and title</span><br>plt.title(<span class="hljs-string">&quot;Bar Chart&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Year&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Unit&quot;</span>)<br><br><span class="hljs-comment"># Display the graph on the screen</span><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287463756180.png" alt="图片"></p><h2 id="22在-X-轴上绘制带有组数据的条形图"><a href="#22在-X-轴上绘制带有组数据的条形图" class="headerlink" title="22在 X 轴上绘制带有组数据的条形图"></a>22在 X 轴上绘制带有组数据的条形图</h2><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs prolog">import pandas as pd<br>import matplotlib.pyplot as plt<br> <br>df = pd.<span class="hljs-symbol">DataFrame</span>([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">1.4</span>, <span class="hljs-number">2.1</span>, <span class="hljs-number">2.8</span>], [<span class="hljs-number">5.5</span>, <span class="hljs-number">1.5</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1.2</span>],<br>                   [<span class="hljs-number">1.5</span>, <span class="hljs-number">1.4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">8</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">8</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>]],<br>                  columns=[<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>, <span class="hljs-string">&#x27;Banana&#x27;</span>, <span class="hljs-string">&#x27;Pear&#x27;</span>],<br>                  index=[<span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">13</span>, <span class="hljs-number">20</span>, <span class="hljs-number">28</span>, <span class="hljs-number">35</span>])<br>width = <span class="hljs-number">2</span><br>bottom = <span class="hljs-number">0</span><br> <br>for i in df.columns:<br>    plt.bar(df.index, df[i], width=width, bottom=bottom)<br>    bottom += df[i]<br> <br>plt.legend(df.columns)<br>plt.tight_layout()<br> <br># <span class="hljs-symbol">Display</span> the graph on the screen<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141079.png" alt="图片"></p><h2 id="23具有不同颜色条形的条形图"><a href="#23具有不同颜色条形的条形图" class="headerlink" title="23具有不同颜色条形的条形图"></a>23具有不同颜色条形的条形图</h2><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mp<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br> <br><span class="hljs-class"><span class="hljs-keyword">data</span> = [8, 6, 7, 12, 9, 10, 5, 8, 9]</span><br> <br><span class="hljs-meta"># Colorize the graph based on likeability:</span><br><span class="hljs-title">likeability_scores</span> = np.array(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br> <br><span class="hljs-title">data_normalizer</span> = mp.colors.<span class="hljs-type">Normalize</span>()<br><span class="hljs-title">color_map</span> = mp.colors.<span class="hljs-type">LinearSegmentedColormap</span>(<br>    <span class="hljs-string">&quot;my_map&quot;</span>,<br>    &#123;<br>        <span class="hljs-string">&quot;red&quot;</span>: [(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>),<br>                (<span class="hljs-number">1.0</span>, .<span class="hljs-number">5</span>, .<span class="hljs-number">5</span>)],<br>        <span class="hljs-string">&quot;green&quot;</span>: [(<span class="hljs-number">0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>),<br>                  (<span class="hljs-number">1.0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)],<br>        <span class="hljs-string">&quot;blue&quot;</span>: [(<span class="hljs-number">0</span>, <span class="hljs-number">0.50</span>, <span class="hljs-number">0.5</span>),<br>                 (<span class="hljs-number">1.0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)]<br>    &#125;<br>)<br> <br><span class="hljs-meta"># Map xs to numbers:</span><br><span class="hljs-type">N</span> = len(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br><span class="hljs-title">x_nums</span> = np.arange(<span class="hljs-number">1</span>, <span class="hljs-type">N</span>+<span class="hljs-number">1</span>)<br> <br><span class="hljs-meta"># Plot a bar graph:</span><br><span class="hljs-title">plt</span>.bar(<br>    x_nums,<br>    <span class="hljs-class"><span class="hljs-keyword">data</span>,</span><br>    align=<span class="hljs-string">&quot;center&quot;</span>,<br>    color=color_map(data_normalizer(likeability_scores))<br>)<br> <br><span class="hljs-title">plt</span>.xticks(x_nums, <span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br><span class="hljs-title">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287451643178.png" alt="图片"></p><h2 id="24使用-Matplotlib-中的特定值改变条形图中每个条的颜色"><a href="#24使用-Matplotlib-中的特定值改变条形图中每个条的颜色" class="headerlink" title="24使用 Matplotlib 中的特定值改变条形图中每个条的颜色"></a>24使用 Matplotlib 中的特定值改变条形图中每个条的颜色</h2><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib.cm <span class="hljs-keyword">as</span> cm<br><span class="hljs-title">from</span> matplotlib.colors <span class="hljs-keyword">import</span> Normalize<br><span class="hljs-title">from</span> numpy.random <span class="hljs-keyword">import</span> rand<br><br><span class="hljs-class"><span class="hljs-keyword">data</span> = [2, 3, 5, 6, 8, 12, 7, 5]</span><br><span class="hljs-title">fig</span>, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br><span class="hljs-meta"># Get a color map</span><br><span class="hljs-title">my_cmap</span> = cm.get_cmap(&#x27;jet&#x27;)<br><br><span class="hljs-meta"># Get normalize function (takes data in range [vmin, vmax] -&gt; [0, 1])</span><br><span class="hljs-title">my_norm</span> = <span class="hljs-type">Normalize</span>(vmin=<span class="hljs-number">0</span>, vmax=<span class="hljs-number">8</span>)<br><br><span class="hljs-title">ax</span>.bar(range(<span class="hljs-number">8</span>), rand(<span class="hljs-number">8</span>), color=my_cmap(my_norm(<span class="hljs-class"><span class="hljs-keyword">data</span>)))</span><br><span class="hljs-title">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141180.png" alt="图片"></p><h2 id="25在-Matplotlib-中绘制散点图"><a href="#25在-Matplotlib-中绘制散点图" class="headerlink" title="25在 Matplotlib 中绘制散点图"></a>25在 Matplotlib 中绘制散点图</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>x1 = [<span class="hljs-number">214</span>, <span class="hljs-number">5</span>, <span class="hljs-number">91</span>, <span class="hljs-number">81</span>, <span class="hljs-number">122</span>, <span class="hljs-number">16</span>, <span class="hljs-number">218</span>, <span class="hljs-number">22</span>]<br>x2 = [<span class="hljs-number">12</span>, <span class="hljs-number">125</span>, <span class="hljs-number">149</span>, <span class="hljs-number">198</span>, <span class="hljs-number">22</span>, <span class="hljs-number">26</span>, <span class="hljs-number">28</span>, <span class="hljs-number">32</span>]<br><br>plt.scatter(x1, x2)<br><br># <span class="hljs-keyword">Set</span> X <span class="hljs-keyword">and</span> Y axis labels<br>plt.xlabel(<span class="hljs-string">&#x27;Demand&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Price&#x27;</span>)<br><br><span class="hljs-meta">#Display the graph</span><br>plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141181.png" alt="图片"></p><h2 id="26使用单个标签绘制散点图"><a href="#26使用单个标签绘制散点图" class="headerlink" title="26使用单个标签绘制散点图"></a>26使用单个标签绘制散点图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>import matplotlib.pyplot as plt<br><br>N = 6<br>data = np.random.random((N, 4))<br>labels = [<span class="hljs-string">&#x27;point&#123;0&#125;&#x27;</span>.format(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(N)]<br><br>plt.subplots_adjust(<span class="hljs-attribute">bottom</span>=0.1)<br>plt.scatter(<br>    data[:, 0], data[:, 1], <span class="hljs-attribute">marker</span>=<span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-attribute">c</span>=data[:, 2], <span class="hljs-attribute">s</span>=data[:, 3] * 1500,<br>    <span class="hljs-attribute">cmap</span>=plt.get_cmap(&#x27;Spectral&#x27;))<br><br><span class="hljs-keyword">for</span> label, x, y <span class="hljs-keyword">in</span> zip(labels, data[:, 0], data[:, 1]):<br>    plt.annotate(<br>        label,<br>        xy=(x, y), xytext=(-20, 20),<br>        <span class="hljs-attribute">textcoords</span>=<span class="hljs-string">&#x27;offset points&#x27;</span>, <span class="hljs-attribute">ha</span>=<span class="hljs-string">&#x27;right&#x27;</span>, <span class="hljs-attribute">va</span>=<span class="hljs-string">&#x27;bottom&#x27;</span>,<br>        <span class="hljs-attribute">bbox</span>=dict(boxstyle=&#x27;round,pad=0.5&#x27;, <span class="hljs-attribute">fc</span>=<span class="hljs-string">&#x27;yellow&#x27;</span>, <span class="hljs-attribute">alpha</span>=0.5),<br>        <span class="hljs-attribute">arrowprops</span>=dict(arrowstyle=&#x27;-&gt;&#x27;, <span class="hljs-attribute">connectionstyle</span>=<span class="hljs-string">&#x27;arc3,rad=0&#x27;</span>))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287441540176.png" alt="图片"></p><h2 id="27用标记大小绘制散点图"><a href="#27用标记大小绘制散点图" class="headerlink" title="27用标记大小绘制散点图"></a>27用标记大小绘制散点图</h2><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>x1 = [<span class="hljs-number">214</span>, <span class="hljs-number">5</span>, <span class="hljs-number">91</span>, <span class="hljs-number">81</span>, <span class="hljs-number">122</span>, <span class="hljs-number">16</span>, <span class="hljs-number">218</span>, <span class="hljs-number">22</span>]<br>x2 = [<span class="hljs-number">12</span>, <span class="hljs-number">125</span>, <span class="hljs-number">149</span>, <span class="hljs-number">198</span>, <span class="hljs-number">22</span>, <span class="hljs-number">26</span>, <span class="hljs-number">28</span>, <span class="hljs-number">32</span>]<br><br>plt.figure(<span class="hljs-number">1</span>)<br><span class="hljs-comment"># You can specify the marker size two ways directly:</span><br>plt.plot(x1, <span class="hljs-string">&#x27;bo&#x27;</span>, markersize=<span class="hljs-number">20</span>)  <span class="hljs-comment"># blue circle with size 10 </span><br>plt.plot(x2, <span class="hljs-string">&#x27;ro&#x27;</span>, ms=<span class="hljs-number">10</span>,)  <span class="hljs-comment"># ms is just an alias for markersize</span><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141182.png" alt="图片"></p><h2 id="28在散点图中调整标记大小和颜色"><a href="#28在散点图中调整标记大小和颜色" class="headerlink" title="28在散点图中调整标记大小和颜色"></a>28在散点图中调整标记大小和颜色</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>import matplotlib.colors<br><br><span class="hljs-comment"># Prepare a list of integers</span><br>val = [2, 3, 6, 9, 14]<br><br><span class="hljs-comment"># Prepare a list of sizes that increases with values in val</span><br>sizevalues = [i*<span class="hljs-number">*2</span><span class="hljs-number">*50</span>+50 <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> val]<br><br><span class="hljs-comment"># Prepare a list of colors</span><br>plotcolor = [<span class="hljs-string">&#x27;red&#x27;</span>,<span class="hljs-string">&#x27;orange&#x27;</span>,<span class="hljs-string">&#x27;yellow&#x27;</span>,<span class="hljs-string">&#x27;green&#x27;</span>,<span class="hljs-string">&#x27;blue&#x27;</span>]<br><br><span class="hljs-comment"># Draw a scatter plot of val points with sizes in sizevalues and</span><br><span class="hljs-comment"># colors in plotcolor</span><br>plt.scatter(val, val, <span class="hljs-attribute">s</span>=sizevalues, <span class="hljs-attribute">c</span>=plotcolor)<br><br><span class="hljs-comment"># Set axis limits to show the markers completely</span><br>plt.xlim(0, 20)<br>plt.ylim(0, 20)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141183.png" alt="图片"></p><h2 id="29在-Matplotlib-中应用样式表"><a href="#29在-Matplotlib-中应用样式表" class="headerlink" title="29在 Matplotlib 中应用样式表"></a>29在 Matplotlib 中应用样式表</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>import matplotlib.colors<br>import matplotlib as mpl<br><br>mpl.style.use(<span class="hljs-string">&#x27;seaborn-darkgrid&#x27;</span>)<br><br><span class="hljs-comment"># Prepare a list of integers</span><br>val = [2, 3, 6, 9, 14]<br><br><span class="hljs-comment"># Prepare a list of sizes that increases with values in val</span><br>sizevalues = [i*<span class="hljs-number">*2</span><span class="hljs-number">*50</span>+50 <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> val]<br><br><span class="hljs-comment"># Prepare a list of colors</span><br>plotcolor = [<span class="hljs-string">&#x27;red&#x27;</span>,<span class="hljs-string">&#x27;orange&#x27;</span>,<span class="hljs-string">&#x27;yellow&#x27;</span>,<span class="hljs-string">&#x27;green&#x27;</span>,<span class="hljs-string">&#x27;blue&#x27;</span>]<br><br><span class="hljs-comment"># Draw a scatter plot of val points with sizes in sizevalues and</span><br><span class="hljs-comment"># colors in plotcolor</span><br>plt.scatter(val, val, <span class="hljs-attribute">s</span>=sizevalues, <span class="hljs-attribute">c</span>=plotcolor)<br><br><span class="hljs-comment"># Draw grid lines with red color and dashed style</span><br>plt.grid(<span class="hljs-attribute">color</span>=<span class="hljs-string">&#x27;blue&#x27;</span>, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&#x27;-.&#x27;</span>, <span class="hljs-attribute">linewidth</span>=0.7)<br><br><span class="hljs-comment"># Set axis limits to show the markers completely</span><br>plt.xlim(0, 20)<br>plt.ylim(0, 20)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287424985174.png" alt="图片"></p><h2 id="30自定义网格颜色和样式"><a href="#30自定义网格颜色和样式" class="headerlink" title="30自定义网格颜色和样式"></a>30自定义网格颜色和样式</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>import matplotlib.colors<br><br><span class="hljs-comment"># Prepare a list of integers</span><br>val = [2, 3, 6, 9, 14]<br><br><span class="hljs-comment"># Prepare a list of sizes that increases with values in val</span><br>sizevalues = [i*<span class="hljs-number">*2</span><span class="hljs-number">*50</span>+50 <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> val]<br><br><span class="hljs-comment"># Prepare a list of colors</span><br>plotcolor = [<span class="hljs-string">&#x27;red&#x27;</span>,<span class="hljs-string">&#x27;orange&#x27;</span>,<span class="hljs-string">&#x27;yellow&#x27;</span>,<span class="hljs-string">&#x27;green&#x27;</span>,<span class="hljs-string">&#x27;blue&#x27;</span>]<br><br><span class="hljs-comment"># Draw a scatter plot of val points with sizes in sizevalues and</span><br><span class="hljs-comment"># colors in plotcolor</span><br>plt.scatter(val, val, <span class="hljs-attribute">s</span>=sizevalues, <span class="hljs-attribute">c</span>=plotcolor)<br><br><span class="hljs-comment"># Draw grid lines with red color and dashed style</span><br>plt.grid(<span class="hljs-attribute">color</span>=<span class="hljs-string">&#x27;red&#x27;</span>, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&#x27;-.&#x27;</span>, <span class="hljs-attribute">linewidth</span>=0.7)<br><br><span class="hljs-comment"># Set axis limits to show the markers completely</span><br>plt.xlim(0, 20)<br>plt.ylim(0, 20)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141184.png" alt="图片"></p><h2 id="31在-Python-Matplotlib-中绘制饼图"><a href="#31在-Python-Matplotlib-中绘制饼图" class="headerlink" title="31在 Python Matplotlib 中绘制饼图"></a>31在 Python Matplotlib 中绘制饼图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br><br><br>labels = [<span class="hljs-string">&#x27;India&#x27;</span>, <span class="hljs-string">&#x27;Canada&#x27;</span>, <span class="hljs-string">&#x27;Japan&#x27;</span>, <span class="hljs-string">&#x27;Australia&#x27;</span>, <span class="hljs-string">&#x27;Russia&#x27;</span>]<br>sizes = [31, 19, 15, 14, 21]  # <span class="hljs-built_in">Add</span> upto 100%<br><br><span class="hljs-comment"># Plot the pie chart</span><br>plt.pie(sizes, <span class="hljs-attribute">labels</span>=labels, <span class="hljs-attribute">autopct</span>=<span class="hljs-string">&#x27;%1.1f%%&#x27;</span>, <span class="hljs-attribute">startangle</span>=90)<br><br><span class="hljs-comment"># Equal aspect ratio ensures that pie is drawn as a circle.</span><br>plt.axis(<span class="hljs-string">&#x27;equal&#x27;</span>)<br><br><span class="hljs-comment"># Display the graph onto the screen</span><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287410451172.png" alt="图片"></p><h2 id="32在-Matplotlib-饼图中为楔形设置边框"><a href="#32在-Matplotlib-饼图中为楔形设置边框" class="headerlink" title="32在 Matplotlib 饼图中为楔形设置边框"></a>32在 Matplotlib 饼图中为楔形设置边框</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br> <br>labels = [<span class="hljs-string">&#x27;India&#x27;</span>, <span class="hljs-string">&#x27;Canada&#x27;</span>, <span class="hljs-string">&#x27;Japan&#x27;</span>, <span class="hljs-string">&#x27;Australia&#x27;</span>, <span class="hljs-string">&#x27;Russia&#x27;</span>]<br>sizes = [31, 19, 15, 14, 21]  # <span class="hljs-built_in">Add</span> upto 100%<br> <br><span class="hljs-comment"># Plot the pie chart</span><br>plt.pie(sizes, <span class="hljs-attribute">labels</span>=labels, <span class="hljs-attribute">autopct</span>=<span class="hljs-string">&#x27;%1.1f%%&#x27;</span>, <span class="hljs-attribute">startangle</span>=90,<br>        wedgeprops=&#123;<span class="hljs-string">&quot;edgecolor&quot;</span>:<span class="hljs-string">&quot;0&quot;</span>,<span class="hljs-string">&#x27;linewidth&#x27;</span>: 1,<br>                    <span class="hljs-string">&#x27;linestyle&#x27;</span>: <span class="hljs-string">&#x27;dashed&#x27;</span>, <span class="hljs-string">&#x27;antialiased&#x27;</span>: <span class="hljs-literal">True</span>&#125;)<br> <br><span class="hljs-comment"># Equal aspect ratio ensures that pie is drawn as a circle.</span><br>plt.axis(<span class="hljs-string">&#x27;equal&#x27;</span>)<br> <br><span class="hljs-comment"># Display the graph onto the screen</span><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141185.png" alt="图片"></p><h2 id="33在-Python-Matplotlib-中设置饼图的方向"><a href="#33在-Python-Matplotlib-中设置饼图的方向" class="headerlink" title="33在 Python Matplotlib 中设置饼图的方向"></a>33在 Python Matplotlib 中设置饼图的方向</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br> <br>labels = [<span class="hljs-string">&#x27;India&#x27;</span>, <span class="hljs-string">&#x27;Canada&#x27;</span>, <span class="hljs-string">&#x27;Japan&#x27;</span>, <span class="hljs-string">&#x27;Australia&#x27;</span>, <span class="hljs-string">&#x27;Russia&#x27;</span>]<br>sizes = [31, 19, 15, 14, 21]  # <span class="hljs-built_in">Add</span> upto 100%<br> <br><span class="hljs-comment"># Plot the pie chart</span><br>plt.pie(sizes, <span class="hljs-attribute">labels</span>=labels, <span class="hljs-attribute">counterclock</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">startangle</span>=90)<br> <br><span class="hljs-comment"># Equal aspect ratio ensures that pie is drawn as a circle.</span><br>plt.axis(<span class="hljs-string">&#x27;equal&#x27;</span>)<br> <br><span class="hljs-comment"># Display the graph onto the screen</span><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287393020170.png" alt="图片"></p><h2 id="34在-Matplotlib-中绘制具有不同颜色主题的饼图"><a href="#34在-Matplotlib-中绘制具有不同颜色主题的饼图" class="headerlink" title="34在 Matplotlib 中绘制具有不同颜色主题的饼图"></a>34在 Matplotlib 中绘制具有不同颜色主题的饼图</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import matplotlib.pyplot as plt<br><br>sizes = [<span class="hljs-number">12</span>, <span class="hljs-number">23</span>, <span class="hljs-number">11</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>, <span class="hljs-number">24</span>, <span class="hljs-number">29</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">9</span>, <span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]<br><span class="hljs-built_in">labels</span> = [<span class="hljs-string">&quot;Market %s&quot;</span> <span class="hljs-symbol">%</span> i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sizes]<br><br>fig1, ax1 = plt.subplots(figsize=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>fig1.subplots_adjust(<span class="hljs-number">0.3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>theme = plt.get_cmap(&#x27;copper&#x27;)<br>ax1.set_prop_cycle(<span class="hljs-string">&quot;color&quot;</span>, [theme(<span class="hljs-number">1</span>. * i / len(sizes))<br>                             <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(len(sizes))])<br><br><span class="hljs-symbol">_</span>, <span class="hljs-symbol">_</span> = ax1.pie(sizes, startangle=<span class="hljs-number">90</span>, <span class="hljs-built_in">radius</span>=<span class="hljs-number">1800</span>)<br><br>ax1.axis(&#x27;<span class="hljs-built_in">equal</span>&#x27;)<br><br>total = <span class="hljs-built_in">sum</span>(sizes)<br>plt.<span class="hljs-built_in">legend</span>(<br>    loc=&#x27;upper left&#x27;,<br>    <span class="hljs-built_in">labels</span>=[&#x27;<span class="hljs-built_in">%s</span>, <span class="hljs-symbol">%</span><span class="hljs-number">1.</span>1f%%&#x27; <span class="hljs-symbol">%</span> (<br>        l, (<span class="hljs-built_in">float</span>(s) / total) * <span class="hljs-number">100</span>)<br>            <span class="hljs-keyword">for</span> l, s <span class="hljs-keyword">in</span> zip(<span class="hljs-built_in">labels</span>, sizes)],<br>    prop=&#123;&#x27;size&#x27;: <span class="hljs-number">11</span>&#125;,<br>    bbox_to_anchor=(<span class="hljs-number">0.0</span>, <span class="hljs-number">1</span>),<br>    bbox_transform=fig1.transFigure<br>)<br><br>plt.<span class="hljs-built_in">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287384415168.png" alt="图片"></p><h2 id="35在-Python-Matplotlib-中打开饼图的轴"><a href="#35在-Python-Matplotlib-中打开饼图的轴" class="headerlink" title="35在 Python Matplotlib 中打开饼图的轴"></a>35在 Python Matplotlib 中打开饼图的轴</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-attribute">sizes</span> =<span class="hljs-meta"> [12, 23, 11, 17, 19, 24, 29, 11, 12, 9, 7, 5, 3, 2, 1]</span><br><span class="hljs-attribute">labels</span> =<span class="hljs-meta"> [&quot;Market %s&quot; % i for i in sizes]</span><br><br><span class="hljs-attribute">fig1</span>, ax1 = plt.subplots(figsize=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><span class="hljs-attribute">fig1</span>.subplots_adjust(<span class="hljs-number">0</span>.<span class="hljs-number">1</span>, <span class="hljs-number">0</span>.<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br><span class="hljs-attribute">theme</span> = plt.get_cmap(&#x27;jet&#x27;)<br><span class="hljs-attribute">ax1</span>.set_prop_cycle(<span class="hljs-string">&quot;color&quot;</span>,<span class="hljs-meta"> [theme(1. * i / len(sizes))</span><br><span class="hljs-meta">                             for i in range(len(sizes))])</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">_, _ = ax1.pie(sizes, startangle=90, radius=1800, frame=True)</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">ax1.axis(&#x27;equal&#x27;)</span><br><span class="hljs-meta">plt.show()</span><br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141186.png" alt="图片"></p><h2 id="36具有特定颜色和位置的饼图"><a href="#36具有特定颜色和位置的饼图" class="headerlink" title="36具有特定颜色和位置的饼图"></a>36具有特定颜色和位置的饼图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>import matplotlib.pyplot as plt<br><br>fig =plt.figure(figsize = (4,4))<br>ax11 = fig.add_subplot(111)<br><span class="hljs-comment"># Data to plot</span><br>labels = <span class="hljs-string">&#x27;Python&#x27;</span>, <span class="hljs-string">&#x27;C++&#x27;</span>, <span class="hljs-string">&#x27;Ruby&#x27;</span>, <span class="hljs-string">&#x27;Java&#x27;</span><br>sizes = [250, 130, 75, 200]<br>colors = [<span class="hljs-string">&#x27;gold&#x27;</span>, <span class="hljs-string">&#x27;yellowgreen&#x27;</span>, <span class="hljs-string">&#x27;lightcoral&#x27;</span>, <span class="hljs-string">&#x27;lightskyblue&#x27;</span>]<br><br><span class="hljs-comment"># Plot</span><br>w,l,p = ax11.pie(sizes,  <span class="hljs-attribute">labels</span>=labels, <span class="hljs-attribute">colors</span>=colors,<br>                 <span class="hljs-attribute">autopct</span>=<span class="hljs-string">&#x27;%1.1f%%&#x27;</span>, <span class="hljs-attribute">startangle</span>=140, <span class="hljs-attribute">pctdistance</span>=1, <span class="hljs-attribute">radius</span>=0.5)<br><br>pctdists = [.8, .5, .4, .2]<br><br><span class="hljs-keyword">for</span> t,d <span class="hljs-keyword">in</span> zip(p, pctdists):<br>    xi,yi = t.get_position()<br>    ri = np.sqrt(xi*<span class="hljs-number">*2</span>+yi*<span class="hljs-number">*2</span>)<br>    phi = np.arctan2(yi,xi)<br>    x = d*ri*np.cos(phi)<br>    y = d*ri*np.sin(phi)<br>    t.set_position((x,y))<br><br>plt.axis(<span class="hljs-string">&#x27;equal&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287366841166.png" alt="图片"></p><h2 id="37在-Matplotlib-中绘制极坐标图"><a href="#37在-Matplotlib-中绘制极坐标图" class="headerlink" title="37在 Matplotlib 中绘制极坐标图"></a>37在 Matplotlib 中绘制极坐标图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>employee = [<span class="hljs-string">&quot;Sam&quot;</span>, <span class="hljs-string">&quot;Rony&quot;</span>, <span class="hljs-string">&quot;Albert&quot;</span>, <span class="hljs-string">&quot;Chris&quot;</span>, <span class="hljs-string">&quot;Jahrum&quot;</span>]<br>actual = [<span class="hljs-number">45</span>, <span class="hljs-number">53</span>, <span class="hljs-number">55</span>, <span class="hljs-number">61</span>, <span class="hljs-number">57</span>, <span class="hljs-number">45</span>]<br>expected = [<span class="hljs-number">50</span>, <span class="hljs-number">55</span>, <span class="hljs-number">60</span>, <span class="hljs-number">65</span>, <span class="hljs-number">55</span>, <span class="hljs-number">50</span>]<br><br><span class="hljs-comment"># Initialise the spider plot by setting figure size and polar projection</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.subplot(polar=<span class="hljs-literal">True</span>)<br><br>theta = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">2</span> * np.pi, <span class="hljs-built_in">len</span>(actual))<br><br><span class="hljs-comment"># Arrange the grid into number of sales equal parts in degrees</span><br>lines, labels = plt.thetagrids(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">360</span>, <span class="hljs-built_in">int</span>(<span class="hljs-number">360</span>/<span class="hljs-built_in">len</span>(employee))), (employee))<br><br><span class="hljs-comment"># Plot actual sales graph</span><br>plt.plot(theta, actual)<br>plt.fill(theta, actual, <span class="hljs-string">&#x27;b&#x27;</span>, alpha=<span class="hljs-number">0.1</span>)<br><br><span class="hljs-comment"># Plot expected sales graph</span><br>plt.plot(theta, expected)<br><br><span class="hljs-comment"># Add legend and title for the plot</span><br>plt.legend(labels=(<span class="hljs-string">&#x27;Actual&#x27;</span>, <span class="hljs-string">&#x27;Expected&#x27;</span>), loc=<span class="hljs-number">1</span>)<br>plt.title(<span class="hljs-string">&quot;Actual vs Expected sales by Employee&quot;</span>)<br><br><span class="hljs-comment"># Dsiplay the plot on the screen</span><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141187.png" alt="图片"></p><h2 id="38在-Matplotlib-中绘制半极坐标图"><a href="#38在-Matplotlib-中绘制半极坐标图" class="headerlink" title="38在 Matplotlib 中绘制半极坐标图"></a>38在 Matplotlib 中绘制半极坐标图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>import numpy as np<br><br>theta = np.linspace(0, np.pi)<br>r = np.sin(theta)<br><br>fig = plt.figure()<br>ax = fig.add_subplot(111, <span class="hljs-attribute">polar</span>=<span class="hljs-literal">True</span>)<br>c = ax.scatter(theta, r, <span class="hljs-attribute">c</span>=r, <span class="hljs-attribute">s</span>=10, <span class="hljs-attribute">cmap</span>=<span class="hljs-string">&#x27;hsv&#x27;</span>, <span class="hljs-attribute">alpha</span>=0.75)<br><br>ax.set_thetamin(0)<br>ax.set_thetamax(180)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141188.png" alt="图片"></p><h2 id="39Matplotlib-中的极坐标等值线图"><a href="#39Matplotlib-中的极坐标等值线图" class="headerlink" title="39Matplotlib 中的极坐标等值线图"></a>39Matplotlib 中的极坐标等值线图</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import numpy as <span class="hljs-built_in">np</span><br>import matplotlib.pyplot as plt<br><br># Using linspace so that the endpoint of <span class="hljs-number">360</span> <span class="hljs-built_in">is</span> included<br>actual = <span class="hljs-built_in">np</span>.radians(<span class="hljs-built_in">np</span>.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">360</span>, <span class="hljs-number">20</span>))<br>expected = <span class="hljs-built_in">np</span>.arange(<span class="hljs-number">0</span>, <span class="hljs-number">70</span>, <span class="hljs-number">10</span>)<br><br>r, theta = <span class="hljs-built_in">np</span>.meshgrid(expected, actual)<br><span class="hljs-built_in">values</span> = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">random</span>.<span class="hljs-built_in">random</span>((actual.size, expected.size))<br><br>fig, ax = plt.subplots(subplot_kw=dict(projection=&#x27;<span class="hljs-built_in">polar</span>&#x27;))<br>ax.contourf(theta, r, <span class="hljs-built_in">values</span>)<br><br>plt.<span class="hljs-built_in">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141189.png" alt="图片"></p><h2 id="40绘制直方图"><a href="#40绘制直方图" class="headerlink" title="40绘制直方图"></a>40绘制直方图</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># Data in numpy array</span><br><span class="hljs-attribute">exp_data</span> = np.array([<span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">13</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">11</span>, <span class="hljs-number">19</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">19</span>, <span class="hljs-number">13</span>, <br>                    <span class="hljs-attribute">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <br>                    <span class="hljs-attribute">12</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <br>                    <span class="hljs-attribute">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <br>                    <span class="hljs-attribute">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <br>                    <span class="hljs-attribute">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">18</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>, <br>                    <span class="hljs-attribute">19</span>, <span class="hljs-number">19</span>, <span class="hljs-number">17</span>, <span class="hljs-number">16</span>, <span class="hljs-number">11</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-attribute">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">19</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-attribute">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">18</span>, <span class="hljs-number">13</span>])<br><br><span class="hljs-comment"># Plot the distribution of numpy data</span><br><span class="hljs-attribute">plt</span>.hist(exp_data, bins = <span class="hljs-number">19</span>)<br><br><span class="hljs-comment"># Add axis labels</span><br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;Year&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Salary&quot;</span>)<br><span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Example of Histogram Plot&quot;</span>)<br><br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-164628699141190.png" alt="图片"></p><h2 id="41在-Matplotlib-直方图中选择-bins"><a href="#41在-Matplotlib-直方图中选择-bins" class="headerlink" title="41在 Matplotlib 直方图中选择 bins"></a>41在 Matplotlib 直方图中选择 bins</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># Data in numpy array</span><br><span class="hljs-attribute">data</span> = np.array([<span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">13</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">11</span>, <span class="hljs-number">19</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">19</span>, <span class="hljs-number">13</span>, <br>                    <span class="hljs-attribute">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <br>                    <span class="hljs-attribute">12</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <br>                    <span class="hljs-attribute">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <br>                    <span class="hljs-attribute">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <br>                    <span class="hljs-attribute">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">18</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>, <br>                    <span class="hljs-attribute">19</span>, <span class="hljs-number">19</span>, <span class="hljs-number">17</span>, <span class="hljs-number">16</span>, <span class="hljs-number">11</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-attribute">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">19</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-attribute">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">18</span>, <span class="hljs-number">13</span>])<br><br><span class="hljs-comment"># Plot the distribution of numpy data</span><br><span class="hljs-attribute">ax</span> = plt.hist(data, bins=np.arange(min(data), max(data) + <span class="hljs-number">0</span>.<span class="hljs-number">25</span>, <span class="hljs-number">0</span>.<span class="hljs-number">25</span>), align=&#x27;left&#x27;)<br><span class="hljs-comment"># Add axis labels</span><br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;Year&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Salary&quot;</span>)<br><span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Example of Histogram Plot&quot;</span>)<br><br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287338815164.png" alt="图片"></p><h2 id="42在-Matplotlib-中绘制没有条形的直方图"><a href="#42在-Matplotlib-中绘制没有条形的直方图" class="headerlink" title="42在 Matplotlib 中绘制没有条形的直方图"></a>42在 Matplotlib 中绘制没有条形的直方图</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># Data in numpy array</span><br><span class="hljs-attribute">data</span> = np.array([<span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">13</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">11</span>, <span class="hljs-number">19</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">19</span>, <span class="hljs-number">13</span>,<br>                 <span class="hljs-attribute">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>,<br>                 <span class="hljs-attribute">12</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>,<br>                 <span class="hljs-attribute">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>,<br>                 <span class="hljs-attribute">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>,<br>                 <span class="hljs-attribute">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">18</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>,<br>                 <span class="hljs-attribute">19</span>, <span class="hljs-number">19</span>, <span class="hljs-number">17</span>, <span class="hljs-number">16</span>, <span class="hljs-number">11</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>,<br>                 <span class="hljs-attribute">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">19</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>,<br>                 <span class="hljs-attribute">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">18</span>, <span class="hljs-number">13</span>])<br><br><span class="hljs-attribute">bins</span>, edges = np.histogram(data, <span class="hljs-number">21</span>, normed=<span class="hljs-number">1</span>)<br><span class="hljs-attribute">left</span>, right = edges[:-<span class="hljs-number">1</span>], edges[<span class="hljs-number">1</span>:]<br><span class="hljs-attribute">X</span> = np.array([left, right]).T.flatten()<br><span class="hljs-attribute">Y</span> = np.array([bins, bins]).T.flatten()<br><br><span class="hljs-attribute">plt</span>.plot(X, Y)<br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287332420162.png" alt="图片"></p><h2 id="43使用-Matplotlib-同时绘制两个直方图"><a href="#43使用-Matplotlib-同时绘制两个直方图" class="headerlink" title="43使用 Matplotlib 同时绘制两个直方图"></a>43使用 Matplotlib 同时绘制两个直方图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>import matplotlib.pyplot as plt<br><br>age = np.random.normal(<span class="hljs-attribute">loc</span>=1, <span class="hljs-attribute">size</span>=100) # a normal distribution<br>salaray = np.random.normal(<span class="hljs-attribute">loc</span>=-1, <span class="hljs-attribute">size</span>=10000) # a normal distribution<br><br>_, bins, _ = plt.hist(age, <span class="hljs-attribute">bins</span>=50, range=[-6, 6], <span class="hljs-attribute">density</span>=<span class="hljs-literal">True</span>)<br>_ = plt.hist(salaray, <span class="hljs-attribute">bins</span>=bins, <span class="hljs-attribute">alpha</span>=0.5, <span class="hljs-attribute">density</span>=<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287326594160.png" alt="图片"></p><h2 id="44绘制具有特定颜色、边缘颜色和线宽的直方图"><a href="#44绘制具有特定颜色、边缘颜色和线宽的直方图" class="headerlink" title="44绘制具有特定颜色、边缘颜色和线宽的直方图"></a>44绘制具有特定颜色、边缘颜色和线宽的直方图</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># Data in numpy array</span><br><span class="hljs-attribute">exp_data</span> = np.array([<span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">13</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">11</span>, <span class="hljs-number">19</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">19</span>, <span class="hljs-number">13</span>, <br>                    <span class="hljs-attribute">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <br>                    <span class="hljs-attribute">12</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <br>                    <span class="hljs-attribute">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <br>                    <span class="hljs-attribute">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <br>                    <span class="hljs-attribute">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">18</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>, <br>                    <span class="hljs-attribute">19</span>, <span class="hljs-number">19</span>, <span class="hljs-number">17</span>, <span class="hljs-number">16</span>, <span class="hljs-number">11</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-attribute">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">19</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-attribute">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">18</span>, <span class="hljs-number">13</span>])<br><br><span class="hljs-comment"># Plot the distribution of numpy data</span><br><span class="hljs-attribute">plt</span>.hist(exp_data, bins=<span class="hljs-number">21</span>, align=&#x27;left&#x27;, color=&#x27;b&#x27;, edgecolor=&#x27;red&#x27;,<br>              <span class="hljs-attribute">linewidth</span>=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Add axis labels</span><br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;Year&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Salary&quot;</span>)<br><span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Example of Histogram Plot&quot;</span>)<br><br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287319271158.png" alt="图片"></p><h2 id="45用颜色图绘制直方图"><a href="#45用颜色图绘制直方图" class="headerlink" title="45用颜色图绘制直方图"></a>45用颜色图绘制直方图</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-comment"># Data in numpy array</span><br><span class="hljs-attribute">data</span> = np.array([<span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">13</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">11</span>, <span class="hljs-number">19</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">19</span>, <span class="hljs-number">13</span>, <br>                    <span class="hljs-attribute">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <br>                    <span class="hljs-attribute">12</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <br>                    <span class="hljs-attribute">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <br>                    <span class="hljs-attribute">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <br>                    <span class="hljs-attribute">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">18</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>, <br>                    <span class="hljs-attribute">19</span>, <span class="hljs-number">19</span>, <span class="hljs-number">17</span>, <span class="hljs-number">16</span>, <span class="hljs-number">11</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-attribute">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">19</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-attribute">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">18</span>, <span class="hljs-number">13</span>])<br><br><span class="hljs-attribute">cm</span> = plt.cm.RdBu_r<br><br><span class="hljs-attribute">n</span>, bins, patches = plt.hist(data, <span class="hljs-number">25</span>, normed=<span class="hljs-number">1</span>, color=&#x27;green&#x27;)<br><span class="hljs-attribute">for</span> i, p in enumerate(patches):<br>    <span class="hljs-attribute">plt</span>.setp(p, &#x27;facecolor&#x27;, cm(i/<span class="hljs-number">25</span>)) # notice the i/<span class="hljs-number">25</span><br><br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287312250156.png" alt="图片"></p><h2 id="46更改直方图上特定条的颜色"><a href="#46更改直方图上特定条的颜色" class="headerlink" title="46更改直方图上特定条的颜色"></a>46更改直方图上特定条的颜色</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>s = pd.Series([<span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">13</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">11</span>, <span class="hljs-number">19</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">19</span>, <span class="hljs-number">13</span>, <br>                    <span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <br>                    <span class="hljs-number">12</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <br>                    <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <br>                    <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <br>                    <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">18</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>, <br>                    <span class="hljs-number">19</span>, <span class="hljs-number">19</span>, <span class="hljs-number">17</span>, <span class="hljs-number">16</span>, <span class="hljs-number">11</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">19</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <br>                    <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">18</span>, <span class="hljs-number">13</span>])<br><br>p = s.plot(kind=<span class="hljs-string">&#x27;hist&#x27;</span>, bins=<span class="hljs-number">50</span>, color=<span class="hljs-string">&#x27;orange&#x27;</span>)<br><br>bar_value_to_label = <span class="hljs-number">5</span><br><br>min_distance = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)  <span class="hljs-comment"># initialize min_distance with infinity</span><br>index_of_bar_to_label = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i, rectangle <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(p.patches):  <span class="hljs-comment"># iterate over every bar</span><br>    tmp = <span class="hljs-built_in">abs</span>(  <span class="hljs-comment"># tmp = distance from middle of the bar to bar_value_to_label</span><br>        (rectangle.get_x() +<br>            (rectangle.get_width() * (<span class="hljs-number">1</span> / <span class="hljs-number">2</span>))) - bar_value_to_label)<br>    <span class="hljs-keyword">if</span> tmp &lt; min_distance:  <span class="hljs-comment"># we are searching for the bar with x cordinate</span><br>                            <span class="hljs-comment"># closest to bar_value_to_label</span><br>        min_distance = tmp<br>        index_of_bar_to_label = i<br>p.patches[index_of_bar_to_label].set_color(<span class="hljs-string">&#x27;b&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287297868152.png" alt="图片"></p><h2 id="47箱线图"><a href="#47箱线图" class="headerlink" title="47箱线图"></a>47箱线图</h2><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs prolog">import matplotlib.pyplot as plt<br>import pandas as pd<br><br>df = pd.<span class="hljs-symbol">DataFrame</span>([[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">14</span>, <span class="hljs-number">21</span>, <span class="hljs-number">28</span>], [<span class="hljs-number">15</span>, <span class="hljs-number">15</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>],<br>                   [<span class="hljs-number">15</span>, <span class="hljs-number">14</span>, <span class="hljs-number">1</span>, <span class="hljs-number">8</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">8</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>]],<br>                  columns=[<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>, <span class="hljs-string">&#x27;Banana&#x27;</span>, <span class="hljs-string">&#x27;Pear&#x27;</span>],<br>                  index=[<span class="hljs-string">&#x27;Basket1&#x27;</span>, <span class="hljs-string">&#x27;Basket2&#x27;</span>, <span class="hljs-string">&#x27;Basket3&#x27;</span>, <span class="hljs-string">&#x27;Basket4&#x27;</span>,<br>                         <span class="hljs-string">&#x27;Basket5&#x27;</span>, <span class="hljs-string">&#x27;Basket6&#x27;</span>])<br><br>df.boxplot([<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>, <span class="hljs-string">&#x27;Banana&#x27;</span>, <span class="hljs-string">&#x27;Pear&#x27;</span>])<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287292341150.png" alt="图片"></p><h2 id="48箱型图按列数据分组"><a href="#48箱型图按列数据分组" class="headerlink" title="48箱型图按列数据分组"></a>48箱型图按列数据分组</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br> <br>employees = pd.DataFrame(&#123;<br>    <span class="hljs-string">&#x27;EmpCode&#x27;</span>: [<span class="hljs-string">&#x27;Emp001&#x27;</span>, <span class="hljs-string">&#x27;Emp002&#x27;</span>, <span class="hljs-string">&#x27;Emp003&#x27;</span>, <span class="hljs-string">&#x27;Emp004&#x27;</span>, <span class="hljs-string">&#x27;Emp005&#x27;</span>, <span class="hljs-string">&#x27;Emp006&#x27;</span><br>                , <span class="hljs-string">&#x27;Emp007&#x27;</span>, <span class="hljs-string">&#x27;Emp008&#x27;</span>, <span class="hljs-string">&#x27;Emp009&#x27;</span>, <span class="hljs-string">&#x27;Emp010&#x27;</span>, <span class="hljs-string">&#x27;Emp011&#x27;</span>, <span class="hljs-string">&#x27;Emp012&#x27;</span><br>                , <span class="hljs-string">&#x27;Emp013&#x27;</span>, <span class="hljs-string">&#x27;Emp014&#x27;</span>, <span class="hljs-string">&#x27;Emp015&#x27;</span>, <span class="hljs-string">&#x27;Emp016&#x27;</span>, <span class="hljs-string">&#x27;Emp017&#x27;</span>, <span class="hljs-string">&#x27;Emp018&#x27;</span><br>                , <span class="hljs-string">&#x27;Emp019&#x27;</span>, <span class="hljs-string">&#x27;Emp020&#x27;</span>],<br>    <span class="hljs-string">&#x27;Occupation&#x27;</span>: [<span class="hljs-string">&#x27;Chemist&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>,<br>                   <span class="hljs-string">&#x27;Programmer&#x27;</span>, <span class="hljs-string">&#x27;Chemist&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>,<br>                   <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Programmer&#x27;</span>, <span class="hljs-string">&#x27;Chemist&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>,<br>                   <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Programmer&#x27;</span>, <span class="hljs-string">&#x27;Chemist&#x27;</span>,<br>                   <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Programmer&#x27;</span><br>                   ],<br>    <span class="hljs-string">&#x27;Age&#x27;</span>: [<span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">34</span>, <span class="hljs-number">29</span>, <span class="hljs-number">40</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">29</span>, <span class="hljs-number">40</span>, <span class="hljs-number">41</span>, <span class="hljs-number">40</span>, <span class="hljs-number">35</span>, <span class="hljs-number">41</span>, <span class="hljs-number">29</span>, <span class="hljs-number">33</span>, <span class="hljs-number">35</span>,<br>            <span class="hljs-number">29</span>, <span class="hljs-number">30</span>, <span class="hljs-number">36</span>, <span class="hljs-number">37</span>]&#125;)<br> <br>employees.boxplot(column=[<span class="hljs-string">&#x27;Age&#x27;</span>], <span class="hljs-keyword">by</span>=[<span class="hljs-string">&#x27;Occupation&#x27;</span>])<br> <br>plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287282925148.png" alt="图片"></p><h2 id="49更改箱线图中的箱体颜色"><a href="#49更改箱线图中的箱体颜色" class="headerlink" title="49更改箱线图中的箱体颜色"></a>49更改箱线图中的箱体颜色</h2><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs prolog">import matplotlib.pyplot as plt<br>import pandas as pd<br><br>df = pd.<span class="hljs-symbol">DataFrame</span>([[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">14</span>, <span class="hljs-number">21</span>, <span class="hljs-number">28</span>], [<span class="hljs-number">15</span>, <span class="hljs-number">15</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>],<br>                   [<span class="hljs-number">15</span>, <span class="hljs-number">14</span>, <span class="hljs-number">1</span>, <span class="hljs-number">8</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">8</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>]],<br>                  columns=[<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>, <span class="hljs-string">&#x27;Banana&#x27;</span>, <span class="hljs-string">&#x27;Pear&#x27;</span>],<br>                  index=[<span class="hljs-string">&#x27;Basket1&#x27;</span>, <span class="hljs-string">&#x27;Basket2&#x27;</span>, <span class="hljs-string">&#x27;Basket3&#x27;</span>, <span class="hljs-string">&#x27;Basket4&#x27;</span>,<br>                         <span class="hljs-string">&#x27;Basket5&#x27;</span>, <span class="hljs-string">&#x27;Basket6&#x27;</span>])<br><br>box = plt.boxplot(df, patch_artist=<span class="hljs-symbol">True</span>)<br><br>colors = [<span class="hljs-string">&#x27;blue&#x27;</span>, <span class="hljs-string">&#x27;green&#x27;</span>, <span class="hljs-string">&#x27;purple&#x27;</span>, <span class="hljs-string">&#x27;tan&#x27;</span>, <span class="hljs-string">&#x27;pink&#x27;</span>, <span class="hljs-string">&#x27;red&#x27;</span>]<br><br>for patch, color in zip(box[<span class="hljs-string">&#x27;boxes&#x27;</span>], colors):<br>    patch.set_facecolor(color)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287275306146.png" alt="图片"></p><h2 id="50更改-Boxplot-标记样式、标记颜色和标记大小"><a href="#50更改-Boxplot-标记样式、标记颜色和标记大小" class="headerlink" title="50更改 Boxplot 标记样式、标记颜色和标记大小"></a>50更改 Boxplot 标记样式、标记颜色和标记大小</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>import pandas as pd<br><br>df = pd.DataFrame([[10, 20, 30, 40], [7, 14, 21, 128], [15, 15, 89, 12],<br>                   [-15, 14, 1, 8], [7, -11, 1, 8], [5, 4, 9, 2]],<br>                  columns=[<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>, <span class="hljs-string">&#x27;Banana&#x27;</span>, <span class="hljs-string">&#x27;Pear&#x27;</span>],<br>                  index=[<span class="hljs-string">&#x27;Basket1&#x27;</span>, <span class="hljs-string">&#x27;Basket2&#x27;</span>, <span class="hljs-string">&#x27;Basket3&#x27;</span>, <span class="hljs-string">&#x27;Basket4&#x27;</span>,<br>                         <span class="hljs-string">&#x27;Basket5&#x27;</span>, <span class="hljs-string">&#x27;Basket6&#x27;</span>])<br><br><br>flierprops = dict(<span class="hljs-attribute">marker</span>=<span class="hljs-string">&#x27;+&#x27;</span>, <span class="hljs-attribute">markerfacecolor</span>=<span class="hljs-string">&#x27;g&#x27;</span>, <span class="hljs-attribute">markersize</span>=15,<br>                  <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&#x27;none&#x27;</span>, <span class="hljs-attribute">markeredgecolor</span>=<span class="hljs-string">&#x27;r&#x27;</span>)<br><br>df.boxplot([<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>, <span class="hljs-string">&#x27;Banana&#x27;</span>, <span class="hljs-string">&#x27;Pear&#x27;</span>], <span class="hljs-attribute">flierprops</span>=flierprops)<br><br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287268238144.png" alt="图片"></p><h2 id="51用数据系列绘制水平箱线图"><a href="#51用数据系列绘制水平箱线图" class="headerlink" title="51用数据系列绘制水平箱线图"></a>51用数据系列绘制水平箱线图</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><br><span class="hljs-attribute">data</span> =<span class="hljs-meta"> [-12, 15, 13, -20, 19, 20, 11, 19, -11, 12, 19, 10, </span><br><span class="hljs-meta">                    12, 10, 6, 19, 3, 1, 1, 0, 4, 49, 6, 5, 3, 7, </span><br><span class="hljs-meta">                    12, 77, 9, 8, 12, 11, 11, 18, 19, 18, 19, 3, 6, </span><br><span class="hljs-meta">                    5, 6, 9, 11, 10, 18, 14, 16, 17, 17, 19, 0, 2, </span><br><span class="hljs-meta">                    0, 3, 1, 4, 6, 6, 8, 7, 7, 69, 79, 11, 11, 10, </span><br><span class="hljs-meta">                    11, 10, 13, 13, 15, 18, 20, 19, 1, 11, 8, 16, </span><br><span class="hljs-meta">                    19, 89, 17, 16, 11, 1, 110, 13, 15, 3, 8, 6, 99, </span><br><span class="hljs-meta">                    10, 15, 19, 2, 4, 5, 6, 9, 11, 10, 9, 10, 99, </span><br><span class="hljs-meta">                    15, 16, 18, 13]</span><br><br><span class="hljs-attribute">fig</span> = plt.figure(figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">3</span>), dpi=<span class="hljs-number">100</span>)<br><span class="hljs-attribute">ax</span> = plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br><br><span class="hljs-attribute">ax</span>.boxplot(data, False, sym=&#x27;rs&#x27;, vert=False, whis=<span class="hljs-number">0</span>.<span class="hljs-number">75</span>, positions=[<span class="hljs-number">0</span>], widths=[<span class="hljs-number">0</span>.<span class="hljs-number">5</span>])<br><br><span class="hljs-attribute">plt</span>.tight_layout()<br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287262078142.png" alt="图片"></p><h2 id="52箱线图调整底部和左侧"><a href="#52箱线图调整底部和左侧" class="headerlink" title="52箱线图调整底部和左侧"></a>52箱线图调整底部和左侧</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import matplotlib<span class="hljs-selector-class">.pyplot</span> as plt<br>import pandas as pd<br><br>x = <span class="hljs-selector-attr">[[1.2, 2.3, 3.0, 4.5]</span>,<br>     <span class="hljs-selector-attr">[1.1, 2.2, 2.9, 5.0]</span>]<br><br>df = pd<span class="hljs-selector-class">.DataFrame</span>(x, index=<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>]</span>)<br>df<span class="hljs-selector-class">.T</span><span class="hljs-selector-class">.boxplot</span>()<br><br>plt<span class="hljs-selector-class">.subplots_adjust</span>(<span class="hljs-attribute">bottom</span>=<span class="hljs-number">0.25</span>)<br><br>plt<span class="hljs-selector-class">.show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287254755140.png" alt="图片"></p><h2 id="53使用-Pandas-数据在-Matplotlib-中生成热图"><a href="#53使用-Pandas-数据在-Matplotlib-中生成热图" class="headerlink" title="53使用 Pandas 数据在 Matplotlib 中生成热图"></a>53使用 Pandas 数据在 Matplotlib 中生成热图</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import matplotlib<span class="hljs-selector-class">.pyplot</span> as plt<br>import pandas as pd<br><br>df = pd<span class="hljs-selector-class">.DataFrame</span>(<span class="hljs-selector-attr">[[10, 20, 30, 40]</span>, <span class="hljs-selector-attr">[7, 14, 21, 28]</span>, <span class="hljs-selector-attr">[55, 15, 8, 12]</span>,<br>                    <span class="hljs-selector-attr">[15, 14, 1, 8]</span>],<br>                  <span class="hljs-attribute">columns</span>=[<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>, <span class="hljs-string">&#x27;Banana&#x27;</span>, <span class="hljs-string">&#x27;Pear&#x27;</span>],<br>                  index=<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Basket1&#x27;</span>, <span class="hljs-string">&#x27;Basket2&#x27;</span>, <span class="hljs-string">&#x27;Basket3&#x27;</span>, <span class="hljs-string">&#x27;Basket4&#x27;</span>]</span><br>                  )<br><br>plt<span class="hljs-selector-class">.imshow</span>(df, cmap=<span class="hljs-string">&quot;YlGnBu&quot;</span>)<br>plt<span class="hljs-selector-class">.colorbar</span>()<br>plt<span class="hljs-selector-class">.xticks</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df)),df<span class="hljs-selector-class">.columns</span>, rotation=<span class="hljs-number">20</span>)<br>plt<span class="hljs-selector-class">.yticks</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df)),df.index)<br>plt<span class="hljs-selector-class">.show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287239467138.png" alt="图片"></p><h2 id="54带有中间颜色文本注释的热图"><a href="#54带有中间颜色文本注释的热图" class="headerlink" title="54带有中间颜色文本注释的热图"></a>54带有中间颜色文本注释的热图</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>data = &#123;<br>        <span class="hljs-string">&#x27;Basket1&#x27;</span>: [<span class="hljs-number">90</span>, <span class="hljs-number">95</span>, <span class="hljs-number">99</span>, <span class="hljs-number">50</span>, <span class="hljs-number">50</span>, <span class="hljs-number">45</span>, <span class="hljs-number">81</span>],<br>        <span class="hljs-string">&#x27;Basket2&#x27;</span>: [<span class="hljs-number">91</span>, <span class="hljs-number">98</span>, <span class="hljs-number">89</span>, <span class="hljs-number">75</span>, <span class="hljs-number">98</span>, <span class="hljs-number">49</span>, <span class="hljs-number">80</span>],<br>        <span class="hljs-string">&#x27;Basket3&#x27;</span>: [<span class="hljs-number">92</span>, <span class="hljs-number">97</span>, <span class="hljs-number">99</span>, <span class="hljs-number">85</span>, <span class="hljs-number">96</span>, <span class="hljs-number">75</span>, <span class="hljs-number">88</span>],<br>        <span class="hljs-string">&#x27;Basket4&#x27;</span>: [<span class="hljs-number">94</span>, <span class="hljs-number">96</span>, <span class="hljs-number">88</span>, <span class="hljs-number">79</span>, <span class="hljs-number">98</span>, <span class="hljs-number">69</span>, <span class="hljs-number">86</span>]<br>        &#125;<br><br>fig, ax = plt.subplots(figsize=(<span class="hljs-number">9</span>, <span class="hljs-number">4</span>))<br>df = pd.DataFrame.from_dict(data, orient=<span class="hljs-string">&#x27;index&#x27;</span>)<br><br>im = ax.imshow(df.<span class="hljs-keyword">values</span>, cmap=&quot;YlGnBu&quot;)<br>fig.colorbar(im)<br><br># <span class="hljs-keyword">Loop</span> <span class="hljs-keyword">over</span> data dimensions <span class="hljs-keyword">and</span> <span class="hljs-keyword">create</span> <span class="hljs-type">text</span> annotations<br>textcolors = [&quot;k&quot;, &quot;w&quot;]<br>threshold = <span class="hljs-number">55</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(df)):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(df.<span class="hljs-keyword">columns</span>)):<br>        <span class="hljs-type">text</span> = ax.text(j, i, df.<span class="hljs-keyword">values</span>[i, j],<br>                       ha=&quot;center&quot;, va=&quot;center&quot;,<br>                       color=textcolors[df.<span class="hljs-keyword">values</span>[i, j] &gt; threshold])<br><br>plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287232541136.png" alt="图片"></p><h2 id="55热图显示列和行的标签并以正确的方向显示数据"><a href="#55热图显示列和行的标签并以正确的方向显示数据" class="headerlink" title="55热图显示列和行的标签并以正确的方向显示数据"></a>55热图显示列和行的标签并以正确的方向显示数据</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>import numpy as np<br><br>column_labels = list(<span class="hljs-string">&#x27;ABCDEFGH&#x27;</span>)<br>row_labels = list(<span class="hljs-string">&#x27;12345678&#x27;</span>)<br><br>data = np.random.rand(8, 8)<br><br>fig, ax = plt.subplots()<br>heatmap = ax.pcolor(data, <span class="hljs-attribute">cmap</span>=plt.cm.Reds)<br><br><span class="hljs-comment"># Put the major ticks at the middle of each cell</span><br>ax.set_xticks(np.arange(data.shape[0]), <span class="hljs-attribute">minor</span>=<span class="hljs-literal">False</span>)<br>ax.set_yticks(np.arange(data.shape[0]), <span class="hljs-attribute">minor</span>=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># Want a more natural, table-like display</span><br>ax.invert_yaxis()<br>ax.xaxis.tick_top()<br><br>ax.set_xticklabels(row_labels, <span class="hljs-attribute">minor</span>=<span class="hljs-literal">False</span>)<br>ax.set_yticklabels(column_labels, <span class="hljs-attribute">minor</span>=<span class="hljs-literal">False</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287225520134.png" alt="图片"></p><h2 id="56将-NA-cells-与-HeatMap-中的其他-cells-区分开来"><a href="#56将-NA-cells-与-HeatMap-中的其他-cells-区分开来" class="headerlink" title="56将 NA cells 与 HeatMap 中的其他 cells 区分开来"></a>56将 NA cells 与 HeatMap 中的其他 cells 区分开来</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>import matplotlib.patches as patches<br> <br>import numpy as np<br> <br>column_labels = list(<span class="hljs-string">&#x27;ABCDEFGH&#x27;</span>)<br>row_labels = list(<span class="hljs-string">&#x27;12345678&#x27;</span>)<br> <br>data = np.random.rand(8, 8)<br>data = np.ma.masked_greater(data, 0.8)<br> <br>fig, ax = plt.subplots()<br>heatmap = ax.pcolor(data, <span class="hljs-attribute">cmap</span>=plt.cm.gray, <span class="hljs-attribute">edgecolors</span>=<span class="hljs-string">&#x27;blue&#x27;</span>, <span class="hljs-attribute">linewidths</span>=1,<br>                   <span class="hljs-attribute">antialiased</span>=<span class="hljs-literal">True</span>)<br> <br>fig.colorbar(heatmap)<br>ax.patch.<span class="hljs-built_in">set</span>(<span class="hljs-attribute">hatch</span>=<span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-attribute">edgecolor</span>=<span class="hljs-string">&#x27;red&#x27;</span>)<br> <br><span class="hljs-comment"># Put the major ticks at the middle of each cell</span><br>ax.set_xticks(np.arange(data.shape[0]), <span class="hljs-attribute">minor</span>=<span class="hljs-literal">False</span>)<br>ax.set_yticks(np.arange(data.shape[0]), <span class="hljs-attribute">minor</span>=<span class="hljs-literal">False</span>)<br> <br><span class="hljs-comment"># Want a more natural, table-like display</span><br>ax.invert_yaxis()<br>ax.xaxis.tick_top()<br> <br>ax.set_xticklabels(row_labels, <span class="hljs-attribute">minor</span>=<span class="hljs-literal">False</span>)<br>ax.set_yticklabels(column_labels, <span class="hljs-attribute">minor</span>=<span class="hljs-literal">False</span>)<br> <br> <br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287218026132.png" alt="图片"></p><h2 id="57在-matplotlib-中创建径向热图"><a href="#57在-matplotlib-中创建径向热图" class="headerlink" title="57在 matplotlib 中创建径向热图"></a>57在 matplotlib 中创建径向热图</h2><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-built_in">from</span> mpl_toolkits.mplot3d import Axes3D<br>import numpy <span class="hljs-keyword">as</span> np<br><br>fig = plt.figure()<br>ax = Axes3D(fig)<br><br>n = <span class="hljs-number">12</span><br>m = <span class="hljs-number">24</span><br>rad = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, m)<br><span class="hljs-keyword">a</span> = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">2</span> * np.<span class="hljs-literal">pi</span>, n)<br>r, th = np.meshgrid(rad, <span class="hljs-keyword">a</span>)<br><br>z = np.<span class="hljs-built_in">random</span>.uniform(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, (n,m))<br>plt.subplot(projection=<span class="hljs-string">&quot;polar&quot;</span>)<br><br>plt.pcolormesh(th, r, z, cmap = <span class="hljs-string">&#x27;Blues&#x27;</span>)<br><br>plt.plot(<span class="hljs-keyword">a</span>, r, ls=<span class="hljs-string">&#x27;none&#x27;</span>, color = <span class="hljs-string">&#x27;k&#x27;</span>) <br>plt.grid()<br>plt.colorbar()<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287206149130.png" alt="图片"></p><h2 id="58在-Matplotlib-中组合两个热图"><a href="#58在-Matplotlib-中组合两个热图" class="headerlink" title="58在 Matplotlib 中组合两个热图"></a>58在 Matplotlib 中组合两个热图</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matplotlib.pyplot as plt<br>import numpy as np<br>import pandas as pd<br>import seaborn as sns<br><br>df1 = pd.DataFrame(np.random.rand(20, 4), <span class="hljs-attribute">columns</span>=list(&quot;ABCD&quot;))<br>df2 = pd.DataFrame(np.random.rand(20, 4), <span class="hljs-attribute">columns</span>=list(&quot;WXYZ&quot;))<br><br>fig, (ax1, ax2) = plt.subplots(<span class="hljs-attribute">ncols</span>=2)<br>fig.subplots_adjust(<span class="hljs-attribute">wspace</span>=0.01)<br><br>sns.heatmap(df1, <span class="hljs-attribute">cmap</span>=<span class="hljs-string">&quot;rocket&quot;</span>, <span class="hljs-attribute">ax</span>=ax1, <span class="hljs-attribute">cbar</span>=<span class="hljs-literal">False</span>)<br>fig.colorbar(ax1.collections[0], <span class="hljs-attribute">ax</span>=ax1, <span class="hljs-attribute">location</span>=<span class="hljs-string">&quot;left&quot;</span>, <span class="hljs-attribute">use_gridspec</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">pad</span>=0.2)<br><br>sns.heatmap(df2, <span class="hljs-attribute">cmap</span>=<span class="hljs-string">&quot;icefire&quot;</span>, <span class="hljs-attribute">ax</span>=ax2, <span class="hljs-attribute">cbar</span>=<span class="hljs-literal">False</span>)<br>fig.colorbar(ax2.collections[0], <span class="hljs-attribute">ax</span>=ax2, <span class="hljs-attribute">location</span>=<span class="hljs-string">&quot;right&quot;</span>, <span class="hljs-attribute">use_gridspec</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">pad</span>=0.2)<br><br>ax2.yaxis.tick_right()<br>ax2.tick_params(<span class="hljs-attribute">rotation</span>=0)<br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287194349128.png" alt="图片"></p><h2 id="59使用-Numpy-和-Matplotlib-创建热图日历"><a href="#59使用-Numpy-和-Matplotlib-创建热图日历" class="headerlink" title="59使用 Numpy 和 Matplotlib 创建热图日历"></a>59使用 Numpy 和 Matplotlib 创建热图日历</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs css">import datetime as <span class="hljs-selector-tag">dt</span><br>import matplotlib<span class="hljs-selector-class">.pyplot</span> as plt<br>import numpy as np<br><br><br>def <span class="hljs-selector-tag">main</span>():<br>    dates, data = <span class="hljs-built_in">generate_data</span>()<br>    fig, ax = plt.<span class="hljs-built_in">subplots</span>(figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">10</span>))<br>    <span class="hljs-built_in">calendar_heatmap</span>(ax, dates, data)<br>    plt.<span class="hljs-built_in">show</span>()<br><br><br>def <span class="hljs-built_in">generate_data</span>():<br>    num = <span class="hljs-number">60</span><br>    data = np.random.<span class="hljs-built_in">randint</span>(<span class="hljs-number">0</span>, <span class="hljs-number">20</span>, num)<br>    start = dt.<span class="hljs-built_in">datetime</span>(<span class="hljs-number">2018</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    dates = [start + dt.<span class="hljs-built_in">timedelta</span>(days=i) for i in <span class="hljs-built_in">range</span>(num)]<br>    return dates, data<br><br><br>def <span class="hljs-built_in">calendar_array</span>(dates, data):<br>    i, j = <span class="hljs-built_in">zip</span>(*[d.<span class="hljs-built_in">isocalendar</span>()[<span class="hljs-number">1</span>:] for d in dates])<br>    i = np.<span class="hljs-built_in">array</span>(i) - <span class="hljs-built_in">min</span>(i)<br>    j = np.<span class="hljs-built_in">array</span>(j) - <span class="hljs-number">1</span><br>    ni = <span class="hljs-built_in">max</span>(i) + <span class="hljs-number">1</span><br><br>    calendar = np.nan * np.<span class="hljs-built_in">zeros</span>((ni, <span class="hljs-number">7</span>))<br>    calendar[i, j] = data<br>    return i, j, calendar<br><br><br>def <span class="hljs-built_in">calendar_heatmap</span>(ax, dates, data):<br>    i, j, calendar = <span class="hljs-built_in">calendar_array</span>(dates, data)<br>    im = ax.<span class="hljs-built_in">imshow</span>(calendar, interpolation=<span class="hljs-string">&#x27;none&#x27;</span>, cmap=<span class="hljs-string">&#x27;summer&#x27;</span>)<br>    <span class="hljs-built_in">label_days</span>(ax, dates, i, j, calendar)<br>    <span class="hljs-built_in">label_months</span>(ax, dates, i, j, calendar)<br>    ax.figure.<span class="hljs-built_in">colorbar</span>(im)<br><br><br>def <span class="hljs-built_in">label_days</span>(ax, dates, i, j, calendar):<br>    ni, nj = calendar.shape<br>    day_of_month = np.nan * np.<span class="hljs-built_in">zeros</span>((ni, <span class="hljs-number">7</span>))<br>    day_of_month[i, j] = [d.day for d in dates]<br><br>    for (i, j), day in np.<span class="hljs-built_in">ndenumerate</span>(day_of_month):<br>        if np.<span class="hljs-built_in">isfinite</span>(day):<br>            ax.<span class="hljs-built_in">text</span>(j, i, <span class="hljs-built_in">int</span>(day), ha=<span class="hljs-string">&#x27;center&#x27;</span>, va=<span class="hljs-string">&#x27;center&#x27;</span>)<br><br>    ax.<span class="hljs-built_in">set</span>(xticks=np.<span class="hljs-built_in">arange</span>(<span class="hljs-number">7</span>),<br>           xticklabels=[<span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;W&#x27;</span>, <span class="hljs-string">&#x27;R&#x27;</span>, <span class="hljs-string">&#x27;F&#x27;</span>, <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;S&#x27;</span>])<br>    ax.xaxis.<span class="hljs-built_in">tick_top</span>()<br><br><br>def <span class="hljs-built_in">label_months</span>(ax, dates, i, j, calendar):<br>    month_labels = np.<span class="hljs-built_in">array</span>([<span class="hljs-string">&#x27;Jan&#x27;</span>, <span class="hljs-string">&#x27;Feb&#x27;</span>, <span class="hljs-string">&#x27;Mar&#x27;</span>, <span class="hljs-string">&#x27;Apr&#x27;</span>, <span class="hljs-string">&#x27;May&#x27;</span>, <span class="hljs-string">&#x27;Jun&#x27;</span>, <span class="hljs-string">&#x27;Jul&#x27;</span>,<br>                             <span class="hljs-string">&#x27;Aug&#x27;</span>, <span class="hljs-string">&#x27;Sep&#x27;</span>, <span class="hljs-string">&#x27;Oct&#x27;</span>, <span class="hljs-string">&#x27;Nov&#x27;</span>, <span class="hljs-string">&#x27;Dec&#x27;</span>])<br>    months = np.<span class="hljs-built_in">array</span>([d.month for d in dates])<br>    uniq_months = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(months))<br>    yticks = [i[months == m].<span class="hljs-built_in">mean</span>() for m in uniq_months]<br>    labels = [month_labels[m - <span class="hljs-number">1</span>] for m in uniq_months]<br>    ax.<span class="hljs-built_in">set</span>(yticks=yticks)<br>    ax.<span class="hljs-built_in">set_yticklabels</span>(labels, rotation=<span class="hljs-number">90</span>)<br><br><br><span class="hljs-built_in">main</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287182971126.png" alt="图片"></p><h2 id="60在-Python-中创建分类气泡图"><a href="#60在-Python-中创建分类气泡图" class="headerlink" title="60在 Python 中创建分类气泡图"></a>60在 Python 中创建分类气泡图</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;Company1&#x27;</span>:[<span class="hljs-string">&#x27;Chemist&#x27;</span>, <span class="hljs-string">&#x27;Scientist&#x27;</span>, <span class="hljs-string">&#x27;Worker&#x27;</span>,<br>                               <span class="hljs-string">&#x27;Accountant&#x27;</span>, <span class="hljs-string">&#x27;Programmer&#x27;</span>, <span class="hljs-string">&#x27;Chemist&#x27;</span>,<br>                              <span class="hljs-string">&#x27;Scientist&#x27;</span>, <span class="hljs-string">&#x27;Worker&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>,<br>                              <span class="hljs-string">&#x27;Programmer&#x27;</span>, <span class="hljs-string">&#x27;Chemist&#x27;</span>, <span class="hljs-string">&#x27;Accountant&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>,<br>                               <span class="hljs-string">&#x27;Scientist&#x27;</span>, <span class="hljs-string">&#x27;Accountant&#x27;</span>, <span class="hljs-string">&#x27;Chemist&#x27;</span>,<br>                              <span class="hljs-string">&#x27;Scientist&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>,<br>                              <span class="hljs-string">&#x27;Programmer&#x27;</span>],                   <br>                   <span class="hljs-string">&#x27;Company2&#x27;</span>:[<span class="hljs-string">&#x27;Programmer&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Scientist&#x27;</span>,<br>                               <span class="hljs-string">&#x27;Statistician&#x27;</span>, <span class="hljs-string">&#x27;Worker&#x27;</span>, <span class="hljs-string">&#x27;Chemist&#x27;</span>,<br>                              <span class="hljs-string">&#x27;Accountant&#x27;</span>, <span class="hljs-string">&#x27;Accountant&#x27;</span>, <span class="hljs-string">&#x27;Statistician&#x27;</span>,<br>                              <span class="hljs-string">&#x27;Chemist&#x27;</span>, <span class="hljs-string">&#x27;Programmer&#x27;</span>, <span class="hljs-string">&#x27;Scientist&#x27;</span>, <span class="hljs-string">&#x27;Scientist&#x27;</span>,<br>                               <span class="hljs-string">&#x27;Accountant&#x27;</span>, <span class="hljs-string">&#x27;Programmer&#x27;</span>, <span class="hljs-string">&#x27;Chemist&#x27;</span>,<br>                              <span class="hljs-string">&#x27;Accountant&#x27;</span>, <span class="hljs-string">&#x27;Scientist&#x27;</span>, <span class="hljs-string">&#x27;Scientist&#x27;</span>,<br>                              <span class="hljs-string">&#x27;Worker&#x27;</span>],<br>                   <span class="hljs-string">&#x27;Count&#x27;</span>:[<span class="hljs-number">53</span>, <span class="hljs-number">15</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">22</span>, <span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">15</span>, <span class="hljs-number">15</span>,<br>                             <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">22</span>, <span class="hljs-number">22</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>]<br>                   &#125;)<br><br># <span class="hljs-keyword">Create</span> padding <span class="hljs-keyword">column</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">values</span> <span class="hljs-keyword">for</span> circles that are neither too small nor too <span class="hljs-keyword">large</span><br>df[&quot;padd&quot;] = <span class="hljs-number">2.5</span> * (df.Count - df.Count.min()) / (df.Count.max() - df.Count.min()) + <span class="hljs-number">0.5</span><br><br>fig = plt.figure()<br># <span class="hljs-keyword">Prepare</span> the axes <span class="hljs-keyword">for</span> the plot - you can <span class="hljs-keyword">also</span> <span class="hljs-keyword">order</span> your categories at this step<br>s = plt.scatter(sorted(df.Company1.<span class="hljs-keyword">unique</span>()),<br>                sorted(df.Company2.<span class="hljs-keyword">unique</span>(), <span class="hljs-keyword">reverse</span> = <span class="hljs-keyword">True</span>), s = <span class="hljs-number">0</span>)<br>s.remove<br># Plot data <span class="hljs-keyword">row</span>-wise <span class="hljs-keyword">as</span> <span class="hljs-type">text</span> <span class="hljs-keyword">with</span> <span class="hljs-type">circle</span> radius according <span class="hljs-keyword">to</span> Count<br><span class="hljs-keyword">for</span> <span class="hljs-keyword">row</span> <span class="hljs-keyword">in</span> df.itertuples():<br>    bbox_props = dict(boxstyle = &quot;circle, pad = &#123;&#125;&quot;.format(<span class="hljs-keyword">row</span>.padd),<br>                      fc = &quot;w&quot;, ec = &quot;b&quot;, lw = <span class="hljs-number">2</span>)<br>    plt.annotate(str(<span class="hljs-keyword">row</span>.Count), xy = (<span class="hljs-keyword">row</span>.Company1, <span class="hljs-keyword">row</span>.Company2),<br>                 bbox = bbox_props, ha=&quot;center&quot;, va=&quot;center&quot;, zorder = <span class="hljs-number">2</span>,<br>                 clip_on = <span class="hljs-keyword">True</span>)<br><br># Plot grid behind markers<br>plt.grid(ls = &quot;--&quot;, zorder = <span class="hljs-number">1</span>)<br><br># Take care <span class="hljs-keyword">of</span> long labels<br>fig.autofmt_xdate()<br>plt.tight_layout()<br>plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287172939124.png" alt="图片"></p><h2 id="61使用-Numpy-和-Matplotlib-创建方形气泡图"><a href="#61使用-Numpy-和-Matplotlib-创建方形气泡图" class="headerlink" title="61使用 Numpy 和 Matplotlib 创建方形气泡图"></a>61使用 Numpy 和 Matplotlib 创建方形气泡图</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> random<br><br><span class="hljs-attribute">xs</span> = np.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br><span class="hljs-attribute">ys</span> = np.arange(<span class="hljs-number">0</span>.<span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">1</span>)<br><br><span class="hljs-attribute">colors</span> =<span class="hljs-meta"> [&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;]</span><br><br><br><span class="hljs-attribute">def</span> square_size_color(value):<br><br>    <span class="hljs-attribute">square_color</span> = random.choice(colors)<br><br>    <span class="hljs-attribute">if</span> value &lt; <span class="hljs-number">1</span>:<br>        <span class="hljs-attribute">square_size</span> = random.choice(range(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>))<br>    <span class="hljs-attribute">if</span> <span class="hljs-number">3</span> &gt; value &gt; <span class="hljs-number">1</span>:<br>        <span class="hljs-attribute">square_size</span> = random.choice(range(<span class="hljs-number">10</span>, <span class="hljs-number">25</span>))<br>    <span class="hljs-attribute">else</span>:<br>        <span class="hljs-attribute">square_size</span> = random.choice(range(<span class="hljs-number">25</span>, <span class="hljs-number">35</span>))<br><br>    <span class="hljs-attribute">return</span> square_size, square_color<br><br><br><span class="hljs-attribute">for</span> x in xs:<br>    <span class="hljs-attribute">for</span> y in ys:<br>        <span class="hljs-attribute">square_size</span>, square_color = square_size_color(y)<br>        <span class="hljs-attribute">plt</span>.plot(x, y, linestyle=<span class="hljs-string">&quot;None&quot;</span>, marker=<span class="hljs-string">&quot;s&quot;</span>,<br>                 <span class="hljs-attribute">markersize</span>=square_size, mfc=square_color, mec=square_color)<br><br><span class="hljs-attribute">plt</span>.grid(visible=True, axis=&#x27;y&#x27;)<br><span class="hljs-attribute">plt</span>.xlim(<span class="hljs-number">0</span>.<span class="hljs-number">5</span>, <span class="hljs-number">4</span>.<span class="hljs-number">5</span>)<br><span class="hljs-attribute">plt</span>.ylim(-<span class="hljs-number">0</span>.<span class="hljs-number">5</span>, <span class="hljs-number">6</span>.<span class="hljs-number">5</span>)<br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287156311122.png" alt="图片"></p><h2 id="62使用-Numpy-和-Matplotlib-创建具有气泡大小的图例"><a href="#62使用-Numpy-和-Matplotlib-创建具有气泡大小的图例" class="headerlink" title="62使用 Numpy 和 Matplotlib 创建具有气泡大小的图例"></a>62使用 Numpy 和 Matplotlib 创建具有气泡大小的图例</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br> <br>N = <span class="hljs-number">50</span><br>M = <span class="hljs-number">5</span> # Number <span class="hljs-keyword">of</span> bins<br> <br>x = np.random.rand(N)<br>y = np.random.rand(N)<br>a2 = <span class="hljs-number">400</span>*np.random.rand(N)<br> <br># <span class="hljs-keyword">Create</span> the DataFrame <span class="hljs-keyword">from</span> your randomised data <span class="hljs-keyword">and</span> bin it <span class="hljs-keyword">using</span> groupby.<br>df = pd.DataFrame(data=dict(x=x, y=y, a2=a2))<br>bins = np.linspace(df.a2.min(), df.a2.max(), M)<br>grouped = df.groupby(np.digitize(df.a2, bins))<br> <br># <span class="hljs-keyword">Create</span> <span class="hljs-keyword">some</span> sizes <span class="hljs-keyword">and</span> <span class="hljs-keyword">some</span> labels.<br>sizes = [<span class="hljs-number">50</span>*(i+<span class="hljs-number">1.</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(M)]<br>labels = [<span class="hljs-string">&#x27;Tiny&#x27;</span>, <span class="hljs-string">&#x27;Small&#x27;</span>, <span class="hljs-string">&#x27;Medium&#x27;</span>, <span class="hljs-string">&#x27;Large&#x27;</span>, <span class="hljs-string">&#x27;Huge&#x27;</span>]<br> <br><span class="hljs-keyword">for</span> i, (<span class="hljs-type">name</span>, <span class="hljs-keyword">group</span>) <span class="hljs-keyword">in</span> enumerate(grouped):<br>    plt.scatter(<span class="hljs-keyword">group</span>.x, <span class="hljs-keyword">group</span>.y, s=sizes[i], alpha=<span class="hljs-number">0.5</span>, label=labels[i])<br> <br>plt.legend()<br>plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287147615120.png" alt="图片"></p><h2 id="63使用-Matplotlib-堆叠条形图"><a href="#63使用-Matplotlib-堆叠条形图" class="headerlink" title="63使用 Matplotlib 堆叠条形图"></a>63使用 Matplotlib 堆叠条形图</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>df = pd.DataFrame([[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">14</span>, <span class="hljs-number">21</span>, <span class="hljs-number">28</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]],<br>                  <span class="hljs-keyword">columns</span>=[<span class="hljs-string">&#x27;Apple&#x27;</span>, <span class="hljs-string">&#x27;Orange&#x27;</span>, <span class="hljs-string">&#x27;Banana&#x27;</span>, <span class="hljs-string">&#x27;Pear&#x27;</span>],<br>                  <span class="hljs-keyword">index</span>=[<span class="hljs-string">&#x27;Basket1&#x27;</span>, <span class="hljs-string">&#x27;Basket2&#x27;</span>, <span class="hljs-string">&#x27;Basket3&#x27;</span>])<br><br>ax = df.plot(kind=<span class="hljs-string">&#x27;bar&#x27;</span>, <span class="hljs-keyword">stacked</span>=<span class="hljs-keyword">True</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;DataFrame Values&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;Basket&#x27;</span>)<br>plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287129791118.png" alt="图片"></p><h2 id="64在同一图中绘制多个堆叠条"><a href="#64在同一图中绘制多个堆叠条" class="headerlink" title="64在同一图中绘制多个堆叠条"></a>64在同一图中绘制多个堆叠条</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>df = pd.DataFrame(dict(<br>    A=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],<br>    B=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],<br>    C=[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]<br>))<br><br>fig, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>), sharey=<span class="hljs-keyword">True</span>)<br><br>df.plot.bar(ax=axes[<span class="hljs-number">0</span>])<br>df.diff(axis=<span class="hljs-number">1</span>).fillna(df).astype(df.dtypes).plot.bar(ax=axes[<span class="hljs-number">1</span>], <span class="hljs-keyword">stacked</span>=<span class="hljs-keyword">True</span>)<br><br>plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287105490116.png" alt="图片"></p><h2 id="65Matplotlib-中的水平堆积条形图"><a href="#65Matplotlib-中的水平堆积条形图" class="headerlink" title="65Matplotlib 中的水平堆积条形图"></a>65Matplotlib 中的水平堆积条形图</h2><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import numpy <span class="hljs-keyword">as</span> np<br>import matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>people = (<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-string">&#x27;B&#x27;</span>,<span class="hljs-string">&#x27;C&#x27;</span>,<span class="hljs-string">&#x27;D&#x27;</span>,<span class="hljs-string">&#x27;E&#x27;</span>,<span class="hljs-string">&#x27;F&#x27;</span>,<span class="hljs-string">&#x27;G&#x27;</span>,<span class="hljs-string">&#x27;H&#x27;</span>)<br><span class="hljs-keyword">segments</span> = <span class="hljs-number">4</span><br><br><span class="hljs-comment"># generate some multi-dimensional data &amp; arbitrary labels</span><br>data = <span class="hljs-number">3</span> + <span class="hljs-number">10</span>* np.<span class="hljs-built_in">random</span>.rand(<span class="hljs-keyword">segments</span>, <span class="hljs-built_in">len</span>(people))<br>percentages = (np.<span class="hljs-built_in">random</span>.randint(<span class="hljs-number">5</span>,<span class="hljs-number">20</span>, (<span class="hljs-built_in">len</span>(people), <span class="hljs-keyword">segments</span>)))<br>y_pos = np.arange(<span class="hljs-built_in">len</span>(people))<br><br>fig = plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>ax = fig.add_subplot(<span class="hljs-number">111</span>)<br><br>colors =<span class="hljs-string">&#x27;rgbwmc&#x27;</span><br>patch_handles = []<br>left = np.zeros(<span class="hljs-built_in">len</span>(people)) <span class="hljs-comment"># left alignment of data starts at zero</span><br><span class="hljs-keyword">for</span> i, d <span class="hljs-keyword">in</span> enumerate(data):<br>    patch_handles.append(ax.barh(y_pos, d, <br>      color=colors[i%<span class="hljs-built_in">len</span>(colors)], align=<span class="hljs-string">&#x27;center&#x27;</span>, <br>      left=left))<br>    <span class="hljs-comment"># accumulate the left-hand offsets</span><br>    left += d<br><br><span class="hljs-comment"># go through all of the bar segments and annotate</span><br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-built_in">len</span>(patch_handles)):<br>    <span class="hljs-keyword">for</span> i, patch <span class="hljs-keyword">in</span> enumerate(patch_handles[j].get_children()):<br>        bl = patch.get_xy()<br>        x = <span class="hljs-number">0.5</span>*patch.get_width() + bl[<span class="hljs-number">0</span>]<br>        y = <span class="hljs-number">0.5</span>*patch.get_height() + bl[<span class="hljs-number">1</span>]<br>        ax.<span class="hljs-keyword">text</span>(x,y, <span class="hljs-string">&quot;%d%%&quot;</span> % (percentages[i,j]), ha=<span class="hljs-string">&#x27;center&#x27;</span>)<br><br>ax.set_yticks(y_pos)<br>ax.set_yticklabels(people)<br>ax.set_xlabel(<span class="hljs-string">&#x27;Distance&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>Output:</p><p><img src="/%5Csrc%5C640-1646287095116114.png" alt="图片">set_yticklabels(people)<br>ax.set_xlabel(‘Distance’)</p><p>plt.show()</p><p>&#96;&#96;&#96;</p><p>Output:</p><p><img src="/..%5Csrc%5C640-1646287095116114.png" alt="图片"></p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python Logging</title>
    <link href="/2022/08/01/Python/Logging/"/>
    <url>/2022/08/01/Python/Logging/</url>
    
    <content type="html"><![CDATA[<h1 id="Logging-理解"><a href="#Logging-理解" class="headerlink" title="Logging 理解"></a>Logging 理解</h1><p>Logging作为Python的标准库之一，日志是对软件执行时所发生事件的一种追踪方式，所以他可以十分灵活的帮助我们记录程序运行过程中的一些信息。这里给出<a href="https://docs.python.org/zh-cn/3/library/logging.html#">官方文档</a>，可以进行参考</p><h2 id="什么时候使用日志"><a href="#什么时候使用日志" class="headerlink" title="什么时候使用日志"></a>什么时候使用日志</h2><p>对于简单的日志使用来说日志功能提供了一系列便利的函数。它们是 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.debug"><code>debug()</code></a>，<a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.info"><code>info()</code></a>，<a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.warning"><code>warning()</code></a>，<a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.error"><code>error()</code></a> 和 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.critical"><code>critical()</code></a>。想要决定何时使用日志，请看下表，其中显示了对于每个通用任务集合来说最好的工具。</p><table><thead><tr><th align="left">你想要执行的任务</th><th align="left">此任务最好的工具</th></tr></thead><tbody><tr><td align="left">对于命令行或程序的应用，结果显示在控制台。</td><td align="left"><a href="https://docs.python.org/zh-cn/3.10/library/functions.html#print"><code>print()</code></a></td></tr><tr><td align="left">在对程序的普通操作发生时提交事件报告(比如：状态监控和错误调查)</td><td align="left"><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.info"><code>logging.info()</code></a> 函数(当有诊断目的需要详细输出信息时使用 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.debug"><code>logging.debug()</code></a> 函数)</td></tr><tr><td align="left">提出一个警告信息基于一个特殊的运行时事件</td><td align="left"><a href="https://docs.python.org/zh-cn/3.10/library/warnings.html#warnings.warn"><code>warnings.warn()</code></a> 位于代码库中，该事件是可以避免的，需要修改客户端应用以消除告警<a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.warning"><code>logging.warning()</code></a> 不需要修改客户端应用，但是该事件还是需要引起关注</td></tr><tr><td align="left">对一个特殊的运行时事件报告错误</td><td align="left">引发异常</td></tr><tr><td align="left">报告错误而不引发异常(如在长时间运行中的服务端进程的错误处理)</td><td align="left"><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.error"><code>logging.error()</code></a>, <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.exception"><code>logging.exception()</code></a> 或 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.critical"><code>logging.critical()</code></a> 分别适用于特定的错误及应用领域</td></tr></tbody></table><p>日志功能应以所追踪事件级别或严重性而定。各级别适用性如下（以严重性递增）：</p><table><thead><tr><th align="left">级别</th><th>级别数值</th><th align="left">何时使用</th><th></th></tr></thead><tbody><tr><td align="left"><code>DEBUG</code></td><td>10</td><td align="left">细节信息，仅当诊断问题时适用。</td><td></td></tr><tr><td align="left"><code>INFO</code></td><td>20</td><td align="left">确认程序按预期运行。</td><td></td></tr><tr><td align="left"><code>WARNING</code></td><td>30</td><td align="left">表明有已经或即将发生的意外（例如：磁盘空间不足）。程序仍按预期进行。</td><td></td></tr><tr><td align="left"><code>ERROR</code></td><td>40</td><td align="left">由于严重的问题，程序的某些功能已经不能正常执行</td><td></td></tr><tr><td align="left"><code>CRITICAL</code></td><td>50</td><td align="left">严重的错误，表明程序已不能继续执行</td><td></td></tr></tbody></table><p>默认的级别是 <code>WARNING</code>，意味着只会追踪该级别及以上的事件，除非更改日志配置。</p><p>所追踪事件可以以不同形式处理。最简单的方式是输出到控制台。另一种常用的方式是写入磁盘文件。</p><h2 id="日志的输出格式"><a href="#日志的输出格式" class="headerlink" title="日志的输出格式"></a>日志的输出格式</h2><p><img src="/src/1620.png" alt="img"></p><h2 id="怎么使用日志"><a href="#怎么使用日志" class="headerlink" title="怎么使用日志"></a>怎么使用日志</h2><p><strong>最简单的方法</strong>当然是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">logging.warning(<span class="hljs-string">&#x27;Watch out!&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这样就会直接在控制台中输出<code>WARNING:root:Watch out!</code></p><p>同样可以在消息中<strong>填加一些事件的属性</strong>，例如时间、日期等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br>logging.basicConfig(<span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%(asctime)s %(message)s&#x27;</span>)<br>logging.warning(<span class="hljs-string">&#x27;is when this event was logged.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>那么它的输出就是<code>2022-07-18 16:08:42,612 is when this event was logged.</code></p><p>当然这里还存在一些高级的<strong>进项使用方法</strong></p><p>日志库采用模块化方法，并提供几类组件（对象）：**<code>Logger</code><strong>日志、</strong><code>LogRecord</code><strong>记录器、</strong><code>Filter过滤器</code><strong>、</strong><code>Handler</code><strong>处理器、</strong><code>Formatter</code>**格式器。</p><ul><li>记录器暴露了应用程序代码直接使用的接口。</li><li>处理器将日志记录（由记录器创建）发送到适当的目标。</li><li>过滤器提供了更细粒度的功能，用于确定要输出的日志记录。</li><li>格式器指定最终输出中日志记录的样式。</li></ul><p><img src="/src/logging_flow.png" alt="/_images/logging_flow.png"></p><p>Logging流程解释：</p><ol><li>判断 Logger 对象对于设置的级别是否可用，如果可用，则往下执行，否则，流程结束。</li><li>创建 LogRecord 对象，如果注册到 Logger 对象中的 Filter 对象过滤后返回 False，则不记录日志，流程结束，否则，则向下执行。</li><li>LogRecord 对象将 Handler 对象传入当前的 Logger 对象，（图中的子流程）如果 Handler 对象的日志级别大于设置的日志级别，再判断注册到 Handler 对象中的 Filter 对象过滤后是否返回 True 而放行输出日志信息，否则不放行，流程结束。</li><li>如果传入的 Handler 大于 Logger 中设置的级别，也即 Handler 有效，则往下执行，否则，流程结束。</li><li>判断这个 Logger 对象是否还有父 Logger 对象，如果没有（代表当前 Logger 对象是最顶层的 Logger 对象 root Logger），流程结束。否则将 Logger 对象设置为它的父 Logger 对象，重复上面的 3、4 两步，输出父类 Logger 对象中的日志输出，直到是 root Logger 为止。</li></ol><h3 id="记录器"><a href="#记录器" class="headerlink" title="记录器"></a>记录器</h3><p><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger"><code>Logger</code></a> 对象有三重任务。首先，它们向应用程序代码公开了几种方法，以便应用程序可以在运行时记录消息。其次，记录器对象根据严重性（默认过滤工具）或过滤器对象确定要处理的日志消息。第三，记录器对象将相关的日志消息传递给所有感兴趣的日志处理器。</p><p><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.getLogger"><code>getLogger()</code></a> 返回对具有指定名称的记录器实例的引用（如果已提供），或者如果没有则返回 <code>root</code> 。</p><p>记录器对象上使用最广泛的方法分为两类：配置和消息发送。这些是最常见的配置方法：</p><ul><li><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.setLevel"><code>Logger.setLevel()</code></a> 指定记录器将处理的最低严重性日志消息，其中 debug 是最低内置严重性级别， critical 是最高内置严重性级别。 例如，如果严重性级别为 INFO ，则记录器将仅处理 INFO 、 WARNING 、 ERROR 和 CRITICAL 消息，并将忽略 DEBUG 消息。</li><li><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.addHandler"><code>Logger.addHandler()</code></a> 和 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.removeHandler"><code>Logger.removeHandler()</code></a> 从记录器对象中添加和删除处理器对象。处理器在以下内容中有更详细的介绍 <a href="https://docs.python.org/zh-cn/3.10/howto/logging.html#handler-basic">处理器</a> 。</li><li><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.addFilter"><code>Logger.addFilter()</code></a> 和 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.removeFilter"><code>Logger.removeFilter()</code></a> 可以添加或移除记录器对象中的过滤器。 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#filter">过滤器对象</a> 包含更多的过滤器细节。</li></ul><p>配置记录器对象后，以下方法将创建日志消息：</p><ul><li><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.debug"><code>Logger.debug()</code></a> 、 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.info"><code>Logger.info()</code></a> 、 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.warning"><code>Logger.warning()</code></a> 、 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.error"><code>Logger.error()</code></a> 和 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.critical"><code>Logger.critical()</code></a> 都创建日志记录，包含消息和与其各自方法名称对应的级别。该消息实际上是一个格式化字符串，它可能包含标题字符串替换语法 <code>%s</code> 、 <code>%d</code> 、 <code>%f</code> 等等。其余参数是与消息中的替换字段对应的对象列表。关于 <code>**kwargs</code> ，日志记录方法只关注 <code>exc_info</code> 的关键字，并用它来确定是否记录异常信息。</li><li><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.exception"><code>Logger.exception()</code></a> 创建与 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.error"><code>Logger.error()</code></a> 相似的日志信息。 不同之处是， <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.exception"><code>Logger.exception()</code></a> 同时还记录当前的堆栈追踪。仅从异常处理程序调用此方法。</li><li><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.log"><code>Logger.log()</code></a> 将日志级别作为显式参数。对于记录消息而言，这比使用上面列出的日志级别便利方法更加冗长，但这是使用自定义日志级别的方法。</li></ul><h3 id="处理器"><a href="#处理器" class="headerlink" title="处理器"></a>处理器</h3><p><a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Handler"><code>Handler</code></a> 对象负责将适当的日志消息（基于日志消息的严重性）分派给处理器的指定目标。 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger"><code>Logger</code></a> 对象可以使用 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Logger.addHandler"><code>addHandler()</code></a> 方法向自己添加零个或多个处理器对象。</p><p>注意不要直接实例化 <a href="https://docs.python.org/zh-cn/3/library/logging.html#logging.Handler"><code>Handler</code></a> ；这个类用来派生其他更有用的子类。但是，子类的 <code>__init__()</code> 方法需要调用 <a href="https://docs.python.org/zh-cn/3/library/logging.html#logging.Handler.__init__"><code>Handler.__init__()</code></a> 。</p><h3 id="格式器"><a href="#格式器" class="headerlink" title="格式器"></a>格式器</h3><p>格式化器对象配置日志消息的最终顺序、结构和内容。 与 <a href="https://docs.python.org/zh-cn/3.10/library/logging.html#logging.Handler"><code>logging.Handler</code></a> 类不同，应用程序代码可以实例化格式器类，但如果应用程序需要特殊行为，则可能会对格式化器进行子类化定制。</p><p>构造函数有三个可选参数 —— 消息格式字符串、日期格式字符串和样式指示符。</p><p>一般情况下，它们负责将 <a href="https://docs.python.org/zh-cn/3/library/logging.html#logging.LogRecord"><code>LogRecord</code></a> 转换为可由人或外部系统解释的字符串。基础的 <a href="https://docs.python.org/zh-cn/3/library/logging.html#logging.Formatter"><code>Formatter</code></a> 允许指定格式字符串。如果未提供任何值，则使用默认值 <code>&#39;%(message)s&#39;</code> ，它仅将消息包括在日志记录调用中。</p><h3 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h3><p><code>Filters</code> 可被 <code>Handlers</code> 和 <code>Loggers</code> 用来实现比按层级提供更复杂的过滤操作。 基本过滤器类只允许低于日志记录器层级结构中低于特定层级的事件。</p><p>在上面的使用中我们用到了<code>basicConfig()</code>，这个方法就可基本满足一般使用需求。在没有传入参数时，会根据默认的配置来创建一个Logger对象，把日志级别默认设置为Warning。具体的参数为：</p><table><thead><tr><th align="left">参数名称</th><th align="left">参数描述</th></tr></thead><tbody><tr><td align="left">filename</td><td align="left">日志输出到文件的文件名</td></tr><tr><td align="left">filemode</td><td align="left">文件模式，r[+]、w[+]、a[+]</td></tr><tr><td align="left">format</td><td align="left">日志输出的格式</td></tr><tr><td align="left">datefat</td><td align="left">日志附带日期时间的格式</td></tr><tr><td align="left">style</td><td align="left">格式占位符，默认为 “%” 和 “{}”</td></tr><tr><td align="left">level</td><td align="left">设置日志输出级别</td></tr><tr><td align="left">stream</td><td align="left">定义输出流，用来初始化 StreamHandler 对象，不能 filename 参数一起使用，否则会ValueError 异常</td></tr><tr><td align="left">handles</td><td align="left">定义处理器，用来创建 Handler 对象，不能和 filename 、stream 参数一起使用，否则也会抛出 ValueError 异常</td></tr></tbody></table><p>例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><br>logging.basicConfig()<br>logging.debug(<span class="hljs-string">&#x27;This is a debug message&#x27;</span>)<br>logging.info(<span class="hljs-string">&#x27;This is an info message&#x27;</span>)<br>logging.warning(<span class="hljs-string">&#x27;This is a warning message&#x27;</span>)<br>logging.error(<span class="hljs-string">&#x27;This is an error message&#x27;</span>)<br>logging.critical(<span class="hljs-string">&#x27;This is a critical message&#x27;</span>)<br></code></pre></td></tr></table></figure><p>结果如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">WARNING:root:This <span class="hljs-keyword">is</span> a warning message<br>ERROR:root:This <span class="hljs-keyword">is</span> an error message<br>CRITICAL:root:This <span class="hljs-keyword">is</span> a critical message<br></code></pre></td></tr></table></figure><p>当然我们也可以自定义去设计一个Logger</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_file</span>():<br>    log_file = <span class="hljs-string">&#x27;testfun.log&#x27;</span><br>    handler_test = logging.FileHandler(log_file,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-comment"># stdout to file</span><br>    handler_control = logging.StreamHandler()    <span class="hljs-comment"># stdout to console</span><br>    handler_test.setLevel(<span class="hljs-string">&#x27;ERROR&#x27;</span>)               <span class="hljs-comment"># 设置ERROR级别</span><br>    handler_control.setLevel(<span class="hljs-string">&#x27;INFO&#x27;</span>)             <span class="hljs-comment"># 设置INFO级别</span><br><br>    selfdef_fmt = <span class="hljs-string">&#x27;%(asctime)s - %(funcName)s - %(levelname)s - %(message)s&#x27;</span><br>    formatter = logging.Formatter(selfdef_fmt)<br>    handler_test.setFormatter(formatter)<br>    handler_control.setFormatter(formatter)<br><br>    logger = logging.getLogger(<span class="hljs-string">&#x27;updateSecurity&#x27;</span>)<br>    logger.setLevel(<span class="hljs-string">&#x27;DEBUG&#x27;</span>)           <span class="hljs-comment">#设置了这个才会把debug以上的输出到控制台</span><br><br>    logger.addHandler(handler_test)    <span class="hljs-comment">#添加handler</span><br>    logger.addHandler(handler_control)<br>    logger.info(<span class="hljs-string">&#x27;info,一般的信息输出&#x27;</span>)<br>    logger.warning(<span class="hljs-string">&#x27;waring，用来用来打印警告信息&#x27;</span>)<br>    logger.error(<span class="hljs-string">&#x27;error，一般用来打印一些错误信息&#x27;</span>)<br>    logger.critical(<span class="hljs-string">&#x27;critical，用来打印一些致命的错误信息，等级最高&#x27;</span>)<br><br>log_file()<br></code></pre></td></tr></table></figure><h2 id="实战问题"><a href="#实战问题" class="headerlink" title="实战问题"></a>实战问题</h2><h3 id="中文乱码"><a href="#中文乱码" class="headerlink" title="中文乱码"></a><strong>中文乱码</strong></h3><p>FileHandler 创建对象时可以设置文件编码，如果将文件编码设置为 “utf-8”（utf-8 和 utf8 等价），就可以解决中文乱码问题</p><h3 id="临时禁用日志输出"><a href="#临时禁用日志输出" class="headerlink" title="临时禁用日志输出"></a><strong>临时禁用日志输出</strong></h3><p>有时候我们又不想让日志输出，但在这后又想输出日志。如果我们打印信息用的是 print() 方法，那么就需要把所有的 print() 方法都注释掉，而使用了 logging 后，我们就有了一键开关闭日志的 “魔法”。一种方法是在使用默认配置时，给 logging.disabled() 方法传入禁用的日志级别，就可以禁止设置级别以下的日志输出了，另一种方法时在自定义 Logger 时，Logger 对象的 disable 属性设为 True，默认值是 False，也即不禁用。</p><h3 id="日志文件按照时间划分或者按照大小划分"><a href="#日志文件按照时间划分或者按照大小划分" class="headerlink" title="日志文件按照时间划分或者按照大小划分"></a><strong>日志文件按照时间划分或者按照大小划分</strong></h3><p>如果将日志保存在一个文件中，那么时间一长，或者日志一多，单个日志文件就会很大，既不利于备份，也不利于查看。我们会想到能不能按照时间或者大小对日志文件进行划分呢？答案肯定是可以的，并且还很简单，logging 考虑到了我们这个需求。logging.handlers 文件中提供了 <strong>TimedRotatingFileHandler</strong> 和 <strong>RotatingFileHandler</strong> 类分别可以实现按时间和大小划分。打开这个 handles 文件，可以看到还有其他功能的 Handler 类，它们都继承自基类 <strong>BaseRotatingHandler</strong>。<br>*。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning Foundation——高等数学</title>
    <link href="/2022/08/01/MachineLearning/%E9%AB%98%E6%95%B0%E5%9F%BA%E7%A1%80/"/>
    <url>/2022/08/01/MachineLearning/%E9%AB%98%E6%95%B0%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="高等数学"><a href="#高等数学" class="headerlink" title="高等数学"></a>高等数学</h1><h4 id="1-导数定义："><a href="#1-导数定义：" class="headerlink" title="1.导数定义："></a>1.导数定义：</h4><p>导数和微分的概念</p><p>$$<br>f’({ {x}<em>{0} })&#x3D;\underset{\Delta x\to 0}{\mathop{\lim } },\frac{f({ {x}</em>{0} }+\Delta x)-f({ {x}_{0} })}{\Delta x} （1）<br>$$</p><p>或者：</p><p>$$<br>f’({ {x}<em>{0} })&#x3D;\underset{x\to { {x}</em>{0} } }{\mathop{\lim } },\frac{f(x)-f({ {x}<em>{0} })}{x-{ {x}</em>{0} } }（2）<br>$$</p><h4 id="2-左右导数导数的几何意义和物理意义"><a href="#2-左右导数导数的几何意义和物理意义" class="headerlink" title="2.左右导数导数的几何意义和物理意义"></a>2.左右导数导数的几何意义和物理意义</h4><p>函数$f(x)$在$x_0$处的左、右导数分别定义为：</p><p>左导数：<br>$$<br>{ { {f}’}<em>{-} }({ {x}</em>{0} })&#x3D;\underset{\Delta x\to { {0}^{-} } }{\mathop{\lim } },\frac{f({ {x}<em>{0} }+\Delta x)-f({ {x}</em>{0} })}{\Delta x}&#x3D;\underset{x\to x_{0}^{-} }{\mathop{\lim } },\frac{f(x)-f({ {x}<em>{0} })}{x-{ {x}</em>{0} } },(x&#x3D;{ {x}<em>{0} }+\Delta x)<br>$$<br>右导数：<br>$$<br>{ { {f}’}</em>{+} }({ {x}<em>{0} })&#x3D;\underset{\Delta x\to { {0}^{+} } }{\mathop{\lim } },\frac{f({ {x}</em>{0} }+\Delta x)-f({ {x}<em>{0} })}{\Delta x}&#x3D;\underset{x\to x</em>{0}^{+} }{\mathop{\lim } },\frac{f(x)-f({ {x}<em>{0} })}{x-{ {x}</em>{0} } }<br>$$</p><h4 id="3-函数的可导性与连续性之间的关系"><a href="#3-函数的可导性与连续性之间的关系" class="headerlink" title="3.函数的可导性与连续性之间的关系"></a>3.函数的可导性与连续性之间的关系</h4><p>Th1: 函数$f(x)$在$x_0$处可微$\Leftrightarrow f(x)$在$x_0$处可导</p><p>Th2: 若函数在点$x_0$处可导，则$y&#x3D;f(x)$在点$x_0$处连续，反之则不成立。即函数连续不一定可导。</p><p>Th3: ${f}’({ {x}<em>{0} })$存在$\Leftrightarrow { { {f}’}</em>{-} }({ {x}<em>{0} })&#x3D;{ { {f}’}</em>{+} }({ {x}_{0} })$</p><h4 id="4-平面曲线的切线和法线"><a href="#4-平面曲线的切线和法线" class="headerlink" title="4.平面曲线的切线和法线"></a>4.平面曲线的切线和法线</h4><p>切线方程 : $y-{ {y}<em>{0} }&#x3D;f’({ {x}</em>{0} })(x-{ {x}_{0} })$</p><p>法线方程：$y-{ {y}<em>{0} }&#x3D;-\frac{1}{f’({ {x}</em>{0} })}(x-{ {x}<em>{0} }),f’({ {x}</em>{0} })\ne 0$</p><h4 id="5-四则运算法则"><a href="#5-四则运算法则" class="headerlink" title="5.四则运算法则"></a>5.四则运算法则</h4><p>设函数$u&#x3D;u(x)，v&#x3D;v(x)$]在点$x$可导则</p><p>(1) $(u\pm v{)}’&#x3D;{u}’\pm {v}’$ $d(u\pm v)&#x3D;du\pm dv$</p><p>(2)$(uv{)}’&#x3D;u{v}’+v{u}’$ $d(uv)&#x3D;udv+vdu$</p><p>(3) $(\frac{u}{v}{)}’&#x3D;\frac{v{u}’-u{v}’}{ { {v}^{2} } }(v\ne 0)$ $d(\frac{u}{v})&#x3D;\frac{vdu-udv}{ { {v}^{2} } }$</p><h4 id="6-基本导数与微分表"><a href="#6-基本导数与微分表" class="headerlink" title="6.基本导数与微分表"></a>6.基本导数与微分表</h4><p>(1) $y&#x3D;c$（常数） ${y}’&#x3D;0$ $dy&#x3D;0$</p><p>(2) $y&#x3D;{ {x}^{\alpha } }$($\alpha $为实数) ${y}’&#x3D;\alpha { {x}^{\alpha -1} }$ $dy&#x3D;\alpha { {x}^{\alpha -1} }dx$</p><p>(3) $y&#x3D;{ {a}^{x} }$ ${y}’&#x3D;{ {a}^{x} }\ln a$ $dy&#x3D;{ {a}^{x} }\ln adx$<br>特例: $({ { {e} }^{x} }{)}’&#x3D;{ { {e} }^{x} }$ $d({ { {e} }^{x} })&#x3D;{ { {e} }^{x} }dx$</p><p>(4) $y&#x3D;{ {\log }_{a} }x$ ${y}’&#x3D;\frac{1}{x\ln a}$</p><p>$dy&#x3D;\frac{1}{x\ln a}dx$<br>特例:$y&#x3D;\ln x$ $(\ln x{)}’&#x3D;\frac{1}{x}$ $d(\ln x)&#x3D;\frac{1}{x}dx$</p><p>(5) $y&#x3D;\sin x$</p><p>${y}’&#x3D;\cos x$ $d(\sin x)&#x3D;\cos xdx$</p><p>(6) $y&#x3D;\cos x$</p><p>${y}’&#x3D;-\sin x$ $d(\cos x)&#x3D;-\sin xdx$</p><p>(7) $y&#x3D;\tan x$</p><p>${y}’&#x3D;\frac{1}{ { {\cos }^{2} }x}&#x3D;{ {\sec }^{2} }x$ $d(\tan x)&#x3D;{ {\sec }^{2} }xdx$</p><p>(8) $y&#x3D;\cot x$ ${y}’&#x3D;-\frac{1}{ { {\sin }^{2} }x}&#x3D;-{ {\csc }^{2} }x$ $d(\cot x)&#x3D;-{ {\csc }^{2} }xdx$</p><p>(9) $y&#x3D;\sec x$ ${y}’&#x3D;\sec x\tan x$</p><p>$d(\sec x)&#x3D;\sec x\tan xdx$<br>(10) $y&#x3D;\csc x$ ${y}’&#x3D;-\csc x\cot x$</p><p>$d(\csc x)&#x3D;-\csc x\cot xdx$<br>(11) $y&#x3D;\arcsin x$</p><p>${y}’&#x3D;\frac{1}{\sqrt{1-{ {x}^{2} } } }$</p><p>$d(\arcsin x)&#x3D;\frac{1}{\sqrt{1-{ {x}^{2} } } }dx$<br>(12) $y&#x3D;\arccos x$</p><p>${y}’&#x3D;-\frac{1}{\sqrt{1-{ {x}^{2} } } }$ $d(\arccos x)&#x3D;-\frac{1}{\sqrt{1-{ {x}^{2} } } }dx$</p><p>(13) $y&#x3D;\arctan x$</p><p>${y}’&#x3D;\frac{1}{1+{ {x}^{2} } }$ $d(\arctan x)&#x3D;\frac{1}{1+{ {x}^{2} } }dx$</p><p>(14) $y&#x3D;\operatorname{arc}\cot x$</p><p>${y}’&#x3D;-\frac{1}{1+{ {x}^{2} } }$</p><p>$d(\operatorname{arc}\cot x)&#x3D;-\frac{1}{1+{ {x}^{2} } }dx$<br>(15) $y&#x3D;shx$</p><p>${y}’&#x3D;chx$ $d(shx)&#x3D;chxdx$</p><p>(16) $y&#x3D;chx$</p><p>${y}’&#x3D;shx$ $d(chx)&#x3D;shxdx$</p><h4 id="7-复合函数，反函数，隐函数以及参数方程所确定的函数的微分法"><a href="#7-复合函数，反函数，隐函数以及参数方程所确定的函数的微分法" class="headerlink" title="7.复合函数，反函数，隐函数以及参数方程所确定的函数的微分法"></a>7.复合函数，反函数，隐函数以及参数方程所确定的函数的微分法</h4><p>(1) 反函数的运算法则: 设$y&#x3D;f(x)$在点$x$的某邻域内单调连续，在点$x$处可导且${f}’(x)\ne 0$，则其反函数在点$x$所对应的$y$处可导，并且有$\frac{dy}{dx}&#x3D;\frac{1}{\frac{dx}{dy} }$</p><p>(2) 复合函数的运算法则:若$\mu &#x3D;\varphi (x)$在点$x$可导,而$y&#x3D;f(\mu )$在对应点$\mu $($\mu &#x3D;\varphi (x)$)可导,则复合函数$y&#x3D;f(\varphi (x))$在点$x$可导,且${y}’&#x3D;{f}’(\mu )\cdot {\varphi }’(x)$</p><p>(3) 隐函数导数$\frac{dy}{dx}$的求法一般有三种方法：</p><p>1)方程两边对$x$求导，要记住$y$是$x$的函数，则$y$的函数是$x$的复合函数.例如$\frac{1}{y}$，${ {y}^{2} }$，$ln y$，${ { {e} }^{y} }$等均是$x$的复合函数.<br>对$x$求导应按复合函数连锁法则做.</p><p>2)公式法.由$F(x,y)&#x3D;0$知 $\frac{dy}{dx}&#x3D;-\frac{ { { { {F}’} }<em>{x} }(x,y)}{ { { { {F}’} }</em>{y} }(x,y)}$,其中，${ { {F}’}<em>{x} }(x,y)$，<br>${ { {F}’}</em>{y} }(x,y)$分别表示$F(x,y)$对$x$和$y$的偏导数</p><p>3)利用微分形式不变性</p><h4 id="8-常用高阶导数公式"><a href="#8-常用高阶导数公式" class="headerlink" title="8.常用高阶导数公式"></a>8.常用高阶导数公式</h4><p>（1）$({ {a}^{x} }){ {,}^{(n)} }&#x3D;{ {a}^{x} }{ {\ln }^{n} }a\quad (a&gt;{0})\quad \quad ({ { {e} }^{x} }){ {,}^{(n)} }&#x3D;{e}{ {,}^{x} }$</p><p>（2）$(\sin kx{)}{ {,}^{(n)} }&#x3D;{ {k}^{n} }\sin (kx+n\cdot \frac{\pi }{ {2} })$</p><p>（3）$(\cos kx{)}{ {,}^{(n)} }&#x3D;{ {k}^{n} }\cos (kx+n\cdot \frac{\pi }{ {2} })$</p><p>（4）$({ {x}^{m} }){ {,}^{(n)} }&#x3D;m(m-1)\cdots (m-n+1){ {x}^{m-n} }$</p><p>（5）$(\ln x){ {,}^{(n)} }&#x3D;{ {(-{1})}^{(n-{1})} }\frac{(n-{1})!}{ { {x}^{n} } }$</p><p>（6）莱布尼兹公式：若$u(x),,v(x)$均$n$阶可导，则<br>${ {(uv)}^{(n)} }&#x3D;\sum\limits_{i&#x3D;{0} }^{n}{c_{n}^{i}{ {u}^{(i)} }{ {v}^{(n-i)} } }$，其中${ {u}^{({0})} }&#x3D;u$，${ {v}^{({0})} }&#x3D;v$</p><h4 id="9-微分中值定理，泰勒公式"><a href="#9-微分中值定理，泰勒公式" class="headerlink" title="9.微分中值定理，泰勒公式"></a>9.微分中值定理，泰勒公式</h4><p><strong>Th1:(费马定理)</strong></p><p>若函数$f(x)$满足条件：</p><p>(1)函数$f(x)$在${ {x}<em>{0} }$的某邻域内有定义，并且在此邻域内恒有<br>$f(x)\le f({ {x}</em>{0} })$或$f(x)\ge f({ {x}_{0} })$,</p><p>(2) $f(x)$在${ {x}<em>{0} }$处可导,则有 ${f}’({ {x}</em>{0} })&#x3D;0$</p><p><strong>Th2:(罗尔定理)</strong></p><p>设函数$f(x)$满足条件：</p><p>(1)在闭区间$[a,b]$上连续；</p><p>(2)在$(a,b)$内可导；</p><p>(3)$f(a)&#x3D;f(b)$；</p><p>则在$(a,b)$内一存在个$\xi $，使 ${f}’(\xi )&#x3D;0$</p><p><strong>Th3: (拉格朗日中值定理)</strong></p><p>设函数$f(x)$满足条件：</p><p>(1)在$[a,b]$上连续；</p><p>(2)在$(a,b)$内可导；</p><p>则在$(a,b)$内一存在个$\xi $，使 $\frac{f(b)-f(a)}{b-a}&#x3D;{f}’(\xi )$</p><p><strong>Th4: (柯西中值定理)</strong></p><p>设函数$f(x)$，$g(x)$满足条件：<br>(1) 在$[a,b]$上连续；</p><p>(2) 在$(a,b)$内可导且${f}’(x)$，${g}’(x)$均存在，且${g}’(x)\ne 0$</p><p>则在$(a,b)$内存在一个$\xi $，使 $\frac{f(b)-f(a)}{g(b)-g(a)}&#x3D;\frac{ {f}’(\xi )}{ {g}’(\xi )}$</p><h4 id="10-洛必达法则"><a href="#10-洛必达法则" class="headerlink" title="10.洛必达法则"></a>10.洛必达法则</h4><p><strong>法则 Ⅰ ($\frac{0}{0}$型)</strong></p><p>设函数$f\left( x \right),g\left( x \right)$</p><p>满足条件：</p><p>$\underset{x\to { {x}<em>{0} } }{\mathop{\lim } },f\left( x \right)&#x3D;0,\underset{x\to { {x}</em>{0} } }{\mathop{\lim } },g\left( x \right)&#x3D;0$;</p><p>$f\left( x \right),g\left( x \right)$在${ {x}<em>{0} }$的邻域内可导，(在${ {x}</em>{0} }$处可除外)且${g}’\left( x \right)\ne 0$;</p><p>$\underset{x\to { {x}_{0} } }{\mathop{\lim } },\frac{ {f}’\left( x \right)}{ {g}’\left( x \right)}$存在(或$\infty $)。</p><p>则:<br>$\underset{x\to { {x}<em>{0} } }{\mathop{\lim } },\frac{f\left( x \right)}{g\left( x \right)}&#x3D;\underset{x\to { {x}</em>{0} } }{\mathop{\lim } },\frac{ {f}’\left( x \right)}{ {g}’\left( x \right)}$。<br>法则${ {I}’}$ ($\frac{0}{0}$型)</p><p>设函数$f\left( x \right),g\left( x \right)$</p><p>满足条件：</p><p>$\underset{x\to \infty }{\mathop{\lim } },f\left(x \right)&#x3D;0,\underset{x \to \infty}{\mathop{\lim} },g\left(x \right)&#x3D;0$;</p><p>存在一个$X&gt;0$,当$\left| x \right|&gt;X$时,$f\left( x \right),g\left( x \right)$可导,且${g}’\left( x \right)\ne 0$;$\underset{x\to { {x}_{0} } }{\mathop{\lim} },\frac{ {f}’\left(x \right)}{ {g}’\left(x \right)}$存在(或$\infty $)。</p><p>则:<br>$\underset{x\to { {x}<em>{0} } }{\mathop{\lim } },\frac{f\left( x \right)}{g\left( x \right)}&#x3D;\underset{x\to { {x}</em>{0} } }{\mathop{\lim } },\frac{ {f}’\left( x \right)}{ {g}’\left( x \right)}$</p><p><strong>法则 Ⅱ($\frac{\infty }{\infty }$型)</strong></p><p>设函数$f\left( x \right),g\left( x \right)$满足条件：<br>$\underset{x\to { {x}<em>{0} } }{\mathop{\lim } },f\left( x \right)&#x3D;\infty ,\underset{x\to { {x}</em>{0} } }{\mathop{\lim } },g\left( x \right)&#x3D;\infty$;</p><p>$f\left( x \right),g\left( x \right)$在${ {x}<em>{0} }$ 的邻域内可导(在${ {x}</em>{0} }$处可除外)且${g}’\left( x \right)\ne 0$;$\underset{x\to { {x}_{0} } }{\mathop{\lim } },\frac{ {f}’\left( x \right)}{ {g}’\left( x \right)}$存在(或$\infty$)。</p><p>则</p><p>$\underset{x\to { {x}<em>{0} } }{\mathop{\lim } },\frac{f\left( x \right)}{g\left( x \right)}&#x3D;\underset{x\to { {x}</em>{0} } }{\mathop{\lim } },\frac{ {f}’\left( x \right)}{ {g}’\left( x \right)}.$</p><p>同理法则${I{I}’}$($\frac{\infty }{\infty }$型)仿法则${ {I}’}$可写出。</p><h4 id="11-泰勒公式"><a href="#11-泰勒公式" class="headerlink" title="11.泰勒公式"></a>11.泰勒公式</h4><p>设函数$f(x)$在点${ {x}<em>{0} }$处的某邻域内具有$n+1$阶导数，则对该邻域内异于${ {x}</em>{0} }$的任意点$x$，在${ {x}_{0} }$与$x$之间至少存在<br>一个$\xi$，使得：</p><p>$f(x)&#x3D;f({ {x}<em>{0} })+{f}’({ {x}</em>{0} })(x-{ {x}<em>{0} })+\frac{1}{2!}{f}’’({ {x}</em>{0} }){ {(x-{ {x}_{0} })}^{2} }+\cdots$</p><p>$+\frac{ { {f}^{(n)} }({ {x}<em>{0} })}{n!}{ {(x-{ {x}</em>{0} })}^{n} }+{ {R}_{n} }(x)$</p><p>其中 ${ {R}<em>{n} }(x)&#x3D;\frac{ { {f}^{(n+1)} }(\xi )}{(n+1)!}{ {(x-{ {x}</em>{0} })}^{n+1} }$称为$f(x)$在点${ {x}_{0} }$处的$n$阶泰勒余项。</p><p>令${ {x}<em>{0} }&#x3D;0$，则$n$阶泰勒公式<br>$f(x)&#x3D;f(0)+{f}’(0)x+\frac{1}{2!}{f}’’(0){ {x}^{2} }+\cdots +\frac{ { {f}^{(n)} }(0)}{n!}{ {x}^{n} }+{ {R}</em>{n} }(x)$……(1)</p><p>其中 ${ {R}_{n} }(x)&#x3D;\frac{ { {f}^{(n+1)} }(\xi )}{(n+1)!}{ {x}^{n+1} }$，$\xi $在 0 与$x$之间.(1)式称为<strong>麦克劳林公式</strong></p><p>常用五种函数在${ {x}_{0} }&#x3D;0$处的泰勒公式</p><p>(1) ${ { {e} }^{x} }&#x3D;1+x+\frac{1}{2!}{ {x}^{2} }+\cdots +\frac{1}{n!}{ {x}^{n} }+\frac{ { {x}^{n+1} } }{(n+1)!}{ {e}^{\xi } }$</p><p>或 $&#x3D;1+x+\frac{1}{2!}{ {x}^{2} }+\cdots +\frac{1}{n!}{ {x}^{n} }+o({ {x}^{n} })$</p><p>(2) $\sin x&#x3D;x-\frac{1}{3!}{ {x}^{3} }+\cdots +\frac{ { {x}^{n} } }{n!}\sin \frac{n\pi }{2}+\frac{ { {x}^{n+1} } }{(n+1)!}\sin (\xi +\frac{n+1}{2}\pi )$</p><p>或 $&#x3D;x-\frac{1}{3!}{ {x}^{3} }+\cdots +\frac{ { {x}^{n} } }{n!}\sin \frac{n\pi }{2}+o({ {x}^{n} })$</p><p>(3) $\cos x&#x3D;1-\frac{1}{2!}{ {x}^{2} }+\cdots +\frac{ { {x}^{n} } }{n!}\cos \frac{n\pi }{2}+\frac{ { {x}^{n+1} } }{(n+1)!}\cos (\xi +\frac{n+1}{2}\pi )$</p><p>或 $&#x3D;1-\frac{1}{2!}{ {x}^{2} }+\cdots +\frac{ { {x}^{n} } }{n!}\cos \frac{n\pi }{2}+o({ {x}^{n} })$</p><p>(4) $\ln (1+x)&#x3D;x-\frac{1}{2}{ {x}^{2} }+\frac{1}{3}{ {x}^{3} }-\cdots +{ {(-1)}^{n-1} }\frac{ { {x}^{n} } }{n}+\frac{ { {(-1)}^{n} }{ {x}^{n+1} } }{(n+1){ {(1+\xi )}^{n+1} } }$</p><p>或 $&#x3D;x-\frac{1}{2}{ {x}^{2} }+\frac{1}{3}{ {x}^{3} }-\cdots +{ {(-1)}^{n-1} }\frac{ { {x}^{n} } }{n}+o({ {x}^{n} })$</p><p>(5) ${ {(1+x)}^{m} }&#x3D;1+mx+\frac{m(m-1)}{2!}{ {x}^{2} }+\cdots +\frac{m(m-1)\cdots (m-n+1)}{n!}{ {x}^{n} }$<br>$+\frac{m(m-1)\cdots (m-n+1)}{(n+1)!}{ {x}^{n+1} }{ {(1+\xi )}^{m-n-1} }$</p><p>或 ${ {(1+x)}^{m} }&#x3D;1+mx+\frac{m(m-1)}{2!}{ {x}^{2} }+\cdots$ $+\frac{m(m-1)\cdots(m-n+1)}{n!}{ {x}^{n} }+o({ {x}^{n} })$</p><h4 id="12-函数单调性的判断"><a href="#12-函数单调性的判断" class="headerlink" title="12.函数单调性的判断"></a>12.函数单调性的判断</h4><p><strong>Th1:</strong> </p><p>设函数$f(x)$在$(a,b)$区间内可导，如果对$\forall x\in (a,b)$，都有$f,‘(x)&gt;0$（或$f,‘(x)&lt;0$），则函数$f(x)$在$(a,b)$内是单调增加的（或单调减少）</p><p><strong>Th2:</strong> </p><p>（取极值的必要条件）设函数$f(x)$在${ {x}<em>{0} }$处可导，且在${ {x}</em>{0} }$处取极值，则$f,‘({ {x}_{0} })&#x3D;0$。</p><p><strong>Th3:</strong> </p><p>（取极值的第一充分条件）设函数$f(x)$在${ {x}<em>{0} }$的某一邻域内可微，且$f,‘({ {x}</em>{0} })&#x3D;0$（或$f(x)$在${ {x}<em>{0} }$处连续，但$f,‘({ {x}</em>{0} })$不存在。）</p><p>(1)若当$x$经过${ {x}<em>{0} }$时，$f,‘(x)$由“+”变“-”，则$f({ {x}</em>{0} })$为极大值；</p><p>(2)若当$x$经过${ {x}<em>{0} }$时，$f,‘(x)$由“-”变“+”，则$f({ {x}</em>{0} })$为极小值；</p><p>(3)若$f,‘(x)$经过$x&#x3D;{ {x}<em>{0} }$的两侧不变号，则$f({ {x}</em>{0} })$不是极值。</p><p><strong>Th4:</strong> </p><p>(取极值的第二充分条件)设$f(x)$在点${ {x}<em>{0} }$处有$f’’(x)\ne 0$，且$f,‘({ {x}</em>{0} })&#x3D;0$，则 当$f’,‘({ {x}<em>{0} })&lt;0$时，$f({ {x}_{0} })$为极大值；<br>当$f’,‘({ {x}_{0} })&gt;0$时，$f({ {x}</em>{0} })$为极小值。<br>注：如果$f’,‘({ {x}_{0} })&lt;0$，此方法失效。</p><h4 id="13-渐近线的求法"><a href="#13-渐近线的求法" class="headerlink" title="13.渐近线的求法"></a>13.渐近线的求法</h4><p>(1)水平渐近线 若$\underset{x\to +\infty }{\mathop{\lim } },f(x)&#x3D;b$，或$\underset{x\to -\infty }{\mathop{\lim } },f(x)&#x3D;b$，则</p><p>$y&#x3D;b$称为函数$y&#x3D;f(x)$的水平渐近线。</p><p>(2)铅直渐近线 若$\underset{x\to x_{0}^{-} }{\mathop{\lim } },f(x)&#x3D;\infty $，或$\underset{x\to x_{0}^{+} }{\mathop{\lim } },f(x)&#x3D;\infty $，则</p><p>$x&#x3D;{ {x}_{0} }$称为$y&#x3D;f(x)$的铅直渐近线。</p><p>(3)斜渐近线 若$a&#x3D;\underset{x\to \infty }{\mathop{\lim } },\frac{f(x)}{x},\quad b&#x3D;\underset{x\to \infty }{\mathop{\lim } },[f(x)-ax]$，则<br>$y&#x3D;ax+b$称为$y&#x3D;f(x)$的斜渐近线。</p><h4 id="14-函数凹凸性的判断"><a href="#14-函数凹凸性的判断" class="headerlink" title="14.函数凹凸性的判断"></a>14.函数凹凸性的判断</h4><p><strong>Th1:</strong> (凹凸性的判别定理）若在 I 上$f’’(x)&lt;0$（或$f’’(x)&gt;0$），则$f(x)$在 I 上是凸的（或凹的）。</p><p><strong>Th2:</strong> (拐点的判别定理 1)若在${ {x}<em>{0} }$处$f’’(x)&#x3D;0$，（或$f’’(x)$不存在），当$x$变动经过${ {x}</em>{0} }$时，$f’’(x)$变号，则$({ {x}<em>{0} },f({ {x}</em>{0} }))$为拐点。</p><p><strong>Th3:</strong> (拐点的判别定理 2)设$f(x)$在${ {x}<em>{0} }$点的某邻域内有三阶导数，且$f’’(x)&#x3D;0$，$f’’’(x)\ne 0$，则$({ {x}</em>{0} },f({ {x}_{0} }))$为拐点。</p><h4 id="15-弧微分"><a href="#15-弧微分" class="headerlink" title="15.弧微分"></a>15.弧微分</h4><p>$dS&#x3D;\sqrt{1+y{ {‘}^{2} } }dx$</p><h4 id="16-曲率"><a href="#16-曲率" class="headerlink" title="16.曲率"></a>16.曲率</h4><p>曲线$y&#x3D;f(x)$在点$(x,y)$处的曲率$k&#x3D;\frac{\left| y’’ \right|}{ { {(1+y{ {‘}^{2} })}^{\tfrac{3}{2} } } }$。<br>对于参数方程$\left{ \begin{align}  &amp; x&#x3D;\varphi (t) \  &amp; y&#x3D;\psi (t) \ \end{align} \right.,$$k&#x3D;\frac{\left| \varphi ‘(t)\psi ‘’(t)-\varphi ‘’(t)\psi ‘(t) \right|}{ { {[\varphi { {‘}^{2} }(t)+\psi { {‘}^{2} }(t)]}^{\tfrac{3}{2} } } }$。</p><h4 id="17-曲率半径"><a href="#17-曲率半径" class="headerlink" title="17.曲率半径"></a>17.曲率半径</h4><p>曲线在点$M$处的曲率$k(k\ne 0)$与曲线在点$M$处的曲率半径$\rho $有如下关系：$\rho &#x3D;\frac{1}{k}$。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Mathematics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning Foundation——统计学习与机器学习的关联</title>
    <link href="/2022/08/01/MachineLearning/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%B3%E8%81%94/"/>
    <url>/2022/08/01/MachineLearning/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%B3%E8%81%94/</url>
    
    <content type="html"><![CDATA[<p>统计学和机器学习之间的界定一直很模糊。</p><p>无论是业界还是学界一直认为机器学习只是统计学披了一层光鲜的外衣。</p><p>而机器学习支撑的人工智能也被称为“统计学的外延”。</p><p>例如，诺奖得主托马斯·萨金特曾经说过人工智能其实就是统计学，只不过用了一个很华丽的辞藻。</p><p><img src="/src/640-165778097574451.jpeg" alt="图片"></p><p>萨金特在世界科技创新论坛上表示，人工智能其实就是统计学</p><p>当然也有一些不同的声音。但是这一观点的正反双方在争吵中充斥着一堆看似高深实则含糊的论述，着实让人摸不着头脑。</p><p>一位名叫Matthew Stewart的哈佛大学博士生从统计与机器学习的不同；统计模型与机器学习的不同，这两个角度论证了机器学习和统计学并不是互为代名词。</p><h2 id="机器学习和统计的主要区别在于它们的目的"><a href="#机器学习和统计的主要区别在于它们的目的" class="headerlink" title="机器学习和统计的主要区别在于它们的目的"></a><strong>机器学习和统计的主要区别在于它们的目的</strong></h2><p><img src="/src/640-165778097574452.jpeg" alt="图片"></p><p>与大部分人所想的正相反，机器学习其实已经存在几十年了。当初只是因为那时的计算能力无法满足它对大量计算的需求，而渐渐被人遗弃。然而，近年来，由于信息爆炸所带来的数据和算力优势，机器学习正快速复苏。</p><p>言归正传，如果说机器学习和统计学是互为代名词，那为什么我们没有看到每所大学的统计学系都关门大吉而转投’机器学习’系呢？因为它们是不一样的!</p><p>我经常听到一些关于这个话题的含糊论述，最常见的是这样的说法:</p><p>“机器学习和统计的主要区别在于它们的目的。机器学习模型旨在使最准确的预测成为可能。统计模型是为推断变量之间的关系而设计的。</p><p>虽然技术上来说这是正确的，但这样的论述并没有给出特别清晰和令人满意的答案。机器学习和统计之间的一个主要区别确实是它们的目的。</p><p>然而，说机器学习是关于准确的预测，而统计模型是为推理而设计，几乎是毫无意义的说法，除非你真的精通这些概念。</p><p><img src="/src/640-165778097574553.jpeg" alt="图片"></p><p>首先，我们必须明白，统计和统计建模是不一样的。统计是对数据的数学研究。除非有数据，否则无法进行统计。统计模型是数据的模型，主要用于推断数据中不同内容的关系，或创建能够预测未来值的模型。通常情况下，这两者是相辅相成的。</p><p>因此，实际上我们需要从两方面来论述：第一，统计与机器学习有何不同；第二，统计模型与机器学习有何不同？</p><p>说得更直白些就是，有很多统计模型可以做出预测，但预测效果比较差强人意。</p><p>而机器学习通常会牺牲可解释性以获得强大的预测能力。例如，从线性回归到神经网络，尽管解释性变差，但是预测能力却大幅提高。</p><p>从宏观角度来看，这是一个很好的答案。至少对大多数人来说已经足够好。然而，在有些情况下，这种说法容易让我们对机器学习和统计建模之间的差异产生误解。让我们看一下线性回归的例子。</p><h2 id="统计模型与机器学习在线性回归上的差异"><a href="#统计模型与机器学习在线性回归上的差异" class="headerlink" title="统计模型与机器学习在线性回归上的差异"></a><strong>统计模型与机器学习在线性回归上的差异</strong></h2><p><img src="/src/640-165778097574554.png" alt="图片"></p><p>或许是因为统计建模和机器学习中使用方法的相似性，使人们认为它们是同一个东西。对于这个我可以理解，但事实上不是这样。</p><p>最明显的例子是线性回归，这可能是造成这种误解的主要原因。线性回归是一种统计方法，通过这种方法我们既可以训练一个线性回归器，又可以通过最小二乘法拟合一个统计回归模型。</p><p>可以看到，在这个案例中，前者做的事儿叫”训练”模型，它只用到了数据的一个子集，而训练得到的模型究竟表现如何需要通过数据的另一个子集测试集测试之后才能知道。在这个例子中，机器学习的最终目的是在测试集上获得最佳性能。</p><p>对于后者，我们则事先假设数据是一个具有高斯噪声的线性回归量，然后试图找到一条线，最大限度地减少了所有数据的均方误差。不需要训练或测试集，在许多情况下，特别是在研究中(如下面的传感器示例)，建模的目的是描述数据与输出变量之间的关系, 而不是对未来数据进行预测。我们称此过程为统计推断，而不是预测。尽管我们可以使用此模型进行预测，这也可能是你所想的，但评估模型的方法不再是测试集，而是评估模型参数的显著性和健壮性。</p><p>机器学习（这里特指有监督学习）的目的是获得一个可反复预测的模型。我们通常不关心模型是否可以解释。机器学习只在乎结果。就好比对公司而言，你的价值只用你的表现来衡量。而统计建模更多的是为了寻找变量之间的关系和确定关系的显著性，恰巧迎合了预测。</p><p>下面我举一个自己的例子，来说明两者的区别。我是一名环境科学家。工作的主要内容是和传感器数据打交道。如果我试图证明传感器能够对某种刺激(如气体浓度)做出反应, 那么我将使用统计模型来确定信号响应是否具有统计显著性。我会尝试理解这种关系，并测试其可重复性，以便能够准确地描述传感器的响应，并根据这些数据做出推断。我还可能测试，响应是否是线性的？响应是否归因于气体浓度而不是传感器中的随机噪声？等等。</p><p>而同时，我也可以拿着从20个不同传感器得到的数据, 去尝试预测一个可由他们表征的传感器的响应。如果你对传感器了解不多，这可能会显得有些奇怪，但目前这确实是环境科学的一个重要研究领域。</p><p>用一个包含20个不同变量的模型来表征传感器的输出显然是一种预测，而且我也没期待模型是可解释的。要知道，由于化学动力学产生的非线性以及物理变量与气体浓度之间的关系等等因素，可能会使这个模型非常深奥，就像神经网络那样难以解释。尽管我希望这个模型能让人看懂, 但其实只要它能做出准确的预测，我就相当高兴了。</p><p>如果我试图证明数据变量之间的关系在某种程度上具有统计显著性，以便我可以在科学论文中发表，我将使用统计模型而不是机器学习。这是因为我更关心变量之间的关系，而不是做出预测。做出预测可能仍然很重要，但是大多数机器学习算法缺乏可解释性，这使得很难证明数据中存在的关系。</p><p><img src="/src/640-165778097574555.jpeg" alt="图片"></p><p>很明显，这两种方法在目标上是不同的，尽管使用了相似的方法来达到目标。机器学习算法的评估使用测试集来验证其准确性。然而，对于统计模型，通过置信区间、显著性检验和其他检验对回归参数进行分析，可以用来评估模型的合法性。因为这些方法产生相同的结果，所以很容易理解为什么人们会假设它们是相同的。</p><h2 id="统计与机器学习在线性回归上的差异"><a href="#统计与机器学习在线性回归上的差异" class="headerlink" title="统计与机器学习在线性回归上的差异"></a><strong>统计与机器学习在线性回归上的差异</strong></h2><p>有一个误解存在了10年：仅基于它们都利用相同的基本概率概念这一事实，来混淆这两个术语是不合理的。</p><p><img src="/src/640-165778097574556.jpeg" alt="图片"></p><p>然而，仅仅基于这两个术语都利用了概率里相同的基本概念这一事实而将他们混为一谈是不合理的。就好比，如果我们仅仅把机器学习当作披了一层光鲜外衣的统计，我们也可以这样说：</p><ul><li>物理只是数学的一种更好听的说法。</li><li>动物学只是邮票收藏的一种更好听的说法。</li><li>建筑学只是沙堡建筑的一种更好听的说法。</li></ul><p>这些说法（尤其是最后一个）非常荒谬，完全混淆了两个类似想法的术语。</p><p>实际上，物理是建立在数学基础上的，理解现实中的物理现象是数学的应用。物理学还包括统计学的各个方面，而现代统计学通常是建立在Zermelo-Frankel集合论与测量理论相结合的框架中，以产生概率空间。它们有很多共同点，因为它们来自相似的起源，并运用相似的思想得出一个逻辑结论。同样，建筑学和沙堡建筑可能有很多共同点，但即使我不是一个建筑师，也不能给出一个清晰的解释，但也看得出它们显然不一样。</p><p>在我们进一步讨论之前，需要简要澄清另外两个与机器学习和统计有关的常见误解。这就是人工智能不同于机器学习，数据科学不同于统计学。这些都是没有争议的问题，所以很快就能说清楚。</p><p>数据科学本质上是应用于数据的计算和统计方法，包括小数据集或大数据集。它也包括诸如探索性数据分析之类的东西，例如对数据进行检查和可视化，以帮助科学家更好地理解数据，并从中做出推论。数据科学还包括诸如数据包装和预处理之类的东西，因此涉及到一定程度的计算机科学，因为它涉及编码和建立数据库、Web服务器之间的连接和流水线等等。</p><p>要进行统计，你并不一定得依靠电脑，但如果是数据科学缺了电脑就没法操作了。这就再次说明了虽然数据科学借助统计学，这两者不是一个概念。</p><p>同理，机器学习也并非人工智能；事实上，机器学习是人工智能的一个分支。这一点挺明显的，因为我们基于以往的数据“教”（训练）机器对特定类型的数据进行概括性的预测。</p><h2 id="机器学习是基于统计学"><a href="#机器学习是基于统计学" class="headerlink" title="机器学习是基于统计学"></a><strong>机器学习是基于统计学</strong></h2><p>在我们讨论统计学和机器学习之间的区别前，我们先来说说其相似性，其实文章的前半段已经对此有过一些探讨了。</p><p>机器学习基于统计的框架，因为机器学习涉及数据，而数据必须基于统计学框架来进行描述，所以这点十分明显。然而，扩展至针对大量粒子的热力学的统计机制，同样也建立在统计学框架之下。</p><p>压力的概念其实是数据，温度也是一种数据。你可能觉得这听起来不合理，但这是真的。这就是为什么你不能描述一个分子的温度或压力，这不合理。温度是分子相撞产生的平均能量的显示。而例如房屋或室外这种拥有大量分子的，我们能用温度来描述也就合理了。</p><p>你会认为热力学和统计学是一个东西吗？当然不会，热力学借助统计学来帮助我们理解运动的相互作用以及转移现象中产生的热。</p><p>事实上，热力学基于多种学科而非仅仅统计学。类似地，机器学习基于许多其他领域的内容，比如数学和计算机科学。举例来说：</p><ul><li>机器学习的理论来源于数学和统计学；</li><li>机器学习算法基于优化理论、矩阵代数和微积分；</li><li>机器学习的实现来源于计算机科学和工程学概念，比如核映射、特征散列等。</li></ul><p>当一个人开始用Python开始编程，突然从Sklearn程序库里找出并使用这些算法，许多上述的概念都比较抽象，因此很难看出其中的区别。这样的情况下，这种抽象定义也就致使了对机器学习真正包含的内容一定程度上的无知。</p><h2 id="统计学习理论——机器学习的统计学基础"><a href="#统计学习理论——机器学习的统计学基础" class="headerlink" title="统计学习理论——机器学习的统计学基础"></a><strong>统计学习理论——机器学习的统计学基础</strong></h2><p>统计学和机器学习之间最主要的区别在于统计学完全基于概率空间。你可以从集合论中推导出全部的统计学内容，集合论讨论了我们如何将数据归类（这些类被称为“集”），然后对这个集进行某种测量保证其总和为1.我们将这种方法成为概率空间。</p><p>统计学除了对这些集合和测量有所定义之外没有其他假设。这就是为什么我们对概率空间的定义非常严谨的原因。一个概率空间，其数学符号写作(Ω,F,P)，包含三部分：</p><ul><li>一个样本空间，Ω，也就是所有可能结果的集合。</li><li>一个事件集合，F，每个事件都包含0或者其它值。</li><li>对每个事件发生的可能性赋予概率，P，这是一个从事件到概率的函数。</li></ul><p>机器学习基于统计学习理论，统计学习理论也依旧基于对概率空间的公理化语言。这个理论基于传统的统计学理论，并发展于19世纪60年代。</p><p>机器学习分为多个类别，这篇文章我仅着眼于监督学习理论，因为它最容易解释（虽然因其充斥数学概念依然显得晦涩难懂）。</p><p>统计学习理论中的监督学习，给了我们一个数据集，我们将其标为S&#x3D; {(xᵢ,yᵢ)}，也就是说我们有一个包含N个数据点的数据集，每个数据点由被称为“特征”的其它值描述，这些特征用x描述，这些特征通过特定函数来描绘以返回我们想要的y值。</p><p>已知这个数据集，问如何找到将x值映射到y值的函数。我们将所有可能的描述映射过程的函数集合称为假设空间。</p><p>为了找到这个函数，我们需要给算法一些方法来“学习”如何最好地着手处理这个问题，而这由一个被称为“损失函数”的概念来提供。因此，对我们所有的每个假设（也即提议的函数），我们要通过比较所有数据下其预期风险的值来衡量这个函数的表现。</p><p>预期风险本质上就是损失函数之和乘以数据的概率分布。如果我们知道这个映射的联合概率分布，找到最优函数就很简单了。但是这个联合概率分布通常是未知的，因此我们最好的方式就是猜测一个最优函数，再实证验证损失函数是否得到优化。我们将这种称为实证风险。</p><p>之后，我们就可以比较不同函数，找出最小预期风险的那个假设，也就是所有函数中得出最小下确界值的那个假设。</p><p>然而，为了最小化损失函数，算法有通过过度拟合来作弊的倾向。这也是为什么要通过训练集“学习”函数，之后在训练集之外的数据集，测试集里对函数进行验证。</p><p>我们如何定义机器学习的本质引出了过度拟合的问题，也对需要区分训练集和测试集作出了解释。而我们在统计学中无需试图最小化实证风险，过度拟合不是统计学的固有特征。最小化统计学中无需视图程向于一个从函数中选取最小化实证风险的学习算法被称为实证风险最小化。</p><h2 id="例证"><a href="#例证" class="headerlink" title="例证"></a><strong>例证</strong></h2><p>以线性回归做一个简单例子。在传统概念中，我们试图最小化数据中的误差找到能够描述数据的函数，这种情况下，我们通常使用均值方差。使用平方数是为了不让正值和负值互相抵消。然后我们可以使用闭合表达式来求出回归系数。</p><p>如果我们将损失函数计为均值方差，并基于统计学习理论进行最小化实证风险，碰巧就能得到传统线性回归分析同样的结果。</p><p>这个巧合是因为两个情况是相同的，对同样的数据以相同的方式求解最大概率自然会得出相同的结果。最大化概率有不同的方法来实现同样的目标，但没人会去争论说最大化概率与线性回归是一个东西。这个最简单的例子显然没能区分开这些方法。</p><p>这里要指出的第二点在于，传统的统计方法中没有训练集和测试集的概念，但我们会使用不同的指标来帮助验证模型。验证过程虽然不同，但两种方法都能够给我们统计稳健的结果。</p><p>另外要指出的一点在于，传统统计方法给了我们一个闭合形式下的最优解，它没有对其它可能的函数进行测试来收敛出一个结果。相对的，机器学习方法尝试了一批不同的模型，最后结合回归算法的结果，收敛出一个最终的假设。</p><p>如果我们用一个不同的损失函数，结果可能并不收敛。例如，如果我们用了铰链损失（使用标准梯度下降时不太好区分，因此需要使用类似近梯度下降等其它方法），那么结果就不会相同了。</p><p>最后可以对模型偏差进行区分。你可以用机器学习算法来测试线性模型以及多项式模型，指数模型等，来检验这些假设是否相对我们的先验损失函数对数据集给出更好的拟合度。在传统统计学概念中，我们选择一个模型，评估其准确性，但无法自动从100个不同的模型中摘出最优的那个。显然，由于最开始选择的算法不同，找出的模型总会存在一些偏误。选择算法是非常必要的，因为为数据集找出最优的方程是一个NP-hard问题。</p><h2 id="那么哪个方法更优呢？"><a href="#那么哪个方法更优呢？" class="headerlink" title="那么哪个方法更优呢？"></a><strong>那么哪个方法更优呢？</strong></h2><p>这个问题其实很蠢。没有统计学，机器学习根本没法存在，但由于当代信息爆炸，人类能接触到的大量数据，机器学习是非常有用的。</p><p>对比机器学习和统计模型还要更难一些，你需要视乎你的目标而定究竟选择哪种。如果你只是想要创建一个高度准确的预测房价的算法，或者从数据中找出哪类人更容易得某种疾病，机器学习可能是更好的选择。如果你希望找出变量之间的关系或从数据中得出推论，选择统计模型会更好。</p><p><img src="/src/640-165778097574557.jpeg" alt="图片"></p><blockquote><p>图中文字：</p><p>这是你的机器学习系统？</p><p>对的，你从这头把数据都倒进这一大堆或者线性代数里，然后从那头里拿答案就好了。</p><p>答案错了咋整？</p><p>那就搅搅，搅到看起来对了为止。</p></blockquote><p>如果你统计学基础不够扎实，你依然可以学习机器学习并使用它——机器学习程序库里的抽象概念能够让你以业余者的身份来轻松使用它们，但你还是得对统计概念有所了解，从而避免模型过度拟合或得出些貌似合理的推论。��的推论。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning Foundation——线性代数</title>
    <link href="/2022/08/01/MachineLearning/%E7%BA%BF%E4%BB%A3%E5%9F%BA%E7%A1%80/"/>
    <url>/2022/08/01/MachineLearning/%E7%BA%BF%E4%BB%A3%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><h2 id="1-基本符号"><a href="#1-基本符号" class="headerlink" title="1 基本符号"></a>1 基本符号</h2><ul><li><p>$A \in \mathbb{R}^{m \times n}$，表示 $A$ 为由实数组成具有$m$行和$n$列的矩阵。</p></li><li><p>$x \in \mathbb{R}^{ n}$，表示具有$n$个元素的向量。 通常，向量$x$将表示列向量: 即，具有$n$行和$1$列的矩阵。 如果我们想要明确地表示行向量: 具有 $1$ 行和$n$列的矩阵 - 我们通常写$x^T$（这里$x^T$x$的转置）。</p></li><li><p>$x_i$表示向量$x$的第$i$个元素</p></li></ul><p>$$<br>x&#x3D;\left[\begin{array}{c}{x_{1} } \ {x_{2} } \ {\vdots} \ {x_{n} }\end{array}\right]<br>$$</p><ul><li>我们使用符号 $a_{ij}$（或$A_{ij}$,$A_{i,j}$等）来表示第 $i$ 行和第$j$列中的 $A$ 的元素：</li></ul><p>$$<br>A&#x3D;\left[\begin{array}{cccc}{a_{11} } &amp; {a_{12} } &amp; {\cdots} &amp; {a_{1 n} } \ {a_{21} } &amp; {a_{22} } &amp; {\cdots} &amp; {a_{2 n} } \ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \ {a_{m 1} } &amp; {a_{m 2} } &amp; {\cdots} &amp; {a_{m n} }\end{array}\right]<br>$$</p><ul><li>我们用$a^j$或者$A_{:,j}$表示矩阵$A$的第$j$列：</li></ul><p>$$<br>A&#x3D;\left[\begin{array}{llll}{ |} &amp; { |} &amp; {} &amp; { |} \ {a^{1} } &amp; {a^{2} } &amp; {\cdots} &amp; {a^{n} } \ { |} &amp; { |} &amp; {} &amp; { |}\end{array}\right]<br>$$</p><ul><li><p>我们用$a^T_i$或者$A_{i,:}$表示矩阵$A$的第$i$行：</p><p>$$<br>A&#x3D;\left[\begin{array}{c}{-a_{1}^{T}-} \ {-a_{2}^{T}-} \ {\vdots} \ {-a_{m}^{T}-}\end{array}\right]<br>$$</p></li></ul><h2 id="2-行列式"><a href="#2-行列式" class="headerlink" title="2 行列式"></a>2 行列式</h2><p><strong>行列式按行（列）展开定理</strong></p><p>(1) 设$A &#x3D; ( a_{ {ij} } )<em>{n \times n}$，则：$a</em>{i1}A_{j1} +a_{i2}A_{j2} + \cdots + a_{ {in} }A_{ {jn} } &#x3D; \begin{cases}|A|,i&#x3D;j\ 0,i \neq j\end{cases}$</p><p>或$a_{1i}A_{1j} + a_{2i}A_{2j} + \cdots + a_{ {ni} }A_{ {nj} } &#x3D; \begin{cases}|A|,i&#x3D;j\ 0,i \neq j\end{cases}$即 $AA^{<em>} &#x3D; A^{</em>}A &#x3D; \left| A \right|E,$其中：$A^{*} &#x3D; \begin{pmatrix} A_{11} &amp; A_{12} &amp; \ldots &amp; A_{1n} \ A_{21} &amp; A_{22} &amp; \ldots &amp; A_{2n} \ \ldots &amp; \ldots &amp; \ldots &amp; \ldots \ A_{n1} &amp; A_{n2} &amp; \ldots &amp; A_{ {nn} } \ \end{pmatrix} &#x3D; (A_{ {ji} }) &#x3D; {(A_{ {ij} })}^{T}$</p><p>$D_{n} &#x3D; \begin{vmatrix} 1 &amp; 1 &amp; \ldots &amp; 1 \ x_{1} &amp; x_{2} &amp; \ldots &amp; x_{n} \ \ldots &amp; \ldots &amp; \ldots &amp; \ldots \ x_{1}^{n - 1} &amp; x_{2}^{n - 1} &amp; \ldots &amp; x_{n}^{n - 1} \ \end{vmatrix} &#x3D; \prod_{1 \leq j &lt; i \leq n}^{},(x_{i} - x_{j})$</p><p>(2) 设$A,B$为$n$阶方阵，则$\left| {AB} \right| &#x3D; \left| A \right|\left| B \right| &#x3D; \left| B \right|\left| A \right| &#x3D; \left| {BA} \right|$，但$\left| A \pm B \right| &#x3D; \left| A \right| \pm \left| B \right|$不一定成立。</p><p>(3) $\left| {kA} \right| &#x3D; k^{n}\left| A \right|$,$A$为$n$阶方阵。</p><p>(4) 设$A$为$n$阶方阵，$|A^{T}| &#x3D; |A|;|A^{- 1}| &#x3D; |A|^{- 1}$（若$A$可逆），$|A^{*}| &#x3D; |A|^{n - 1}$</p><p>$n \geq 2$</p><p>(5) $\left| \begin{matrix}  &amp; {A\quad O} \  &amp; {O\quad B} \ \end{matrix} \right| &#x3D; \left| \begin{matrix}  &amp; {A\quad C} \  &amp; {O\quad B} \ \end{matrix} \right| &#x3D; \left| \begin{matrix}  &amp; {A\quad O} \  &amp; {C\quad B} \ \end{matrix} \right| &#x3D;| A||B|$<br>，$A,B$为方阵，但$\left| \begin{matrix} {O} &amp; A_{m \times m} \  B_{n \times n} &amp; { O} \ \end{matrix} \right| &#x3D; ({- 1)}^{ {mn} }|A||B|$ 。</p><p>(6) 范德蒙行列式$D_{n} &#x3D; \begin{vmatrix} 1 &amp; 1 &amp; \ldots &amp; 1 \ x_{1} &amp; x_{2} &amp; \ldots &amp; x_{n} \ \ldots &amp; \ldots &amp; \ldots &amp; \ldots \ x_{1}^{n - 1} &amp; x_{2}^{n 1} &amp; \ldots &amp; x_{n}^{n - 1} \ \end{vmatrix} &#x3D;  \prod_{1 \leq j &lt; i \leq n}^{},(x_{i} - x_{j})$</p><p>设$A$是$n$阶方阵，$\lambda_{i}(i &#x3D; 1,2\cdots,n)$是$A$的$n$个特征值，则<br>$|A| &#x3D; \prod_{i &#x3D; 1}^{n}\lambda_{i}$</p><h2 id="3-矩阵"><a href="#3-矩阵" class="headerlink" title="3 矩阵"></a>3 矩阵</h2><p>矩阵：$m \times n$个数$a_{ {ij} }$排成$m$行$n$列的表格$\begin{bmatrix}  a_{11}\quad a_{12}\quad\cdots\quad a_{1n} \ a_{21}\quad a_{22}\quad\cdots\quad a_{2n} \ \quad\cdots\cdots\cdots\cdots\cdots \  a_{m1}\quad a_{m2}\quad\cdots\quad a_{ {mn} } \ \end{bmatrix}$ 称为矩阵，简记为$A$，或者$\left( a_{ {ij} } \right)_{m \times n}$ 。若$m &#x3D; n$，则称$A$是$n$阶矩阵或$n$阶方阵。</p><h3 id="3-1-矩阵的属性"><a href="#3-1-矩阵的属性" class="headerlink" title="3.1 矩阵的属性"></a><strong>3.1 矩阵的属性</strong></h3><h4 id="3-1-1-单位矩阵和对角矩阵"><a href="#3-1-1-单位矩阵和对角矩阵" class="headerlink" title="3.1.1 单位矩阵和对角矩阵"></a>3.1.1 单位矩阵和对角矩阵</h4><p><strong>单位矩阵</strong>,$I \in \mathbb{R}^{n \times n} $，它是一个方阵，对角线的元素是1，其余元素都是0：<br>$<br>I_{i j}&#x3D;\left{\begin{array}{ll}{1} &amp; {i&#x3D;j} \ {0} &amp; {i \neq j}\end{array}\right.<br>$对于所有$A \in \mathbb{R}^ {m \times n}$，有：<br>$I &#x3D; A &#x3D; IA<br>$，在某种意义上，单位矩阵的表示法是不明确的，因为它没有指定$I$的维数。通常，$I$的维数是从上下文推断出来的，以便使矩阵乘法成为可能。 例如，在上面的等式中，$AI &#x3D; A$中的I是$n\times n$矩阵，而$A &#x3D; IA$中的$I$是$m\times m$矩阵。</p><p>对角矩阵是一种这样的矩阵：对角线之外的元素全为0。对角阵通常表示为：$D&#x3D; diag(d_1, d_2, . . . , d_n)$，其中：<br>$<br>D_{i j}&#x3D;\left{\begin{array}{ll}{d_{i} } &amp; {i&#x3D;j} \ {0} &amp; {i \neq j}\end{array}\right.<br>$很明显：单位矩阵$ I &#x3D; diag(1, 1, . . . , 1)$。</p><h4 id="3-1-2-转置"><a href="#3-1-2-转置" class="headerlink" title="3.1.2 转置"></a>3.1.2 转置</h4><p>矩阵的转置是指翻转矩阵的行和列。</p><p>给定一个矩阵：</p><p>$A \in \mathbb{R}^ {m \times n}$, 它的转置为$n \times m$的矩阵$A^T \in \mathbb{R}^ {n \times m}$ ，其中的元素为：<br>$<br>(A^T)<em>{ij} &#x3D; A</em>{ji}<br>$事实上，我们在描述行向量时已经使用了转置，因为列向量的转置自然是行向量。</p><p>转置的以下属性很容易验证：</p><ul><li>$(A^T )^T &#x3D; A$</li><li>$ (AB)^T &#x3D; B^T A^T$</li><li>$(A + B)^T &#x3D; A^T + B^T$</li></ul><h4 id="3-1-3-矩阵的逆"><a href="#3-1-3-矩阵的逆" class="headerlink" title="3.1.3 矩阵的逆"></a>3.1.3 矩阵的逆</h4><p>方阵$A  \in \mathbb{R}^{n \times n}$的倒数表示为$A^{-1}$，并且是这样的独特矩阵:<br>$<br>A^{-1}A&#x3D;I&#x3D;AA^{-1}<br>$请注意，并非所有矩阵都具有逆。 例如，非方形矩阵根据定义没有逆。 然而，对于一些方形矩阵$A$，可能仍然存在$A^{-1}$可能不存在的情况。 特别是，如果$A^{-1}$存在，我们说$A$是<strong>可逆</strong>的或<strong>非奇异</strong>的，否则就是<strong>不可逆</strong>或<strong>奇异</strong>的。<br>为了使方阵A具有逆$A^{-1}$，则$A$必须是满秩。 我们很快就会发现，除了满秩之外，还有许多其它的充分必要条件。<br>以下是逆的属性; 假设$A,B  \in \mathbb{R}^{n \times n}$，而且是非奇异的：</p><ul><li>$(A^{-1})^{-1} &#x3D; A$</li><li>$(AB)^{-1} &#x3D; B^{-1}A^{-1}$</li><li>$(A^{-1})^{T} &#x3D;(A^{T})^{-1} $因此，该矩阵通常表示为$A^{-T}$。<br>作为如何使用逆的示例，考虑线性方程组，$Ax &#x3D; b$，其中$A  \in \mathbb{R}^{n \times n}$，$x,b\in \mathbb{R}$， 如果$A$是非奇异的（即可逆的），那么$x &#x3D; A^{-1}b$。</li></ul><h4 id="3-1-4-对称矩阵"><a href="#3-1-4-对称矩阵" class="headerlink" title="3.1.4 对称矩阵"></a>3.1.4 对称矩阵</h4><p>如果$A &#x3D;  A^T$，则矩阵$A \in \mathbb{R}^ {n \times n}$是对称矩阵。 如果$ A &#x3D;  -  A^T$，它是反对称的。 很容易证明，对于任何矩阵$A \in \mathbb{R}^ {n \times n}$，矩阵$A  +  A^ T$是对称的，矩阵$A -A^T$是反对称的。 由此得出，任何方矩阵$A \in \mathbb{R}^ {n \times n}$可以表示为对称矩阵和反对称矩阵的和，所以：<br>$<br>A&#x3D;\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)<br>$上面公式的右边的第一个矩阵是对称矩阵，而第二个矩阵是反对称矩阵。 事实证明，对称矩阵在实践中用到很多，它们有很多很好的属性，我们很快就会看到它们。<br>通常将大小为$n$的所有对称矩阵的集合表示为$\mathbb{S}^n$，因此$A \in \mathbb{S}^n$意味着$A$是对称的$n\times n$矩阵;</p><h4 id="3-1-5-正交阵"><a href="#3-1-5-正交阵" class="headerlink" title="3.1.5  正交阵"></a>3.1.5  正交阵</h4><p>如果 $x^Ty&#x3D;0$，则两个向量$x,y\in \mathbb{R}^{n}$ 是<strong>正交</strong>的。如果$|x|_2&#x3D;1$，则向量$x\in \mathbb{R}^{n}$ 被归一化。如果一个方阵$U\in \mathbb{R}^{n \times n}$的所有列彼此正交并被归一化（这些列然后被称为正交），则方阵$U$是正交阵（注意在讨论向量时的意义不一样）。</p><p>它可以从正交性和正态性的定义中得出:<br>$<br>U^ TU &#x3D; I &#x3D; U U^T<br>$</p><p>换句话说，正交矩阵的逆是其转置。 注意，如果$U$不是方阵 :即，$U\in \mathbb{R}^{m \times n}$，$n &lt;m$  ，但其列仍然是正交的，则$U^TU &#x3D; I$，但是$UU^T \neq I$。我们通常只使用术语”正交”来描述先前的情况 ，其中$U$是方阵。<br>正交矩阵的另一个好的特性是在具有正交矩阵的向量上操作不会改变其欧几里德范数，即:<br>$<br>|U x|<em>{2}&#x3D;|x|</em>{2}<br>$对于任何 $x\in \mathbb{R}$ , $U\in \mathbb{R}^{n}$是正交的。</p><h4 id="3-1-6-矩阵的迹"><a href="#3-1-6-矩阵的迹" class="headerlink" title="3.1.6 矩阵的迹"></a>3.1.6 矩阵的迹</h4><p>方矩阵$A \in \mathbb{R}^ {n \times n}$的迹，表示为$\operatorname{tr} (A)$（或者只是$\operatorname{tr} A$，如果括号显然是隐含的），是矩阵中对角元素的总和：<br>$<br>\operatorname{tr} A&#x3D;\sum_{i&#x3D;1}^{n} A_{i i}<br>$迹具有以下属性（如下所示）：</p><ul><li><p>对于矩阵$A \in \mathbb{R}^ {n \times n}$，则：$\operatorname{tr}A &#x3D;\operatorname{tr}A^T$</p></li><li><p>对于矩阵$A,B \in \mathbb{R}^ {n \times n}$，则：$\operatorname{tr}(A + B) &#x3D; \operatorname{tr}A + \operatorname{tr}B$</p></li><li><p>对于矩阵$A \in \mathbb{R}^ {n \times n}$，$ t \in \mathbb{R}$，则：$\operatorname{tr}(tA) &#x3D; t\operatorname{tr}A$.</p></li><li><p>对于矩阵 $A$, $B$，$AB$ 为方阵, 则：$\operatorname{tr}AB &#x3D; \operatorname{tr}BA$</p></li><li><p>对于矩阵 $A$, $B$, $C$, $ABC$为方阵, 则：$\operatorname{tr}ABC &#x3D; \operatorname{tr}BCA&#x3D;\operatorname{tr}CAB$, 同理，更多矩阵的积也是有这个性质。</p></li></ul><p>作为如何证明这些属性的示例，我们将考虑上面给出的第四个属性。 假设$A \in \mathbb{R}^ {m \times n}$和$B \in \mathbb{R}^ {n \times m}$（因此$AB \in \mathbb{R}^ {m \times m}$是方阵）。 观察到$BA \in \mathbb{R}^ {n \times n}$也是一个方阵，因此对它们进行迹的运算是有意义的。 要证明$\operatorname{tr}AB &#x3D; \operatorname{tr}BA$，请注意：</p><p>$$<br>\begin{aligned} \operatorname{tr} A B &amp;&#x3D;\sum_{i&#x3D;1}^{m}(A B)<em>{i i}&#x3D;\sum</em>{i&#x3D;1}^{m}\left(\sum_{j&#x3D;1}^{n} A_{i j} B_{j i}\right) \ &amp;&#x3D;\sum_{i&#x3D;1}^{m} \sum_{j&#x3D;1}^{n} A_{i j} B_{j i}&#x3D;\sum_{j&#x3D;1}^{n} \sum_{i&#x3D;1}^{m} B_{j i} A_{i j} \ &amp;&#x3D;\sum_{j&#x3D;1}^{n}\left(\sum_{i&#x3D;1}^{m} B_{j i} A_{i j}\right)&#x3D;\sum_{j&#x3D;1}^{n}(B A)_{j j}&#x3D;\operatorname{tr} B A \end{aligned}<br>$$</p><p>这里，第一个和最后两个等式使用迹运算符和矩阵乘法的定义，重点在第四个等式，使用标量乘法的可交换性来反转每个乘积中的项的顺序，以及标量加法的可交换性和相关性，以便重新排列求和的顺序。</p><h4 id="3-1-7-矩阵的范数"><a href="#3-1-7-矩阵的范数" class="headerlink" title="3.1.7  矩阵的范数"></a>3.1.7  矩阵的范数</h4><p>向量的范数$|x|$是非正式度量的向量的“长度” 。 例如，我们有常用的欧几里德或$\ell_{2}$范数，<br>$<br>|x|<em>{2}&#x3D;\sqrt{\sum</em>{i&#x3D;1}^{n} x_{i}^{2} }<br>$注意：$|x|_{2}^{2}&#x3D;x^{T} x$</p><p>更正式地，范数是满足4个属性的函数（$f : \mathbb{R}^{n} \rightarrow \mathbb{R}$）：</p><ol><li>对于所有的 $x \in \mathbb{R}^ {n}$, $f(x) \geq 0 $(非负).</li><li>当且仅当$x &#x3D; 0$ 时，$f(x) &#x3D; 0$ (明确性).</li><li>对于所有$x \in \mathbb{R}^ {n}$,$t\in \mathbb{R}$，则 $f(tx) &#x3D; \left| t \right|f(x)$ (正齐次性).</li><li>对于所有 $x,y \in \mathbb{R}^ {n}$, $f(x + y) \leq f(x) + f(y)$ (三角不等式)</li></ol><p>其他范数的例子是$\ell_1$范数:<br>$<br>|x|<em>{1}&#x3D;\sum</em>{i&#x3D;1}^{n}\left|x_{i}\right|<br>$和$\ell_{\infty }$范数：<br>$|x|<em>{\infty}&#x3D;\max <em>{i}\left|x</em>{i}\right|<br>$上，到目前为止所提出的所有三个范数都是$\ell_p$范数族的例子，它们由实数$p \geq 1$参数化，并定义为：<br>$|</em>{p}&#x3D;\left(\sum_{i&#x3D;1}^{n}\left|x_{i}\right|^{p}\right)^{1 &#x2F; p}<br>$</p><p>也可以为矩阵定义范数，例如<strong>Frobenius</strong>范数:<br>$<br>|A|<em>{F}&#x3D;\sqrt{\sum</em>{i&#x3D;1}^{m} \sum_{j&#x3D;1}^{n} A_{i j}^{2} }&#x3D;\sqrt{\operatorname{tr}\left(A^{T} A\right)}<br>$</p><h3 id="3-2-矩阵的线性运算"><a href="#3-2-矩阵的线性运算" class="headerlink" title="3.2 矩阵的线性运算"></a><strong>3.2 矩阵的线性运算</strong></h3><h4 id="3-2-1-矩阵的加法"><a href="#3-2-1-矩阵的加法" class="headerlink" title="3.2.1 矩阵的加法"></a><strong>3.2.1 矩阵的加法</strong></h4><p>设$A &#x3D; (a_{ {ij} }),B &#x3D; (b_{ {ij} })$是两个$m \times n$矩阵，则$m \times n$ 矩阵$（C &#x3D; c_{ {ij} }) &#x3D; a_{ {ij} } + b_{ {ij} }$称为矩阵$A$与$B$的和，记为$A + B &#x3D; C$ 。</p><h4 id="3-2-2-矩阵的数乘"><a href="#3-2-2-矩阵的数乘" class="headerlink" title="3.2.2 矩阵的数乘"></a><strong>3.2.2 矩阵的数乘</strong></h4><p>设$A &#x3D; (a_{ {ij} })$是$m \times n$矩阵，$k$是一个常数，则$m \times n$矩阵$(ka_{ {ij} })$称为数$k$与矩阵$A$的数乘，记为${kA}$。</p><h4 id="3-2-3-矩阵的乘法"><a href="#3-2-3-矩阵的乘法" class="headerlink" title="3.2.3 矩阵的乘法"></a><strong>3.2.3 矩阵的乘法</strong></h4><p>设$A &#x3D; (a_{ {ij} })$是$m \times n$矩阵，$B &#x3D; (b_{ {ij} })$是$n \times s$矩阵，那么$m \times s$矩阵$C &#x3D; (c_{ {ij} })$，其中$c_{ {ij} } &#x3D; a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{ {in} }b_{ {nj} } &#x3D; \sum_{k &#x3D;1}^{n}{a_{ {ik} }b_{ {kj} }}$称为${AB}$的乘积，记为$C &#x3D; AB$ 。</p><h4 id="3-2-4-mathbf-A-mathbf-T-、-mathbf-A-mathbf-1-、-mathbf-A-mathbf-三者之间的关系"><a href="#3-2-4-mathbf-A-mathbf-T-、-mathbf-A-mathbf-1-、-mathbf-A-mathbf-三者之间的关系" class="headerlink" title="*3.2.4 ** $\mathbf{A}^{\mathbf{T} }$、$\mathbf{A}^{\mathbf{-1} }$、$\mathbf{A}^{\mathbf{} }$三者之间的关系"></a>*<em>3.2.4 ** $\mathbf{A}^{\mathbf{T} }$<strong>、</strong>$\mathbf{A}^{\mathbf{-1} }$<strong>、</strong>$\mathbf{A}^{\mathbf{</em>} }$<strong>三者之间的关系</strong></h4><p>(1) ${(A^{T})}^{T} &#x3D; A,{(AB)}^{T} &#x3D; B^{T}A^{T},{(kA)}^{T} &#x3D; kA^{T},{(A \pm B)}^{T} &#x3D; A^{T} \pm B^{T}$</p><p>(2) $\left( A^{- 1} \right)^{- 1} &#x3D; A,\left( {AB} \right)^{- 1} &#x3D; B^{- 1}A^{- 1},\left( {kA} \right)^{- 1} &#x3D; \frac{1}{k}A^{- 1},$</p><p>但 ${(A \pm B)}^{- 1} &#x3D; A^{- 1} \pm B^{- 1}$不一定成立。</p><p>(3) $\left( A^{<em>} \right)^{</em>} &#x3D; |A|^{n - 2}\ A\ \ (n \geq 3)$，$\left({AB} \right)^{<em>} &#x3D; B^{</em>}A^{<em>},$ $\left( {kA} \right)^{</em>} &#x3D; k^{n -1}A^{*}{\ \ }\left( n \geq 2 \right)$</p><p>但$\left( A \pm B \right)^{<em>} &#x3D; A^{</em>} \pm B^{*}$不一定成立。</p><p>(4) ${(A^{- 1})}^{T} &#x3D; {(A^{T})}^{- 1},\ \left( A^{- 1} \right)^{<em>} &#x3D;{(AA^{</em>})}^{- 1},{(A^{<em>})}^{T} &#x3D; \left( A^{T} \right)^{</em>}$</p><h4 id="3-2-5-有关-mathbf-A-mathbf-的结论"><a href="#3-2-5-有关-mathbf-A-mathbf-的结论" class="headerlink" title="3.2.5 有关$\mathbf{A}^{\mathbf{*} }$的结论"></a><strong>3.2.5 有关</strong>$\mathbf{A}^{\mathbf{*} }$<strong>的结论</strong></h4><p>(1) $AA^{<em>} &#x3D; A^{</em>}A &#x3D; |A|E$</p><p>(2) $|A^{<em>}| &#x3D; |A|^{n - 1}\ (n \geq 2),\ \ \ \ {(kA)}^{</em>} &#x3D; k^{n -1}A^{<em>},{ {\ \ }\left( A^{</em>} \right)}^{*} &#x3D; |A|^{n - 2}A(n \geq 3)$</p><p>(3) 若$A$可逆，则$A^{<em>} &#x3D; |A|A^{- 1},{(A^{</em>})}^{*} &#x3D; \frac{1}{|A|}A$</p><p>(4) 若$A$为$n$阶方阵，则：</p><p>$r(A^*)&#x3D;\begin{cases}n,\quad r(A)&#x3D;n\ 1,\quad r(A)&#x3D;n-1\ 0,\quad r(A)&lt;n-1\end{cases}$</p><h4 id="3-2-6-有关-mathbf-A-mathbf-1-的结论"><a href="#3-2-6-有关-mathbf-A-mathbf-1-的结论" class="headerlink" title="3.2.6 有关$\mathbf{A}^{\mathbf{- 1} }$的结论"></a><strong>3.2.6 有关</strong>$\mathbf{A}^{\mathbf{- 1} }$<strong>的结论</strong></h4><p>$A$可逆$\Leftrightarrow AB &#x3D; E; \Leftrightarrow |A| \neq 0; \Leftrightarrow r(A) &#x3D; n;$</p><p>$\Leftrightarrow A$可以表示为初等矩阵的乘积；$\Leftrightarrow A;\Leftrightarrow Ax &#x3D; 0$。</p><h4 id="3-2-7-有关矩阵秩的结论"><a href="#3-2-7-有关矩阵秩的结论" class="headerlink" title="3.2.7 有关矩阵秩的结论"></a><strong>3.2.7 有关矩阵秩的结论</strong></h4><p>(1) 秩$r(A)$&#x3D;行秩&#x3D;列秩；</p><p>(2) $r(A_{m \times n}) \leq \min(m,n);$</p><p>(3) $A \neq 0 \Rightarrow r(A) \geq 1$；</p><p>(4) $r(A \pm B) \leq r(A) + r(B);$</p><p>(5) 初等变换不改变矩阵的秩</p><p>(6) $r(A) + r(B) - n \leq r(AB) \leq \min(r(A),r(B)),$特别若$AB &#x3D; O$<br>则：$r(A) + r(B) \leq n$</p><p>(7) 若$A^{- 1}$存在$\Rightarrow r(AB) &#x3D; r(B);$ 若$B^{- 1}$存在<br>$\Rightarrow r(AB) &#x3D; r(A);$</p><p>若$r(A_{m \times n}) &#x3D; n \Rightarrow r(AB) &#x3D; r(B);$ 若$r(A_{m \times s}) &#x3D; n\Rightarrow r(AB) &#x3D; r\left( A \right)$。</p><p>(8) $r(A_{m \times s}) &#x3D; n \Leftrightarrow Ax &#x3D; 0$只有零解</p><h4 id="3-2-8-分块求逆公式"><a href="#3-2-8-分块求逆公式" class="headerlink" title="3.2.8 分块求逆公式"></a><strong>3.2.8 分块求逆公式</strong></h4><p>$\begin{pmatrix} A &amp; O \ O &amp; B \ \end{pmatrix}^{- 1} &#x3D; \begin{pmatrix} A^{-1} &amp; O \ O &amp; B^{- 1} \ \end{pmatrix}$； $\begin{pmatrix} A &amp; C \ O &amp; B \\end{pmatrix}^{- 1} &#x3D; \begin{pmatrix} A^{- 1}&amp; - A^{- 1}CB^{- 1} \ O &amp; B^{- 1} \ \end{pmatrix}$；</p><p>$\begin{pmatrix} A &amp; O \ C &amp; B \ \end{pmatrix}^{- 1} &#x3D; \begin{pmatrix}  A^{- 1}&amp;{O} \   - B^{- 1}CA^{- 1} &amp; B^{- 1} \\end{pmatrix}$； $\begin{pmatrix} O &amp; A \ B &amp; O \ \end{pmatrix}^{- 1} &#x3D;\begin{pmatrix} O &amp; B^{- 1} \ A^{- 1} &amp; O \ \end{pmatrix}$</p><p>这里$A$，$B$均为可逆方阵。</p><h3 id="3-3-矩阵微积分"><a href="#3-3-矩阵微积分" class="headerlink" title="3.3 矩阵微积分"></a>3.3 矩阵微积分</h3><p>虽然前面章节中的主题通常包含在线性代数的标准课程中，但似乎很少涉及（我们将广泛使用）的一个主题是微积分扩展到向量设置展。尽管我们使用的所有实际微积分都是相对微不足道的，但是符号通常会使事情看起来比实际困难得多。 在本节中，我们将介绍矩阵微积分的一些基本定义，并提供一些示例。</p><h4 id="3-3-1-梯度"><a href="#3-3-1-梯度" class="headerlink" title="3.3.1 梯度"></a>3.3.1 梯度</h4><p>假设$f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$是将维度为$m \times n$的矩阵$A\in \mathbb{R}^{m \times n}$作为输入并返回实数值的函数。 然后$f$的梯度（相对于$A\in \mathbb{R}^{m \times n}$）是偏导数矩阵，定义如下：<br>$<br>\nabla_{A} f(A) \in \mathbb{R}^{m \times n}&#x3D;\left[\begin{array}{cccc}{\frac{\partial f(A)}{\partial A_{11} }} &amp; {\frac{\partial f(A)}{\partial A_{12} }} &amp; {\cdots} &amp; {\frac{\partial f(A)}{\partial A_{1n} }} \ {\frac{\partial f(A)}{\partial A_{21} }} &amp; {\frac{\partial f(A)}{\partial A_{22} }} &amp; {\cdots} &amp; {\frac{\partial f(A)}{\partial A_{2 n} }} \ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \ {\frac{\partial f(A)}{\partial A_{m 1} }} &amp; {\frac{\partial f(A)}{\partial A_{m 2} }} &amp; {\cdots} &amp; {\frac{\partial f(A)}{\partial A_{m n} }}\end{array}\right]<br>$即，$m \times n$矩阵:<br>$\left(\nabla_{A} f(A)\right)<em>{i j}&#x3D;\frac{\partial f(A)}{\partial A</em>{i j} }<br>$意，$\nabla_{A} f(A) $的维度始终与$A$的维度相同。特殊情况，如果$A$只是向量$A\in \mathbb{R}^{n}$，则<br>$bla_{x} f(x)&#x3D;\left[\begin{array}{c}{\frac{\partial f(x)}{\partial x_{1} }} \ {\frac{\partial f(x)}{\partial x_{2} }} \ {\vdots} \ {\frac{\partial f(x)}{\partial x_{n} }}\end{array}\right]<br>$要记住，只有当函数是实值时，即如果函数返回标量值，才定义函数的梯度。例如，$A\in \mathbb{R}^{m \times n}$相对于$x$，我们不能取$Ax$的梯度，因为这个量是向量值。<br>它直接从偏导数的等价性质得出：</p><ul><li><p>$\nabla_{x}(f(x)+g(x))&#x3D;\nabla_{x} f(x)+\nabla_{x} g(x)$</p></li><li><p>对于$t \in \mathbb{R}$ ，$\nabla_{x}(t f(x))&#x3D;t \nabla_{x} f(x)$</p></li></ul><p>原则上，梯度是偏导数对多变量函数的自然延伸。然而，在实践中，由于符号的原因，使用梯度有时是很困难的。例如，假设$A\in \mathbb{R}^{m \times n}$是一个固定系数矩阵，假设$b\in \mathbb{R}^{m}$是一个固定系数向量。设$f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$为$f(z)&#x3D;z^Tz$定义的函数，因此$\nabla_{z}f(z)&#x3D;2z$。但现在考虑表达式，<br>$<br>\nabla f(Ax)<br>$该表达式应该如何解释？ 至少有两种可能性：<br>1.在第一个解释中，回想起$\nabla_{z}f(z)&#x3D;2z$。 在这里，我们将$\nabla f(Ax)$解释为评估点$Ax$处的梯度，因此:</p><p>$$<br>\nabla f(A x)&#x3D;2(A x)&#x3D;2 A x \in \mathbb{R}^{m}<br>$$</p><p>2.在第二种解释中，我们将数量$f(Ax)$视为输入变量$x$的函数。 更正式地说，设$g(x) &#x3D;f(Ax)$。 然后在这个解释中:<br>$<br>\nabla f(A x)&#x3D;\nabla_{x} g(x) \in \mathbb{R}^{n}<br>$</p><p>在这里，我们可以看到这两种解释确实不同。 一种解释产生$m$维向量作为结果，而另一种解释产生$n$维向量作为结果！ 我们怎么解决这个问题？</p><p>这里，关键是要明确我们要区分的变量。<br>在第一种情况下，我们将函数$f$与其参数$z$进行区分，然后替换参数$Ax$。<br>在第二种情况下，我们将复合函数$g(x)&#x3D;f(Ax)$直接与$x$进行微分。</p><p>我们将第一种情况表示为$\nabla zf(Ax)$，第二种情况表示为$\nabla xf(Ax)$。</p><p>保持符号清晰是非常重要的，以后完成课程作业时候你就会发现。</p><h4 id="3-3-2-黑塞矩阵"><a href="#3-3-2-黑塞矩阵" class="headerlink" title="3.3.2 黑塞矩阵"></a>3.3.2 黑塞矩阵</h4><p>假设$f: \mathbb{R}^{n} \rightarrow \mathbb{R}$是一个函数，它接受$\mathbb{R}^{n}$中的向量并返回实数。那么关于$x$的<strong>黑塞矩阵</strong>（也有翻译作海森矩阵），写做：$\nabla_x ^2 f(A x)$，或者简单地说，$H$是$n \times n$矩阵的偏导数：<br>$<br>\nabla_{x}^{2} f(x) \in \mathbb{R}^{n \times n}&#x3D;\left[\begin{array}{cccc}{\frac{\partial^{2} f(x)}{\partial x_{1}^{2} }} &amp; {\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2} }} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{n} }} \ {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{1} }} &amp; {\frac{\partial^{2} f(x)}{\partial x_{2}^{2} }} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{n} }} \ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \ {\frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{1} }} &amp; {\frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{2} }} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{n}^{2} }}\end{array}\right]<br>$换句话说，$\nabla_{x}^{2} f(x) \in \mathbb{R}^{n \times n}$，其：</p><p>$$<br>\left(\nabla_{x}^{2} f(x)\right)<em>{i j}&#x3D;\frac{\partial^{2} f(x)}{\partial x</em>{i} \partial x_{j} }<br>$$</p><p>注意：黑塞矩阵通常是对称阵：</p><p>$$<br>\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{j} }&#x3D;\frac{\partial^{2} f(x)}{\partial x_{j} \partial x_{i} }<br>$$</p><p>与梯度相似，只有当$f(x)$为实值时才定义黑塞矩阵。</p><p>很自然地认为梯度与向量函数的一阶导数的相似，而黑塞矩阵与二阶导数的相似（我们使用的符号也暗示了这种关系）。 这种直觉通常是正确的，但需要记住以下几个注意事项。<br>首先，对于一个变量$f: \mathbb{R} \rightarrow \mathbb{R}$的实值函数，它的基本定义：二阶导数是一阶导数的导数，即：<br>$<br>\frac{\partial^{2} f(x)}{\partial x^{2} }&#x3D;\frac{\partial}{\partial x} \frac{\partial}{\partial x} f(x)<br>$然而，对于向量的函数，函数的梯度是一个向量，我们不能取向量的梯度，即:<br>$nabla_{x} \nabla_{x} f(x)&#x3D;\nabla_{x}\left[\begin{array}{c}{\frac{\partial f(x)}{\partial x_{1} }} \ {\frac{\partial f(x)}{\partial x_{2} }} \ {\vdots} \ {\frac{\partial f(x)}{\partial x_{n} }}\end{array}\right]<br>$</p><p>上面这个表达式没有意义。 因此，黑塞矩阵不是梯度的梯度。 然而，下面这种情况却这几乎是正确的：如果我们看一下梯度$\left(\nabla_{x} f(x)\right)<em>{i}&#x3D;\partial f(x) &#x2F; \partial x</em>{i}$的第$i$个元素，并取关于于$x$的梯度我们得到：<br>$<br>\nabla_{x} \frac{\partial f(x)}{\partial x_{i} }&#x3D;\left[\begin{array}{c}{\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{1} }} \ {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{2} }} \ {\vdots} \ {\frac{\partial f(x)}{\partial x_{i} \partial x_{n} }}\end{array}\right]<br>$</p><p>这是黑塞矩阵第$i$行（列）,所以：<br>$<br>\nabla_{x}^{2} f(x)&#x3D;\left[\nabla_{x}\left(\nabla_{x} f(x)\right)<em>{1} \quad \nabla</em>{x}\left(\nabla_{x} f(x)\right)<em>{2} \quad \cdots \quad \nabla</em>{x}\left(\nabla_{x} f(x)\right)<em>{n}\right]<br>$简单地说：我们可以说由于：$\nabla</em>{x}^{2} f(x)&#x3D;\nabla_{x}\left(\nabla_{x} f(x)\right)^{T}$，只要我们理解，这实际上是取$\nabla_{x} f(x)$的每个元素的梯度，而不是整个向量的梯度。</p><p>最后，请注意，虽然我们可以对矩阵$A\in \mathbb{R}^{n}$取梯度，但对于这门课，我们只考虑对向量$x \in \mathbb{R}^{n}$取黑塞矩阵。<br>这会方便很多（事实上，我们所做的任何计算都不要求我们找到关于矩阵的黑森方程），因为关于矩阵的黑塞方程就必须对矩阵所有元素求偏导数$\partial^{2} f(A) &#x2F;\left(\partial A_{i j} \partial A_{k \ell}\right)$，将其表示为矩阵相当麻烦。</p><h4 id="3-3-3-二次函数和线性函数的梯度和黑塞矩阵"><a href="#3-3-3-二次函数和线性函数的梯度和黑塞矩阵" class="headerlink" title="3.3.3 二次函数和线性函数的梯度和黑塞矩阵"></a>3.3.3 二次函数和线性函数的梯度和黑塞矩阵</h4><p>现在让我们尝试确定几个简单函数的梯度和黑塞矩阵。 </p><p>对于$x \in \mathbb{R}^{n}$, 设$f(x)&#x3D;b^Tx$  的某些已知向量$b \in \mathbb{R}^{n}$ ，则：</p><p>$$<br>f(x)&#x3D;\sum_{i&#x3D;1}^{n} b_{i} x_{i}<br>$$</p><p>所以：<br>$<br>\frac{\partial f(x)}{\partial x_{k} }&#x3D;\frac{\partial}{\partial x_{k} } \sum_{i&#x3D;1}^{n} b_{i} x_{i}&#x3D;b_{k}<br>$由此我们可以很容易地看出$\nabla_{x} b^{T} x&#x3D;b$。 这应该与单变量微积分中的类似情况进行比较，其中$\partial &#x2F;(\partial x) a x&#x3D;a$。<br>现在考虑$A\in \mathbb{S}^{n}$的二次函数$f(x)&#x3D;x^TAx$。 记住这一点：<br>$(x)&#x3D;\sum_{i&#x3D;1}^{n} \sum_{j&#x3D;1}^{n} A_{i j} x_{i} x_{j}<br>$取偏导数，我们将分别考虑包括$x_k$和$x_2^k$因子的项：</p><p>$$<br>\begin{aligned} \frac{\partial f(x)}{\partial x_{k} } &amp;&#x3D;\frac{\partial}{\partial x_{k} } \sum_{i&#x3D;1}^{n} \sum_{j&#x3D;1}^{n} A_{i j} x_{i} x_{j} \ &amp;&#x3D;\frac{\partial}{\partial x_{k} }\left[\sum_{i \neq k} \sum_{j \neq k} A_{i j} x_{i} x_{j}+\sum_{i \neq k} A_{i k} x_{i} x_{k}+\sum_{j \neq k} A_{k j} x_{k} x_{j}+A_{k k} x_{k}^{2}\right] \ &amp;&#x3D;\sum_{i \neq k} A_{i k} x_{i}+\sum_{j \neq k} A_{k j} x_{j}+2 A_{k k} x_{k} \ &amp;&#x3D;\sum_{i&#x3D;1}^{n} A_{i k} x_{i}+\sum_{j&#x3D;1}^{n} A_{k j} x_{j}&#x3D;2 \sum_{i&#x3D;1}^{n} A_{k i} x_{i} \end{aligned}<br>$$</p><p>最后一个等式，是因为$A$是对称的（我们可以安全地假设，因为它以二次形式出现）。 注意，$\nabla_{x} f(x)$的第$k$个元素是$A$和$x$的第$k$行的内积。 因此，$\nabla_{x} x^{T} A x&#x3D;2 A x$。 同样，这应该提醒你单变量微积分中的类似事实，即$\partial &#x2F;(\partial x) a x^{2}&#x3D;2 a x$。</p><p>最后，让我们来看看二次函数$f(x)&#x3D;x^TAx$黑塞矩阵（显然，线性函数$b^Tx$的黑塞矩阵为零）。在这种情况下:<br>$<br>\frac{\partial^{2} f(x)}{\partial x_{k} \partial x_{\ell} }&#x3D;\frac{\partial}{\partial x_{k} }\left[\frac{\partial f(x)}{\partial x_{\ell} }\right]&#x3D;\frac{\partial}{\partial x_{k} }\left[2 \sum_{i&#x3D;1}^{n} A_{\ell i} x_{i}\right]&#x3D;2 A_{\ell k}&#x3D;2 A_{k \ell}<br>$因此，应该很清楚$\nabla_{x}^2 x^{T} A x&#x3D;2 A$，这应该是完全可以理解的（同样类似于$\partial^2 &#x2F;(\partial x^2) a x^{2}&#x3D;2a$的单变量事实）。</p><p>简要概括起来：</p><ul><li><p>$\nabla_{x} b^{T} x&#x3D;b$ </p></li><li><p>$\nabla_{x} x^{T} A x&#x3D;2 A x$ (如果$A$是对称阵)</p></li><li><p>$\nabla_{x}^2 x^{T} A x&#x3D;2 A $  (如果$A$是对称阵)</p></li></ul><h4 id="3-3-4-最小二乘法"><a href="#3-3-4-最小二乘法" class="headerlink" title="3.3.4 最小二乘法"></a>3.3.4 最小二乘法</h4><p>让我们应用上一节中得到的方程来推导最小二乘方程。假设我们得到矩阵$A\in \mathbb{R}^{m \times n}$（为了简单起见，我们假设$A$是满秩）和向量$b\in \mathbb{R}^{m}$，从而使$b \notin \mathcal{R}(A)$。在这种情况下，我们将无法找到向量$x\in \mathbb{R}^{n}$，由于$Ax &#x3D; b$，因此我们想要找到一个向量$x$，使得$Ax$尽可能接近 $b$，用欧几里德范数的平方$|A x-b|_{2}^{2} $来衡量。</p><p>使用公式$|x|^{2}&#x3D;x^Tx$，我们可以得到：</p><p>$$<br>\begin{aligned}|A x-b|_{2}^{2} &amp;&#x3D;(A x-b)^{T}(A x-b) \ &amp;&#x3D;x^{T} A^{T} A x-2 b^{T} A x+b^{T} b \end{aligned}<br>$$</p><p>根据$x$的梯度，并利用上一节中推导的性质：<br>$<br>\begin{aligned} \nabla_{x}\left(x^{T} A^{T} A x-2 b^{T} A x+b^{T} b\right) &amp;&#x3D;\nabla_{x} x^{T} A^{T} A x-\nabla_{x} 2 b^{T} A x+\nabla_{x} b^{T} b \ &amp;&#x3D;2 A^{T} A x-2 A^{T} b \end{aligned}<br>$将最后一个表达式设置为零，然后解出$x$，得到了正规方程：<br>$ &#x3D; (A^TA)^{-1}A^Tb<br>$我们在课堂上得到的相同。</p><h4 id="3-3-5-行列式的梯度"><a href="#3-3-5-行列式的梯度" class="headerlink" title="3.3.5 行列式的梯度"></a>3.3.5 行列式的梯度</h4><p>现在让我们考虑一种情况，我们找到一个函数相对于矩阵的梯度，也就是说，对于$A\in \mathbb{R}^{n \times n}$，我们要找到$\nabla_{A}|A|$。回想一下我们对行列式的讨论：<br>$<br>|A|&#x3D;\sum_{i&#x3D;1}^{n}(-1)^{i+j} A_{i j}\left|A_{\backslash i, \backslash j}\right| \quad(\text { for any } j \in 1, \ldots, n)<br>$所以：<br>$frac{\partial}{\partial A_{k \ell} }|A|&#x3D;\frac{\partial}{\partial A_{k \ell} } \sum_{i&#x3D;1}^{n}(-1)^{i+j} A_{i j}\left|A_{\backslash i, \backslash j}\right|&#x3D;(-1)^{k+\ell}\left|A_{\backslash k,\backslash \ell}\right|&#x3D;(\operatorname{adj}(A))<em>{\ell k}<br>$里可以知道，它直接从伴随矩阵的性质得出：<br>$bla</em>{A}|A|&#x3D;(\operatorname{adj}(A))^{T}&#x3D;|A| A^{-T}<br>$来考虑函数$f : \mathbb{S}<em>{++}^{n} \rightarrow \mathbb{R}$，$f(A)&#x3D;\log |A|$。注意，我们必须将$f$的域限制为正定矩阵，因为这确保了$|A|&gt;0$，因此$|A|$的对数是实数。在这种情况下，我们可以使用链式法则（没什么奇怪的，只是单变量演算中的普通链式法则）来看看：<br>${\partial \log |A|}{\partial A</em>{i j} }&#x3D;\frac{\partial \log |A|}{\partial|A|} \frac{\partial|A|}{\partial A_{i j} }&#x3D;\frac{1}{|A|} \frac{\partial|A|}{\partial A_{i j} }<br>$明显看出：</p><p>$$<br>\nabla_{A} \log |A|&#x3D;\frac{1}{|A|} \nabla_{A}|A|&#x3D;A^{-1}<br>$$</p><p>我们可以在最后一个表达式中删除转置，因为$A$是对称的。注意与单值情况的相似性，其中$\partial &#x2F;(\partial x) \log x&#x3D;1 &#x2F; x$。</p><h4 id="3-3-6-特征值优化"><a href="#3-3-6-特征值优化" class="headerlink" title="3.3.6 特征值优化"></a>3.3.6 特征值优化</h4><p>最后，我们使用矩阵演算以直接导致特征值&#x2F;特征向量分析的方式求解优化问题。 考虑以下等式约束优化问题：</p><p>$$<br>\max <em>{x \in \mathbb{R}^{n} } x^{T} A x \quad \text { subject to }|x|</em>{2}^{2}&#x3D;1<br>$$</p><p>对于对称矩阵$A\in \mathbb{S}^{n}$。求解等式约束优化问题的标准方法是采用<strong>拉格朗日</strong>形式，一种包含等式约束的目标函数，在这种情况下，拉格朗日函数可由以下公式给出：</p><p>$$<br>\mathcal{L}(x, \lambda)&#x3D;x^{T} A x-\lambda x^{T} x<br>$$</p><p>其中，$\lambda $被称为与等式约束关联的拉格朗日乘子。可以确定，要使$x^*$成为问题的最佳点，拉格朗日的梯度必须在$x^*$处为零（这不是唯一的条件，但它是必需的）。也就是说，<br>$<br>\nabla_{x} \mathcal{L}(x, \lambda)&#x3D;\nabla_{x}\left(x^{T} A x-\lambda x^{T} x\right)&#x3D;2 A^{T} x-2 \lambda x&#x3D;0<br>$请注意，这只是线性方程$Ax &#x3D;\lambda x$。 这表明假设$x^T x &#x3D; 1$，可能最大化（或最小化）$x^T Ax$的唯一点是$A$的特征向量。</p><h2 id="4-向量"><a href="#4-向量" class="headerlink" title="4 向量"></a>4 向量</h2><h3 id="4-1-有关向量组的线性表示"><a href="#4-1-有关向量组的线性表示" class="headerlink" title="4.1 有关向量组的线性表示"></a><strong>4.1 有关向量组的线性表示</strong></h3><p>(1)$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性相关$\Leftrightarrow$至少有一个向量可以用其余向量线性表示。</p><p>(2)$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，$\beta$线性相关$\Leftrightarrow \beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$唯一线性表示。</p><p>(3) $\beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性表示<br>$\Leftrightarrow r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s}) &#x3D;r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s},\beta)$ 。</p><h3 id="4-2-有关向量组的线性相关性"><a href="#4-2-有关向量组的线性相关性" class="headerlink" title="4.2 有关向量组的线性相关性"></a><strong>4.2 有关向量组的线性相关性</strong></h3><p>(1)部分相关，整体相关；整体无关，部分无关.</p><p>(2) ① $n$个$n$维向量<br>$\alpha_{1},\alpha_{2}\cdots\alpha_{n}$线性无关$\Leftrightarrow \left|\left\lbrack \alpha_{1}\alpha_{2}\cdots\alpha_{n} \right\rbrack \right| \neq0$， $n$个$n$维向量$\alpha_{1},\alpha_{2}\cdots\alpha_{n}$线性相关<br>$\Leftrightarrow |\lbrack\alpha_{1},\alpha_{2},\cdots,\alpha_{n}\rbrack| &#x3D; 0$<br>。</p><p>② $n + 1$个$n$维向量线性相关。</p><p>③ 若$\alpha_{1},\alpha_{2}\cdots\alpha_{S}$线性无关，则添加分量后仍线性无关；或一组向量线性相关，去掉某些分量后仍线性相关。</p><h3 id="4-3-有关向量组的线性表示"><a href="#4-3-有关向量组的线性表示" class="headerlink" title="4.3 有关向量组的线性表示"></a><strong>4.3 有关向量组的线性表示</strong></h3><p>(1) $\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性相关$\Leftrightarrow$至少有一个向量可以用其余向量线性表示。</p><p>(2) $\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，$\beta$线性相关$\Leftrightarrow\beta$ 可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$唯一线性表示。</p><p>(3) $\beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性表示<br>$\Leftrightarrow r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s}) &#x3D;r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s},\beta)$</p><h3 id="4-4-向量组的秩与矩阵的秩之间的关系"><a href="#4-4-向量组的秩与矩阵的秩之间的关系" class="headerlink" title="4.4 向量组的秩与矩阵的秩之间的关系"></a><strong>4.4 向量组的秩与矩阵的秩之间的关系</strong></h3><p>设$r(A_{m \times n}) &#x3D;r$，则$A$的秩$r(A)$与$A$的行列向量组的线性相关性关系为：</p><p>(1) 若$r(A_{m \times n}) &#x3D; r &#x3D; m$，则$A$的行向量组线性无关。</p><p>(2) 若$r(A_{m \times n}) &#x3D; r &lt; m$，则$A$的行向量组线性相关。</p><p>(3) 若$r(A_{m \times n}) &#x3D; r &#x3D; n$，则$A$的列向量组线性无关。</p><p>(4) 若$r(A_{m \times n}) &#x3D; r &lt; n$，则$A$的列向量组线性相关。</p><h3 id="4-5-mathbf-n-维向量空间的基变换公式及过渡矩阵"><a href="#4-5-mathbf-n-维向量空间的基变换公式及过渡矩阵" class="headerlink" title="**4.5 **$\mathbf{n}$维向量空间的基变换公式及过渡矩阵"></a>**4.5 **$\mathbf{n}$<strong>维向量空间的基变换公式及过渡矩阵</strong></h3><p>若$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$与$\beta_{1},\beta_{2},\cdots,\beta_{n}$是向量空间$V$的两组基，则基变换公式为：</p><p>$(\beta_{1},\beta_{2},\cdots,\beta_{n}) &#x3D; (\alpha_{1},\alpha_{2},\cdots,\alpha_{n})\begin{bmatrix}  c_{11}&amp; c_{12}&amp; \cdots &amp; c_{1n} \  c_{21}&amp; c_{22}&amp;\cdots &amp; c_{2n} \ \cdots &amp; \cdots &amp; \cdots &amp; \cdots \  c_{n1}&amp; c_{n2} &amp; \cdots &amp; c_{ {nn} } \\end{bmatrix} &#x3D; (\alpha_{1},\alpha_{2},\cdots,\alpha_{n})C$</p><p>其中$C$是可逆矩阵，称为由基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$到基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的过渡矩阵。</p><h3 id="4-6-坐标变换公式"><a href="#4-6-坐标变换公式" class="headerlink" title="4.6 坐标变换公式"></a><strong>4.6 坐标变换公式</strong></h3><p>若向量$\gamma$在基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$与基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的坐标分别是<br>$X &#x3D; {(x_{1},x_{2},\cdots,x_{n})}^{T}$，</p><p>$Y &#x3D; \left( y_{1},y_{2},\cdots,y_{n} \right)^{T}$ 即： $\gamma &#x3D;x_{1}\alpha_{1} + x_{2}\alpha_{2} + \cdots + x_{n}\alpha_{n} &#x3D; y_{1}\beta_{1} +y_{2}\beta_{2} + \cdots + y_{n}\beta_{n}$，则向量坐标变换公式为$X &#x3D; CY$ 或$Y &#x3D; C^{- 1}X$，其中$C$是从基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$到基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的过渡矩阵。</p><h3 id="4-7-向量的内积-Inner-Product"><a href="#4-7-向量的内积-Inner-Product" class="headerlink" title="4.7 向量的内积(Inner Product)"></a><strong>4.7 向量的内积(Inner Product)</strong></h3><p>也可以成为点积(Dot Product)或标量积(Scalar Product)</p><p>$(\alpha,\beta) &#x3D; a_{1}b_{1} + a_{2}b_{2} + \cdots + a_{n}b_{n} &#x3D; \alpha^{T}\beta &#x3D; \beta^{T}\alpha$</p><h3 id="4-8-Schmidt-正交化"><a href="#4-8-Schmidt-正交化" class="headerlink" title="4.8 Schmidt 正交化"></a><strong>4.8 Schmidt 正交化</strong></h3><p>若$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，则可构造$\beta_{1},\beta_{2},\cdots,\beta_{s}$使其两两正交，且$\beta_{i}$仅是$\alpha_{1},\alpha_{2},\cdots,\alpha_{i}$的线性组合$(i&#x3D; 1,2,\cdots,n)$，再把$\beta_{i}$单位化，记$\gamma_{i} &#x3D;\frac{\beta_{i} }{\left| \beta_{i}\right|}$，则$\gamma_{1},\gamma_{2},\cdots,\gamma_{i}$是规范正交向量组。其中<br>$\beta_{1} &#x3D; \alpha_{1}$， $\beta_{2} &#x3D; \alpha_{2} -\frac{(\alpha_{2},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}$ ， $\beta_{3} &#x3D;\alpha_{3} - \frac{(\alpha_{3},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1} -\frac{(\alpha_{3},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2}$ ，</p><p>$\beta_{s} &#x3D; \alpha_{s} - \frac{(\alpha_{s},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1} - \frac{(\alpha_{s},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2} - \cdots - \frac{(\alpha_{s},\beta_{s - 1})}{(\beta_{s - 1},\beta_{s - 1})}\beta_{s - 1}$</p><h3 id="4-9-正交基及规范正交基"><a href="#4-9-正交基及规范正交基" class="headerlink" title="4.9 正交基及规范正交基"></a><strong>4.9 正交基及规范正交基</strong></h3><p>向量空间一组基中的向量如果两两正交，就称为正交基；若正交基中每个向量都是单位向量，就称其为规范正交基。</p><h2 id="5-线性方程组"><a href="#5-线性方程组" class="headerlink" title="5 线性方程组"></a>5 线性方程组</h2><p><strong>1. 克莱姆法则</strong></p><p>线性方程组$\begin{cases}  a_{11}x_{1} + a_{12}x_{2} + \cdots +a_{1n}x_{n} &#x3D; b_{1} \   a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &#x3D;b_{2} \   \quad\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots \ a_{n1}x_{1} + a_{n2}x_{2} + \cdots + a_{ {nn} }x_{n} &#x3D; b_{n} \ \end{cases}$，如果系数行列式$D &#x3D; \left| A \right| \neq 0$，则方程组有唯一解，$x_{1} &#x3D; \frac{D_{1} }{D},x_{2} &#x3D; \frac{D_{2} }{D},\cdots,x_{n} &#x3D;\frac{D_{n} }{D}$，其中$D_{j}$是把$D$中第$j$列元素换成方程组右端的常数列所得的行列式。</p><p><strong>2.</strong> $n$阶矩阵$A$可逆$\Leftrightarrow Ax &#x3D; 0$只有零解。$\Leftrightarrow\forall b,Ax &#x3D; b$总有唯一解，一般地，$r(A_{m \times n}) &#x3D; n \Leftrightarrow Ax&#x3D; 0$只有零解。</p><p><strong>3.非奇次线性方程组有解的充分必要条件，线性方程组解的性质和解的结构</strong></p><p>(1) 设$A$为$m \times n$矩阵，若$r(A_{m \times n}) &#x3D; m$，则对$Ax &#x3D;b$而言必有$r(A) &#x3D; r(A \vdots b) &#x3D; m$，从而$Ax &#x3D; b$有解。</p><p>(2) 设$x_{1},x_{2},\cdots x_{s}$为$Ax &#x3D; b$的解，则$k_{1}x_{1} + k_{2}x_{2}\cdots + k_{s}x_{s}$当$k_{1} + k_{2} + \cdots + k_{s} &#x3D; 1$时仍为$Ax &#x3D;b$的解；但当$k_{1} + k_{2} + \cdots + k_{s} &#x3D; 0$时，则为$Ax &#x3D;0$的解。特别$\frac{x_{1} + x_{2} }{2}$为$Ax &#x3D; b$的解；$2x_{3} - (x_{1} +x_{2})$为$Ax &#x3D; 0$的解。</p><p>(3) 非齐次线性方程组${Ax} &#x3D; b$无解$\Leftrightarrow r(A) + 1 &#x3D;r(\overline{A}) \Leftrightarrow b$不能由$A$的列向量$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$线性表示。</p><p><strong>4.奇次线性方程组的基础解系和通解，解空间，非奇次线性方程组的通解</strong></p><p>(1) 齐次方程组${Ax} &#x3D; 0$恒有解(必有零解)。当有非零解时，由于解向量的任意线性组合仍是该齐次方程组的解向量，因此${Ax}&#x3D; 0$的全体解向量构成一个向量空间，称为该方程组的解空间，解空间的维数是$n - r(A)$，解空间的一组基称为齐次方程组的基础解系。</p><p>(2) $\eta_{1},\eta_{2},\cdots,\eta_{t}$是${Ax} &#x3D; 0$的基础解系，即：</p><ol><li><p>$\eta_{1},\eta_{2},\cdots,\eta_{t}$是${Ax} &#x3D; 0$的解；</p></li><li><p>$\eta_{1},\eta_{2},\cdots,\eta_{t}$线性无关；</p></li><li><p>${Ax} &#x3D; 0$的任一解都可以由$\eta_{1},\eta_{2},\cdots,\eta_{t}$线性表出.<br>$k_{1}\eta_{1} + k_{2}\eta_{2} + \cdots + k_{t}\eta_{t}$是${Ax} &#x3D; 0$的通解，其中$k_{1},k_{2},\cdots,k_{t}$是任意常数。</p></li></ol><h2 id="6-矩阵的特征值和特征向量"><a href="#6-矩阵的特征值和特征向量" class="headerlink" title="6 矩阵的特征值和特征向量"></a>6 矩阵的特征值和特征向量</h2><h3 id="6-1-矩阵的特征值和特征向量的概念及性质"><a href="#6-1-矩阵的特征值和特征向量的概念及性质" class="headerlink" title="6.1 矩阵的特征值和特征向量的概念及性质"></a><strong>6.1 矩阵的特征值和特征向量的概念及性质</strong></h3><p>(1) 设$\lambda$是$A$的一个特征值，则 ${kA},{aA} + {bE},A^{2},A^{m},f(A),A^{T},A^{- 1},A^{*}$有一个特征值分别为<br>${kλ},{aλ} + b,\lambda^{2},\lambda^{m},f(\lambda),\lambda,\lambda^{- 1},\frac{|A|}{\lambda},$且对应特征向量相同（$A^{T}$ 例外）。</p><p>(2)若$\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$为$A$的$n$个特征值，则$\sum_{i&#x3D; 1}^{n}\lambda_{i} &#x3D; \sum_{i &#x3D; 1}^{n}a_{ {ii} },\prod_{i &#x3D; 1}^{n}\lambda_{i}&#x3D; |A|$ ,从而$|A| \neq 0 \Leftrightarrow A$没有特征值。</p><p>(3)设$\lambda_{1},\lambda_{2},\cdots,\lambda_{s}$为$A$的$s$个特征值，对应特征向量为$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，</p><p>若: $\alpha &#x3D; k_{1}\alpha_{1} + k_{2}\alpha_{2} + \cdots + k_{s}\alpha_{s}$ ,</p><p>则: $A^{n}\alpha &#x3D; k_{1}A^{n}\alpha_{1} + k_{2}A^{n}\alpha_{2} + \cdots +k_{s}A^{n}\alpha_{s} &#x3D; k_{1}\lambda_{1}^{n}\alpha_{1} +k_{2}\lambda_{2}^{n}\alpha_{2} + \cdots k_{s}\lambda_{s}^{n}\alpha_{s}$ 。</p><h3 id="6-2相似变换、相似矩阵的概念及性质"><a href="#6-2相似变换、相似矩阵的概念及性质" class="headerlink" title="6.2相似变换、相似矩阵的概念及性质"></a><strong>6.2相似变换、相似矩阵的概念及性质</strong></h3><p>(1) 若$A \sim B$，则</p><ol><li><p>$A^{T} \sim B^{T},A^{- 1} \sim B^{- 1},,A^{<em>} \sim B^{</em>}$</p></li><li><p>$|A| &#x3D; |B|,\sum_{i &#x3D; 1}^{n}A_{ {ii} } &#x3D; \sum_{i &#x3D;1}^{n}b_{ {ii} },r(A) &#x3D; r(B)$</p></li><li><p>$|\lambda E - A| &#x3D; |\lambda E - B|$，对$\forall\lambda$成立</p></li></ol><h3 id="6-3-矩阵可相似对角化的充分必要条件"><a href="#6-3-矩阵可相似对角化的充分必要条件" class="headerlink" title="6.3 矩阵可相似对角化的充分必要条件"></a><strong>6.3 矩阵可相似对角化的充分必要条件</strong></h3><p>(1)设$A$为$n$阶方阵，则$A$可对角化$\Leftrightarrow$对每个$k_{i}$重根特征值$\lambda_{i}$，有$n-r(\lambda_{i}E - A) &#x3D; k_{i}$</p><p>(2) 设$A$可对角化，则由$P^{- 1}{AP} &#x3D; \Lambda,$有$A &#x3D; {PΛ}P^{-1}$，从而$A^{n} &#x3D; P\Lambda^{n}P^{- 1}$</p><p>(3) 重要结论</p><ol><li><p>若$A \sim B,C \sim D$，则$\begin{bmatrix}  A &amp; O \ O &amp; C \\end{bmatrix} \sim \begin{bmatrix} B &amp; O \  O &amp; D \\end{bmatrix}$.</p></li><li><p>若$A \sim B$，则$f(A) \sim f(B),\left| f(A) \right| \sim \left| f(B)\right|$，其中$f(A)$为关于$n$阶方阵$A$的多项式。</p></li><li><p>若$A$为可对角化矩阵，则其非零特征值的个数(重根重复计算)＝秩($A$)</p></li></ol><h3 id="6-4-实对称矩阵的特征值、特征向量及相似对角阵"><a href="#6-4-实对称矩阵的特征值、特征向量及相似对角阵" class="headerlink" title="6.4 实对称矩阵的特征值、特征向量及相似对角阵"></a><strong>6.4 实对称矩阵的特征值、特征向量及相似对角阵</strong></h3><p>(1)相似矩阵：设$A,B$为两个$n$阶方阵，如果存在一个可逆矩阵$P$，使得$B &#x3D;P^{- 1}{AP}$成立，则称矩阵$A$与$B$相似，记为$A \sim B$。</p><p>(2)相似矩阵的性质：如果$A \sim B$则有：</p><ol><li><p>$A^{T} \sim B^{T}$</p></li><li><p>$A^{- 1} \sim B^{- 1}$ （若$A$，$B$均可逆）</p></li><li><p>$A^{k} \sim B^{k}$ （$k$为正整数）</p></li><li><p>$\left| {λE} - A \right| &#x3D; \left| {λE} - B \right|$，从而$A,B$<br>有相同的特征值</p></li><li><p>$\left| A \right| &#x3D; \left| B \right|$，从而$A,B$同时可逆或者不可逆</p></li><li><p>秩$\left( A \right) &#x3D;$秩$\left( B \right),\left| {λE} - A \right| &#x3D;\left| {λE} - B \right|$，$A,B$不一定相似</p></li></ol><h2 id="7-二次型"><a href="#7-二次型" class="headerlink" title="7 二次型"></a>7 二次型</h2><h3 id="7-1-mathbf-n-个变量-mathbf-x-mathbf-1-mathbf-mathbf-x-mathbf-2-mathbf-cdots-mathbf-x-mathbf-n-的二次齐次函数"><a href="#7-1-mathbf-n-个变量-mathbf-x-mathbf-1-mathbf-mathbf-x-mathbf-2-mathbf-cdots-mathbf-x-mathbf-n-的二次齐次函数" class="headerlink" title="7.1 $\mathbf{n}$个变量$\mathbf{x}{\mathbf{1} }\mathbf{,}\mathbf{x}{\mathbf{2} }\mathbf{,\cdots,}\mathbf{x}_{\mathbf{n} }$的二次齐次函数"></a><strong>7.1</strong> $\mathbf{n}$<strong>个变量</strong>$\mathbf{x}<em>{\mathbf{1} }\mathbf{,}\mathbf{x}</em>{\mathbf{2} }\mathbf{,\cdots,}\mathbf{x}_{\mathbf{n} }$<strong>的二次齐次函数</strong></h3><p>$f(x_{1},x_{2},\cdots,x_{n}) &#x3D; \sum_{i &#x3D; 1}^{n}{\sum_{j &#x3D;1}^{n}{a_{ {ij} }x_{i}y_{j} }}$，其中$a_{ {ij} } &#x3D; a_{ {ji} }(i,j &#x3D;1,2,\cdots,n)$，称为$n$元二次型，简称二次型. 若令$x &#x3D; \ \begin{bmatrix}x_{1} \ x_{1} \  \vdots \ x_{n} \ \end{bmatrix},A &#x3D; \begin{bmatrix}  a_{11}&amp; a_{12}&amp; \cdots &amp; a_{1n} \  a_{21}&amp; a_{22}&amp; \cdots &amp; a_{2n} \ \cdots &amp;\cdots &amp;\cdots &amp;\cdots \  a_{n1}&amp; a_{n2} &amp; \cdots &amp; a_{ {nn} } \\end{bmatrix}$,这二次型$f$可改写成矩阵向量形式$f &#x3D;x^{T}{Ax}$，其中$A$称为二次型矩阵。写得清楚些，我们可以看到：</p><p>$$<br>x^{T} A x&#x3D;\sum_{i&#x3D;1}^{n} x_{i}(A x)<em>{i}&#x3D;\sum</em>{i&#x3D;1}^{n} x_{i}\left(\sum_{j&#x3D;1}^{n} A_{i j} x_{j}\right)&#x3D;\sum_{i&#x3D;1}^{n} \sum_{j&#x3D;1}^{n} A_{i j} x_{i} x_{j}<br>$$</p><p>注意：<br>$<br>x^{T} A x&#x3D;\left(x^{T} A x\right)^{T}&#x3D;x^{T} A^{T} x&#x3D;x^{T}\left(\frac{1}{2} A+\frac{1}{2} A^{T}\right) x<br>$第一个等号的是因为是标量的转置与自身相等，而第二个等号是因为是我们平均两个本身相等的量。 由此，我们可以得出结论，只有$A$的对称部分有助于形成二次型。 出于这个原因，我们经常隐含地假设以二次型出现的矩阵是对称阵。<br>我们给出以下定义：</p><ul><li><p>对于所有非零向量$x \in \mathbb{R}^n$，$x^TAx&gt;0$，对称阵$A \in \mathbb{S}^n$为<strong>正定</strong>（<strong>positive definite,PD</strong>）。这通常表示为$A\succ0$（或$A&gt;0$），并且通常将所有正定矩阵的集合表示为$\mathbb{S}_{++}^n$。</p></li><li><p>对于所有向量$x^TAx\geq 0$，对称矩阵$A \in \mathbb{S}^n$是<strong>半正定</strong>(<strong>positive semidefinite ,PSD</strong>)。 这写为（或$A \succeq 0$仅$A≥0$），并且所有半正定矩阵的集合通常表示为$\mathbb{S}_+^n$。</p></li><li><p>同样，对称矩阵$A \in \mathbb{S}^n$是<strong>负定</strong>（<strong>negative definite,ND</strong>），如果对于所有非零$x \in \mathbb{R}^n$，则$x^TAx &lt;0$表示为$A\prec0$（或$A &lt;0$）。</p></li><li><p>类似地，对称矩阵$A \in \mathbb{S}^n$是<strong>半负定</strong>(<strong>negative semidefinite,NSD</strong>），如果对于所有$x \in \mathbb{R}^n$，则$x^TAx \leq 0$表示为$A\preceq 0$（或$A≤0$）。</p></li><li><p>最后，对称矩阵$A \in \mathbb{S}^n$是<strong>不定</strong>的，如果它既不是正半定也不是负半定，即，如果存在$x_1,x_2 \in \mathbb{R}^n$，那么$x_1^TAx_1&gt;0$且$x_2^TAx_2&lt;0$。</p></li></ul><p>很明显，如果$A$是正定的，那么$−A$是负定的，反之亦然。同样，如果$A$是半正定的，那么$−A$是是半负定的，反之亦然。如果果$A$是不定的，那么$−A$是也是不定的。</p><p>正定矩阵和负定矩阵的一个重要性质是它们总是满秩，因此是可逆的。为了了解这是为什么，假设某个矩阵$A \in \mathbb{S}^n$不是满秩。然后，假设$A$的第$j$列可以表示为其他$n-1$列的线性组合：<br>$<br>a_{j}&#x3D;\sum_{i \neq j} x_{i} a_{i}<br>$对于某些$x_1,\cdots x_{j-1},x_{j + 1} ,\cdots ,x_n\in \mathbb{R}$。设$x_j &#x3D; -1$，则：<br>$x&#x3D;\sum_{i \neq j} x_{i} a_{i}&#x3D;0<br>$意味着对于某些非零向量$x$，$x^T Ax &#x3D; 0$，因此$A$必须既不是正定也不是负定。如果$A$是正定或负定，则必须是满秩。<br>最后，有一种类型的正定矩阵经常出现，因此值得特别提及。 给定矩阵$A  \in \mathbb{R}^{m \times n}$（不一定是对称或偶数平方），矩阵$G &#x3D; A^T A$（有时称为<strong>Gram矩阵</strong>）总是半正定的。 此外，如果$m\geq n$（同时为了方便起见，我们假设$A$是满秩），则$G &#x3D; A^T A$是正定的。</p><h3 id="7-2-惯性定理，二次型的标准形和规范形"><a href="#7-2-惯性定理，二次型的标准形和规范形" class="headerlink" title="7.2 惯性定理，二次型的标准形和规范形"></a><strong>7.2 惯性定理，二次型的标准形和规范形</strong></h3><p>(1) 惯性定理</p><p>对于任一二次型，不论选取怎样的合同变换使它化为仅含平方项的标准型，其正负惯性指数与所选变换无关，这就是所谓的惯性定理。</p><p>(2) 标准形</p><p>二次型$f &#x3D; \left( x_{1},x_{2},\cdots,x_{n} \right) &#x3D;x^{T}{Ax}$经过合同变换$x &#x3D; {Cy}$化为$f &#x3D; x^{T}{Ax} &#x3D;y^{T}C^{T}{AC}$</p><p>$y &#x3D; \sum_{i &#x3D; 1}^{r}{d_{i}y_{i}^{2} }$称为 $f(r \leq n)$的标准形。在一般的数域内，二次型的标准形不是唯一的，与所作的合同变换有关，但系数不为零的平方项的个数由$r(A)$唯一确定。</p><p>(3) 规范形</p><p>任一实二次型$f$都可经过合同变换化为规范形$f &#x3D; z_{1}^{2} + z_{2}^{2} + \cdots z_{p}^{2} - z_{p + 1}^{2} - \cdots -z_{r}^{2}$，其中$r$为$A$的秩，$p$为正惯性指数，$r -p$为负惯性指数，且规范型唯一。</p><h3 id="7-3-用正交变换和配方法化二次型为标准形，二次型及其矩阵的正定性"><a href="#7-3-用正交变换和配方法化二次型为标准形，二次型及其矩阵的正定性" class="headerlink" title="7.3 用正交变换和配方法化二次型为标准形，二次型及其矩阵的正定性"></a><strong>7.3 用正交变换和配方法化二次型为标准形，二次型及其矩阵的正定性</strong></h3><p>设$A$正定$\Rightarrow {kA}(k &gt; 0),A^{T},A^{- 1},A^{*}$正定；$|A| &gt;0$,$A$可逆；$a_{ {ii} } &gt; 0$，且$|A_{ {ii} }| &gt; 0$</p><p>$A$，$B$正定$\Rightarrow A +B$正定，但${AB}$，${BA}$不一定正定</p><p>$A$正定$\Leftrightarrow f(x) &#x3D; x^{T}{Ax} &gt; 0,\forall x \neq 0$</p><p>$\Leftrightarrow A$的各阶顺序主子式全大于零</p><p>$\Leftrightarrow A$的所有特征值大于零</p><p>$\Leftrightarrow A$的正惯性指数为$n$</p><p>$\Leftrightarrow$存在可逆阵$P$使$A &#x3D; P^{T}P$</p><p>$\Leftrightarrow$存在正交矩阵$Q$，使$Q^{T}{AQ} &#x3D; Q^{- 1}{AQ} &#x3D;\begin{pmatrix} \lambda_{1} &amp; &amp; \ \begin{matrix}  &amp; \  &amp; \ \end{matrix} &amp;\ddots &amp; \  &amp; &amp; \lambda_{n} \ \end{pmatrix},$</p><p>其中$\lambda_{i} &gt; 0,i &#x3D; 1,2,\cdots,n.$正定$\Rightarrow {kA}(k &gt;0),A^{T},A^{- 1},A^{*}$正定； $|A| &gt; 0,A$可逆；$a_{ {ii} } &gt;0$，且$|A_{ {ii} }| &gt; 0$ 。 }| &gt; 0$ 。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Mathematics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning Foundation——矩阵求导</title>
    <link href="/2022/08/01/MachineLearning/%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC/"/>
    <url>/2022/08/01/MachineLearning/%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC/</url>
    
    <content type="html"><![CDATA[<h1 id="常用的矩阵求导"><a href="#常用的矩阵求导" class="headerlink" title="常用的矩阵求导"></a><strong>常用的矩阵求导</strong></h1><p>$$<br>\frac{\partial{\beta^T \mathbb{x}}}{\partial{\mathbb{x}}}&#x3D;\beta \<br>\frac{\partial{\mathbb{x}^T \mathbb{x}}}{\partial{\mathbb{x}}}&#x3D;2\mathbb{x} \<br>\frac{\partial{\mathbb{x}^T A\mathbb{x}}}{\partial{\mathbb{x}}}&#x3D;(A+A^T)\mathbb{x} \<br>tr(a)&#x3D;a \<br>tr(AB)&#x3D;tr(BA) \<br>tr(ABC)&#x3D;tr(BCA)&#x3D;tr(CAB) \<br>\frac{\partial tr(AB)}{\partial A} &#x3D; B^T \<br>tr(A)&#x3D;tr(A^T) \<br>\frac{\partial tr(ABA^TC)}{\partial A}&#x3D;CAB+C^TAB^T<br>$$</p><p><img src="/src/image-20211229220010048.png" alt="image-20211229220010048"></p><p><img src="/%5Csrc%5Cimage-20211229220137867.png" alt="image-20211229220137867"></p><p><img src="/%5Csrc%5Cimage-20211229220247720.png" alt="image-20211229220247720"></p><p><img src="/%5Csrc%5Cimage-20211229214110342.png" alt="image-20211229214110342"></p><h1 id="1-矩阵向量求导引入"><a href="#1-矩阵向量求导引入" class="headerlink" title="1. 矩阵向量求导引入"></a>1. 矩阵向量求导引入</h1><p>　　　　在高等数学里面，我们已经学过了标量对标量的求导，比如标量$y$对标量$x$的求导，可以表示为$\frac{\partial y}{\partial x}$。</p><p>　　　　有些时候，我们会有一组标量$y_i,i&#x3D;1,2,\dots,m$来对一个标量$x$的求导,那么我们会得到一组标量求导的结果：<br>$<br>\frac{\partial y}{\partial x},i&#x3D;1,2,\dots,m<br>$<br>　　　　如果我们把这组标量写成向量的形式，即得到维度为m的一个向量$\mathbb{y}$对一个标量$x$的求导，那么结果也是一个m维的向量：$\frac{\partial \mathbb{y}}{\partial x}$</p><p>　　　　可见，所谓向量对标量的求导，其实就是向量里的每个分量分别对标量求导，最后把求导的结果排列在一起，按一个向量表示而已。类似的结论也存在于标量对向量的求导，向量对向量的求导，向量对矩阵的求导，矩阵对向量的求导，以及矩阵对矩阵的求导等。</p><p>　　　　总而言之，所谓的向量矩阵求导本质上就是多元函数求导，仅仅是把把函数的自变量，因变量以及标量求导的结果排列成了向量矩阵的形式，方便表达与计算，更加简洁而已。</p><p>　　　　为了便于描述，后面如果没有指明，则求导的自变量用$x$表示标量，$ \mathbb{x}$表示n维向量，$X$表示m×n维度的矩阵，求导的因变量用$y$表示标量，$ \mathbb{y}$表示m维向量，$Y$表示p×q维度的矩阵。</p><h1 id="2-矩阵向量求导定义"><a href="#2-矩阵向量求导定义" class="headerlink" title="2. 矩阵向量求导定义"></a>2. 矩阵向量求导定义</h1><p>　　　　根据求导的自变量和因变量是标量，向量还是矩阵，我们有9种可能的矩阵求导定义，如下：</p><table><thead><tr><th>自变量\因变量</th><th>标量$y$</th><th>向量$\mathbb{y}$</th><th>矩阵$Y$</th></tr></thead><tbody><tr><td>标量$x$</td><td>$\frac{\partial y}{\partial x}$</td><td>$\frac{\partial \mathbb{y}}{\partial x}$</td><td>$\frac{\partial Y}{\partial x}$</td></tr><tr><td>向量$ \mathbb{x}$</td><td>$\frac{\partial y}{\partial \mathbb{x}}$</td><td>$\frac{\partial \mathbb{y}}{\partial \mathbb{x}}$</td><td>$\frac{\partial Y}{\partial \mathbb{x}}$</td></tr><tr><td>矩阵$X$</td><td>$\frac{\partial y}{\partial X}$</td><td>$\frac{\partial \mathbb{y}}{\partial X}$</td><td>$\frac{\partial Y}{\partial X}$</td></tr></tbody></table><p>　　　　这9种里面，标量对标量的求导高数里面就有，不需要我们单独讨论，在剩下的8种情况里面，我们先讨论上图中标量对向量或矩阵求导，向量或矩阵对标量求导，以及向量对向量求导这5种情况。另外三种向量对矩阵的求导，矩阵对向量的求导，以及矩阵对矩阵的求导我们在后面再讲。</p><p>　　　　现在我们回看第一节讲到的例子，维度为m的一个向量$\mathbb{y}$对一个标量$x$的求导，那么结果也是一个m维的向量：$\frac{\partial \mathbb{y}}{\partial x}$。这是我们表格里面向量对标量求导的情况。这里有一个问题没有讲到，就是这个m维的求导结果排列成的m维向量到底应该是列向量还是行向量？</p><p>　　　　这个问题的答案是：<strong>行向量或者列向量皆可</strong>！毕竟我们求导的本质只是把标量求导的结果排列起来，至于是按行排列还是按列排列都是可以的。但是这样也有问题，在我们机器学习算法法优化过程中，如果行向量或者列向量随便写，那么结果就不唯一，乱套了。</p><p>　　　　为了解决这个问题，我们引入求导布局的概念。</p><h1 id="3-矩阵向量求导布局"><a href="#3-矩阵向量求导布局" class="headerlink" title="3. 矩阵向量求导布局"></a>3. 矩阵向量求导布局</h1><p>　　　　为了解决矩阵向量求导的结果不唯一，我们引入求导布局。最基本的求导布局有两个：分子布局(numerator layout)和分母布局(denominator layout )。</p><p>　　　　对于分子布局来说，我们求导结果的维度以分子为主，比如对于我们上面对标量求导的例子，结果的维度和分子的维度是一致的。也就是说，如果向量$\mathbb{y}$是一个m维的列向量，那么求导结果$\frac{\partial \mathbb{y}}{\partial x}$也是一个m维列向量。如果如果向量$\mathbb{y}$是一个m维的行向量，那么求导结果$\frac{\partial \mathbb{y}}{\partial x}$也是一个m维行向量。</p><p>　　　　对于分母布局来说，我们求导结果的维度以分母为主，比如对于我们上面对标量求导的例子，如果向量$\mathbb{y}$是一个m维的列向量，那么求导结果$\frac{\partial \mathbb{y}}{\partial x}$是一个m维行向量。如果如果向量$\mathbb{y}$是一个m维的行向量，那么求导结果$\frac{\partial \mathbb{y}}{\partial x}$是一个m维的列向量向量。</p><p>　　　　可见，对于分子布局和分母布局的结果来说，两者相差一个<strong>转置</strong>。</p><p>　　　　再举一个例子，标量$y$对矩阵$X$求导，那么如果按分母布局，则求导结果的维度和矩阵$X$的维度m×n是一致的。如果是分子布局，则求导结果的维度为n×m。</p><p>　　　　这样，对于标量对向量或者矩阵求导，向量或者矩阵对标量求导这4种情况，对应的分子布局和分母布局的排列方式已经确定了。</p><p>　　　　稍微麻烦点的是向量对向量的求导，本文只讨论列向量对列向量的求导，其他的行向量求导只是差一个转置而已。比如m维列向量$\mathbb{y}$对n维列向量$ \mathbb{x}$求导。它的求导结果在分子布局和分母布局各是什么呢？对于这2个向量求导，那么一共有mn个标量对标量的求导。求导的结果一般是排列为一个矩阵。如果是分子布局，则矩阵的第一个维度以分子为准，即结果是一个m×n的矩阵，如下：<br>$<br>\frac{\partial \mathbb{y}}{\partial \mathbb{x}}&#x3D;\begin{pmatrix}<br>\frac{\partial \mathbb{y_1}}{\partial \mathbb{x_1}} &amp; \frac{\partial \mathbb{y_1}}{\partial \mathbb{x_2}} &amp; \ldots &amp; \frac{\partial \mathbb{y_!}}{\partial \mathbb{x_n}} \<br>\frac{\partial \mathbb{y_2}}{\partial \mathbb{x_1}} &amp; \frac{\partial \mathbb{y_2}}{\partial \mathbb{x_2}} &amp; \ldots &amp; \frac{\partial \mathbb{y_2}}{\partial \mathbb{x_n}} \<br>\ldots &amp; \ldots &amp; \ldots &amp; \ldots \<br>\frac{\partial \mathbb{y_m}}{\partial \mathbb{x_1}} &amp; \frac{\partial \mathbb{y_m}}{\partial \mathbb{x_2}} &amp; \ldots &amp; \frac{\partial \mathbb{y_m}}{\partial \mathbb{x_n}} \<br>\end{pmatrix}_{m\times n}<br>$　　　　上边这个按分子布局的向量对向量求导的结果矩阵，我们一般叫做雅克比 (Jacobian)矩阵。有的资料上会使用$\frac{\partial \mathbb{y}}{\partial \mathbb{x}^T}$来定义雅克比矩阵，意义是一样的。</p><p>　　　　如果是按分母布局，则求导的结果矩阵的第一维度会以分母为准，即结果是一个n×mn×m的矩阵，如下：</p><p>$$<br>\frac{\partial \mathbb{y}}{\partial \mathbb{x}}&#x3D;\begin{pmatrix}<br>\frac{\partial \mathbb{y_1}}{\partial \mathbb{x_1}} &amp; \frac{\partial \mathbb{y_2}}{\partial \mathbb{x_1}} &amp; \ldots &amp; \frac{\partial \mathbb{y_m}}{\partial \mathbb{x_1}} \<br>\frac{\partial \mathbb{y_1}}{\partial \mathbb{x_2}} &amp; \frac{\partial \mathbb{y_2}}{\partial \mathbb{x_2}} &amp; \ldots &amp; \frac{\partial \mathbb{y_m}}{\partial \mathbb{x_2}} \<br>\ldots &amp; \ldots &amp; \ldots &amp; \ldots \<br>\frac{\partial \mathbb{y_1}}{\partial \mathbb{x_n}} &amp; \frac{\partial \mathbb{y_2}}{\partial \mathbb{x_n}} &amp; \ldots &amp; \frac{\partial \mathbb{y_m}}{\partial \mathbb{x_n}} \<br>\end{pmatrix}_{n\times m}<br>$$</p><p>　　　　上边这个按分母布局的向量对向量求导的结果矩阵，我们一般叫做梯度矩阵。有的资料上会使用$\frac{\partial \mathbb{y}^T}{\partial \mathbb{x}}$来定义梯度矩阵，意义是一样的。</p><p>　　　　有了布局的概念，我们对于上面5种求导类型，可以各选择一种布局来求导。但是对于某一种求导类型，不能同时使用分子布局和分母布局求导。</p><p>　　　　但是在机器学习算法原理的资料推导里，我们并没有看到说正在使用什么布局，也就是说布局被隐含了，这就需要自己去推演，比较麻烦。但是一般来说我们会使用一种叫混合布局的思路，即如果是向量或者矩阵对标量求导，则使用分子布局为准，如果是标量对向量或者矩阵求导，则以分母布局为准。对于向量对对向量求导，有些分歧，我的所有文章中会以分子布局的雅克比矩阵为主。</p><p>　　　　具体总结如下：</p><table><thead><tr><th>自变量\因变量</th><th>标量$y$</th><th>向量$\mathbb{y}$</th><th>矩阵$Y$</th></tr></thead><tbody><tr><td>标量$x$</td><td>&#x2F;</td><td>$\frac{\partial \mathbb{y}}{\partial x}$<br />分子布局：m维列向量（默认布局）分母布局：m维行向量</td><td>$\frac{\partial Y}{\partial x}$<br />分子布局：p×q矩阵（默认布局）<br />分母布局：q×p矩阵</td></tr><tr><td>列向量$\mathbb{x}$</td><td>$\frac{\partial y}{\partial \mathbb{x}}$<br />分子布局：n维行向量<br />分母布局：n维列向量（默认布局）</td><td>$\frac{\partial \mathbb{y}}{\partial \mathbb{x}}$<br />分子布局：m×n雅克比矩阵（默认布局）<br />分母布局：n×m梯度矩阵</td><td>&#x2F;</td></tr><tr><td>矩阵$X$</td><td>$\frac{\partial y}{\partial X}$<br />分子布局：n×m矩阵<br />分母布局：m×n矩阵（默认布局）</td><td>&#x2F;</td><td>&#x2F;</td></tr></tbody></table><h1 id="4-用定义法求解标量对向量求导"><a href="#4-用定义法求解标量对向量求导" class="headerlink" title="4. 用定义法求解标量对向量求导"></a>4. 用定义法求解标量对向量求导</h1><p>　　　　标量对向量求导，严格来说是实值函数对向量的求导。即定义实值函数$f:\mathbb{R}^n \to \mathbb{R}$,自变量$\mathbb{x}$是n维向量，而输出$y$是标量。对于一个给定的实值函数，如何求解$\frac{\partial y}{\partial\mathbb{x}}$呢？</p><p>　　　　首先我们想到的是基于矩阵求导的定义来做，由于所谓标量对向量的求导，其实就是标量对向量里的每个分量分别求导，最后把求导的结果排列在一起，按一个向量表示而已。那么我们可以将实值函数对向量的每一个分量来求导，最后找到规律，得到求导的结果向量。</p><p>　　　　首先我们来看一个简单的例子：$y&#x3D;a^T\mathbb{x}$,求解$\frac{\partial a^T\mathbb{x}}{\partial\mathbb{x}}$</p><p>　　　　根据定义，我们先对$ \mathbb{x}$的第i个分量进行求导，这是一个标量对标量的求导，如下：<br>$<br>\frac{\partial a^T\mathbb{x}}{\partial\mathbb{x_i}}&#x3D;\frac{\partial \sum_{j&#x3D;1}^n a_j\mathbb{x}_j}{\partial\mathbb{x_i}}&#x3D;\frac{\partial a_i\mathbb{x_i}}{\partial\mathbb{x_i}}&#x3D;a_i<br>$可见，对向量的第i个分量的求导结果就等于向量$a$的第i个分量。由于我们是分母布局，最后所有求导结果的分量组成的是一个n维向量。那么其实就是向量$a$。也就是说：<br>$frac{\partial a^T\mathbb{x}}{\partial\mathbb{x}} &#x3D; a$<br>　同样的思路，我们也可以直接得到：<br>$<br>$ac{\partial \mathbb{x}^T a}{\partial\mathbb{x}} &#x3D;a<br>$　　$再来看一个复杂一点点的例子：$y&#x3D;\mathbb{x}^TA\mathbb{x}$,求解$\frac{\partial \mathbb{x}^TA\mathbb{x}}{\partial\mathbb{x}}$</p><p>　　　　我们对$\mathbb{x}$的第k个分量进行求导如下：<br>$<br>\frac{\partial \mathbb{x}^TA\mathbb{x}}{\partial\mathbb{x}<em>k}&#x3D; \frac{\partial \sum</em>{i&#x3D;1}^n \sum_{j&#x3D;1}^n\mathbb{x}<em>iA</em>{ij}\mathbb{x}<em>j}{\partial\mathbb{x}<em>k}&#x3D;\sum</em>{i&#x3D;1}^nA</em>{ik}x_i+\sum_{j&#x3D;1}^nA_{kj}x_j<br>$　　　　这个第k个分量的求导结果稍微复杂些了，仔细观察一下，第一部分是矩阵$A$的第k列转置后和$ \mathbb{x}$相乘得到，第二部分是矩阵$A$的第k行和$\mathbb{x}$相乘得到，排列好就是:<br>$frac{\partial \mathbb{x}^TA\mathbb{x}}{\partial\mathbb{x}} &#x3D; A^T\mathbb{x}+A\mathbb{x}<br>$　　从上面可以看出，定义法求导对于简单的实值函数是很容易的，但是复杂的实值函数就算求出了任意一个分量的导数，要排列出最终的求导结果还挺麻烦的，因此我们需要找到其他的简便一些的方法来整体求导，而不是每次都先去针对任意一个分量，再进行排列。</p><h1 id="5-标量对向量求导的一些基本法则"><a href="#5-标量对向量求导的一些基本法则" class="headerlink" title="5. 标量对向量求导的一些基本法则"></a>5. 标量对向量求导的一些基本法则</h1><p>　　　　在我们寻找一些简单的方法前，我们简单看下标量对向量求导的一些基本法则，这些法则和标量对标量求导的过程类似。</p><p>　　　　1） 常量对向量的求导结果为0。</p><p>　　　　2）线性法则：如果$f,g$都是实值函数，$c_1,c_2$为常数，则：<br>$<br>\frac{\partial(c_1f(x)+c_2g(x))}{\partial x}&#x3D;c_1\frac{\partial f(x)}{\partial x} +c_2\frac{\partial g(x)}{\partial x}<br>$　　　　3) 乘法法则：如果$f,g$都是实值函数，则：<br>$frac{\partial f(x)g(x)}{\partial x}&#x3D;f(x)\frac{\partial g(x)}{\partial x} +\frac{\partial f(x)}{\partial x}g(x)<br>$　　要注意的是如果不是实值函数，则不能这么使用乘法法则。</p><p>　　　　4) 除法法则：如果$f,g$都是实值函数，且$g(x)\neq 0$，则：<br>$<br>\frac{\partial f(x)&#x2F;g(x)}{\partial x}&#x3D;\frac{1}{g^2(x)}(g(x)\frac{\partial f(x)}{\partial x}-f(x)\frac{\partial g(x)}{\partial x})<br>$</p><h1 id="6-用定义法求解标量对矩阵求导"><a href="#6-用定义法求解标量对矩阵求导" class="headerlink" title="6. 用定义法求解标量对矩阵求导"></a>6. 用定义法求解标量对矩阵求导</h1><p> 　　　现在我们来看看定义法如何解决标量对矩阵的求导问题。其实思路和第一节的标量对向量的求导是类似的,只是最后的结果是一个和自变量同型的矩阵。</p><p>　　　　我们还是以一个例子来说明。$y&#x3D;\mathbb{a}^TX\mathbb{b} $,求解$\frac{\partial \mathbb{a}^TX\mathbb{b}}{\partial X}$</p><p>　　　　其中, $a$是m维向量,$b$是n维向量, $X$是m×n的矩阵。</p><p>　　　　我们对矩阵$X$的任意一个位置的$X_{ij}$求导，如下：<br>$<br>\frac{\partial \mathbb{a}^TX\mathbb{b}}{\partial X_{ij}}&#x3D;\frac{\partial \sum_{p&#x3D;1}^{m}  \sum_{q&#x3D;1}^{n}\mathbb{a}<em>p X</em>{pq}\mathbb{b}<em>q}{\partial X</em>{ij}}&#x3D;\frac{\partial \mathbb{a}<em>i X</em>{ij}\mathbb{b}<em>j}{\partial X</em>{ij}}&#x3D;\mathbb{a}_i\mathbb{b}_j<br>$　　　　即求导结果在$(i,j)$位置的求导结果是$\mathbb{a}$向量第$i$个分量和$\mathbb{b}$第$j$个分量的乘积，将所有的位置的求导结果排列成一个m×n的矩阵，即为$\mathbb{a}\mathbb{b}^T$,这样最后的求导结果为：<br>$frac{\partial \mathbb{a}^TX\mathbb{b}}{\partial X} &#x3D;\mathbb{a}\mathbb{b}^T<br>$　　简单的求导的确不难，但是如果是比较复杂的标量对矩阵求导，比如$y&#x3D;\mathbb{a}^Texp(X\mathbb{b}) $,对任意标量求导容易，排列起来还是蛮麻烦的，也就是我们遇到了和标量对向量求导一样的问题，定义法比较适合解决简单的问题，复杂的求导需要更简便的方法。这个方法我们在下一篇来讲。</p><p>　　　　同时，标量对矩阵求导也有和第二节对向量求导类似的基本法则，这里就不累述了。</p><h1 id="7-用定义法求解向量对向量求导"><a href="#7-用定义法求解向量对向量求导" class="headerlink" title="7. 用定义法求解向量对向量求导"></a>7. 用定义法求解向量对向量求导</h1><p>　　　　这里我们也同样给出向量对向量求导的定义法的具体例子。</p><p>　　　　先来一个简单的例子: $\mathbb{y}&#x3D;A\mathbb{x}$,其中$A$为n×m的矩阵。$\mathbb{x},\mathbb{y}$分别为m,n维向量。需要求导$\frac{\partial A\mathbb{x}}{\partial \mathbb{x}}$,根据定义，结果应该是一个n×m的矩阵</p><p>　　　　先求矩阵的第$i$行和向量的内积对向量的第$j$分量求导，用定义法求解过程如下：<br>$<br>\frac{\partial A_i\mathbb{x}}{\partial \mathbb{x_j}}&#x3D;\frac{\partial A_{ij}\mathbb{x}<em>j}{\partial \mathbb{x_j}}&#x3D;A</em>{ij}<br>$　　　　可见矩阵 $A$的第$i$行和向量的内积对向量的第$j$分量求导的结果就是矩阵 $A$的$(i,j)$位置的值。排列起来就是一个矩阵了，由于我们分子布局，所以排列出的结果是$A$,而不是 $A^T$</p><h1 id="8-矩阵微分"><a href="#8-矩阵微分" class="headerlink" title="8. 矩阵微分"></a>8. 矩阵微分</h1><p>　　　　在高数里面我们学习过标量的导数和微分，他们之间有这样的关系：$df&#x3D;{f}’(x)dx$。如果是多变量的情况，则微分可以写成：<br>$<br>df&#x3D;\sum_{i&#x3D;1}^n\frac{\partial f}{\partial x_i}dx_i&#x3D;(\frac{\partial f}{\partial \mathbb{x}})^Td\mathbb{x}<br>$　　　　从上次我们可以发现标量对向量的求导和它的向量微分有一个转置的关系。</p><p>　　　　现在我们再推广到矩阵。对于矩阵微分，我们的定义为：<br>$<br>df&#x3D;\sum_{i&#x3D;1}^n\sum_{j&#x3D;1}^n\frac{\partial f}{\partial X_{ij}}dX_{ij}&#x3D;tr((\frac{\partial f}{\partial X})^TdX)<br>$　　　　其中第二步使用了矩阵迹的性质，即迹函数等于主对角线的和。即<br>$r(A^TB)&#x3D;\sum_{i,j}A_{ij}B_{ij}<br>$　从上面矩阵微分的式子，我们可以看到矩阵微分和它的导数也有一个转置的关系，不过在外面套了一个迹函数而已。由于标量的迹函数就是它本身，那么矩阵微分和向量微分可以统一表示，即：</p><p>$dtr((\frac{\partial f}{\partial X})^TdX)\<br>df&#x3D;tr((\frac{\partial f}{\partial \mathbb{x}})^Td\mathbb{x})<br>$</p><h1 id="9-矩阵微分的性质F"><a href="#9-矩阵微分的性质F" class="headerlink" title="9. 矩阵微分的性质F"></a>9. 矩阵微分的性质F</h1><p>　　　　我们在讨论如何使用矩阵微分来求导前，先看看矩阵微分的性质：</p><p>　　　　1）微分加减法：$d(X+Y)&#x3D;dX+dY,d(X−Y)&#x3D;dX−dY$</p><p>　　　　2) 微分乘法：$d(XY)&#x3D;(dX)Y+X(dY)$</p><p>　　　　3) 微分转置：$d(X^T)&#x3D;(dX)^T$</p><p>　　　　4) 微分的迹：$dtr(X)&#x3D;tr(dX)$</p><p>　　　　5) 微分哈达马乘积： $d(X⊙Y)&#x3D;X⊙dY+dX⊙Y$</p><p>　　　　6) 逐元素求导：$dσ(X)&#x3D;σ′(X)⊙dX$</p><p>　　　　7) 逆矩阵微分：$dX^{−1}&#x3D;−X^{−1}dXX^{−1}$</p><p>　　　　8) 行列式微分：$d|X|&#x3D;|X|tr(X^{−1}dX)　$</p><p>　　　　有了这些性质，我们再来看看如何由矩阵微分来求导数。</p><h1 id="10-使用微分法求解矩阵向量求导"><a href="#10-使用微分法求解矩阵向量求导" class="headerlink" title="10. 使用微分法求解矩阵向量求导"></a>10. 使用微分法求解矩阵向量求导</h1><p>　　　　由于第一节我们已经得到了矩阵微分和导数关系，现在我们就来使用微分法求解矩阵向量求导。</p><p>　　　　若标量函数$f$是矩阵$X$经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对ff求微分，再使用迹函数技巧给$df$套上迹并将其它项交换至$dX$左侧,那么对于迹函数里面在$dX$左边的部分，我们只需要加一个转置就可以得到导数了。</p><p>　　　　这里需要用到的迹函数的技巧主要有这么几个：</p><p>　　　　1) 标量的迹等于自己：$tr(x)&#x3D;x$</p><p>　　　　2) 转置不变：$tr(A^T)&#x3D;tr(A)$</p><p>　　　　3) 交换率：$tr(AB)&#x3D;tr(BA)$,需要满足$A,B^T$同维度。</p><p>　　　　4) 加减法：$tr(X+Y)&#x3D;tr(X)+tr(Y),tr(X−Y)&#x3D;tr(X)−tr(Y)$</p><p>　　　　5) 矩阵乘法和迹交换：$tr((A⊙B)^TC)&#x3D;tr(A^T(B⊙C))$,需要满足$A,B,C$同维度。</p><p>　　　　我们先看第一个例子，我们使用上一篇定义法中的一个求导问题：<br>$<br>y&#x3D;\mathbb{a}^TX\mathbb{b},\frac{\partial y}{\partial X}<br>$　　　　首先，我们使用微分乘法的性质对$f$求微分，得到：<br>$y&#x3D;d\mathbb{a}^TX\mathbb{b}+\mathbb{a}^TdX\mathbb{b}+\mathbb{a}^TXd\mathbb{b}&#x3D;\mathbb{a}^TdX\mathbb{b}<br>$　　第二步，就是两边套上迹函数，即：<br>$tr(dy)&#x3D;tr(\mathbb{a}^TdX\mathbb{b})&#x3D;tr(\mathbb{b}\mathbb{a}^TdX)<br>$其中第一到第二步使用了上面迹函数性质1，第三步到第四步用到了上面迹函数的性质3.</p><p>　　　　根据我们矩阵导数和微分的定义，迹函数里面在$dX$左边的部分$ba^T$,加上一个转置即为我们要求的导数，即：<br>$<br>\frac{\partial f}{\partial X}&#x3D;(\mathbb{b}\mathbb{a}^T)T&#x3D;\mathbb{a}\mathbb{b}^T<br>$　　　　以上就是微分法的基本流程，先求微分再做迹函数变换，最后得到求导结果。比起定义法，我们现在不需要去对矩阵中的单个标量进行求导了。</p><p>　　　　再来看看<br>$<br>y&#x3D;\mathbb{a}^Texp(X\mathbb{b}) ,\frac{\partial y}{\partial X}<br>$</p><p>$dy&#x3D;tr(dy)\<br>&#x3D;tr(\mathbb{a}^Tdexp(X\mathbb{b}))\<br>&#x3D;tr(\mathbb{a}^T(exp(X\mathbb{b})⊙d(X\mathbb{b}))) \<br>&#x3D;tr((\mathbb{a}⊙exp(X\mathbb{b}))^TdX\mathbb{b}) \<br>&#x3D;tr(\mathbb{b}(\mathbb{a}⊙exp(X\mathbb{b}))^TdX)$</p><p>　　　　其中第三步到第4步使用了上面迹函数的性质5. 这样我们的求导结果为：<br>$<br>\frac{\partial y}{\partial X}&#x3D;(\mathbb{a}⊙exp(X\mathbb{b}))\mathbb{b}^T<br>$</p><p>　　　　以上就是微分法的基本思路。</p><h1 id="11-迹函数对向量矩阵求导"><a href="#11-迹函数对向量矩阵求导" class="headerlink" title="11. 迹函数对向量矩阵求导"></a>11. 迹函数对向量矩阵求导</h1><p>　　　　由于微分法使用了迹函数的技巧，那么迹函数对对向量矩阵求导这一大类问题，使用微分法是最简单直接的。下面给出一些常见的迹函数的求导过程，也顺便给大家熟练掌握微分法的技巧。</p><p>　　　　首先是$\frac{\partial tr(AB)}{\partial A}&#x3D;B^T$,$\frac{\partial tr(AB)}{\partial B}&#x3D;A^T$这个直接根据矩阵微分的定义即可得到。</p><p>　　　　再来看看$\frac{\partial tr(W^TAW)}{\partial W}$:</p><p>$$<br>\begin{align}<br>d(tr(W^TAW))&amp;&#x3D;tr(dW^TAW+W^TAdW) \&amp;&#x3D;tr(dW^TAW)+tr(W^TAdW)\<br>&amp;&#x3D;tr((dW)^TAW)+tr(W^TAdW)\<br>&amp;&#x3D;tr(W^TA^TdW)+tr(W^TAdW) \<br>&amp;&#x3D;tr(W^T(A+A^T)dW)\</p><p>\end{align}<br>$$<br>　因此可以得到：$\frac{\partial tr(W^TAW)}{\partial W} &#x3D;(A+A^T)W$</p><h1 id="12-微分法求导小结"><a href="#12-微分法求导小结" class="headerlink" title="12. 微分法求导小结"></a>12. 微分法求导小结</h1><p>　　　　使用矩阵微分，可以在不对向量或矩阵中的某一元素单独求导再拼接，因此会比较方便，当然熟练使用的前提是对上面矩阵微分的性质，以及迹函数的性质熟练运用。</p><p>　　　　还有一些场景，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。如果我们可以利用一些常用的简单求导结果，再使用链式求导法则，则会非常的方便。因此下一篇我们讨论向量矩阵求导的链式法则。</p><h1 id="13-向量对向量求导的链式法则"><a href="#13-向量对向量求导的链式法则" class="headerlink" title="13. 向量对向量求导的链式法则"></a>13. 向量对向量求导的链式法则</h1><p>　　　　首先我们来看看向量对向量求导的链式法则。假设多个向量存在依赖关系，比如三个向量$\mathbb{x}\to \mathbb{y}\to \mathbb{z}$存在依赖关系，则我们有下面的链式求导法则：<br>$<br>\frac{\partial \mathbb{z}}{\partial \mathbb{x}}&#x3D;\frac{\partial \mathbb{z}}{\partial \mathbb{y}}\frac{\partial \mathbb{y}}{\partial \mathbb{x}}<br>$　　　　该法则也可以推广到更多的向量依赖关系。但是要注意的是要求所有有依赖关系的变量都是向量，如果有一个$Y$是矩阵，比如是$\mathbb{x}\to Y\to \mathbb{z}$， 则上式并不成立。</p><p>　　　　从矩阵维度相容的角度也很容易理解上面的链式法则，假设$\mathbb{x}, \mathbb{y}, \mathbb{z}$分别是m,n,p维向量，则求导结果$\frac{\partial \mathbb{z}}{\partial \mathbb{x}}$是一个p×m的雅克比矩阵，而右边$\frac{\partial \mathbb{z}}{\partial \mathbb{y}}$是一个p×n的雅克比矩阵，$\frac{\partial \mathbb{y}}{\partial \mathbb{x}}$是一个n×m的矩阵，两个雅克比矩阵的乘积维度刚好是p×m，和左边相容。</p><h1 id="14-标量对多个向量的链式求导法则"><a href="#14-标量对多个向量的链式求导法则" class="headerlink" title="14. 标量对多个向量的链式求导法则"></a>14. 标量对多个向量的链式求导法则</h1><p>　在我们的机器学习算法中，最终要优化的一般是一个标量损失函数，因此最后求导的目标是标量，无法使用上一节的链式求导法则，比如2向量，最后到1标量的依赖关系：$\mathbb{x}\to \mathbb{y} \to z$。(采用分子布局)<br>$<br>\frac{\partial z}{\partial \mathbb{x}}&#x3D;\frac{\partial z}{\partial \mathbb{y}}\frac{\partial \mathbb{y}}{\partial \mathbb{x}}<br>$</p><blockquote><p>假设你的布局不一致就会出现，无法直接使用链式法则。</p><p>假设$\mathbb{x}, \mathbb{y}$分别是m,n维向量, 那么$\frac{\partial z}{\partial \mathbb{x}}$的求导结果是一个m×1的向量, 而$\frac{\partial z}{\partial \mathbb{y}}$是一个n×1的向量，$\frac{\partial \mathbb{y}}{\partial \mathbb{x}}$是一个n×m的雅克比矩阵,右边的向量和矩阵是没法直接乘的。</p><p>　　　　但是假如我们把标量求导的部分都做一个转置，那么维度就可以相容了，也就是：<br>$<br>(\frac{\partial z}{\partial \mathbb{x}})^T&#x3D;(\frac{\partial z}{\partial \mathbb{y}})^T\frac{\partial \mathbb{y}}{\partial \mathbb{x}}<br>$　　　　但是毕竟我们要求导的是$\frac{\partial z}{\partial \mathbb{x}}$,而不是它的转置，因此两边转置我们可以得到标量对多个向量求导的链式法则：<br>$frac{\partial z}{\partial \mathbb{x}}&#x3D;(\frac{\partial \mathbb{y}}{\partial \mathbb{x}})^T\frac{\partial z}{\partial \mathbb{y}}<br>$　　如果是标量对更多的向量求导,比如$\mathbb{y}_1\to \mathbb{y}_2  \to \dots \to \mathbb{y}<em>n \to z$，则其链式求导表达式可以表示为：<br>$rac{\partial z}{\partial \mathbb{y}<em>1})^T&#x3D;(\frac{\partial \mathbb{y}<em>n}{\partial \mathbb{y}</em>{n-1}}\frac{\partial \mathbb{y}</em>{n-1}}{\partial \mathbb{y}</em>{n-2}}\dots\frac{\partial \mathbb{y}<em>2}{\partial \mathbb{y}</em>{1}})^T\frac{\partial z}{\partial \mathbb{y}_n}<br>$这里我们给一个最常见的最小二乘法求导的例子。最小二乘法优化的目标是最小化如下损失函数:<br>$theta-y)^T(X\theta-y)<br>$优化的损失函数$l$是一个标量，而模型参数$\theta$是一个向量，期望$L$对$ \theta$求导，并求出导数等于0时候的极值点。我们假设向量$z&#x3D;(X\theta-y)$, 则$l&#x3D;z^Tz,\theta \to z \to l$存在链式求导的关系，因此：<br>$partial l}{\partial \theta}&#x3D;(\frac{\partial z}{\partial \theta})^T\frac{\partial l}{\partial z}&#x3D;X^T(2z)&#x3D;2X^T(X\theta-y)$一步转换使用了如下求导公式：<br>$rtial (X\theta-y)}{\partial \theta}&#x3D;X\<br>\frac{\partial z^Tz}{\partial z}&#x3D;2z<br>$们在前几篇里已有求解过，现在可以直接拿来使用了，非常方便。</p></blockquote><p>　　　　当然上面的问题使用微分法求导数也是非常简单的，这里只是给出链式求导法的思路。</p><h1 id="15-标量对多个矩阵的链式求导法则"><a href="#15-标量对多个矩阵的链式求导法则" class="headerlink" title="15. 标量对多个矩阵的链式求导法则"></a>15. 标量对多个矩阵的链式求导法则</h1><p>　　　　下面我们再来看看标量对多个矩阵的链式求导法则，假设有这样的依赖关系：$X\to T\to z$,那么我们有：<br>$<br>\frac{\partial z}{\partial X_{ij}}&#x3D;\sum_{k,l}\frac{\partial z}{\partial Y_{kl}}\frac{\partial Y_{kl}}{\partial X_{ij}}&#x3D;tr((\frac{\partial z}{\partial Y})^T\frac{\partial Y}{\partial X_{ij}})<br>$　　　　这里大家会发现我们没有给出基于矩阵整体的链式求导法则，主要原因是矩阵对矩阵的求导是比较复杂的定义，我们目前也未涉及。因此只能给出对矩阵中一个标量的链式求导方法。这个方法并不实用，因为我们并不想每次都基于定义法来求导最后再去排列求导结果。</p><p>　　　　虽然我们没有全局的标量对矩阵的链式求导法则，但是对于一些线性关系的链式求导，我们还是可以得到一些有用的结论的。</p><p>　　　　我们来看这个常见问题：$A,B,X,Y$都是矩阵，$z$是标量，其中$z&#x3D;f(Y),Y&#x3D;AX+B$,我们要求出$\frac{\partial z}{\partial X}$,这个问题在机器学习中是很常见的。此时，我们并不能直接整体使用矩阵的链式求导法则，因为矩阵对矩阵的求导结果不好处理。</p><p>　　　　这里我们回归初心，使用定义法试一试,先使用上面的标量链式求导公式：<br>$<br>\frac{\partial z}{\partial X_{ij}}&#x3D;\sum_{k,l}\frac{\partial z}{\partial Y_{kl}}\frac{\partial Y_{kl}}{\partial X_{ij}}<br>$　　　　我们再来看看后半部分的导数：<br>$frac{\partial Y_{kl}}{\partial X_{ij}}&#x3D;\frac{\partial \sum_s(A_{ks}X_{sl})}{\partial X_{ij}}&#x3D;\frac{\partial A_{ki}X_{si}}{\partial X_{ij}}&#x3D;A_{ki}\delta_{lj}<br>$　　其中$\delta_{lj}$在$l&#x3D;j$时为1，否则为0.</p><p>　　　　那么最终的标签链式求导公式转化为：<br>$<br>\frac{\partial z}{\partial X_{ij}}&#x3D;\sum_{k,l}\frac{\partial z}{\partial Y_{kl}}A_{ki}\delta_{lj}&#x3D;\sum_{k}\frac{\partial z}{\partial Y_{k,j}}A_{ki}<br>$　　　　即矩阵$A^T$的第$i$行和$\frac{\partial z}{\partial Y}$的第$j$列的内积。排列成矩阵即为：<br>$frac{\partial z}{\partial X}&#x3D;A^T\frac{\partial z}{\partial Y}<br>$　　总结下就是：<br>$(Y),Y&#x3D;AX+B \to \frac{\partial z}{\partial X}&#x3D;A^T\frac{\partial z}{\partial Y}<br>$这结论在$ \mathbb{x}$是一个向量的时候也成立，即：<br>$mathbb{y}),\mathbb{y}&#x3D;A\mathbb{x}+\mathbb{b} \to \frac{\partial z}{\partial \mathbb{x}}&#x3D;A^T\frac{\partial z}{\partial \mathbb{y}}<br>$要求导的自变量在左边，线性变换在右边，也有类似稍有不同的结论如下，证明方法是类似的，这里直接给出结论：<br>$Y&#x3D;XA+B \to \frac{\partial z}{\partial X}&#x3D;\frac{\partial z}{\partial \mathbb{y}}A^T \<br>z&#x3D;f(\mathbb{y}),\mathbb{y}&#x3D;X\mathbb{a}+\mathbb{b} \to \frac{\partial z}{\partial X}&#x3D;\frac{\partial z}{\partial \mathbb{y}}\mathbb{a}^T<br>$述四个结论，对于机器学习尤其是深度学习里的求导问题可以非常快的解决,大家可以试一试。</p><h1 id="16-矩阵向量求导小结"><a href="#16-矩阵向量求导小结" class="headerlink" title="16. 矩阵向量求导小结"></a>16. 矩阵向量求导小结</h1><p>　　　　矩阵向量求导在前面我们讨论三种方法，定义法，微分法和链式求导法。在同等情况下，优先考虑链式求导法，尤其是第三节的四个结论。其次选择微分法、在没有好的求导方法的时候使用定义法是最后的保底方案。矩阵对矩阵求导一般不常见��常见</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——特征选择方法</title>
    <link href="/2022/08/01/MachineLearning/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95/"/>
    <url>/2022/08/01/MachineLearning/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="特征选择方法总结"><a href="#特征选择方法总结" class="headerlink" title="特征选择方法总结"></a>特征选择方法总结</h2><hr><p><img src="/src/640-165590372499349.png" alt="图片"></p><h1 id="一、背景介绍"><a href="#一、背景介绍" class="headerlink" title="一、背景介绍"></a><strong>一、背景介绍</strong></h1><p>在处理结构型数据时，特征工程中的特征选择是很重要的一个环节，特征选择是选择对模型重要的特征。它的好处[2]在于:</p><p>● 减少训练数据大小，加快模型训练速度。</p><p>● 减少模型复杂度，避免过拟合。</p><p>● 特征数少，有利于解释模型。</p><p>● 如果选择对的特征子集，模型准确率可能会提升。</p><p>我曾在文章CCF大赛项目提到过一个困境，就是当时我在滑动窗口式组织数据 + 多阶统计特征生成后，我的模型就过拟合了，然后我看到某选手开源的代码，他只用了LGBM模型认为重要的TOP几百个特征就能达到跟我用全量特征的效果。所以我就反思到：特征真的越多越好吗？把特征交给模型，模型自己能很好学习到哪些特征有用或者没用吗？当时，我抱着疑问，做了特征选择工作，发现效果居然有提升，可能原因是：</p><p>● 去除冗余无用特征，减低模型学习难度，减少数据噪声。</p><p>● 去除标注性强的特征，例如某些特征在训练集和测试集分布严重不一致，去除他们有利于避免过拟合。</p><p>● 选用不同特征子集去预测不同的目标，比如用不同状态下的作业数特征去预测”提交中的作业数”，而用不同资源使用率的特征去预测“CPU使用率”。</p><p>当时，我是直接一股脑把特征丢进去训练模型，后面比赛完跟其它选手交流才了解到特征选择的重要性。所以这篇文章的出发点是自我查漏补缺，也希望能给大家带来点帮助。</p><p>特征选择方法一共分为3类：**过滤法(Filter)<strong>、</strong>包裹法(Wrapper)<strong>和</strong>嵌入法(Embedded)**。下面我会依次介绍它们。</p><h1 id="二、过滤法-Filter"><a href="#二、过滤法-Filter" class="headerlink" title="二、过滤法(Filter)"></a><strong>二、过滤法(Filter)</strong></h1><p><img src="/src/640-164640125983870.png" alt="图片"></p><p>图1: 过滤法[3]</p><p>过滤法: 选择特征时不管模型，该方法基于特征的通用表现去选择，比如: 目标相关性、自相关性和发散性等。</p><p>● <strong>优点</strong>: 特征选择计算开销小，且能有效避免过拟合。</p><p><strong>● 缺点</strong>: 没考虑针对后续要使用的学习器去选择特征子集，减弱学习器拟合能力。</p><p>当我们使用过滤法去审视变量时，我们会从<strong>单变量自身情况</strong>和<strong>多变量之间</strong>的关系去判断变量是否该被过滤掉。</p><p><img src="/src/640-164640125983971.png" alt="图片"></p><p>图2: 过滤法方法总结</p><h2 id="1-单变量"><a href="#1-单变量" class="headerlink" title="1. 单变量"></a><strong>1. 单变量</strong></h2><p><strong>(1) 缺失百分比(Missing Percentage)</strong></p><p>缺失样本比例过多且难以填补的特征，建议剔除该变量。</p><p><strong>(2) 方差(Variance)</strong></p><p>若某连续型变量的方差接近于0，说明其特征值趋向于单一值的状态，对模型帮助不大，建议剔除该变量。</p><p><strong>(3) 频数(Frequency)</strong></p><p>若某类别型变量的枚举值样本量占比分布，集中在单一某枚举值上，建议剔除该变量。</p><p>这里以波士顿房价数据集举例，样例代码如下:</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># <span class="hljs-keyword">load</span> Boston datasetimport pandas <span class="hljs-keyword">as</span> pdimport numpy <span class="hljs-keyword">as</span> npimport seaborn <span class="hljs-keyword">as</span> snsimport matplotlib.pyplot <span class="hljs-keyword">as</span> pltfrom sklearn.datasets <span class="hljs-keyword">import</span> load_bostonboston = load_boston()df = pd.DataFrame(boston.data, <span class="hljs-keyword">columns</span> = boston.feature_names)<br># Missing Percentage + Variancestat_df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;# of miss&#x27;</span>:df.<span class="hljs-keyword">isnull</span>().sum(),                        <span class="hljs-string">&#x27;% of miss&#x27;</span>:df.<span class="hljs-keyword">isnull</span>().sum() / len(df) * <span class="hljs-number">100</span>,                        <span class="hljs-string">&#x27;var&#x27;</span>:df.var()&#125;)<br># Frequencycat_name = <span class="hljs-string">&#x27;CHAS&#x27;</span>chas = df[cat_name].value_counts().sort_index()cat_df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;enumerate_val&#x27;</span>:list(chas.<span class="hljs-keyword">index</span>), <span class="hljs-string">&#x27;frequency&#x27;</span>:list(chas.<span class="hljs-keyword">values</span>)&#125;)sns.barplot(x = &quot;enumerate_val&quot;, y = &quot;frequency&quot;,data = cat_df, palette=&quot;Set3&quot;)<span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(range(len(cat_df)), cat_df.frequency):    plt.text(x, y, <span class="hljs-string">&#x27;%d&#x27;</span>%y, ha=<span class="hljs-string">&#x27;center&#x27;</span>, va=<span class="hljs-string">&#x27;bottom&#x27;</span>, color=<span class="hljs-string">&#x27;grey&#x27;</span>)plt.title(cat_name)plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><p><img src="/src/640-164640138869579.png" alt="图片"></p><p>由图3发现，NOX方差低和CHAS频次分布严重不平衡，可以考虑剔除。</p><h2 id="2-多变量"><a href="#2-多变量" class="headerlink" title="2. 多变量"></a><strong>2. 多变量</strong></h2><p>研究多变量之间的关系时，主要从两种关系出发:</p><p><strong>●</strong> <strong>自变量与自变量之间的相关性</strong>: 相关性越高，会引发<strong>多重共线性</strong>问题，进而导致模型稳定性变差，样本微小扰动都会带来大的参数变化[5]，建议在具有共线性的特征中选择一个即可，其余剔除。</p><p>● <strong>自变量和因变量之间的相关性</strong>: 相关性越高，说明特征对模型预测目标更重要，建议保留。</p><p>由于变量分连续型变量和类别型变量，所以在研究变量间关系时，也要选用不同的方法:</p><h3 id="2-1-连续型vs连续型"><a href="#2-1-连续型vs连续型" class="headerlink" title="2.1 连续型vs连续型"></a><strong>2.1 连续型vs连续型</strong></h3><h4 id="1-皮尔逊相关系数-Pearson-Correlation-Coefficient"><a href="#1-皮尔逊相关系数-Pearson-Correlation-Coefficient" class="headerlink" title="(1) 皮尔逊相关系数(Pearson Correlation Coefficient)"></a><strong>(1) 皮尔逊相关系数(Pearson Correlation Coefficient)</strong></h4><p>Pearson相关系数是<strong>两个变量的协方差除以两变量的标准差乘积</strong>。协方差能反映两个随机变量的相关程度（协方差大于0的时候表示两者正相关，小于0的时候表示两者负相关），而除以标准差后，Pearson的值范围为[-1,1]。当两个变量的线性关系增强时，相关系数趋于1或-1，正负号指向正负相关关系。[6]<br>$$<br>\begin{align}<br>\rho(X,Y) &amp;&#x3D; \frac{cov(X, Y)}{\sigma_X\sigma_Y}\<br>&amp;&#x3D; \frac{\sum_{i&#x3D;1}^n(X_i-\bar{X})(Y_i-\bar {Y})}{\sqrt{\frac{\sum_{i&#x3D;1}^n(X_i-\mu_X)^2}{n-1}}\sqrt{\frac{\sum_{i&#x3D;1}^n(Y_i-\mu_Y)^2}{n-1}}}\<br>&amp;&#x3D;\frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sqrt{\sum_{i&#x3D;1}^n(X_i-\mu_X)^2}\sqrt{\sum_{i&#x3D;1}^n(Y_i-\mu_Y)^2}}<br>\end{align}<br>$$</p><h4 id="2-斯皮尔曼相关系数-Spearman’s-Rank-Correlation-Coefficient"><a href="#2-斯皮尔曼相关系数-Spearman’s-Rank-Correlation-Coefficient" class="headerlink" title="(2) 斯皮尔曼相关系数(Spearman’s Rank Correlation Coefficient)"></a><strong>(2) 斯皮尔曼相关系数(Spearman’s Rank Correlation Coefficient)</strong></h4><p>Pearson相关系数是建立在变量符合正态分布的基础上，而Spearman相关系数不假设变量服从何种分布，它是基于**等级(rank)**的概念去计算变量间的相关性。如果变量是顺序变量(Ordinal Feature)，推荐使用Spearman相关系数。<br>$$<br>\rho &#x3D; 1- \frac{6\sum_{i&#x3D;1}^nd_i^2}{n(n^2-1)}<br>$$<br>其中， 为两个变量的等级差值， 为等级个数。这里举个例子会更好理解，假设我们要探究连续型变量 和 的Spearman相关系数，计算过程如下：</p><p><img src="/src/640-164640207549781.png" alt="图片"></p><p>同样地，相关系数趋于1或-1，正负号指向正负相关关系。</p><h3 id="2-2-连续型vs类别型"><a href="#2-2-连续型vs类别型" class="headerlink" title="2.2 连续型vs类别型"></a><strong>2.2 连续型vs类别型</strong></h3><h4 id="1-方差分析-Analysis-of-variance-ANOVA"><a href="#1-方差分析-Analysis-of-variance-ANOVA" class="headerlink" title="(1) 方差分析(Analysis of variance, ANOVA)"></a><strong>(1) 方差分析(Analysis of variance, ANOVA)</strong></h4><p>ANOVA的目的是检验<strong>不同组下的平均数是否存在显著差异</strong>。举个例子，我们要判断1,2和3班的同学的数学平均分是否有显著区别？我们能得到，班级为类别型变量，数学分数为连续型变量，如果班级与数学分数有相关性，比如1班同学数学会更好些，则说明不同班的数学平均分有显著区别。为了验证班级与数学分数的相关性，ANOVA会先建立零假设： : (三个班的数学分数没有显著区别)，它的验证方式是看组间方差(Mean Squared Between, MSB)是否大于组内方差(Mean Squared Error, MSE)，如果组间方差&gt;组内方差，说明存在至少一个分布相对于其他分布较远，则可以考虑拒绝零假设。[7]</p><p>$$<br>F&#x3D;\frac{MSB}{MSE}<br>$$<br>基于纽约Johnny哥在知乎“什么是ANOVA”的回答[8]，我们举例来试着计算下：</p><p><img src="/src/640-164640213634983.png" alt="图片"></p><p>注意，ANOVA分析前需要满足3个假设: 每组样本具备方差同质性、组内样本服从正态分布，样本间需要独立。</p><h4 id="2-肯德尔等级相关系数-Kendall-tau-rank-correlation-coefficient"><a href="#2-肯德尔等级相关系数-Kendall-tau-rank-correlation-coefficient" class="headerlink" title="(2) 肯德尔等级相关系数(Kendall tau rank correlation coefficient)"></a><strong>(2) 肯德尔等级相关系数(Kendall tau rank correlation coefficient)</strong></h4><p>假设我们要评价学历与工资的相关性，Kendall系数会对按学历对<strong>样本排序</strong>，若排序后，学历和工资<strong>排名相同，则Kendall系数为1</strong>，两变量正相关。若学历和工资完全相反，则系数为-1，完全负相关。而如果学历和工资完全独立，系数为0。Kendall系数计算公式如下：<br>$$<br>P &#x3D; \frac{N_{Concordant Pairs}-N_{DiscordantPairs}}{\frac{n(n-1)}{2}}<br>$$<br>其中， 为同序对， 为异序对， 为总对数。同样地，我们举例展示下计算过程：</p><p><img src="/src/640-164640225130885.png" alt="图片"></p><h3 id="2-3-类别型vs类别型"><a href="#2-3-类别型vs类别型" class="headerlink" title="2.3 类别型vs类别型"></a><strong>2.3 类别型vs类别型</strong></h3><h4 id="1-卡方检验-Chi-squared-Test"><a href="#1-卡方检验-Chi-squared-Test" class="headerlink" title="(1) 卡方检验(Chi-squared Test)"></a><strong>(1) 卡方检验(Chi-squared Test)</strong></h4><p>卡方检验可用于检验两个类别型变量之间的相关性。它建立的零假设是：两变量之间不相关。卡方值 的计算公式如下：</p><p>$$<br>X^2&#x3D;\sum\frac{(observed-expected)^2}{expected}<br>$$<br>其中， 是实际值， 是理论值。卡方值的目的是<strong>衡量理论和实际的差异程度</strong>。如果我们研究运动的人是否会受伤，计算过程如下：</p><p><img src="/src/640-164640230562487.png" alt="图片"></p><p>卡方值高，说明两变量之间具有相关性的可能性更大。</p><h4 id="2-互信息-Mutual-Information"><a href="#2-互信息-Mutual-Information" class="headerlink" title="(2) 互信息(Mutual Information)"></a><strong>(2) 互信息(Mutual Information)</strong></h4><p>互信息是<strong>衡量变量之间相互依赖程度</strong>，它的计算公式如下：</p><p>$$<br>I(X;Y)&#x3D;\sum_{y\in Y}\sum_{x\in X}p(x,y)log(\frac{p(x, y)}{p(x)p(y)})<br>$$<br>它可以转为熵的表现形式，其中$H(X|Y)$和$H(Y|X)$是条件熵，$H(X,Y)$是联合熵。当$X$与 $Y$独立时，$p(x, y)&#x3D;p(x)p(y)$，则互信息为0。当两个变量完全相同时，互信息最大，因此互信息越大，变量相关性越强。此外，互信息是正数且具有对称性(即$I(X;Y)&#x3D;I(Y;X)$ )。</p><p><img src="/src/640-164640246738989.png" alt="图片"></p><p>图9: 互信息[9]</p><h2 id="3-过滤法总结"><a href="#3-过滤法总结" class="headerlink" title="3. 过滤法总结"></a><strong>3. 过滤法总结</strong></h2><p>总结以上内容，如下图所示:</p><p><img src="/src/640-164640249579091.png" alt="图片"></p><p>图10: 过滤法的度量指标汇总（注：挑选规则是基于自变量和因变量的相关性去挑选）</p><p>我们可以按需使用上面的指标去观察变量间的相关性，然后人工挑选特征。另外，也能使用scikit-learn里的特征选择库sklearn.feature_selection[10]，这里我以SelectKBest(选择Top K个最高得分的特征)为例:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># load Boston dataset<br>import pandas <span class="hljs-keyword">as</span> pd<br>import numpy <span class="hljs-keyword">as</span> np<br>from sklearn.datasets import load_boston<br>from sklearn.feature_selection import SelectKBest<br>from sklearn.feature_selection import f_regression<br><br>boston = load<span class="hljs-constructor">_boston()</span><br>df = pd.<span class="hljs-constructor">DataFrame(<span class="hljs-params">boston</span>.<span class="hljs-params">data</span>, <span class="hljs-params">columns</span> = <span class="hljs-params">boston</span>.<span class="hljs-params">feature_names</span>)</span><br>target = pd.<span class="hljs-constructor">DataFrame(<span class="hljs-params">boston</span>.<span class="hljs-params">target</span>, <span class="hljs-params">columns</span>=[&#x27;MEDV&#x27;])</span><br>print(&#x27;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X</span>.</span></span>shape:&#x27;, df.shape)<br><br># select feature by person coefficient<br>X = np.<span class="hljs-built_in">array</span>(df)<br>Y = np.<span class="hljs-built_in">array</span>(target)<br>skb = <span class="hljs-constructor">SelectKBest(<span class="hljs-params">score_func</span>=<span class="hljs-params">f_regression</span>, <span class="hljs-params">k</span>=5)</span><br>skb.fit(X, <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Y</span>.</span></span>ravel<span class="hljs-literal">()</span>)<br>print(&#x27;选择的特征有:&#x27;, <span class="hljs-literal">[<span class="hljs-identifier">boston</span>.<span class="hljs-identifier">feature_names</span>[<span class="hljs-identifier">i</span>]</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> skb.get<span class="hljs-constructor">_support(<span class="hljs-params">indices</span> = True)</span>])<br>X_selected = skb.transform(X)<br>print(&#x27;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X_selected</span>.</span></span>shape:&#x27;, <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X_selected</span>.</span></span>shape)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5C640-164640256689093.png" alt="图片"></p><h1 id="三、包裹法-Wrapper"><a href="#三、包裹法-Wrapper" class="headerlink" title="三、包裹法(Wrapper)"></a><strong>三、包裹法(Wrapper)</strong></h1><p><img src="/src/640-164640257785995.png" alt="图片"></p><p>图11: 包裹法[3]</p><p>包裹法: 将要使用的学习器的性能作为特征子集的评价准则，目的是为给的学习器选择“量身定做”的特征子集。[4]</p><p>● <strong>优点</strong>: 特征选择比过滤法更具针对性，对模型性能有好处。</p><p>● <strong>缺点</strong>: 计算开销更大。</p><p>包裹法有如下三类搜索方法:</p><p><img src="/src/640-164640258782897.png" alt="图片"></p><p>图12: 包裹法总结 </p><h2 id="1-完全搜索"><a href="#1-完全搜索" class="headerlink" title="1. 完全搜索"></a><strong>1. 完全搜索</strong></h2><p>遍历所有可能组合的特征子集，然后输入模型，选择最佳模型分数的特征子集。不推荐使用，计算开销过大。</p><h2 id="2-启发式搜索"><a href="#2-启发式搜索" class="headerlink" title="2. 启发式搜索"></a><strong>2. 启发式搜索</strong></h2><p>启发式搜索是利用启发式信息不断缩小搜索空间的方法。在特征选择中，<strong>模型分数或特征权重可作为启发式信息</strong>。</p><h3 id="2-1-向前-x2F-向后搜索"><a href="#2-1-向前-x2F-向后搜索" class="headerlink" title="2.1 向前&#x2F;向后搜索"></a><strong>2.1 向前&#x2F;向后搜索</strong></h3><p>向前搜索是先从空集开始，每轮只加入一个特征，然后训练模型，若模型评估分数提高，则保留该轮加入的特征，否则丢弃。反之，向后特征是做减法，先从全特征集开始，每轮减去一个特征，若模型表现减低，则保留特征，否则弃之。</p><h3 id="2-2-递归特征消除"><a href="#2-2-递归特征消除" class="headerlink" title="2.2 递归特征消除"></a><strong>2.2 递归特征消除</strong></h3><p>递归特征消除简称**RFE(Recursive Feature Elimination)**，RFE是使用一个基模型进行多轮训练，每轮训练后，消除若干低权值(例特征权重系数或者特征重要性)的特征，再基于新的特征集进行下一轮训练[1]。RFE使用时，要提前限定最后选择的特征数(n_features_to_select)，这个超参很难保证一次就设置合理，因为设高了，容易特征冗余，设低了，可能会过滤掉相对重要的特征。而且RFE只是单纯基于特征权重去选择，没有考虑模型表现，因此RFECV出现了，REFCV是REF + CV(交叉验证)，它的运行机制是：先使用REF获取各个特征的ranking，然后再基于ranking，依次选择[min_features_to_select, len(feature)]个特征数量的特征子集进行模型训练和交叉验证，最后选择平均分最高的特征子集。</p><p><img src="/src/640-164640261385199.png" alt="图片"></p><p>图13: RFE 与RFECV</p><p>这里不重复造轮子了，可参考wanglei5205提供样例代码[10]:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment">### 生成数据</span><br><span class="hljs-keyword">from</span> sklearn.datasets import make_classification<br>X, y = make_classification(<span class="hljs-attribute">n_samples</span>=1000,         # 样本个数                           <span class="hljs-attribute">n_features</span>=25,          # 特征个数                           <br><span class="hljs-attribute">n_informative</span>=3,        # 有效特征个数                           <br><span class="hljs-attribute">n_redundant</span>=2,          # 冗余特征个数（有效特征的随机组合）                           <span class="hljs-attribute">n_repeated</span>=0,           # 重复特征个数（有效特征和冗余特征的随机组合）<br><span class="hljs-attribute">n_classes</span>=8,            # 样本类别                          <br><span class="hljs-attribute">n_clusters_per_class</span>=1, # 簇的个数                           <br><span class="hljs-attribute">random_state</span>=0)<br><span class="hljs-comment">### 特征选择</span><br><span class="hljs-comment"># RFE</span><br><span class="hljs-keyword">from</span> sklearn.svm import SVC<br>svc = SVC(<span class="hljs-attribute">kernel</span>=<span class="hljs-string">&quot;linear&quot;</span>)<br><span class="hljs-keyword">from</span> sklearn.feature_selection import RFE<br>rfe = RFE(estimator = svc,           # 基分类器          <br>n_features_to_select = 2,  # 选择特征个数          <br><span class="hljs-keyword">step</span> = 1,                  # 每次迭代移除的特征个数           <br>verbose = 0                # 显示中间过程          ).fit(X,y)<br>X_RFE = rfe.transform(X)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;RFE特征选择结果——————————————————————————————————————————————————&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;有效特征个数 : %d&quot;</span> % rfe.n_features_)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;全部特征等级 : %s&quot;</span> % list(rfe.ranking_))<br><br><span class="hljs-comment"># RFECV</span><br><span class="hljs-keyword">from</span> sklearn.svm import SVC<br>svc = SVC(<span class="hljs-attribute">kernel</span>=<span class="hljs-string">&quot;linear&quot;</span>)<br><span class="hljs-keyword">from</span> sklearn.model_selection import StratifiedKFold<br><span class="hljs-keyword">from</span> sklearn.feature_selection import RFECV<br>rfecv = RFECV(<span class="hljs-attribute">estimator</span>=svc,          # 学习器              <br><span class="hljs-attribute">min_features_to_select</span>=2, # 最小选择的特征数量             <br><span class="hljs-attribute">step</span>=1,                 # 移除特征个数             <br><span class="hljs-attribute">cv</span>=StratifiedKFold(2),  # 交叉验证次数             <br><span class="hljs-attribute">scoring</span>=<span class="hljs-string">&#x27;accuracy&#x27;</span>,     # 学习器的评价标准             <br>verbose = 0,              <br>n_jobs = 1).fit(X, y)<br>X_RFECV = rfecv.transform(X)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;RFECV特征选择结果——————————————————————————————————————————————————&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;有效特征个数 : %d&quot;</span> % rfecv.n_features_)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;全部特征等级 : %s&quot;</span> % list(rfecv.ranking_))<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5C640-1646402768789101.png" alt="图片"></p><h2 id="3-随机搜索"><a href="#3-随机搜索" class="headerlink" title="3. 随机搜索"></a><strong>3. 随机搜索</strong></h2><h3 id="3-1-随机特征子集"><a href="#3-1-随机特征子集" class="headerlink" title="3.1 随机特征子集"></a><strong>3.1 随机特征子集</strong></h3><p>随机选择多个特征子集，然后分别评估模型表现，选择评估分数高的特征子集。</p><h3 id="3-2-Null-Importance"><a href="#3-2-Null-Importance" class="headerlink" title="3.2 Null Importance"></a><strong>3.2 Null Importance</strong></h3><p>3年前Kaggle GM Olivier提出Null Importance特征挑选法，最近看完代码，觉得真妙。它成功找出“见风使舵”的特征并剔除了它们，什么是“见风使舵”的特征呢？多见于标识性强或充满噪声的特征，举个例子，如果我们把userID作为特征加入模型，预测不同userID属于哪类消费人群，一个过拟合的模型，可以会学到userID到消费人群的直接映射关系(相当于模型直接记住了这个userID是什么消费人群)，那如果我假装把标签打乱，搞个假标签去重新训练预测，我们会发现模型会把userID又直接映射到打乱的标签上，最后真假标签下，userID“见风使舵”地让都自己变成了最重要的特征。我们怎么找出这类特征呢？Olivier的想法很简单：<strong>真正强健、稳定且重要的特征一定是在真标签下特征很重要，但一旦标签打乱，这些优质特征的重要性就会变差。相反地，如果某特征在原始标签下表现一般，但打乱标签后，居然重要性上升，明显就不靠谱，这类“见风使舵”的特征就得剔除掉。</strong></p><p>Null Importance的计算过程大致如下:</p><p>(1) 在原始数据集运行模型获取特征重要性;</p><p>(2) shuffle多次标签，每次shuffle后获取假标签下的特征重要性;</p><p>(3) 计算真假标签下的特征重要性差异，并基于差异，筛选特征。</p><p><img src="/src/640-1646402789025103.png" alt="图片"></p><p>图14: Null Importance的计算过程示意图</p><p>在图14我们能知道Null Importance的大致运行流程，但这里补充些细节，其中，重要性你可以选择importance_gain或者importance_split。另外，如图14所示，如果我们要比较原始标签和打乱标签下的特征重要性，Olivier提供了两种比较方法：<br>$$<br>FeatureScore&#x3D;log(10^{-10}+\frac{Importance_{real\ label}}{1+percentile(Importance_{shuffle\ label},75)})<br>$$<br>第一种：分位数比较。</p><p>${10}^{-10}$ 和1是为了避免$log(0)$和分母为0的情况。输出样例如下：</p><p><img src="/src/640-1646402931616105.png" alt="图片"></p><p>图15: 比较特征重要性分位数</p><p>第二种：次数占比比较。</p><p>正常来说，单个特征只有1个 ，之所以作者要求25分位数，是考虑到如果使用时，我们也对原始特征反复训练生成多组特征重要性，所以才就加了25分位数。输出样例如下：<br>$$<br>FeatureScore&#x3D;\frac{\sum(Importance_{shuffle\ label})&lt;percentile(Importance_{shuffle\ label},25)}{n_{Importance_{shuffle\ label}}}*100<br>$$<br><img src="/src/640-1646403081500107.png" alt="图片"></p><p>由上可知，第二种方法得到的特征分数是在0-100范围内，因此Olivier选择在第二种方法上，采用不同阈值去筛选特征，然后评估模型表现。推荐阅读Olivier的开源代码[11]，简单易懂。</p><h2 id="4-包裹法总结"><a href="#4-包裹法总结" class="headerlink" title="4. 包裹法总结"></a><strong>4. 包裹法总结</strong></h2><p>实际使用中，推荐RFECV和Null Importance，因为他们既考虑了特征权重也考虑了模型表现。</p><h1 id="四、嵌入法-Embedded"><a href="#四、嵌入法-Embedded" class="headerlink" title="四、嵌入法(Embedded)"></a><strong>四、嵌入法(Embedded)</strong></h1><p><img src="/src/640-1646403109893109.png" alt="图片"></p><p>图17: 嵌入法[3]</p><p>嵌入法: 特征选择被嵌入进学习器训练过程中。不像包裹法，特性选择与学习器训练过程有明显的区分。[4]</p><p>● <strong>优点</strong>: 比包裹法更省时省力，把特征选择交给模型去学习。</p><p>● <strong>缺点</strong>: 增加模型训练负担。</p><p>常见的嵌入法有LASSO的L1正则惩罚项、随机森林构建子树时会选择特征子集。嵌入法的应用比较单调，sklearn有提供SelectFromModel[12]，可以直接调用模型挑选特征。参考样例代码[1]如下:</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-title">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-title">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><span class="hljs-title">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-title">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<br><br><span class="hljs-title">iris</span> = load_iris()<br><span class="hljs-meta"># 将待L1惩罚项的逻辑回归作为基模型的特征选择</span><br><span class="hljs-title">selected_data_lr</span> = <span class="hljs-type">SelectFromModel</span>(<span class="hljs-type">LogisticRegression</span>(penalty=&#x27;l1&#x27;, <span class="hljs-type">C</span> = <span class="hljs-number">0.1</span>, <br>solver = &#x27;liblinear&#x27;), max_features = <span class="hljs-number">3</span>).fit_transform(iris.<span class="hljs-class"><span class="hljs-keyword">data</span>, iris.target)</span><br><span class="hljs-meta"># 将GBDT作为基模型的特征选择</span><br><span class="hljs-title">selected_data_gbdt</span> = <span class="hljs-type">SelectFromModel</span>(<span class="hljs-type">GradientBoostingClassifier</span>(), <br>max_features = <span class="hljs-number">3</span>).fit_transform(iris.<span class="hljs-class"><span class="hljs-keyword">data</span>, iris.target)</span><br><br><span class="hljs-title">print</span>(iris.<span class="hljs-class"><span class="hljs-keyword">data</span>.shape)</span><br><span class="hljs-title">print</span>(selected_data_lr.shape)<br><span class="hljs-title">print</span>(selected_data_gbdt.shape)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5C640-1646403175816111.png" alt="图片"></p><h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a><strong>五、总结</strong></h1><p>在进行特征选择时，建议过滤法、包裹法和嵌入法都尝试使用，前期的特征过滤有利于减轻模型的学习负担。当然最高级的特征选择还是基于业务知识的人工挑选，以上方法挑选出的特征也建议多思考为什么这个特征对模型有帮助，以及挑选的优质特征有没有更进一步深入挖掘的可能。�有更进一步深入挖掘的可能。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——特征归一化的重要性</title>
    <link href="/2022/08/01/MachineLearning/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7/"/>
    <url>/2022/08/01/MachineLearning/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<p><strong>Feature scaling</strong>，常见的提法有“特征归一化”、“标准化”，是数据预处理中的重要技术，有时甚至决定了算法能不能work以及work得好不好。谈到feature scaling的必要性，最常用的2个例子可能是：</p><ul><li><strong>特征间的单位（尺度）可能不同</strong>，比如身高和体重，比如摄氏度和华氏度，比如房屋面积和房间数，一个特征的变化范围可能是[1000, 10000]，另一个特征的变化范围可能是[−0.1,0.2]，在进行距离有关的计算时，单位的不同会导致计算结果的不同，尺度大的特征会起决定性作用，而尺度小的特征其作用可能会被忽略，<strong>为了消除特征间单位和尺度差异的影响，以对每维特征同等看待，需要对特征进行归一化</strong>。</li><li>原始特征下，<strong>因尺度差异，其损失函数的等高线图可能是椭圆形</strong>，梯度方向垂直于等高线，下降会走zigzag路线，而不是指向local minimum。通过对特征进行zero-mean and unit-variance变换后，其损失函数的等高线图更接近圆形，梯度下降的方向震荡更小，收敛更快，如下图所示。</li></ul><p><img src="/src/640-165779004044093.png" alt="图片"></p><p>对于feature scaling中最常使用的Standardization，似乎“无脑上”就行了，本文想多探究一些为什么，</p><h1 id="常用的feature-scaling方法"><a href="#常用的feature-scaling方法" class="headerlink" title="常用的feature scaling方法"></a>常用的feature scaling方法</h1><p>给定数据集，令特征向量为x，维数为D，样本数量为R，可构成D×R的矩阵，一列为一个样本，一行为一维特征，如下图所示，图片来自Hung-yi Lee pdf-Gradient Descent：</p><p><img src="/src/640-165779234453596.jpeg" alt="图片"></p><p>feature matrix</p><p>feature scaling的方法可以分成2类，逐行进行和逐列进行。逐行是对每一维特征操作，逐列是对每个样本操作，上图为逐行操作中特征标准化的示例。</p><p>具体地，常用feature scaling方法如下，来自wiki，</p><ul><li><p>**Rescaling (min-max normalization、range scaling)**：<br>$$<br>x^{\prime}&#x3D;a+\frac{(x-\min (x))(b-a)}{\max (x)-\min (x)}<br>$$<br>将每一维特征线性映射到目标范围[a,b]，即将最小值映射为a，最大值映射为b，常用目标范围为[0,1]和[−1,1]，特别地，映射到[0,1]计算方式为：<br>$$<br>x^{\prime}&#x3D;\frac{x-\min (x)}{\max (x)-\min (x)}<br>$$</p></li><li><p><strong>Mean normalization</strong>：<br>$$<br>x^{\prime}&#x3D;\frac{x-\bar{x}}{\max (x)-\min (x)}<br>$$<br>将<strong>均值映射为0</strong>，同时用最大值最小值的差对特征进行归一化，一种更常见的做法是用标准差进行归一化，如下。</p></li><li><p><strong>Standardization (Z-score Normalization)<strong>：<br>$$<br>x^{\prime}&#x3D;\frac{x-\bar{x}}{\sigma}<br>$$<br>每维特征</strong>0均值1方差（zero-mean and unit-variance）</strong>。</p></li><li><p><strong>Scaling to unit length</strong>：<br>$$<br>x^{\prime}&#x3D;\frac{x}{||x||}<br>$$<br>将每个样本的特征向量除以其长度，即对样本特征向量的长度进行归一化，长度的度量常使用的是L2 norm（欧氏距离），有时也会采用L1 norm，不同度量方式的一种对比可以参见论文“CVPR2005-Histograms of Oriented Gradients for Human Detection”。</p></li></ul><p>上述4种feature scaling方式，前3种为逐行操作，最后1种为逐列操作。</p><p>  <strong>容易让人困惑的一点是指代混淆，Standardization指代比较清晰，但是单说Normalization有时会指代min-max normalization，有时会指代Standardization，有时会指代Scaling to unit length</strong>。</p><h1 id="计算方式上对比分析"><a href="#计算方式上对比分析" class="headerlink" title="计算方式上对比分析"></a>计算方式上对比分析</h1><p>3种feature scaling的计算方式为<strong>减一个统计量再除以一个统计量</strong>，最后1种为<strong>除以向量自身的长度</strong>。</p><ul><li><strong>减一个统计量</strong>可以看成<strong>选哪个值作为原点，是最小值还是均值，并将整个数据集平移到这个新的原点位置</strong>。如果特征间偏置不同对后续过程有负面影响，则该操作是有益的，可以看成是某种<strong>偏置无关操作</strong>；如果原始特征值有特殊意义，比如稀疏性，该操作可能会破坏其稀疏性。</li><li><strong>除以一个统计量</strong>可以看成在<strong>坐标轴方向上对特征进行缩放</strong>，用于<strong>降低特征尺度的影响，可以看成是某种尺度无关操作</strong>。缩放可以使用最大值最小值间的跨度，也可以使用标准差（到中心点的平均距离），前者对outliers敏感，outliers对后者影响与outliers数量和数据集大小有关，outliers越少数据集越大影响越小。</li><li><strong>除以长度</strong>相当于把长度归一化，<strong>把所有样本映射到单位球上</strong>，可以看成是某种<strong>长度无关操作</strong>，比如，词频特征要移除文章长度的影响，图像处理中某些特征要移除光照强度的影响，以及方便计算余弦距离或内积相似度等。</li></ul><p>稀疏数据、outliers相关的更多数据预处理内容可以参见scikit learn-5.3. Preprocessing data。</p><p>从几何上观察上述方法的作用，zero-mean将数据集平移到原点，unit-variance使每维特征上的跨度相当，图中可以明显看出两维特征间存在线性相关性，Standardization操作并没有消除这种相关性。</p><p><img src="/src/640-165779380738299.png" alt="图片">Standardization</p><p>可通过PCA方法移除线性相关性（decorrelation），即引入旋转，找到新的坐标轴方向，在新坐标轴方向上用“标准差”进行缩放，如下图所示，图片来自链接，图中同时描述了unit length的作用——将所有样本映射到单位球上。</p><p><img src="/src/640-1657793807382100.png" alt="图片">Effect of the operations of standardization and length normalization</p><p>当特征维数更多时，对比如下，<img src="/src/640-1657793807382101.png" alt="图片"></p><p>总的来说，<strong>归一化&#x2F;标准化的目的是为了获得某种“无关性”——偏置无关、尺度无关、长度无关……当归一化&#x2F;标准化方法背后的物理意义和几何含义与当前问题的需要相契合时，其对解决该问题就有正向作用，反之，就会起反作用。所以，“何时选择何种方法”取决于待解决的问题，即problem-dependent。</strong></p><h1 id="feature-scaling-需要还是不需要"><a href="#feature-scaling-需要还是不需要" class="headerlink" title="feature scaling 需要还是不需要"></a>feature scaling 需要还是不需要</h1><p>下图对比了几个监督学习算法，最右侧两列为是否需要feature scaling。</p><p><img src="/src/640-1657794028306108.png" alt="图片">Comparing supervised learning algorithms</p><p>下面具体分析一下。</p><h2 id="什么时候需要feature-scaling？"><a href="#什么时候需要feature-scaling？" class="headerlink" title="什么时候需要feature scaling？"></a>什么时候需要feature scaling？</h2><ul><li>涉及或隐含<strong>距离计算</strong>的算法，比如K-means、KNN、PCA、SVM等，一般需要feature scaling，因为：</li></ul><p><strong>zero-mean一般可以增加样本间余弦距离或者内积结果的差异</strong>，区分力更强，假设数据集集中分布在第一象限遥远的右上角，将其平移到原点处，可以想象样本间余弦距离的差异被放大了。在模版匹配中，zero-mean可以明显提高响应结果的区分度。</p><p>就欧式距离而言，<strong>增大某个特征的尺度，相当于增加了其在距离计算中的权重，如果有明确的先验知识表明某个特征很重要，那么适当增加其权重可能有正向效果，但如果没有这样的先验，或者目的就是想知道哪些特征更重要，那么就需要先feature scaling，对各维特征等而视之</strong>。</p><p>增大尺度的同时也增大了该特征维度上的方差，PCA算法倾向于关注方差较大的特征所在的坐标轴方向，其他特征可能会被忽视，因此，在PCA前做Standardization效果可能更好，如下图所示，图片来自scikit learn-Importance of Feature Scaling，</p><ul><li><p><img src="/src/640-1657794800850111.png" alt="图片">PCA and Standardization</p></li><li><p>损失函数中含有<strong>正则项</strong>时，一般需要feature scaling：对于线性模型y&#x3D;wx+b而言，x的任何线性变换（平移、放缩），都可以被w和b“吸收”掉，理论上，不会影响模型的拟合能力。但是，如果损失函数中含有正则项，如λ∣∣w∣∣^2，λ为超参数，其对w的每一个参数施加同样的惩罚，但对于某一维特征xi而言，其scale越大，系数wi越小，其在正则项中的比重就会变小，相当于对wi惩罚变小，即损失函数会相对忽视那些scale增大的特征，这并不合理，所以需要feature scaling，使损失函数平等看待每一维特征。</p></li><li><p><strong>梯度下降算法，需要feature scaling</strong>。梯度下降的参数更新公式如下，</p></li></ul><p><img src="/src/640-1657794800850112.png" alt="图片"></p><p>E(W)为损失函数，<strong>收敛速度取决于：参数的初始位置到local minima的距离，以及学习率η的大小</strong>。一维情况下，<strong>在local minima附近</strong>，不同学习率对梯度下降的影响如下图所示：</p><p><img src="/src/640-1657794800850113.png" alt="图片">Gradient descent for different learning rates</p><p>多维情况下可以分解成多个上图，每个维度上分别下降，参数W为向量，但学习率只有1个，即所有参数维度共用同一个学习率（暂不考虑为每个维度都分配单独学习率的算法）。收敛意味着在每个参数维度上都取得极小值，每个参数维度上的偏导数都为0，但是每个参数维度上的下降速度是不同的，为了每个维度上都能收敛，学习率应取所有维度在当前位置合适步长中最小的那个。下面讨论feature scaling对gradient descent的作用，</p><ul><li><ul><li><strong>zero center与参数初始化相配合，缩短初始参数位置与local minimum间的距离，加快收敛</strong>。模型的最终参数是未知的，所以一般随机初始化，比如从0均值的均匀分布或高斯分布中采样得到，对线性模型而言，其分界面初始位置大致在原点附近，bias经常初始化为0，则分界面直接通过原点。同时，为了收敛，学习率不会很大。而每个数据集的特征分布是不一样的，如果其分布集中且距离原点较远，比如位于第一象限遥远的右上角，分界面可能需要花费很多步骤才能“爬到”数据集所在的位置。所以，无论什么数据集，先平移到原点，再配合参数初始化，可以保证分界面一定会穿过数据集。此外，<strong>outliers常分布在数据集的外围</strong>，与分界面从外部向内挪动相比，从中心区域开始挪动可能受outliers的影响更小。</li><li>对于采用均方误差损失LMS的线性模型，损失函数恰为二阶，如下图所示</li></ul></li><li><p><img src="/src/640-1657794800850114.png" alt="图片"></p></li><li><p><strong>不同方向上的下降速度变化不同（二阶导不同，曲率不同）</strong>，恰由输入的协方差矩阵决定，<strong>通过scaling改变了损失函数的形状，减小不同方向上的曲率差异</strong>。将每个维度上的下降分解来看，给定一个下降步长，如果不够小，有的维度下降的多，有的下降的少，有的还可能在上升，损失函数的整体表现可能是上升也可能是下降，就会不稳定。<strong>scaling后不同方向上的曲率相对更接近，更容易选择到合适的学习率，使下降过程相对更稳定。</strong></p></li><li><ul><li>另有从Hessian矩阵特征值以及condition number角度的理解，详见Lecun paper-Efficient BackProp中的Convergence of Gradient Descent一节，有清晰的数学描述，同时还介绍了白化的作用——解除特征间的线性相关性，使每个维度上的梯度下降可独立看待。</li><li>文章开篇的椭圆形和圆形等高线图，仅在采用均方误差的线性模型上适用，其他损失函数或更复杂的模型，如深度神经网络，<strong>损失函数的error surface可能很复杂，并不能简单地用椭圆和圆来刻画</strong>，所以用它来解释feature scaling对所有损失函数的梯度下降的作用，似乎过于简化，见Hinton vedio-3.2 The error surface for a linear neuron。</li><li>对于损失函数不是均方误差的情况，只要权重w与输入特征x间是相乘关系，损失函数对w的偏导必然含有因子x，w的梯度下降速度就会受到特征x尺度的影响。理论上为每个参数都设置上自适应的学习率，可以吸收掉x尺度的影响，但在实践中出于计算量的考虑，往往还是所有参数共用一个学习率，此时x尺度不同可能会导致不同方向上的下降速度悬殊较大，学习率不容易选择，下降过程也可能不稳定，通过scaling可对不同方向上的下降速度有所控制，使下降过程相对更稳定。</li></ul></li><li><p>对于传统的神经网络，对输入做feature scaling也很重要，因为采用sigmoid等有饱和区的激活函数，如果输入分布范围很广，参数初始化时没有适配好，很容易直接陷入饱和区，导致<strong>梯度消失</strong>，所以，需要对输入做Standardization或映射到[0,1]、[−1,1]，配合精心设计的参数初始化方法，对值域进行控制。但自从有了Batch Normalization，每次线性变换改变特征分布后，都会重新进行Normalization，似乎可以不太需要对网络的输入进行feature scaling了？但习惯上还是会做feature scaling。</p></li></ul><h2 id="什么时候不需要Feature-Scaling？"><a href="#什么时候不需要Feature-Scaling？" class="headerlink" title="什么时候不需要Feature Scaling？"></a>什么时候不需要Feature Scaling？</h2><p>与距离计算无关的概率模型，不需要feature scaling，比如Naive Bayes；</p><p>与距离计算无关的基于树的模型，不需要feature scaling，比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。<br>对大小无关。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——性能指标</title>
    <link href="/2022/08/01/MachineLearning/%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/"/>
    <url>/2022/08/01/MachineLearning/%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<p>衡量一个机器学习算法的好坏需要一个标准来衡量，对于不同场景中的不同任务就需要决定不同的指标来度量。根据没有免费午餐这个归纳偏好，我们可以知道，没有一种模型是完美适用于任何场景。所以选取一个合适的性能指标和选取一个切合的机器学习算法均是很重要的事情。所以本文对此加以整理</p><p>这里先对一些标记作出解释：给定样例集$D&#x3D;{(\mathbb{x}_1,y_1),(\mathbb{x}_2,y_2),\dots,(\mathbb{x}_m,y_m)}$，其中$y_i$是示例$\mathbb{x}_i$的真实标记，要评估学习器$f$的性能，就要把学习预测结果$f(x)（或\hat{y}）$与真实标记$y$进行比较</p><h1 id="分类问题的性能指标"><a href="#分类问题的性能指标" class="headerlink" title="分类问题的性能指标"></a>分类问题的性能指标</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。</p><p><img src="/src/image-20220104111941267.png" alt="image-20220104111941267"></p><p>  TN:真实类别为negative，模型预测的类别也为negative<br>  FN:预测为negative，但真实类别为positive，是漏报<br>  TP:真实类别为positive，模型预测的类别也为positive<br>  FP:预测为positive，但真实类别为negative，是误报<br>$<br>TN+FN+TP+FP&#x3D;m<br>$<br>TP、FP、TN、FN，第二个字母表示样本被预测的类别，第一个字母表示样本的预测类别与真实类别是否一致。</p><h2 id="错误率与精度"><a href="#错误率与精度" class="headerlink" title="错误率与精度"></a>错误率与精度</h2><p>错误率是分类错误的样本数占样本总数的比例</p><p>$$<br>E(f,D)&#x3D;\frac{1}{m}\sum^m_{i&#x3D;1}\mathbb{I}(f(\mathbb{x_i})\neq y_i)\&#x3D;\frac{FP+FN}{m}<br>$$</p><p>精度则是分类正确的样本数占样本总数的比例</p><p>$$<br>Acc(f;D)&#x3D;\frac{1}{m}\sum^m_{i&#x3D;1}\mathbb{I}(f(\mathbb{x_i}) &#x3D; y_i) \<br>&#x3D;1-E(f;D)&#x3D;\frac{TN+TP}{m}<br>$$</p><p>更一般的，对于数据分布$\mathcal{D}$和概率分布函数$p(·)$，二者可表达为<br>$<br>E(f;\mathcal{D})&#x3D;\int_{\mathbb{x}\sim \mathcal{D}}\mathbb{I}(f(\mathbb{x})\neq y)p(\mathbb{x})d\mathbb{x}，\<br>Acc(f;\mathcal{D})&#x3D;\int_{\mathbb{x}\sim \mathcal{D}}\mathbb{I}(f(\mathbb{x})&#x3D; y)p(\mathbb{x})d\mathbb{x}，\<br>$<br>但是对于数据集中样本不均衡的情况来说，精度就会出现问题。例如异常检测，几百万的数据中可能仅会有数十例不正常样本。即使精度达到99%，也并不能保证模型就是好的模型</p><h2 id="查准率与召回率"><a href="#查准率与召回率" class="headerlink" title="查准率与召回率"></a>查准率与召回率</h2><p>查准率P,预测为正例中真正标记为正的概率（是针对<strong>预测结果</strong>而言的）</p><p>$precision&#x3D;\frac{TP}{TP+FP}$</p><p>查准率R,真正标记为正的样本被正确反映出来概率（是针对<strong>原来样本</strong>而言的）<br>$<br>recall&#x3D;\frac{TP}{TP+FN}$</p><h2 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h2><p>查准率和查全率往往是一对矛盾的度量。简单来说，查准率高，查全率就低。反之亦然。我们可以根据学习器的预测结果对样例进行排序，排在前面的是学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”的正例样本。按此顺序逐个把样本作为正例进行预测，每次可以计算出当前的P、R值。我们可以绘制一幅图。</p><p><img src="/src/image-20220104120445935.png" alt="image-20220104120445935"></p><h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>F1 Score为精准率和召回率的<strong>调和均值</strong></p><p>$$<br>\frac{1}{F_1}&#x3D;\frac{1}{2}(\frac{1}{P}+\frac{1}{R})\<br>\Longleftrightarrow F_1&#x3D;\frac{2 \times P \times R}{P+ R} &#x3D; \frac{2 \times TP}{m+TP-TN}<br>$$</p><h2 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h2><p>ROC出现的动机</p><blockquote><p><strong>Motivation1：</strong>在一个二分类模型中，对于所得到的连续结果，假设已确定一个阀值，比如说 0.6，大于这个值的实例划归为正类，小于这个值则划到负类中。如果减小阀值，减到0.5，固然能识别出更多的正类，也就是提高了识别出的正例占所有正例 的比类，即TPR,但同时也将更多的负实例当作了正实例，即提高了FPR。为了形象化这一变化，引入ROC，ROC曲线可以用于评价一个分类器。</p><p><strong>Motivation2：</strong>在类不平衡的情况下,如正样本990个,负样本10个,直接把所有样本分类为正样本,得到识别率为99%。但这显然是没有意义的。单纯根据Precision和Recall来衡量算法的优劣已经不能表征这种病态问题。</p></blockquote><p>维基百科中的定义：In signal detection theory, a receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system <strong>as its discrimination threshold is varied.</strong></p><p>ROC关注两个指标：</p><blockquote><p><strong>True  Positive Rate:</strong>  TPR &#x3D; TP &#x2F; (TP+FN) → 将正例分对的概率<br> <strong>Fales Positive Rate:</strong>  FPR &#x3D; FP &#x2F; (FP+TN) → 将负例错分为正例的概率</p></blockquote><p>在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off。</p><p><img src="/src/image-20220104115651709.png" alt="image-20220104115651709"></p><p>ROC曲线中的四个点和一条线:</p><ul><li>点(0,1)：即FPR&#x3D;0, TPR&#x3D;1，意味着FN＝0且FP＝0，将所有的样本都正确分类；</li><li>点(1,0)：即FPR&#x3D;1，TPR&#x3D;0，最差分类器，避开了所有正确答案；</li><li>点(0,0)：即FPR&#x3D;TPR&#x3D;0，FP＝TP＝0，分类器预测所有的样本都为负样本（negative）；</li><li>点(1,1)：分类器实际上预测所有的样本都为正样本。</li><li><strong>总之：</strong>ROC曲线越接近左上角，该分类器的性能越好。</li></ul><p>两个模型用这个指标做评价的时候，如果一个模型A的ROC曲线可以包裹住另一个模型B，那么这个模型A就优于模型B。如果A和B两个模型有交叉，那么这个就看它们曲线下的面积大了，面积大的模型性能更优。</p><h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是 AUC 值。<br> <strong>总之：</strong>AUC值越大的分类器，正确率越高。</p><ul><li>AUC &#x3D; 1：绝对完美分类器，理想状态下，100%完美识别正负类，不管阈值怎么设定都能得出完美预测，绝大多数预测不存在完美分类器；</li><li>0.5&lt;AUC&lt;1：优于随机猜测。这个分类器（模型）妥善设定阈值的话，可能有预测价值；</li><li>AUC&#x3D;0.5：跟随机猜测一样（例：随机丢N次硬币，正反出现的概率为50%），模型没有预测价值；</li><li>AUC&lt;0.5：比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在AUC&lt;0.5的状况。</li></ul><p><img src="/src/image-20220104115856538.png" alt="image-20220104115856538"></p><p>同样的AUC的计算不局限于ROC曲线下,相同的情况放在PR曲线也可以。</p><p>AUC的计算方法：</p><p>第一种方法:AUC为ROC曲线下的面积,那我们直接计算面积可得。面积为一个个小的梯形面积之和。计算的精度与阈值的精度有关。</p><p>第二种方法:根据AUC的物理意义,我们计算正样本score大于负样本的score的概率。取$NM$(N为正样本数,M为负样本数)个二元组,比较score,最后得到AUC。时间复杂度为$O(NM)$。</p><p>第三种方法:与第二种方法相似,直接计算正样本score大于负样本的概率。我们首先把所有样本按照score排序,依次用rank表示他们,如最大score的样本,rank&#x3D;n(n&#x3D;N M),其次为n-1。那么对于正样本中rank最大的样本,rank_max,有M-1个其他正样本比他score小,那么就有(rank_max-1)-(M-1)个负样本比他score小。其次为(rank_second-1)-(M-2)。最后我们得到正样本大于负样本的概率为</p><p>$$<br>\frac{\sum_{所有正样本}rank-M(M+1)&#x2F;2}{M * N} \</p><p>时间复杂度为O(N+M)。<br>$$</p><h1 id="回归问题的性能指标"><a href="#回归问题的性能指标" class="headerlink" title="回归问题的性能指标"></a>回归问题的性能指标</h1><h2 id="平均平方误差-MSE"><a href="#平均平方误差-MSE" class="headerlink" title="平均平方误差 MSE"></a>平均平方误差 MSE</h2><p>均方误差，也称平均平方误差Mean Square Error（MSE）,即$L_2$范数损失</p><p>对于一般离散的样本点，我们可以有以下的方式计算</p><p>$$<br>E(f;D)&#x3D;\frac{1}{m}\sum^m_{i&#x3D;1}\mathbb{I}(f(\mathbb{x_i})-y_i)^2\<br>&#x3D;\frac{1}{m}\sum^m_{i&#x3D;1} {\vert \vert f(\mathbb{x_i})-y_i\vert \vert}^2<br>$$</p><p>更一般的，对于数据分布$\mathcal{D}$和概率分布函数$p(·)$，可表达为</p><p>$$<br>vE(f;\mathcal{D})&#x3D;\int_{\mathbb{x}\sim \mathcal{D}}(f(\mathbb{x})-y)^2p(\mathbb{x})d\mathbb{x}vv<br>$$</p><h2 id="平均绝对误差-MAE"><a href="#平均绝对误差-MAE" class="headerlink" title="平均绝对误差 MAE"></a>平均绝对误差 MAE</h2><p>平均绝对误差 MAE，即是$L_1$范数损失。</p><p>$E(f;D)&#x3D;\frac{1}{m}\sum^m_{i&#x3D;1} {\vert f(\mathbb{x_i})-y_i \vert}$<br>vert}$</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——回归模型</title>
    <link href="/2022/08/01/MachineLearning/%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/08/01/MachineLearning/%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是回归分析？"><a href="#什么是回归分析？" class="headerlink" title="什么是回归分析？"></a><strong>什么是回归分析？</strong></h2><p>回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。例如，司机的鲁莽驾驶与道路交通事故数量之间的关系，最好的研究方法就是回归。</p><p>回归分析是建模和分析数据的重要工具。在这里，我们使用曲线&#x2F;线来拟合这些数据点，在这种方式下，从曲线或线到数据点的距离差异最小。我会在接下来的部分详细解释这一点。</p><p><img src="/src/640.png" alt="图片"></p><h2 id="我们为什么使用回归分析？"><a href="#我们为什么使用回归分析？" class="headerlink" title="我们为什么使用回归分析？"></a><strong>我们为什么使用回归分析？</strong></h2><p>如上所述，回归分析估计了两个或多个变量之间的关系。下面，让我们举一个简单的例子来理解它：</p><p>比如说，在当前的经济条件下，你要估计一家公司的销售额增长情况。现在，你有公司最新的数据，这些数据显示出销售额增长大约是经济增长的2.5倍。那么使用回归分析，我们就可以根据当前和过去的信息来预测未来公司的销售情况。</p><p>使用回归分析的好处良多。具体如下：</p><ol><li><p>它表明自变量和因变量之间的显著关系；</p></li><li><p>它表明多个自变量对一个因变量的影响强度。</p></li></ol><p>回归分析也允许我们去比较那些衡量不同尺度的变量之间的相互影响，如价格变动与促销活动数量之间联系。这些有利于帮助市场研究人员，数据分析人员以及数据科学家排除并估计出一组最佳的变量，用来构建预测模型。</p><h2 id="我们有多少种回归技术？"><a href="#我们有多少种回归技术？" class="headerlink" title="我们有多少种回归技术？"></a><strong>我们有多少种回归技术？</strong></h2><p>有各种各样的回归技术用于预测。这些技术主要有三个度量（自变量的个数，因变量的类型以及回归线的形状）。我们将在下面的部分详细讨论它们。</p><p><img src="/src/640-16559035604247.jpeg" alt="图片"></p><p>对于那些有创意的人，如果你觉得有必要使用上面这些参数的一个组合，你甚至可以创造出一个没有被使用过的回归模型。但在你开始之前，先了解如下最常用的回归方法：</p><h3 id="1-Linear-Regression线性回归"><a href="#1-Linear-Regression线性回归" class="headerlink" title="1.Linear Regression线性回归"></a><strong>1.Linear Regression线性回归</strong></h3><p>它是最为人熟知的建模技术之一。线性回归通常是人们在学习预测模型时首选的技术之一。在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。</p><p>线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。</p><p>用一个方程式来表示它，即$Y&#x3D;a+b*X+e$，其中a表示截距，b表示直线的斜率，e是误差项。这个方程可以根据给定的预测变量（s）来预测目标变量的值。</p><p><img src="/src/640-165590356364210.png" alt="图片"></p><p>一元线性回归和多元线性回归的区别在于，多元线性回归有（&gt;1）个自变量，而一元线性回归通常只有1个自变量。现在的问题是“我们如何得到一个最佳的拟合线呢？”。</p><h4 id="如何获得最佳拟合线（a和b的值）？"><a href="#如何获得最佳拟合线（a和b的值）？" class="headerlink" title="如何获得最佳拟合线（a和b的值）？"></a><strong>如何获得最佳拟合线（a和b的值）？</strong></h4><p>这个问题可以使用最小二乘法轻松地完成。最小二乘法也是用于拟合回归线最常用的方法。对于观测数据，它通过最小化每个数据点到线的垂直偏差平方和来计算最佳拟合线。因为在相加时，偏差先平方，所以正值和负值没有抵消。</p><p><img src="/src/640-165590356617213.jpeg" alt="图片"></p><p>我们可以使用R-square指标来评估模型性能。想了解这些指标的详细信息，可以阅读：模型性能指标Part 1,Part 2.</p><blockquote><p><strong>要点：</strong></p><p>1.自变量与因变量之间必须有线性关系</p><p>2.多元回归存在多重共线性，自相关性和异方差性。</p><p>3.线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。</p><p>4.多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感。结果就是系数估计值不稳定</p><p>5.在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。</p></blockquote><h3 id="2-Logistic-Regression逻辑回归"><a href="#2-Logistic-Regression逻辑回归" class="headerlink" title="2.Logistic Regression逻辑回归"></a><strong>2.Logistic Regression逻辑回归</strong></h3><p>逻辑回归是用来计算“事件&#x3D;Success”和“事件&#x3D;Failure”的概率。当因变量的类型属于二元（1 &#x2F; 0，真&#x2F;假，是&#x2F;否）变量时，我们就应该使用逻辑回归。这里，Y的值从0到1，它可以用下方程表示。</p><p>$odds&#x3D; \frac{p}{1-p} &#x3D; \frac{probability \ of \ event \ occurrence}{probability\ of\ not\ event\ occurrence\ ln(odds)} &#x3D; ln(\frac{p}{1-p})logit(p) &#x3D; ln(\frac{p}{1-p}) &#x3D; b_0+b_1X_1+b_2X_2+b_3X_3+b_kX_k$</p><p>上述式子中，p表述具有某个特征的概率。你应该会问这样一个问题：“我们为什么要在公式中使用对数log呢？”。</p><p>因为在这里我们使用的是的二项分布（因变量），我们需要选择一个对于这个分布最佳的连结函数。它就是Logit函数。在上述方程中，通过观测样本的极大似然估计值来选择参数，而不是最小化平方和误差（如在普通回归使用的）。</p><p><img src="/src/640-165590356921716.jpeg" alt="图片"></p><blockquote><p><strong>要点：</strong></p><p>1.它广泛的用于分类问题。</p><p>2.逻辑回归不要求自变量和因变量是线性关系。它可以处理各种类型的关系，因为它对预测的相对风险指数OR使用了一个非线性的log转换。</p><p>3.为了避免过拟合和欠拟合，我们应该包括所有重要的变量。有一个很好的方法来确保这种情况，就是使用逐步筛选方法来估计逻辑回归。</p><p>4.它需要大的样本量，因为在样本数量较少的情况下，极大似然估计的效果比普通的最小二乘法差。</p><p>5.自变量不应该相互关联的，即不具有多重共线性。然而，在分析和建模中，我们可以选择包含分类变量相互作用的影响。</p><p>6.如果因变量的值是定序变量，则称它为序逻辑回归。</p><p>7.如果因变量是多类的话，则称它为多元逻辑回归。</p></blockquote><h3 id="3-Polynomial-Regression多项式回归"><a href="#3-Polynomial-Regression多项式回归" class="headerlink" title="3.Polynomial Regression多项式回归"></a><strong>3.Polynomial Regression多项式回归</strong></h3><p>对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。如下方程所示：$y&#x3D;a+b*x^2$</p><p>在这种回归技术中，最佳拟合线不是直线。而是一个用于拟合数据点的曲线。</p><p><img src="/src/640-165590357936919.png" alt="图片"></p><blockquote><p>重点：</p><p>虽然会有一个诱导可以拟合一个高次多项式并得到较低的错误，但这可能会导致过拟合。你需要经常画出关系图来查看拟合情况，并且专注于保证拟合合理，既没有过拟合又没有欠拟合。</p></blockquote><p>下面是一个图例，可以帮助理解：</p><p><img src="/src/640-165590358261822.png" alt="图片"></p><p>明显地向两端寻找曲线点，看看这些形状和趋势是否有意义。更高次的多项式最后可能产生怪异的推断结果。</p><h3 id="4-Stepwise-Regression逐步回归"><a href="#4-Stepwise-Regression逐步回归" class="headerlink" title="4.Stepwise Regression逐步回归"></a><strong>4.Stepwise Regression逐步回归</strong></h3><p>在处理多个自变量时，我们可以使用这种形式的回归。在这种技术中，自变量的选择是在一个自动的过程中完成的，其中包括非人为操作。</p><p>这一壮举是通过观察统计的值，如R-square，t-stats和AIC指标，来识别重要的变量。逐步回归通过同时添加&#x2F;删除基于指定标准的协变量来拟合模型。</p><p>下面列出了一些最常用的逐步回归方法：</p><ul><li>标准逐步回归法做两件事情。即增加和删除每个步骤所需的预测。</li><li>向前选择法从模型中最显著的预测开始，然后为每一步添加变量。</li><li>向后剔除法与模型的所有预测同时开始，然后在每一步消除最小显着性的变量。</li></ul><p>这种建模技术的目的是使用最少的预测变量数来最大化预测能力。这也是处理高维数据集的方法之一。</p><h3 id="5-Ridge-Regression岭回归"><a href="#5-Ridge-Regression岭回归" class="headerlink" title="5.Ridge Regression岭回归"></a><strong>5.Ridge Regression岭回归</strong></h3><p>岭回归分析是一种用于存在多重共线性（自变量高度相关）数据的技术。在多重共线性情况下，尽管最小二乘法（OLS）对每个变量很公平，但它们的差异很大，使得观测值偏移并远离真实值。岭回归通过给回归估计上增加一个偏差度，来降低标准误差。</p><p>上面，我们看到了线性回归方程。还记得吗？它可以表示为：</p><p>$y&#x3D;a+b*x$这个方程也有一个误差项。完整的方程是：</p><p>y&#x3D;a+b*x+e (error term),  [error term is the value needed to correct for a prediction error between the observed and predicted value]</p><p>&#x3D;&gt; y&#x3D;a+y&#x3D; a+ b1x1+ b2x2++e, for multiple independent variables.</p><p>在一个线性方程中，预测误差可以分解为2个子分量。一个是偏差，一个是方差。预测错误可能会由这两个分量或者这两个中的任何一个造成。在这里，我们将讨论由方差所造成的有关误差。</p><p>岭回归通过收缩参数λ（lambda）解决多重共线性问题。看下面的公式</p><p><img src="/src/640-165590358924025.png" alt="图片"></p><p>在这个公式中，有两个组成部分。第一个是最小二乘项，另一个是β2（β-平方）的λ倍，其中β是相关系数。为了收缩参数把它添加到最小二乘项中以得到一个非常低的方差。</p><blockquote><p>要点：</p><p>1.除常数项以外，这种回归的假设与最小二乘回归类似；</p><p>2.它收缩了相关系数的值，但没有达到零，这表明它没有特征选择功能</p><p>3.这是一个正则化方法，并且使用的是L2正则化。</p></blockquote><h3 id="6-Lasso-Regression套索回归"><a href="#6-Lasso-Regression套索回归" class="headerlink" title="6.Lasso Regression套索回归"></a><strong>6.Lasso Regression套索回归</strong></h3><p>它类似于岭回归，Lasso （Least Absolute Shrinkage and Selection Operator）也会惩罚回归系数的绝对值大小。此外，它能够减少变化程度并提高线性回归模型的精度。看看下面的公式：</p><p><img src="/src/640-165590359151028.jpeg" alt="图片"></p><p>Lasso 回归与Ridge回归有一点不同，它使用的惩罚函数是绝对值，而不是平方。这导致惩罚（或等于约束估计的绝对值之和）值使一些参数估计结果等于零。使用惩罚值越大，进一步估计会使得缩小值趋近于零。这将导致我们要从给定的n个变量中选择变量。</p><blockquote><p>要点：</p><p>1.除常数项以外，这种回归的假设与最小二乘回归类似；</p><p>2.它收缩系数接近零（等于零），这确实有助于特征选择；</p><p>3.这是一个正则化方法，使用的是L1正则化；</p><p>如果预测的一组变量是高度相关的，Lasso 会选出其中一个变量并且将其它的收缩为零。</p></blockquote><h3 id="7-ElasticNet回归"><a href="#7-ElasticNet回归" class="headerlink" title="7.ElasticNet回归"></a><strong>7.ElasticNet回归</strong></h3><p>ElasticNet是Lasso和Ridge回归技术的混合体。它使用L1来训练并且L2优先作为正则化矩阵。当有多个相关的特征时，ElasticNet是很有用的。Lasso 会随机挑选他们其中的一个，而ElasticNet则会选择两个。</p><p><img src="/src/640-165590359434431.png" alt="图片"></p><p>Lasso和Ridge之间的实际的优点是，它允许ElasticNet继承循环状态下Ridge的一些稳定性。</p><blockquote><p><strong>要点：</strong></p><p>1.在高度相关变量的情况下，它会产生群体效应；</p><p>2.选择变量的数目没有限制；</p><p>3.它可以承受双重收缩。</p></blockquote><h2 id="如何正确选择回归模型？"><a href="#如何正确选择回归模型？" class="headerlink" title="如何正确选择回归模型？"></a><strong>如何正确选择回归模型？</strong></h2><p>当你只知道一个或两个技术时，生活往往很简单。我知道的一个培训机构告诉他们的学生，如果结果是连续的，就使用线性回归。如果是二元的，就使用逻辑回归！然而，在我们的处理中，可选择的越多，选择正确的一个就越难。类似的情况下也发生在回归模型中。</p><p>在多类回归模型中，基于自变量和因变量的类型，数据的维数以及数据的其它基本特征的情况下，选择最合适的技术非常重要。以下是你要选择正确的回归模型的<strong>关键因素</strong>：</p><ol><li>数据探索是构建预测模型的必然组成部分。在选择合适的模型时，比如识别变量的关系和影响时，它应该首选的一步。</li><li>比较适合于不同模型的优点，我们可以分析不同的指标参数，如统计意义的参数，R-square，Adjusted R-square，AIC，BIC以及误差项，另一个是Mallows’ Cp准则。这个主要是通过将模型与所有可能的子模型进行对比（或谨慎选择他们），检查在你的模型中可能出现的偏差。</li><li>交叉验证是评估预测模型最好额方法。在这里，将你的数据集分成两份（一份做训练和一份做验证）。使用观测值和预测值之间的一个简单均方差来衡量你的预测精度。</li><li>如果你的数据集是多个混合变量，那么你就不应该选择自动模型选择方法，因为你应该不想在同一时间把所有变量放在同一个模型中。</li><li>它也将取决于你的目的。可能会出现这样的情况，一个不太强大的模型与具有高度统计学意义的模型相比，更易于实现。</li><li>回归正则化方法（Lasso，Ridge和ElasticNet）在高维和数据集变量之间多重共线性情况下运行良好。�性情况下运行良好。</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——关于ELBO的推演</title>
    <link href="/2022/08/01/MachineLearning/%E5%85%B3%E4%BA%8EELBO%E7%9A%84%E6%8E%A8%E6%BC%94/"/>
    <url>/2022/08/01/MachineLearning/%E5%85%B3%E4%BA%8EELBO%E7%9A%84%E6%8E%A8%E6%BC%94/</url>
    
    <content type="html"><![CDATA[<p>当我们想研究一个变量的分布的时候，我们直接通过采样来推测整体分布，需要保持极大的个数。所以我们可以通过搭建一个隐变量，来计算出变量的分布。<br>$$<br>\phi: X \to Z \iff p_{\phi}(z|x)  \quad \text{variational parameter} \<br>\theta: Z \to \hat{X} \iff p_{\theta}(\hat{x}|z) \quad \text{generative parameter}<br>$$<br>因此，我们可以推导出一个目标函数：<br>$$<br>min_{\phi,\theta}L(x,(\phi,\theta)x)<br>$$<br>求出（2）式，就可以近似的得到了x的分布情况。</p><p>但是很遗憾的是关于以下两个公式没有办法计算出来。<br>$$<br>p_{\theta}(x)&#x3D;\int p_{\theta}(x|z)p_{\theta}(z)dz \<br>p_{\theta}(z|x) &#x3D; \frac{p(x|z)p(z)}{p(x)}<br>$$<br>OK，没有办法直接推算出来，那我们可以引入一个变量$q_{\phi}(z|x)$来接近$p_{\theta}(z|x)$。这里就可以利用KL散度来计算两个分布的近似程度。<br>$$<br>\begin{align}<br>D_{KL}(q_{\phi}(z|x)\  ||\ p_{\theta}(z|x)) &amp;&#x3D;E_{z\sim q_{\phi}(z|x)}[logq_{\phi}(z|x)-logp_{\theta}(z|x)] \<br>&amp;&#x3D;E_{z\sim q_{\phi}(z|x)}[logq_{\phi}(z|x)-logp_{\theta}(z,x)]+E_{z\sim q_{\phi}(z|x)}(logp_{\theta}(x))<br>\end{align}<br>$$<br>从上式我们可以看到式（5）后一部分是我们极感兴趣的东西。</p><p>OK,我们可以从$log(x)$​出发寻找一些有用的东西<br>$$<br>\begin{align}<br>\sum_{i}logp(x^i;\theta)&amp;&#x3D;\sum_{i}log\sum_{z_i}p(x^i,z_i;\theta)\<br>&amp;&#x3D;\sum_{i}log\sum_{z^i}q_i(x^i,z^i)\frac{p(x^i,z^i;\theta)}{q_i(x^i,z^i)} \<br>&amp;\ge  \sum_{i}\sum_{z^i}q_i(x^i,z^i)log\frac{p(x^i,z^i;\theta)}{q_i(x^i,z^i)} (Jensen \ Inequality)\<br>             \text{for i,get evidence lower bound ELBO:} \<br>logp(x^i;\theta)&amp;&#x3D;\sum_{z^j}q_i(x^i,z^j)log\frac{p(x^i,z^j;\theta)}{q_i(x^i,z^j)}\<br>&amp;&#x3D;E_{z\sim q_{i}(z|x^i)}[logp_{i}(z,x^i)-logq_{i}(z|x^i)] \<br>&amp;&#x3D;E_{z\sim q_{i}(z|x^i)}[log\frac{p_{i}(z,x^i)}{p(z)}-log\frac{q_{i}(z|x^i)}{p(z)}]\<br>&amp;&#x3D;E_{z\sim q_{i}(z|x^i)}[logp(x^i|z)]-D_{KL}(q_{i}(z|x^i) \ || \ p(z))</p><p>\end{align}<br>$$<br>OK，所以我们接下来需要进一步对式(11)进行细化<br>$$<br>\begin{align}<br>logp(x^i;\theta)&amp;&#x3D;E_{z\sim q_{i}(z|x^i)}[logp_{i}(z,x^i)-logq_{i}(z|x^i)] \<br>&amp;&#x3D;E_{z\sim q_{i}(z|x^i)}[log\frac{p_{i}(z,x^i)}{p(z)}-log\frac{q_{i}(z|x^i)}{p(z)}]\<br>&amp;&#x3D;E_{z\sim q_{i}(z|x^i)}[logp(x^i|z)]-D_{KL}(q_{i}(z|x^i) \ || \ p(z))<br>\end{align}<br>$$</p><p>$$<br>Jensen\ Inequality:\quad if \ f(x) \ is \ convex, \ f(E(x)) \ge E(f(x))<br>$$</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Auto Encoder</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——交叉验证</title>
    <link href="/2022/08/01/MachineLearning/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%96%B9%E6%B3%95/"/>
    <url>/2022/08/01/MachineLearning/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>在任何有监督机器学习项目的模型构建阶段，我们训练模型的目的是从标记的示例中学习所有权重和偏差的最佳值。</p><p>如果我们使用相同的标记示例来测试我们的模型，那么这将是一个方法论错误，因为一个只会重复刚刚看到的样本标签的模型将获得完美的分数，但无法预测任何有用的东西 - 未来的数据，这种情况称为过拟合。</p><p>为了克服过度拟合的问题，我们使用交叉验证。所以你必须知道什么是交叉验证？以及如何解决过拟合的问题？<img src="/src/2274220-20211212113146635-514573712.webp" alt="图片"></p><h1 id="什么是交叉验证？"><a href="#什么是交叉验证？" class="headerlink" title="什么是交叉验证？"></a>什么是交叉验证？</h1><p>交叉验证是一种用于估计机器学习模型性能的统计方法，它是一种评估统计分析结果如何推广到独立数据集的方法。</p><h1 id="它是如何解决过拟合问题的？"><a href="#它是如何解决过拟合问题的？" class="headerlink" title="它是如何解决过拟合问题的？"></a>它是如何解决过拟合问题的？</h1><p>在交叉验证中，我们将训练数据生成多个小的训练测试分割，使用这些拆分来调整您的模型。例如，在标准的 k 折交叉验证中，我们将数据划分为 k 个子集。然后，我们在 k-1 个子集上迭代训练算法，同时使用剩余的子集作为测试集。通过这种方式，我们可以在未参与训练的数据上测试我们的模型。</p><p>下面列出了这些技术方法：</p><ul><li><p>HoldOut 交叉验证</p></li><li><p>K-Fold 交叉验证</p></li><li><p>分层 K-Fold交叉验证</p></li><li><p>Leave P Out 交叉验证</p></li><li><p>留一交叉验证</p></li><li><p>蒙特卡洛 (Shuffle-Split)</p></li><li><p>时间序列（滚动交叉验证）</p></li></ul><h2 id="1、HoldOut-交叉验证"><a href="#1、HoldOut-交叉验证" class="headerlink" title="1、HoldOut 交叉验证"></a>1、HoldOut 交叉验证</h2><p>在这种交叉验证技术中，整个数据集被随机划分为训练集和验证集。根据经验，整个数据集的近 70% 用作训练集，其余 30% 用作验证集。<img src="/src/2274220-20211212113146701-1213545127.png" alt="图片"><strong>优点</strong></p><p>1.快速执行：因为我们必须将数据集拆分为训练集和验证集一次，并且模型将在训练集上仅构建一次，因此可以快速执行。</p><p><strong>缺点</strong></p><ol><li><p>不适合不平衡数据集：假设我们有一个不平衡数据集，它具有“0”类和“1”类。假设 80% 的数据属于“0”类，其余 20% 的数据属于“1”类。在训练集大小为 80%，测试数据大小为数据集的 20% 的情况下进行训练-测试分割。可能会发生“0”类的所有 80% 数据都在训练集中，而“1”类的所有数据都在测试集中。所以我们的模型不能很好地概括我们的测试数据，因为它之前没有看到过“1”类的数据。</p></li><li><p>大量数据无法训练模型。</p></li></ol><p>在小数据集的情况下，将保留一部分用于测试模型，其中可能具有我们的模型可能会错过的重要特征，因为它没有对该数据进行训练。</p><p><strong>代码片段</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris <br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split <br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression <br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>iris = load_iris()<br>X = iris.data<br>Y = iris.target <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Size of Dataset &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(X)))<br>logreg = LogisticRegression()<br>x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">42</span>)<br>logreg.fit(x_train, y_train)<br>predict = logreg.predict(x_test) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy score on training set is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(accuracy_score(logreg.predict(x_train), y_train))) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy score on test set is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(accuracy_score(predict, y_test)))<br></code></pre></td></tr></table></figure><blockquote><p> <img src="https://src2020.cnblogs.com/blog/2274220/202112/2274220-20211212122517173-1672251919.png"></p></blockquote><h2 id="2、K-折交叉验证"><a href="#2、K-折交叉验证" class="headerlink" title="2、K 折交叉验证"></a>2、K 折交叉验证</h2><p>在这种 K 折交叉验证技术中，整个数据集被划分为 K 个相等大小的部分。每个分区称为一个“折叠”。因此，因为我们有 K 个部分，所以我们称之为 K 折叠。一折用作验证集，其余 K-1 折用作训练集。</p><p>该技术重复 K 次，直到每个折叠用作验证集，其余折叠用作训练集。</p><p>模型的最终精度是通过取 k-models 验证数据的平均精度来计算的。<img src="/src/2274220-20211212113146944-1491600070.png" alt="图片"><strong>优点</strong></p><ol><li>整个数据集既用作训练集又用作验证集：</li></ol><p><strong>缺点</strong></p><ol><li><p>不用于不平衡的数据集：正如在 HoldOut 交叉验证的情况下所讨论的，在 K-Fold 验证的情况下也可能发生训练集的所有样本都没有样本形式类“1”，并且只有 类“0”。验证集将有一个类“1”的样本。</p></li><li><p>不适合时间序列数据：对于时间序列数据，样本的顺序很重要。但是在 K 折交叉验证中，样本是按随机顺序选择的。</p></li></ol><p><strong>代码片段</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris <br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score,KFold <br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br>iris = load_iris()<br>X = iris.data<br>Y = iris.target<br>logreg = LogisticRegression()<br>kf = KFold(n_splits=<span class="hljs-number">5</span>)<br>score = cross_val_score(logreg, X, Y, cv=kf) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Cross Validation Scores are &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score)) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Average Cross Validation score :&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score.mean()))<br></code></pre></td></tr></table></figure><h2 id="3、分层-K-折交叉验证"><a href="#3、分层-K-折交叉验证" class="headerlink" title="3、分层 K 折交叉验证"></a>3、分层 K 折交叉验证</h2><p>分层 K-Fold 是 K-Fold 交叉验证的增强版本，主要用于不平衡的数据集。就像 K-fold 一样，整个数据集被分成大小相等的 K-fold。</p><p>但是在这种技术中，每个折叠将具有与整个数据集中相同的目标变量实例比率。</p><p><img src="/src/2274220-20211212113147123-1500880612.png" alt="图片"><strong>优点</strong></p><ol><li>对于不平衡数据非常有效：分层交叉验证中的每个折叠都会以与整个数据集中相同的比率表示所有类别的数据。</li></ol><p><strong>缺点</strong></p><ol><li>不适合时间序列数据：对于时间序列数据，样本的顺序很重要。但在分层交叉验证中，样本是按随机顺序选择的。</li></ol><p><strong>代码片段</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris <br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score, StratifiedKFold <br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br>iris = load_iris()<br>X = iris.data<br>Y = iris.target<br>logreg = LogisticRegression()<br>stratifiedkf = StratifiedKFold(n_splits=<span class="hljs-number">5</span>)<br>score = cross_val_score(logreg, X, Y, cv=stratifiedkf) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Cross Validation Scores are &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score)) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Average Cross Validation score :&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score.mean()))<br></code></pre></td></tr></table></figure><h2 id="4、Leave-P-Out-交叉验证"><a href="#4、Leave-P-Out-交叉验证" class="headerlink" title="4、Leave P Out  交叉验证"></a>4、Leave P Out  交叉验证</h2><p>Leave P Out 交叉验证是一种详尽的交叉验证技术，其中 p 样本用作验证集，剩余的 np 样本用作训练集。</p><p>假设我们在数据集中有 100 个样本。如果我们使用 p&#x3D;10，那么在每次迭代中，10 个值将用作验证集，其余 90 个样本将用作训练集。</p><p>重复这个过程，直到整个数据集在 p-样本和 n-p 训练样本的验证集上被划分。</p><p><strong>优点</strong></p><p>所有数据样本都用作训练和验证样本。</p><p><strong>缺点</strong></p><ol><li><p>计算时间长：由于上述技术会不断重复，直到所有样本都用作验证集，因此计算时间会更长。</p></li><li><p>不适合不平衡数据集：与 K 折交叉验证相同，如果在训练集中我们只有 1 个类的样本，那么我们的模型将无法推广到验证集。</p></li></ol><p><strong>代码片段</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> LeavePOut, cross_val_score <br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris <br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br>iris = load_iris()<br>X = iris.data<br>Y = iris.target<br>lpo = LeavePOut(p=<span class="hljs-number">2</span>)<br>lpo.get_n_splits(X)<br>tree = RandomForestClassifier(n_estimators=<span class="hljs-number">10</span>, max_depth=<span class="hljs-number">5</span>, n_jobs=-<span class="hljs-number">1</span>)<br>score = cross_val_score(tree, X, Y, cv=lpo) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Cross Validation Scores are &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score)) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Average Cross Validation score :&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score.mean()))<br></code></pre></td></tr></table></figure><h2 id="5、留一交叉验证"><a href="#5、留一交叉验证" class="headerlink" title="5、留一交叉验证"></a>5、留一交叉验证</h2><p>留一交叉验证是一种详尽的交叉验证技术，其中 1 个样本点用作验证集，其余 n-1 个样本用作训练集。</p><p>假设我们在数据集中有 100 个样本。然后在每次迭代中，1 个值将用作验证集，其余 99 个样本作为训练集。因此，重复该过程，直到数据集的每个样本都用作验证点。</p><p>它与使用 p&#x3D;1 的 LeavePOut 交叉验证相同。<img src="/src/2274220-20211212113147240-1106296430.png" alt="图片"><strong>「代码片段」</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris <br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier <br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> LeaveOneOut, cross_val_score<br>iris = load_iris()<br>X = iris.data<br>Y = iris.target<br>loo = LeaveOneOut()<br>tree = RandomForestClassifier(n_estimators=<span class="hljs-number">10</span>, max_depth=<span class="hljs-number">5</span>, n_jobs=-<span class="hljs-number">1</span>)<br>score = cross_val_score(tree, X, Y, cv=loo) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Cross Validation Scores are &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score)) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Average Cross Validation score :&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score.mean()))<br></code></pre></td></tr></table></figure><h2 id="6、蒙特卡罗交叉验证（Shuffle-Split）"><a href="#6、蒙特卡罗交叉验证（Shuffle-Split）" class="headerlink" title="6、蒙特卡罗交叉验证（Shuffle Split）"></a>6、蒙特卡罗交叉验证（Shuffle Split）</h2><p>蒙特卡罗交叉验证，也称为Shuffle Split交叉验证，是一种非常灵活的交叉验证策略。在这种技术中，数据集被随机划分为训练集和验证集。</p><p>我们已经决定了要用作训练集的数据集的百分比和用作验证集的百分比。如果训练集和验证集大小的增加百分比总和不是 100，则剩余的数据集不会用于训练集或验证集。</p><p>假设我们有 100 个样本，其中 60% 的样本用作训练集，20% 的样本用作验证集，那么剩下的 20%( 100-(60+20)) 将不被使用。</p><p>这种拆分将重复我们必须指定的“n”次。</p><p><img src="/src/2274220-20211212113146806-2032744793.png" alt="图片"><strong>优点</strong></p><p>1.我们可以自由使用训练和验证集的大小。</p><p>2.我们可以选择重复的次数，而不依赖于重复的折叠次数。</p><p><strong>缺点</strong></p><ol><li><p>可能不会为训练集或验证集选择很少的样本。</p></li><li><p>不适合不平衡的数据集：在我们定义了训练集和验证集的大小后，所有的样本都是随机选择的，所以训练集可能没有测试中的数据类别 设置，并且该模型将无法概括为看不见的数据。</p></li></ol><p><strong>代码片段</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> ShuffleSplit, cross_val_score <br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris <br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression  <br>iris = load_iris()<br>logreg = LogisticRegression()<br>shuffle_split = ShuffleSplit(test_size=<span class="hljs-number">0.3</span>, train_size=<span class="hljs-number">0.5</span>, n_splits=<span class="hljs-number">10</span>)<br>scores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;cross Validation scores:n &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(scores)) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Average Cross Validation score :&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(scores.mean()))<br></code></pre></td></tr></table></figure><h2 id="7、时间序列交叉验证"><a href="#7、时间序列交叉验证" class="headerlink" title="7、时间序列交叉验证"></a>7、时间序列交叉验证</h2><p>什么是时间序列数据？</p><p>时间序列数据是在不同时间点收集的数据。由于数据点是在相邻时间段收集的，因此观测值之间可能存在相关性。这是区分时间序列数据与横截面数据的特征之一。</p><p>在时间序列数据的情况下如何进行交叉验证？</p><p>在时间序列数据的情况下，我们不能选择随机样本并将它们分配给训练集或验证集，因为使用未来数据中的值来预测过去数据的值是没有意义的。</p><p>由于数据的顺序对于时间序列相关问题非常重要，所以我们根据时间将数据拆分为训练集和验证集，也称为“前向链”方法或滚动交叉验证。</p><p>我们从一小部分数据作为训练集开始。基于该集合，我们预测稍后的数据点，然后检查准确性。</p><p>然后将预测样本作为下一个训练数据集的一部分包括在内，并对后续样本进行预测。</p><p><img src="/src/2274220-20211212113146957-262206822.png" alt="图片"><strong>优点</strong></p><p>最好的技术之一。</p><p><strong>缺点</strong></p><p>不适用于其他数据类型的验证：与其他技术一样，我们选择随机样本作为训练或验证集，但在该技术中数据的顺序非常重要。</p><p><strong>代码片段</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> TimeSeriesSplit<br>X = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>y = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>time_series = TimeSeriesSplit() <span class="hljs-built_in">print</span>(time_series) <br><span class="hljs-keyword">for</span> train_index, test_index <span class="hljs-keyword">in</span> time_series.split(X): <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;TRAIN:&quot;</span>, train_index, <span class="hljs-string">&quot;TEST:&quot;</span>, test_index)<br>    X_train, X_test = X[train_index], X[test_index]<br>    y_train, y_test = y[train_index], y[test_index]<br></code></pre></td></tr></table></figure><p>_index]</p><pre><code class="hljs"></code></pre>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——Regularization</title>
    <link href="/2022/08/01/MachineLearning/Regularization/"/>
    <url>/2022/08/01/MachineLearning/Regularization/</url>
    
    <content type="html"><![CDATA[<p>个人认为正则化这个字眼有点太过抽象和宽泛，<strong>其实正则化的本质很简单，就是对某一问题加以先验的限制或约束以达到某种特定目的的一种手段或操作。在算法中使用正则化的目的是防止模型出现过拟合。</strong>一提到正则化，很多同学可能马上会想到常用的L1范数和L2范数，在汇总之前，我们先看下LP范数是什么鬼。</p><h1 id="LP范数"><a href="#LP范数" class="headerlink" title="LP范数"></a>LP范数</h1><p>可以参考<a href="F:\notes\机器学习\Norm&Distance.md">Norm</a></p><p>范数简单可以理解为用来表征向量空间中的距离，而距离的定义很抽象，<strong>只要满足非负、自反、三角不等式就可以称之为距离。</strong></p><p>LP范数不是一个范数，而是一组范数，其定义如下：<br>$$<br>{\vert \vert x \vert \vert}_p &#x3D; (\sum^n_ix^p_i)^\frac{1}{p}<br>$$<br>p的范围是[1,$+\infty$]。<strong>p在(0,1)范围内定义的并不是范数，因为违反了三角不等式。</strong></p><p>根据p的变化，范数也有着不同的变化，借用一个经典的有关P范数的变化图如下：</p><p><img src="F:\notes\机器学习\src\640-1640743227862.webp" alt="图片"></p><p>上图表示了p从0到正无穷变化时，单位球（unit ball）的变化情况。在P范数下定义的单位球都是凸集，但是当0&lt;p&lt;1时，该定义下的单位球不是凸集（这个我们之前提过，当0&lt;p&lt;1时并不是范数）。</p><p>那问题来了，L0范数是啥玩意？</p><p>L0范数表示向量中非零元素的个数，用公式表示如下：<br>$$<br>{\vert \vert x \vert \vert}_0&#x3D;\mathbb{I}(i|x \neq 0)<br>$$<br>我们可以通过最小化L0范数，来寻找最少最优的稀疏特征项。但不幸的是，<strong>L0范数的最优化问题是一个NP hard问题（L0范数同样是非凸的）。因此，在实际应用中我们经常对L0进行凸松弛，理论上有证明，L1范数是L0范数的最优凸近似，因此通常使用L1范数来代替直接优化L0范数。</strong></p><h2 id="L1范数"><a href="#L1范数" class="headerlink" title="L1范数"></a>L1范数</h2><p>根据LP范数的定义我们可以很轻松的得到L1范数的数学形式：<br>$$<br>{\vert \vert x \vert \vert}_1 &#x3D; \sum^n_i \vert x \vert<br>$$<br>通过上式可以看到，L1范数就是向量各元素的绝对值之和，也被称为是”稀疏规则算子”（Lasso regularization）。那么问题来了，为什么我们希望稀疏化？稀疏化有很多好处，最直接的两个：</p><ul><li><strong>特征选择</strong></li><li><strong>可解释性</strong></li></ul><h2 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h2><p>L2范数是最熟悉的，它就是欧几里得距离，公式如下：<br>$$<br>{\vert \vert x \vert \vert}_2 &#x3D; (\sum^n_ix^2_i)^\frac{1}{2}<br>$$<br>L2范数有很多名称，有人把它的回归叫“岭回归”（Ridge Regression），也有人叫它“权值衰减”（Weight Decay）。<strong>以L2范数作为正则项可以得到稠密解，即每个特征对应的参数w都很小，接近于0但是不为0；此外，L2范数作为正则化项，可以防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。</strong></p><h2 id="L1范数和L2范数的区别"><a href="#L1范数和L2范数的区别" class="headerlink" title="L1范数和L2范数的区别"></a>L1范数和L2范数的区别</h2><p>L1范数可以导致稀疏解，L2范数导致稠密解。</p><p>从贝叶斯先验的角度看，当训练一个模型时，仅依靠当前的训练数据集是不够的，为了实现更好的泛化能力，往往需要加入先验项，而加入正则项相当于加入了一种先验。</p><ul><li><strong>L1范数相当于加入了一个Laplacean先验；</strong></li><li><strong>L2范数相当于加入了一个Gaussian先验。</strong></li></ul><p>如下图所示：<img src="F:\notes\机器学习\src\640-1640743227869.webp" alt="图片"></p><p>从数学角度来解释这方面的东西，使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为<em>特征选择</em>，这可能是其他场景下需要的。</p><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>我们介绍了通过惩罚权重的$L_2$范数来正则化统计模型的经典方法。 在概率角度看，我们可以通过以下论证来证明这一技术的合理性： 我们已经假设了一个先验，即权重的值取自均值为0的高斯分布。 更直观的是，我们希望模型深度挖掘特征，即将其权重分散到许多特征中， 而不是过于依赖少数潜在的虚假关联。</p><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p><p>泛化性和灵活性之间的这种基本权衡被描述为<em>偏差-方差权衡</em>（bias-variance tradeoff）。 线性模型有很高的偏差：它们只能表示一小类函数。 然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出了相似的结果。</p><p><strong>深度神经网络</strong>位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。</p><p>Dropout是深度学习中经常采用的一种正则化方法。它的做法可以简单的理解为在DNNs训练的过程中以概率p丢弃部分神经元，即使得被丢弃的神经元输出为0。Dropout可以实例化的表示为下图：</p><p><img src="F:\notes\机器学习\src\640-1640743227877.webp" alt="图片"></p><p>我们可以从两个方面去直观地理解Dropout的正则化效果：</p><ul><li>在Dropout每一轮训练过程中随机丢失神经元的操作相当于多个DNNs进行取平均，因此用于预测具有vote的效果。</li><li><strong>减少神经元之间复杂的共适应性。当隐藏层神经元被随机删除之后，使得全连接网络具有了一定的稀疏化，从而有效地减轻了不同特征的协同效应。</strong>也就是说，有些特征可能会依赖于固定关系的隐含节点的共同作用，而通过Dropout的话，就有效地组织了某些特征在其他特征存在下才有效果的情况，增加了神经网络的鲁棒性。</li></ul><p>Dropout会用于在层(多半是全连接层)之间产生噪声，但是并不希望改变原有的期望所以产生的噪音$\xi \sim N(0, \sigma^2)$，对原有的输出进行变化$x’ &#x3D; x + \xi$,从而产生扰动，预期是$E[x’]&#x3D;x$。</p><p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值$h$以<em>暂退概率</em>$p$由随机变量$h’$替换，如下所示：<br>$$<br>h’&#x3D;\left{\begin{array}{ll}<br>        {0} &amp; {\text {该点概率为p}} \<br>        {\frac{h}{1-p}} &amp; {\text { otherwise }}<br>        \end{array}\right.<br>$$<br>根据此模型的设计，其期望值保持不变，即$E[h’]&#x3D;h$</p><p>所以经过Dropout后的层$h$输出的期望不变，层内每个节点$x$输出的期望不变</p><h1 id="归一化、标准化-amp-正则化"><a href="#归一化、标准化-amp-正则化" class="headerlink" title="归一化、标准化 &amp; 正则化"></a>归一化、标准化 &amp; 正则化</h1><p>正则化我们以及提到过了，这里简单提一下归一化和标准化。</p><p>归一化（Normalization）：归一化的目标是找到某种映射关系，将原数据映射到[a,b]区间上。一般a，b会取[-1,1]，[0,1]这些组合 。</p><p>一般有两种应用场景：</p><ul><li><strong>把数变为(0, 1)之间的小数</strong></li><li><strong>把有量纲的数转化为无量纲的数</strong></li></ul><p>常用min-max normalization：</p><p><strong>标准化（Standardization）：用大数定理将数据转化为一个标准正态分布</strong>，标准化公式为：<br>$$<br>x^{‘} &#x3D; \frac{x-\mu}{\sigma}<br>$$<br><strong>归一化和标准化的区别：</strong></p><p>我们可以这样简单地解释：</p><p><strong>归一化的缩放是“拍扁”统一到区间（仅由极值决定），而标准化的缩放是更加“弹性”和“动态”的，和整体样本的分布有很大的关系。</strong></p><p>值得注意：</p><p>归一化：缩放仅仅跟最大、最小值的差别有关。</p><p>标准化：缩放和每个点都有关系，通过方差（variance）体现出来。与归一化对比，标准化中所有数据点都有贡献（通过均值和标准差造成影响）。</p><p><strong>为什么要标准化和归一化？</strong></p><ul><li><strong>提升模型精度：</strong>归一化后，不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。</li><li><strong>加速模型收敛：</strong>标准化后，最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解。如下图所示：</li></ul><p><img src="F:\notes\机器学习\src\640-1640743227884.webp" alt="图片"></p><p><img src="F:\notes\机器学习\src\640-1640743227893.webp" alt="图片"></p><h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="什么是Batch-Normalization"><a href="#什么是Batch-Normalization" class="headerlink" title="什么是Batch Normalization"></a>什么是Batch Normalization</h2><p>批规范化（Batch Normalization）严格意义上讲属于归一化手段，主要用于加速网络的收敛，但也具有一定程度的正则化效果。</p><p>这里借鉴下魏秀参博士的知乎回答中对covariate shift的解释。</p><blockquote><p><strong>大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。</strong>如果不一致，那么就出现了新的机器学习问题，如transfer learning&#x2F;domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。</p><p>BN的基本思想其实相当直观，因为神经网络在做非线性变换前的激活输入值（X &#x3D; WU + B，U是输入），随着网络深度加深，其分布逐渐发生偏移或者变动（即上述的covariate shift）。之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值（X &#x3D; WU + B）是大的负值和正值。所以这导致后向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。<strong>而 BN 就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，避免因为激活函数导致的梯度弥散问题。所以与其说BN的作用是缓解covariate shift，倒不如说BN可缓解梯度弥散问题。</strong></p></blockquote><p>谷歌在2015年就提出了Batch Normalization(BN)，该方法对每个mini-batch都进行normalize，下图是BN的计算方式，会把mini-batch中的数据正规化到均值为0，标准差为1，同时还引入了两个可以学的参数，分别为scale和shift，让模型学习其适合的分布。</p><p><img src="F:\notes\机器学习\src\640-1640746074269.webp" alt="图片"></p><p>  <strong>那么为什么在做过正规化后，又要scale和shift呢？</strong></p><p>  当通过正规化后，把尺度缩放到0均值，再scale和shift，不是有可能把数据变回”原样”？因为scale和shift是模型自动学习的，神经网络可以自己琢磨前面的正规化有没有起到优化作用，没有的话就”反”正规化，抵消之前的正规化操作带来的影响。</p><p>  BatchNormalization是对<strong>一批样本</strong>进行处理, 对一批样本的<strong>每个特征</strong>分别进行归一化,举个简单的例子,加入我有一批样本, 每个样本有三个特征,,分别是身高,体重,年龄,那么我做归一化的时候,就是对体重做归一化,对身高做归一化,对年龄做归一化,三者之间不会有交叉影响。</p><p><img src="F:\notes\机器学习\src\640-1640746074268.webp" alt="图片"></p><h2 id="什么是LayerNormalization"><a href="#什么是LayerNormalization" class="headerlink" title="什么是LayerNormalization?"></a><strong>什么是LayerNormalization?</strong></h2><p>  LayerNormalization是对一个样本进行处理，对一个样本的所有特征进行归一化，乍一看很没有道理，因为如果对身高体重和年龄一起求一个均值方差，都不知道这些值有什么含义，但存在一些场景却非常有效果——NLP领域。</p><p><img src="F:\notes\机器学习\src\640-1640746074271.webp" alt="图片"></p><p> 在NLP中，N个特征都可能表示不同的词，这个时候我们仍然采用BatchNormalization的话，对第一个词进行操作，很显然意义就不是非常大了，因为任何一个词都可以放在第一个位置，而且很多时候词序对于我们对于句子的影响没那么大，而此时我们对N个词进行Normalization等操作可以很好地反映句子的分布。(LN一般用在第三维度，[batchsize, seq_len,dims])，因为该维度特征的量纲是相同的，所以并没有太多区别。</p><h2 id="为什么要用Normalization"><a href="#为什么要用Normalization" class="headerlink" title="为什么要用Normalization"></a>为什么要用Normalization</h2><h3 id="解决梯度消失问题"><a href="#解决梯度消失问题" class="headerlink" title="解决梯度消失问题"></a>解决梯度消失问题</h3><p>拿sigmoid激活函数距离，从图中，我们很容易知道，数据值越靠近0梯度越大，越远离0梯度越接近0，我们通过BN改变数据分布到0附近，从而解决梯度消失问题。</p><p><img src="F:\notes\机器学习\src\640-1640746459970.webp"></p><h3 id="解决了Internal-Covariate-Shift-ICS-问题"><a href="#解决了Internal-Covariate-Shift-ICS-问题" class="headerlink" title="解决了Internal Covariate Shift(ICS)问题"></a><strong>解决了Internal Covariate Shift(ICS)问题</strong></h3><blockquote><p>we define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training</p></blockquote><p>由于训练过程中参数的变化，导致各层数据分布变化较大，神经网络就要学习新的分布，随着层数的加深，学习过程就变的愈加困难，要解决这个问题需要使用较低的学习率，由此又产生收敛速度慢，因此引入BN可以很有效的解决这个问题。</p><h3 id="加速模型收敛"><a href="#加速模型收敛" class="headerlink" title="加速模型收敛"></a><strong>加速模型收敛</strong></h3><p>和对原始特征做归一化类似，BN使得每一维数据对结果的影响是相同的，由此就能加速模型的收敛速度。</p><p><img src="F:\notes\机器学习\src\640-1640746649895.webp" alt="图片"></p><h3 id="具有正则化的效果"><a href="#具有正则化的效果" class="headerlink" title="具有正则化的效果"></a>具有正则化的效果</h3><p> BatchNormalization层和正规化&#x2F;归一化不同，BatchNormalization层是在mini-batch中计算均值方差，因此会带来一些较小的噪声，在神经网络中添加随机噪声可以带来正则化的效果。</p><h2 id="在CTR问题中的蜜汁效果"><a href="#在CTR问题中的蜜汁效果" class="headerlink" title="在CTR问题中的蜜汁效果"></a><strong>在CTR问题中的蜜汁效果</strong></h2><p>  在非常多CTR相关的论文中,很多工作主要都Focus在模型结构方面的优化或者引入新的信息等,而这么做往往都忽略了模型中的一些小的模块的作用,例如Normalization,在CTR相关的结构中我们发现,大家经常会把BatchNorm放在最后的MLP层, 但是这么做够吗？Normalization是否发挥了最大的作用？是否是最优的方案？本文通过大量的系统的实验,给出了结论：<strong>没有,还有更好的方案</strong>, 本文通过在CTR模型的不同地方加入不同的正则化策略(BatchNorm,LayerNorm等),最终取得了非常好的效果。那究竟是怎么做的呢？我们继续往下看，下面的框架很简单，显示作者提出模型的核心组成部分VO-LayerNorm,然后是基于此提出的新的NormDNN，最后是实验验证部分。</p><h3 id="Variance-Only-LayerNorm"><a href="#Variance-Only-LayerNorm" class="headerlink" title="Variance-Only LayerNorm"></a><strong>Variance-Only LayerNorm</strong></h3><p>  这是一个经验得出来的操作,作者在大量的实验中发现，原始的LayerNorm有些复杂化了,在对其进行不断的精简实验后,作者发现在CTR数据集上的效果并没有带来下降，反而更好了。下面我们看看这一步步精简的操作：</p><h3 id="复杂版本LayerNorm"><a href="#复杂版本LayerNorm" class="headerlink" title="复杂版本LayerNorm"></a><strong>复杂版本LayerNorm</strong></h3><p>假设我们一个Batch有H个样本,$x_1,x_2,\dots,x_{H}$ ,那么我们的LayerNorm可以通过下面的方式计算得到：$h &#x3D; g\bigodot N(x)+ b,\  N(x)&#x3D;\frac{x-\mu }{\sigma}$,其中$ \mu&#x3D;\frac{1}{H}\sum_{i&#x3D;1}^Hx_i, \ \sigma&#x3D;\sqrt{\frac{1}{H}\sum_{i&#x3D;1}^H(x_i-u)^2}$,LayerNorm在NLP任务中取得了非常好的效果,但是实践中,Xu等人发现这个LayerNorm的bias和gain增加了过拟合的风险,并且经过试验他们发现简化LayerNorm也可以取的非常不错的效果。</p><h3 id="简化版的LayerNorm"><a href="#简化版的LayerNorm" class="headerlink" title="简化版的LayerNorm"></a><strong>简化版的LayerNorm</strong></h3><p>我们把bias以及gain同时删除,得到精简版的LayerNorm,</p><p>$h&#x3D;N(x),\ N(x)&#x3D;\frac{x-\mu }{\sigma}$,其中$ \mu&#x3D;\frac{1}{H}\sum_{i&#x3D;1}^Hx_i, \ \sigma&#x3D;\sqrt{\frac{1}{H}\sum_{i&#x3D;1}^H(x_i-u)^2}$,在大量的实验中,我们发现简化版本的LayerNorm并没有什么性能损失,相反的还可以取的更好的效果。然后作者在CTR相关的数据集上又进行了大量的实验,发现对模型效果影响最大的不是re-centering等操作,反而方差带来的影响更大，于是作者提出了新的LayerNorm。</p><p>Variance-Only LayerNorm<br>$$<br>h&#x3D;\frac{x}{\sigma}<br>$$<br>其中$\mu&#x3D;\frac{1}{H}\sum_{i&#x3D;1}^Hx_i, \ \sigma&#x3D;\sqrt{\frac{1}{H}\sum_{i&#x3D;1}^H(x_i-u)^2}$,此处,作者直接除以了方差,虽然更加精简了,但是实验效果却显示这么做在CTR数据集上却可以取得更好的效果。</p><h2 id="NormDNN"><a href="#NormDNN" class="headerlink" title="NormDNN"></a><strong>NormDNN</strong></h2><p>在不同的地方使用不同形式的Normalization策略会带来什么样的影响呢？此处作者主要探索了两个部分, 特征Embedding部分的Normalization以及MLP部分的Normalization。在大量的实验之后，作者得到了一种提升最大的方案：</p><ol><li>对于数值类的特征，我们使用Variance-Only LayerNorm或者LayerNorm;</li><li>对于类别特征，我们使用BatchNorm;</li><li>对于MLP部分，我们使用LayerNorm;</li></ol><h3 id="在特征Embedding层的Normalization"><a href="#在特征Embedding层的Normalization" class="headerlink" title="在特征Embedding层的Normalization"></a>在特征Embedding层的Normalization</h3><p>假设我们有个域,我们原始的embedding可以表示为:</p><p>$V_{emb}&#x3D;concat(e_1,e_2,\dots,e_i,\dots,e_f),\ e_i\in\mathbb{R}^k$,</p><p>表示每个field的embedding的维度;</p><p>我们在该基础上加入Normalization,得到</p><p>$N(V_{emb})&#x3D;concat(N(e_1),N(e_2),\dots,N(e_i),\dots,N(e_f)),\ e_i\in\mathbb{R}^k$,</p><p>此处的可以是LayerNorm,BatchNorm等。</p><h3 id="在MLP处加入Normalization"><a href="#在MLP处加入Normalization" class="headerlink" title="在MLP处加入Normalization"></a>在MLP处加入Normalization</h3><p>此处作者发现在非线性的激活函数之前加入Normalization操作的效果是比先使用激活函数再做Normalization处理要好的。</p><h3 id="为什么Normalization是有效的？"><a href="#为什么Normalization是有效的？" class="headerlink" title="为什么Normalization是有效的？"></a>为什么Normalization是有效的？</h3><p>作者最后分析了一下Normalization为什么有效,并绘制了不同的Normalization对于我们均值和方差带来的影响,从图中以及实验中看来,我们发现 Normalization有效的最大一个原因在于方差的影响而不是均值。</p><p><img src="F:\notes\机器学习\src\640-1640746764463.webp" alt="图片"></p><p>同时我们发现很多神经元的输出大部分在使用Variance-Only LayerNorm之后都会被push输出一个负数的值，然后被RELU过滤掉,这可以减少噪音带来的影响,同样的，我们将Normalization的导数表示为：<br>$$<br>\frac{\partial L}{\partial x_i } &#x3D; \frac{\partial L}{\partial h }* \frac{\partial L}{\partial x_i } \<br>\frac{\partial h}{\partial x_i }&#x3D; \frac{1}{\sigma}-\frac{x_i(x_i-\mu)}{\sigma^3\cdot H}<br>$$<br>从上面的式子中我们也发现我们的Normalization对于$\sigma$是非常敏感的。</p><h3 id="特征Embedding上加入Normalization是否有效？"><a href="#特征Embedding上加入Normalization是否有效？" class="headerlink" title="特征Embedding上加入Normalization是否有效？"></a>特征Embedding上加入Normalization是否有效？</h3><p><img src="F:\notes\机器学习\src\640-1640746764470.webp" alt="图片"></p><p>从上面的实验中,我们发现,<strong>在特征Embedding层加入Normalization都是有效的,而且LayerNorm以及相关的变种是效果相对稳定以及最好的</strong>;</p><h3 id="Normalization对于MLP的影响"><a href="#Normalization对于MLP的影响" class="headerlink" title="Normalization对于MLP的影响"></a>Normalization对于MLP的影响</h3><p><img src="F:\notes\机器学习\src\640-1640746764476.webp" alt="图片"></p><p>从上面的实验中,我们发现,<strong>在MLP层加入Normalization都是有效的,但是具体选用哪种Normalization需要依据不同的任务进行选择</strong>;</p><h3 id="Normalization对于Feature-EMbedding-amp-MLP的影响"><a href="#Normalization对于Feature-EMbedding-amp-MLP的影响" class="headerlink" title="Normalization对于Feature EMbedding &amp; MLP的影响"></a>Normalization对于Feature EMbedding &amp; MLP的影响</h3><p><img src="F:\notes\机器学习\src\640-1640746764483.webp" alt="图片"></p><p>从上面的实验中,我们发现,<strong>在MLP层以及特征Embedding层都加入Normalization都是比单个加入都有效的,在MLP侧加入VO-LN的Normalization往往能取得更好的效果</strong>;</p><h3 id="Normalization对于数值以及类别特征的-EMbedding的影响"><a href="#Normalization对于数值以及类别特征的-EMbedding的影响" class="headerlink" title="Normalization对于数值以及类别特征的 EMbedding的影响"></a>Normalization对于数值以及类别特征的 EMbedding的影响</h3><p><img src="F:\notes\机器学习\src\640-1640746764489.webp" alt="图片"></p><p>从上面的实验中,我们发现,<strong>对数值的EMbedding使用LayerNorm相关的效果更好,对数值特征使用LayerNorm相关的正则化方法,在MLP处使用VO-LN往往可以取得最好的效果.</strong></p><h2 id="NormDNN-与-其他网络比较"><a href="#NormDNN-与-其他网络比较" class="headerlink" title="NormDNN 与 其他网络比较"></a><strong>NormDNN 与 其他网络比较</strong></h2><p><img src="F:\notes\机器学习\src\640-1640746764503.webp" alt="图片"><br>出乎意料,在三个数据集上,我们只需要对不同层做Normalization的处理就可以取得比DeepFM,xDeepFM更好的效果;</p><blockquote><p>NormDNN: Numerical Embedding用LayerNorm相关的处理; Categorical Feature使用BatchNorm相关的处理; 在MLP部分使用VO-LN</p></blockquote><h3 id="泛化到其他Deep相关的模型"><a href="#泛化到其他Deep相关的模型" class="headerlink" title="泛化到其他Deep相关的模型"></a>泛化到其他Deep相关的模型</h3><p><img src="F:\notes\机器学习\src\640-1640746764525.webp" alt="图片"></p><p>  我们把初始化的方案应用到更加复杂的网络结构上也都取得了更好的效果;也就是说这种Normalization的方案可以扩充到其他的所有最新网络结构上;</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h3><p>  从上面的内容来看,Normalization对于模型的帮助是非常大的; 对Embedding之后的特征进行Normalization(数值Embedding处用LayerNorm相关的Normalization,Categorical部分使用BatchNorm相关的处理，MLP部分使用VO-LN)可以取得非常大的提升;非常值得一试。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——Perceptron</title>
    <link href="/2022/08/01/MachineLearning/Perceptron/"/>
    <url>/2022/08/01/MachineLearning/Perceptron/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h1><p>感知机模型(Perceptron)是一个最简单的有监督的二分类线性模型。他可以从两个方面进行介绍</p><h2 id="方面一-问题分析"><a href="#方面一-问题分析" class="headerlink" title="方面一 问题分析"></a>方面一 问题分析</h2><h3 id="问题（一维）：儿童免票乘车问题（孩子身高低于1-2m可以免票上车）"><a href="#问题（一维）：儿童免票乘车问题（孩子身高低于1-2m可以免票上车）" class="headerlink" title="问题（一维）：儿童免票乘车问题（孩子身高低于1.2m可以免票上车）"></a>问题（一维）：儿童免票乘车问题（孩子身高低于1.2m可以免票上车）</h3><p>这转换成数学表达式就是 $x:$身高，$y:{-1:$免票 ,$1:$购票$}$</p><p>$$ y&#x3D;\left{\begin{matrix}+1,x\ge1.2m \ -1,x\lt1.2m\end{matrix}\right. $$</p><p><img src="/img/2274220-20211218214342516-2127817773.png" alt="img"></p><p>如果把$y$用函数$f(x;b)$表达出来就如下所示</p><p>$$ y&#x3D;f(x;b)&#x3D;\left{\begin{matrix}+1,z&#x3D;x+b\ge0 \ -1,z&#x3D;x+b \lt0 \end{matrix}\right. $$</p><p><img src="/img/2274220-20211218214401828-471520394.png" alt="img"></p><p>用机器学习的话来说变量$x$就是一个<strong>特征</strong>,$y$就是一个<strong>标记</strong>，$b$就是一个<strong>偏置</strong></p><h3 id="问题（二维）：西瓜分类（依靠西瓜的颜色和敲击的响声分类）"><a href="#问题（二维）：西瓜分类（依靠西瓜的颜色和敲击的响声分类）" class="headerlink" title="问题（二维）：西瓜分类（依靠西瓜的颜色和敲击的响声分类）"></a>问题（二维）：西瓜分类（依靠西瓜的颜色和敲击的响声分类）</h3><p>这里的$x$就从一个一维特征编程了二维的特称向量${x_1:$颜色 ,$x_2:$响声$}$ $y$仍然是一个标记 $y:{-1:$坏瓜 ,$1:$好瓜$}$ 数据如图所示：</p><p><img src="/img/2274220-20211219113413328-528436610.png" alt="img"></p><p>用一般的数学方法，我们希望将两个变量$x_1,x_2$映射到一个维度上去 就会出现如下三种经典的情况</p><p>映射到水平方向<img src="/img/2274220-20211218221216356-994026222.png" alt="img"></p><p>映射到竖直方向<img src="/img/2274220-20211218221328431-310696772.png" alt="img"></p><p>映射到任意方向<img src="/img/2274220-20211218221424196-524684068.png" alt="img"></p><p>二维向量映射到一维空间需要有映射的方向。此时就需要一定的方向向量$\mathbb{w}&#x3D;(w_1 \ w_2)^T$ 此时，如果把$y$用函数$f(\mathbb{x};\mathbb{w},b)$来表示</p><p>$$ y&#x3D;f(\mathbb{x};\mathbb{w},b)&#x3D;\left{\begin{matrix}+1,\mathbb{w}^T\mathbb{x}+b\ge0 \ -1,\mathbb{w}^T\mathbb{x}+b\lt0 \end{matrix}\right. $$</p><p>综上所述，可以类推到n维。只要将$\mathbb{x},\mathbb{w}升维就可以了$</p><h2 id="方面二-模型发展"><a href="#方面二-模型发展" class="headerlink" title="方面二 模型发展"></a>方面二 模型发展</h2><p>1943年，McCulloch和Pits结合神经元模型Neuron提出来经典的抽象的M-P神经元模型</p><p>神经元的生物学结构<img src="/img/2274220-20211219123951945-1354915555.png" alt="img"></p><p>M-P神经元模型 <img src="/img/2274220-20211219124043889-525716909.png" alt="img"></p><p>而感知机模型是从神经元模型发展过来的。感知机是由两层神经元组成的一个简单模型。只有输出层是M-P 神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（ functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。示意图如下：</p><p><img src="/img/2274220-20211219130328971-1222419824.png" alt="img"></p><h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h1><p>假设输入空间（特征空间）是$\mathcal{X} \subset \mathbb{R}^n$，输出空间是$\mathcal{Y}&#x3D;{-1,+1}$。输入$x\in \mathcal{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in \mathcal{Y}$表示类别。由输入空间到输出空间的如下函数： $$ f(x)&#x3D;sign(w \cdot x+b) $$ 成为感知机。其中，$w,b$成为感知机模型的参数,$w \in \mathbb{R}^n$叫做权值（weight）或权值向量（weight vector），$b \in \mathbb{R}$叫做偏置（bias），$w \cdot x$表示两者的内积。$sign$是符号函数，即 $$ sign(a)&#x3D;\left{\begin{matrix}+1,a\ge0\-1,a\lt0\end{matrix}\right. $$ 感知机是一种线性分类器，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数模型${f,|,f(x)&#x3D;w \cdot x +b }$</p><h1 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h1><p>感知机学习的策略就是最小化损失函数。对于感知机来说损失函数有两种方式。</p><p>\1. 统计误分类点： $$ l(w)&#x3D;\sum_{i&#x3D;1}^N \ \mathbb{I}(y_i \cdot (w^Tx +b ) &lt; 0) $$ 　但是指示函数不连续，在计算过程中不太友好</p><p>\2. 统计误分类点到分来超平面的距离： $$ \begin{align}l(w) &amp;&#x3D;|w^Tx +b |\ &amp;&#x3D;\sum_{i&#x3D;1}^N - ,y_i \cdot (w^Tx +b )\end{align} $$</p><h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><p>为了找到更好的$w,b$。使用随机梯度下降SGD(Stochastic Gradient Descend), 但是该方法容易受特征缩放、学习率因素影响陷入局部最小值。 $$ w_{n+1} \leftarrow w_n +\eta \frac{\partial l}{\partial w}\ b_{n+1} \leftarrow b_n +\eta \frac{\partial l}{\partial b} $$</p><p>对该算法的超参数：学习率$\eta$，误差上限$\varepsilon$，学习次数$epoch$</p><h1 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h1><p>设训练数据集$T&#x3D;{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$是线性可分的，其中$x_i\in \mathcal{X}&#x3D;\mathbb{R}^n,\ y_i\in\mathcal{Y}&#x3D;{+1,-1},\ i&#x3D;1,2,\cdots,N$则:</p><ul><li>存在满足条件$||\hat{w}<em>{opt}||&#x3D;1$的超平面$\hat{w}</em>{opt} \cdot \hat{x}&#x3D;w_{opt}\cdot x +b_{opt}&#x3D;0$将训练数据集完全正确分开；且存在$\gamma&gt;0$,对所有$i&#x3D;1,2,\cdots,N$: $$ y_i (\hat{w}<em>{opt} \cdot \hat{x})&#x3D;y_i(w</em>{opt}\cdot x +b_{opt})\geq \gamma $$</li><li>令$R&#x3D;max_{1 \leq i \leq N} ||\hat{x}_i||$,则感知机算法在训练数据集上的误分类次数$k$满足不等式: $$ k \leq (\frac{R}{\gamma})^2 $$</li></ul><h1 id="Disadvantage"><a href="#Disadvantage" class="headerlink" title="Disadvantage"></a>Disadvantage</h1><p>\1. 存在多解<br>\2. 依赖初值，也依赖选择顺序<br>\3. 当数据集线性不可分就会产生迭代震荡。例如XOR异或问题</p><p>针对感知机算法的缺点，提出SVM解决多解问题，Pocket Algorithm使感知机容忍一些错误。</p><h1 id="Python代码实现"><a href="#Python代码实现" class="headerlink" title="Python代码实现"></a>Python代码实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>data1 = [[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">5</span>,<span class="hljs-number">2</span>]]<br>lable1 = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>]<br><br>data2 = [[<span class="hljs-number">3</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]]<br>lable2 = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>]<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,data</span>):<br>        self.w = np.zeros(np.shape(data)[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 权重参数</span><br>        self.b = <span class="hljs-number">0</span>  <span class="hljs-comment"># 偏置</span><br>        self.l_rate = <span class="hljs-number">1</span>  <span class="hljs-comment"># 学习率</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sign</span>(<span class="hljs-params">self, x, w, b</span>):<br>        y = np.dot(x, w) + b<br>        <span class="hljs-keyword">return</span> y<br><br>    <span class="hljs-comment"># 随机梯度下降法</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, data, lable</span>):<br>        is_wrong = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> is_wrong:<br>            wrong_count = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data)):<br>                X = data[d]<br>                y = lable[d]<br>                judge = y * self.sign(X, self.w, self.b)<br>                <span class="hljs-keyword">if</span> judge &lt;= <span class="hljs-number">0</span>:<br>                    self.w = self.w + self.l_rate * np.dot(y, X)<br>                    self.b = self.b + self.l_rate * y<br>                    <span class="hljs-keyword">if</span> judge == <span class="hljs-number">0</span>:<br>                        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;未分类=&#x27;</span>, d + <span class="hljs-number">1</span>, <span class="hljs-string">&#x27; &#x27;</span>, data[d])<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;误分类=&#x27;</span>, d + <span class="hljs-number">1</span>, <span class="hljs-string">&#x27; &#x27;</span>, data[d])<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;w = &#x27;</span>,self.w,<span class="hljs-string">&#x27;b= &#x27;</span>,self.b)<br>                    wrong_count += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> wrong_count == <span class="hljs-number">0</span>:<br>                is_wrong = <span class="hljs-literal">True</span><br><br>perception1 = Model(data1)<br>perception1.fit(data1,lable1)<br></code></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p>李航 统计学习方法</p><p>周志华 机器学习(西瓜书)</p><p>shuhuai008 <a href="https://www.bilibili.com/video/BV1aE411o7qd?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">机器学习白板推导</a>ion.content.click)</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——PCA</title>
    <link href="/2022/08/01/MachineLearning/PCA/"/>
    <url>/2022/08/01/MachineLearning/PCA/</url>
    
    <content type="html"><![CDATA[<h2 id="一、PCA算法的直觉理解"><a href="#一、PCA算法的直觉理解" class="headerlink" title="一、PCA算法的直觉理解"></a>一、PCA算法的直觉理解</h2><p>从直觉上看，PCA主成分分析类似于依次寻找一群样本点的各个位置差异最大的方向长轴。</p><p><img src="/src/640-1640494499177.webp" alt="图片"></p><p>假定把一个人身上的所有细胞看成一个一个的样本点。这些样本点可以用3个坐标来表示，从左到右为x方向，从前到后为y方向，从下到上为z方向。</p><p>那么它们的第一个主成分是什么呢？第一个主成分对应的长轴是沿着人的脚到头的方向，也就是通常的上下方向，即z方向。这个方向是最主要的长轴。这些样本点的位置差异基本上70%以上来自于这个方向上的差异。</p><p>它们的第二个主成分是什么呢？第二个主成分对应的方向是沿着人的左臂到右臂的方向，也就通常的左右方向，即y方向。这个方向和第一个主成分长轴垂直，这些样本点的位置差异大概有20%左右来自这个方向上的差异。</p><p>它们的第三个主成分是什么呢？第三个主成分方向是沿着人的前胸到后背的方向，也就是通常的前后方向，即x方向。这个方向和前两个主成分长轴垂直，样本点的位置差异有一般只有不到10%来自这个方向的差异，当然，有些身材比较圆润的同学除外。</p><p>现在，如果要将这些样本点的坐标减少到用2个来表示，并尽可能多的保留样本点之间位置差异的信息，那么，显然，应该保留第一个主成分坐标和第二个主成分坐标。</p><p>现在，假定这个人躺在一个斜的躺椅上，那么现在这些样本点的第一主成分显然不再是从下到上的z方向。我们应该将我们的坐标系作一个旋转，让z轴和斜着的躺椅方向一致，这个新的z方向是这些样本点的第一主成分方向。类似地，也需要旋转x轴和y轴得到新的第二主成分方向和第三主成分方向。</p><p>这个旋转旧坐标系以找到主成分方向的过程就是PCA主成分分析。</p><h2 id="二、PCA算法的数学说明"><a href="#二、PCA算法的数学说明" class="headerlink" title="二、PCA算法的数学说明"></a>二、PCA算法的数学说明</h2><p>PCA主成分分析(Principal Components Analysis)是一种通过正交线性组合方式，最大化保留样本间方差的降维方法。</p><p>用几何观点来看，PCA主成分分析方法可以看成通过<strong>正交变换</strong>，对坐标系进行旋转和平移，并保留样本点投影坐标方差最大的前几个新的坐标。</p><p>这里有几个关键词需要说明:</p><ul><li><strong>降维：</strong>将样本原来的m维特征用更少的k个特征取代。降维算法可以理解成一种数据压缩方法，它可能会丢失一些信息。</li><li><strong>正交线性组合：</strong>k个新特征是通过对m个旧特征进行线性组合产生的，并且k个线性组合的系数向量为单位向量，且彼此相互正交。</li><li><strong>最大化保留样本间方差：</strong>第1个主成分特征最大化样本间特征方差，第2个主成分特征在满足与第1个主成分正交约束条件下最大化样本间特征方差……</li></ul><p><img src="/src/640-1640494499121.webp" alt="图片"></p><h2 id="三、PCA算法的调包范例"><a href="#三、PCA算法的调包范例" class="headerlink" title="三、PCA算法的调包范例"></a>三、PCA算法的调包范例</h2><p>下面的范例我们调用sklearn中的PCA降维算法接口，对波士顿房价数据集进行降维(13维降到7维)。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-title">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><br><span class="hljs-title">boston</span> = datasets.load_boston()<br><span class="hljs-title">dfdata</span> = pd.<span class="hljs-type">DataFrame</span>(boston.<span class="hljs-class"><span class="hljs-keyword">data</span>,columns = boston.feature_names)</span><br><span class="hljs-title">dfdata</span>.head()<br></code></pre></td></tr></table></figure><p><img src="/src/640-1640494499120.webp" alt="图片"></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 不同特征取值范围相差较大，我们首先进行标准正态归一化</span><br><span class="hljs-comment"># 归一化的结果作为 PCA降维的输入</span><br><span class="hljs-keyword">from</span> sklearn.preprocessing import StandardScaler<br>scaler = StandardScaler()<br>scaler.fit(dfdata.values)<br>X_input = scaler.transform(dfdata.values)<br><br><span class="hljs-comment"># 我们的输入有506个样本，13维特征</span><br><span class="hljs-built_in">print</span>(X_input.shape)<br>(506, 13)<br><span class="hljs-comment"># 应用PCA进行降维</span><br><br><span class="hljs-keyword">from</span> sklearn.decomposition import PCA<br>pca = PCA(<span class="hljs-attribute">n_components</span>=7)<br>pca.fit(X_input)<br>X_output = pca.transform(X_input)<br><br><span class="hljs-comment"># 降维后，只有7维特征</span><br><span class="hljs-built_in">print</span>(X_output.shape)<br>(506, 7)<br><span class="hljs-comment"># 查看各个主成分对应的方差大小和占全部方差的比例</span><br><span class="hljs-comment"># 可以看到前7个主成分已经解释了样本分布的90%的差异了。</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;explained_variance:&quot;</span>)<br><span class="hljs-built_in">print</span>(pca.explained_variance_)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;explained_variance_ratio:&quot;</span>)<br><span class="hljs-built_in">print</span>(pca.explained_variance_ratio_)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;total explained variance ratio of first 7 principal components:&quot;</span>)<br><span class="hljs-built_in">print</span>(sum(pca.explained_variance_ratio_)<br>explained_variance:<br>[6.1389812  1.43611329 1.2450773  0.85927328 0.83646904 0.65870897<br> 0.5364162 ]<br>explained_variance_ratio:<br>[0.47129606 0.11025193 0.0955859  0.06596732 0.06421661 0.05056978<br> 0.04118124]<br>total explained variance ratio of first 7 principal components:<br>0.8990688406240493<br><span class="hljs-comment">#可视化各个主成分贡献的方差</span><br>%matplotlib inline<br>%config InlineBackend.figure_format = <span class="hljs-string">&#x27;svg&#x27;</span><br><br>import matplotlib.pyplot as plt<br>plt.figure()<br>plt.plot(np.arange(1,8),pca.explained_variance_,<span class="hljs-attribute">linewidth</span>=2)<br>plt.xlabel(<span class="hljs-string">&#x27;components_n&#x27;</span>, <span class="hljs-attribute">fontsize</span>=16)<br>plt.ylabel(<span class="hljs-string">&#x27;explained_variance_&#x27;</span>, <span class="hljs-attribute">fontsize</span>=16)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs subunit"># 查看降维对应的正交变换矩阵，即各个投影向量<br>W = pca.components_<br># 验证正交关系<br>np.round(np.dot(W,np.transpose(W)),6)<br>array([[ 1.,  0., <span class="hljs-string">-0</span>., <span class="hljs-string">-0</span>., <span class="hljs-string">-0</span>.,  0., <span class="hljs-string">-0</span>.],<br>       [ 0.,  1., <span class="hljs-string">-0</span>.,  0.,  0., <span class="hljs-string">-0</span>., <span class="hljs-string">-0</span>.],<br>       [<span class="hljs-string">-0</span>., <span class="hljs-string">-0</span>.,  1.,  0., <span class="hljs-string">-0</span>.,  0., <span class="hljs-string">-0</span>.],<br>       [<span class="hljs-string">-0</span>.,  0.,  0.,  1., <span class="hljs-string">-0</span>., <span class="hljs-string">-0</span>.,  0.],<br>       [<span class="hljs-string">-0</span>.,  0., <span class="hljs-string">-0</span>., <span class="hljs-string">-0</span>.,  1.,  0.,  0.],<br>       [ 0., <span class="hljs-string">-0</span>.,  0., <span class="hljs-string">-0</span>.,  0.,  1., <span class="hljs-string">-0</span>.],<br>       [<span class="hljs-string">-0</span>., <span class="hljs-string">-0</span>., <span class="hljs-string">-0</span>.,  0.,  0., <span class="hljs-string">-0</span>.,  1.]])<br># 对降维后的数据的前两维进行散点图可视化<br><br>%matplotlib inline<br>%config InlineBackend.figure_format = &#x27;svg&#x27;<br><br>plt.scatter(X_output[:,0],X_output[:,1])<br>plt.xlabel(&quot;components_1&quot;)<br>plt.ylabel(&quot;components_2&quot;)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/src/640-1640494499119.webp" alt="图片"></p><h2 id="四、PCA算法的数学原理"><a href="#四、PCA算法的数学原理" class="headerlink" title="四、PCA算法的数学原理"></a>四、PCA算法的数学原理</h2><p>前方高数提醒。</p><p>下面进行PCA算法的数学原理推演，证明第k个主成分投影向量恰好是样本的协方差矩阵的第k大特征值对应的特征向量。</p><p>下面的推演会会用到高等数学中的一些线性代数知识和微积分知识。</p><p>没有相关数学基础的同学可以跳过，在实践中只要掌握PCA算法的直觉概念和调包使用方法，基本就够用了。</p><p>假定样本特征用矩阵$X$表示， 其每一行表示一个样本，每一列表示一个特征。</p><p>假定$X$是$n \times m$维的，即$n$个样本，$m$维特征。</p><p>现在寻找$X$的第一个主成分。</p><p>假定第一个主成分投影向量是$w$，它是一个单位列向量，维数为$m$，</p><p>则$X$投影后的坐标是<br>$$<br>y&#x3D;Xw<br>$$<br>投影后的样本间的坐标方差可以表示为<br>$$<br>var(y)&#x3D;(Xw-\bar{Xw})^T(Xw-\bar{Xw})<br>$$<br>杠杠表示对样本求平均，由于与样本无关，可以化简为:<br>$$<br>var(y)&#x3D;((X-\bar{X})w)^T((X-\bar{X})w)\<br>var(y)&#x3D;w^T(X-\bar{X})^T(X-\bar{X})w<br>$$<br>注意到$(X-\bar{X})^T(X-\bar{X})$恰好为$X$的协方差矩阵。</p><p>下面我们要让这个坐标方差最大化，同时$w$要满足单位长度约束:<br>$$<br>w^Tw&#x3D;1<br>$$<br>对于含有约束条件的极值条件求解，我们可以应用微积分中的拉格朗日乘子法。构造如下拉格朗日函数。<br>$$<br>L(w,\lambda)&#x3D;w^T(X-\bar{X})^T(X-\bar{X})w-\lambda(w^Tw-1)<br>$$<br>可以利用下标表示方法证明，对于二次型，存在如下向量求导公式：<br>$$<br>\frac{\partial x^TAx}{\partial x}&#x3D;A^Tx+Ax<br>$$<br>可以推导得到：</p><p>$$<br>\frac{\partial L}{\partial w}&#x3D;2(X-\bar{X})^T(X-\bar{X})w-2\lambda w&#x3D;0\<br>(X-\bar{X})^T(X-\bar{X})w&#x3D;\lambda w<br>$$<br>可以看出， $\lambda$为$X$协方差矩阵的特征值，$w$为对应的特征向量。由于协方差矩阵为实对称矩阵，其特征值必定大于等于0。</p><p>同时，投影后的样本间的坐标方差可以表示为：</p><p>$$<br>var(y)&#x3D;w^T(X-\bar{X})^T(X-\bar{X})w&#x3D;\lambda w^Tw&#x3D;\lambda \ge 0<br>$$<br>为了最大化样本间的坐标方差，$\lambda$应当取$X$协方差矩阵的最大的特征值，则$w$应为$X$协方差矩阵最大特征值对应的特征向量。</p><p>类似地，可以证明第k个主成分投影向量为$X$协方差矩阵的第k大特征值对应的特征向量。</p><p>由于矩阵不同特征值的特征向量相互正交，因此这些投影向量满足正交条件。</p><hr><p>PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍PCA的基本数学原理，帮助读者了解PCA的工作机制是什么。</p><p>数据的向量表示及降维问题</p><p>一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下：</p><p>(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额)</p><p>其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子：</p><p><img src="/src/640-1640495441440.webp" alt="图片"></p><p>注意这里我用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时我会省略转置符号，但我们说到向量默认都是指列向量。</p><p>我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。</p><p>降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。</p><p>举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。</p><p>当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。</p><p>这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。</p><p>上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？</p><p>要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。</p><p>向量的表示及基变换</p><p>既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。</p><p><strong>内积与投影</strong></p><p>下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为：</p><p><img src="/src/640-1640495441325.webp" alt="图片"></p><p>内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则A&#x3D;(x1,y1)A&#x3D;(x1,y1)，B&#x3D;(x2,y2)B&#x3D;(x2,y2)。则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：</p><p><img src="/src/640-1640495441323.webp" alt="图片"></p><p>好，现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为<img src="/src/640-1640495441408.webp" alt="图片">其中</p><p><img src="/src/640-1640495441352.webp" alt="图片">是向量A的模，也就是A线段的标量长度。</p><p>注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。</p><p>到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式：</p><p>A⋅B&#x3D;|A||B|cos(a)</p><p>现在事情似乎是有点眉目了：A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让|B|&#x3D;1，那么就变成了：</p><p>A⋅B&#x3D;|A|cos(a)</p><p>也就是说，设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！这就是内积的一种几何解释，也是我们得到的第一个重要结论。在后面的推导中，将反复使用这个结论。</p><p><strong>基</strong></p><p>下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量：</p><p><img src="/src/640-1640495441314.webp" alt="图片"></p><p>在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。</p><p>不过我们常常忽略，只有一个(3,2)本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。</p><p>更正式的说，向量(x,y)实际上表示线性组合：</p><p><img src="/src/640-1640495441328.webp" alt="图片"></p><p>不难证明所有二维向量都可以表示为这样的线性组合。此处(1,0)和(0,1)叫做二维空间中的一组基。</p><p><img src="/src/640-1640495441327.webp" alt="图片"></p><p>所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。</p><p>我们之所以默认选择(1,0)和(0,1)为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。</p><p>例如，(1,1)和(-1,1)也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为<img src="/src/640-1640495441325.webp" alt="图片"></p><p>现在，我们想获得(3,2)在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为<img src="/src/640-1640495441318.webp" alt="图片"></p><p>下图给出了新的基以及(3,2)在新基上坐标值的示意图：</p><p><img src="/src/640-1640495441354.webp" alt="图片"></p><p>另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。</p><p><strong>基变换的矩阵表示</strong></p><p>下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将(3,2)变换为新基上的坐标，就是用(3,2)与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换：</p><p><img src="/src/640-1640495441408.webp" alt="图片"></p><p>太漂亮了！其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky2jXhnzqYC7JdLQBWPVKFGc68nlISpkibRGs2uWYyqPIVnwYZUx6GfcExtLkbzUCqaTSib1oHSQ5JA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>于是一组向量的基变换被干净的表示为矩阵的相乘。</p><p>一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。</p><p>数学表示为：</p><p><img src="/src/640-1640495441322.webp" alt="图片"></p><p>其中pi是一个行向量，表示第i个基，aj是一个列向量，表示第j个原始数据记录</p><p>特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。</p><p>最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。</p><p>协方差矩阵及优化目标</p><p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？</p><p>要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。</p><p>为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式：</p><p><img src="/src/640-1640495441323.webp" alt="图片"></p><p>其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。</p><p>我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后：</p><p><img src="/src/640-1640495441437.webp" alt="图片"></p><p>我们可以看下五条数据在平面直角坐标系内的样子：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkyQPnnP8sxicPCfWP6sZKy4VY6cTVl9YN0Q6EpFqRA3EB02zqvAicKqutInibic1SCVfXNAKIcCvX0rsg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？</p><p>通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。</p><p>那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。</p><p>以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。</p><p>下面，我们用数学方法表述这个问题。</p><p><strong>方差</strong></p><p>上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即：</p><p><img src="/src/640-1640495441353.webp" alt="图片"></p><p>由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：</p><p><img src="/src/640-1640495441327.webp" alt="图片"></p><p>于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</p><p><strong>协方差</strong></p><p>对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。</p><p>如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。</p><p>数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：</p><p><img src="/src/640-1640495441324.webp" alt="图片"></p><p>可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。</p><p>当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。</p><p>至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。</p><p><strong>协方差矩阵</strong></p><p>上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。</p><p>我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：</p><p>假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：</p><p><img src="/src/640-1640495441409.webp" alt="图片"></p><p>然后我们用X乘以X的转置，并乘上系数1&#x2F;m：</p><p><img src="/src/640-1640495441326.webp" alt="图片"></p><p>奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。</p><p>根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：</p><p>设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设<img src="/src/640-1640495441438.webp" alt="图片"></p><p>则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。</p><p><strong>协方差矩阵对角化</strong></p><p>根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p><p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的</p><p>矩阵，设Y&#x3D;PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p><p><img src="/src/640-1640495441432.webp" alt="图片"></p><p>现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，<img src="/src/640-1640495441433.webp" alt="图片">满足 是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p><p>至此，我们离“发明”PCA还有仅一步之遥！</p><p>现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。</p><p>由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：</p><p>1）实对称矩阵不同特征值对应的特征向量必然正交。</p><p>2）设特征向量λ重数为r，则必然存在r个线性无关的特征向量对应于λλ，因此可以将这r个特征向量单位正交化。</p><p>由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为e1,e2,⋯,en，我们将其按列组成矩阵：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky2jXhnzqYC7JdLQBWPVKFGg6kzxoUe6Aa2GZ7BktHDtVicKz5eKWLicaPfA5pk34VEtaPXqUY3IYlw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>则对协方差矩阵C有如下结论：</p><p><img src="/src/640-1640495441434.webp" alt="图片"></p><p>其中Λ为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。</p><p>以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。</p><p>到这里，我们发现我们已经找到了需要的矩阵P:</p><p><img src="/src/640-1640495441435.webp" alt="图片"></p><p>P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照Λ中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p><p>至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。</p><p>算法及实例</p><p>为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。</p><p><strong>PCA算法</strong></p><p>总结一下PCA的算法步骤：</p><p>设有m条n维数据。</p><p>1）将原始数据按列组成n行m列矩阵X</p><p>2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</p><p>3）求出协方差矩阵<img src="/src/640-1640495441436.webp" alt="图片"></p><p>4）求出协方差矩阵的特征值及对应的特征向量</p><p>5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</p><p>6）Y&#x3D;PX即为降维到k维后的数据</p><p><strong>实例</strong></p><p>这里是上文提到的</p><p><img src="/src/640-1640495441436.webp" alt="图片"></p><p>为例，我们用PCA方法将这组二维数据其降到一维。</p><p>因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵：</p><p><img src="/src/640-1640495441438.webp" alt="图片"></p><p>然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为：</p><p><img src="/src/640-1640495441440.webp" alt="图片"></p><p>其对应的特征向量分别是：</p><p><img src="/src/640-1640495441441.webp" alt="图片"></p><p>其中对应的特征向量分别是一个通解，c1c1和c2c2可取任意实数。那么标准化后的特征向量为：</p><p><img src="/src/640-1640495441442.webp" alt="图片"></p><p>因此我们的矩阵P是：</p><p><img src="/src/640-1640495441442.webp" alt="图片"></p><p>可以验证协方差矩阵C的对角化：</p><p><img src="/src/640-1640495441443.webp" alt="图片"></p><p>最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示：</p><p><img src="/src/640-1640495441443.webp" alt="图片"></p><p>降维投影结果如下图：</p><p><img src="/src/640-1640495441464.webp" alt="图片"></p><p>进一步讨论</p><p>根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。</p><p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p><p>最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</p><p>在机器学习的领域中，我们对原始数据进行特征提取，经常会得到高维度的特征向量。在这些多特征的高维空间中，会包含一些冗余和噪声。所以我们希望通过降维的方式来寻找数据内部的特性，提升特征表达能力，降低模型的训练成本。PCA是一种降维的经典算法，属于<strong>线性、非监督、全局</strong>的降维方法。��维的经典算法，属于<strong>线性、非监督、全局</strong>的降维方法。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——Norm</title>
    <link href="/2022/08/01/MachineLearning/Norm&amp;Distance/"/>
    <url>/2022/08/01/MachineLearning/Norm&amp;Distance/</url>
    
    <content type="html"><![CDATA[<h1 id="范数-Norm"><a href="#范数-Norm" class="headerlink" title="范数 Norm"></a>范数 Norm</h1><p>范数本质是向量或者矩阵映射到实数域的单值函数。</p><blockquote><p>假设$N(x)&#x3D;\Vert x \Vert$是定义在$R^n$上的函数，她需要满足以下三个条件：</p><ul><li>非负性：$\Vert x \Vert \ge 0 $，当且仅当$x&#x3D;0$时，$\Vert x \Vert &#x3D; 0 $</li><li>齐次性：$\Vert kx \Vert &#x3D;  \Vert x \Vert *\Vert k \Vert, \quad k \in R$</li><li>三角不等性：对于$\forall x,y \in R^n, \quad \Vert x+y \Vert \le \Vert x \Vert+\Vert y \Vert$</li></ul><p>则称$N(x)&#x3D;\Vert x \Vert$在$R^n$上向量$x$的范数</p></blockquote><p>那么，向量范数和矩阵范数输出都是一个值，实数值。只不过输入空间不同罢了。他们都是为了能够更显性的体现出一个向量和矩阵的大小。</p><p>下面是一些范数的图示：</p><p><img src="/img/image-20211214115802005.png" alt="image-20211214115802005"></p><p>同样，范数也可以有相对应的变化趋势，从下图中不难发现一些东西：</p><p><img src="/img/IMG_0522.JPG" alt="QQ图片20211216152534"></p><h2 id="Vector-范数计算公式"><a href="#Vector-范数计算公式" class="headerlink" title="Vector 范数计算公式:"></a>Vector 范数计算公式:</h2><p>$$<br>\Vert x\Vert_1&#x3D; \sum_{i&#x3D;1}^N |x_i| \<br>\Vert x\Vert_2&#x3D; \sqrt {\sum_{i&#x3D;1}^N x_i^2} \<br>\Vert x\Vert_p&#x3D; ({\sum_{i&#x3D;1}^N x_i^p})^{\frac{1}{p}} \<br>\Vert x\Vert _\infty&#x3D; \max|x_i|<br>$$</p><h2 id="Matrix-范数计算公式"><a href="#Matrix-范数计算公式" class="headerlink" title="Matrix 范数计算公式:"></a>Matrix 范数计算公式:</h2><p>$$<br>A_{m\times n}\<br>\Vert A\Vert_1&#x3D; \max_j \sum_{i&#x3D;1}^m |a_{ij}| \<br>\Vert A\Vert_2&#x3D; \sqrt {\lambda_{max}(A^TA)} \<br>\Vert A\Vert_F&#x3D; ({\sum_{i&#x3D;1}^m \sum_{j&#x3D;1}^n |a_{ij}|^2})^{\frac{1}{2}} \<br>\Vert A\Vert_\infty&#x3D; \max_i \sum_{j&#x3D;1}^n|a_{ij}|<br>$$</p><p>范数的应用：在机器学习中，范数最常见的作用是来对目标函数进行惩罚，达到正则化的效果，以免目标函数过拟合。当然面对模型过拟合的方法不止正则化这一种，还有增加数据，特征提取等方式。</p><p>$L_1$范数(Lasso)和$L_0$范数通过正则化得到稀疏解<br>$L_2$范数(Ridge)通过正则化得到稠密解，这种方法有个专有的名字：权值衰减</p><p>首先写出L1正则化和L2正则化的公式：</p><h2 id="用等高线展示L1，L2正则化的正确姿势"><a href="#用等高线展示L1，L2正则化的正确姿势" class="headerlink" title="用等高线展示L1，L2正则化的正确姿势"></a>用等高线展示L1，L2正则化的正确姿势</h2><p><img src="/img/640-16463600254073.png" alt="图片">L1，L2正则化示意图</p><p>图中：</p><ul><li>蓝色的一层层的线，代表<strong>正则项的等高线</strong>，对于L1，它是菱形的，对于L2，它是圆形的；</li><li>绿色的一层层的圆圈，代表<strong>原始损失函数的等高线图</strong>；</li><li>黑色的是坐标轴，这里展示的是二维特征的坐标轴。</li></ul><p><strong>关键的关键</strong>：</p><ol><li>蓝色的等高线和绿色的等高线，分别代表了两个优化问题。对原始的损失函数J添加了正则像之后，优化问题就变成了两个子优化问题的博弈。</li><li>当J和正则项之和最小时，上述的博弈取得平衡。而此时平衡点一定是相切点&#x2F;端点。相切点的具体位置，取决于正则项的惩罚力度，也就是公式里的。每一个平衡点，对应着一个的设置。</li></ol><ul><li>可以想象：<strong>当惩罚力度大时，蓝色的线希望扯着绿色的线，往靠近坐标轴的方向移动</strong>，而<strong>惩罚力度小时，绿色的线希望扯着蓝色的线，往远离坐标轴的方向移动</strong>。</li></ul><p>那么就好理解了，我们可以发现：</p><ul><li>对于L1正则化，蓝线和绿线的相切点，随着蓝线不断靠近坐标轴，早晚会碰到坐标轴，抵达坐标轴之后，最优点会保持在L1等高线的端点处，依然在坐标轴上，故某个特征的值会变为0。</li><li>而L2的相切点则只能无限接近坐标轴，惩罚力度再大，都到不了0。</li></ul><h3 id="上面这个图还是复杂了点儿，能不能分解-x2F-分步一下？"><a href="#上面这个图还是复杂了点儿，能不能分解-x2F-分步一下？" class="headerlink" title="上面这个图还是复杂了点儿，能不能分解&#x2F;分步一下？"></a>上面这个图还是复杂了点儿，能不能分解&#x2F;分步一下？</h3><p>当然，真正让我彻底理解的，是当我画出下面这个图的时候：</p><p><img src="/img/640-16463600254074.png" alt="图片">L1正则化示意图</p><p><img src="/img/640-16463600254075.gif" alt="图片"></p><p>图中展示的都是达到最优的时候的两个等高线的关系。图注都写在图片里了，随着的增大，L1的蓝色的方框不断缩小，拉扯着J的绿色圆圈变大，第三张子图的时候，最优点到达了L1的顶点，后面如果继续增大的话，最优点会沿着纵坐标往下滑。所以，当超过某个阈值的时候，最优解中的w1就会总等于0.</p><p><img src="/img/640-16463600254076.png" alt="图片">L2理解起来就简单多了，两个圆一直都是相切的状态，切点永远到不了原点，也就没法让某个特征等于0。</p><p>这两张图，才真正展示了一般情况下的L1和L2正则化的特点，解释了为什么L1正则化会导致稀疏解。</p><h2 id="最后辅以推导"><a href="#最后辅以推导" class="headerlink" title="最后辅以推导"></a>最后辅以推导</h2><p>只有从直觉上，道理上理解了L1和L2的特点，我们再去用数学推导才有意义。</p><p>要看是否会导致稀疏解，我们可以看0附近的导数是否导致0是其邻域内的极值点：</p><p><img src="/img/640-16463600254087.png" alt="图片"></p><p><img src="/img/640-16463600254088.png" alt="图片">假设J在0处的导数是一个很小的正数，那么在0的邻域内，J的导数和J的大致的样子</p><p>上面的推导表明，对于L1，0的邻域内，存在某种条件，使得导数先负后正，即函数先减后增，所以0是极值点。而条件是：原损失函数J在某参数为0处的导数在范围内。这个条件，说直白一点就是<strong>有没有某特征对损失函数影响不大</strong>。</p><p>而L2在0处的导数，就等于J在0处的导数。只有当原始优化问题的最优解本身就是稀疏解的时候，才会使得该参数为0，而这个显然不常见。</p><p>最后的最后补充一句，前面我一直说的“导致”稀疏解，搞的好像稀疏解是某种不好的后果一样，不是的。其实L1这个特点，经常被用来进行特征选择，因为它可以找出那些不重要的特征。</p><h1 id="距离-Distance："><a href="#距离-Distance：" class="headerlink" title="距离 Distance："></a>距离 Distance：</h1><p>既然说了范数，距离比较是两个<br>$p \in \mathbb{R}^m,\ q \in \mathbb{R}^m$</p><p>$$<br>L_1(p,q)&#x3D; \sum_{i&#x3D;1}^m |p_i-q_i|\<br>L_2(p,q)&#x3D; [\sum_{i&#x3D;1}^m (p_i-q_i)^2]^\frac{1}{2}\<br>L_p(p,q)&#x3D; \max |p_i-q_i|<br>$$</p><p>$L_1$称街区距离，又称Manhattan Distance<br>$L_2$称欧式距离，又称Euclidean Distance<br>$L_1$称切比雪夫距离，又称Minkowski Distance</p><p>距离(distance，差异程度)、相似度(similarity，相似程度)方法可以看作是以某种的距离函数计算元素间的距离，这些方法作为机器学习的基础概念，广泛应用于如：Kmeans聚类、协同过滤推荐算法、相似度算法、MSE损失函数等等。本文对常用的距离计算方法进行归纳以及解析，分为以下几类展开：</p><h2 id="闵氏距离（Minkowski-Distance）类"><a href="#闵氏距离（Minkowski-Distance）类" class="headerlink" title="闵氏距离（Minkowski Distance）类"></a>闵氏距离（Minkowski Distance）类</h2><h3 id="闵氏距离（Minkowski-Distance）"><a href="#闵氏距离（Minkowski-Distance）" class="headerlink" title="闵氏距离（Minkowski Distance）"></a>闵氏距离（Minkowski Distance）</h3><p>对于点x&#x3D;(x1,x2.xn) 与点y&#x3D;(y1,y2.yn) , 闵氏距离可以用下式表示：<br><a href="https://camo.githubusercontent.com/4623a6a2ac3e0f0b122f6ea01512b9bb109a9ef9bd36d1f83fb3d568d25ee125/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d363261656565393930313837396330382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/4623a6a2ac3e0f0b122f6ea01512b9bb109a9ef9bd36d1f83fb3d568d25ee125/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d363261656565393930313837396330382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img-1.png"></a></p><p>闵氏距离是对多个距离度量公式的概括性的表述，p&#x3D;1退化为曼哈顿距离；p&#x3D;2退化为欧氏距离；切比雪夫距离是闵氏距离取极限的形式。</p><h3 id="曼哈顿距离（Manhattan-Distance）VS-欧几里得距离（Euclidean-Distance）"><a href="#曼哈顿距离（Manhattan-Distance）VS-欧几里得距离（Euclidean-Distance）" class="headerlink" title="曼哈顿距离（Manhattan Distance）VS 欧几里得距离（Euclidean Distance）"></a>曼哈顿距离（Manhattan Distance）VS 欧几里得距离（Euclidean Distance）</h3><p>曼哈顿距离 公式：</p><p><a href="https://camo.githubusercontent.com/c6d190e0f05b6be207141bb173883768f2d7d9de4192e3a824b59e7e13c122e5/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d383666393835326261386361343936612e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/c6d190e0f05b6be207141bb173883768f2d7d9de4192e3a824b59e7e13c122e5/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d383666393835326261386361343936612e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><p>欧几里得距离公式：</p><p><a href="https://camo.githubusercontent.com/c88270c10bd37a31194b953d24a814e9e5c72ce9f9f824089390171836e90ac9/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d306431313631616131303636396535392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/c88270c10bd37a31194b953d24a814e9e5c72ce9f9f824089390171836e90ac9/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d306431313631616131303636396535392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><p>如下图蓝线的距离即是曼哈顿距离（想象你在曼哈顿要从一个十字路口开车到另外一个十字路口实际驾驶距离就是这个“曼哈顿距离”，此即曼哈顿距离名称的来源，也称为城市街区距离），红线为欧几里得距离：</p><p>[<img src="https://camo.githubusercontent.com/2ec05a4b25f50cff3e51999d90ca96ca5bfc4c554c221fba02c91a3ff1026bfd/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d373066626637643862363130323464662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></p><h3 id="切比雪夫距离（Chebyshev-Distance）"><a href="#切比雪夫距离（Chebyshev-Distance）" class="headerlink" title="切比雪夫距离（Chebyshev Distance）"></a>切比雪夫距离（Chebyshev Distance）</h3><p>切比雪夫距离起源于国际象棋中国王的走法，国际象棋中国王每次只能往周围的8格中走一步，那么如果要从棋盘中A格(x1,y1)走到B格(x2,y2)最少需要走几步？你会发现最少步数总是max(|x2-x1|,|y2-y1|)步。有一种类似的一种距离度量方法叫切比雪夫距离。</p><p><a href="https://camo.githubusercontent.com/7144c9f505ae682075bc81922bfb44d7e737fffc73057a4dc456d308777ca96d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d653866306436313763653665393036342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/7144c9f505ae682075bc81922bfb44d7e737fffc73057a4dc456d308777ca96d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d653866306436313763653665393036342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><p>切比雪夫距离就是当p趋向于无穷大时的闵氏距离：</p><p><a href="https://camo.githubusercontent.com/dbb3c1b8e27f59afda6c9092c5a52fa1ebacde632cdfda1fd5f1fed244b544cd/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d623239623737633263636466346238652e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/dbb3c1b8e27f59afda6c9092c5a52fa1ebacde632cdfda1fd5f1fed244b544cd/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d623239623737633263636466346238652e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><h2 id="闵氏距离的相关知识"><a href="#闵氏距离的相关知识" class="headerlink" title="闵氏距离的相关知识"></a>闵氏距离的相关知识</h2><h3 id="距离度量的定义"><a href="#距离度量的定义" class="headerlink" title="距离度量的定义"></a>距离度量的定义</h3><p>距离函数并不一定是距离度量，当距离函数要作为距离度量，需要满足：<br><a href="https://camo.githubusercontent.com/b41d73b203a6f172280a80af85066ef590bbc7b0d7832b1ffdebeb354ff74594/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d613464623231383730346430393936362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/b41d73b203a6f172280a80af85066ef590bbc7b0d7832b1ffdebeb354ff74594/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d613464623231383730346430393936362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><br>由此可见，闵氏距离可以作为距离度量，而大部分的相似度并不能作为距离度量。</p><h3 id="Lp范数"><a href="#Lp范数" class="headerlink" title="Lp范数"></a>Lp范数</h3><blockquote><p>向量的范数可以简单形象的理解为向量的长度，或者向量到零点的距离，或者相应的两个点之间的距离。</p></blockquote><p>闵氏距离也是Lp范数（如p&#x3D;&#x3D;2为常用L2范数正则化）的一般化定义。<br>下图给出了一个Lp球（ ||X||p &#x3D; 1 ）的形状随着P的减少的可视化图：</p><p>[<img src="https://camo.githubusercontent.com/30b84347233498518d718b53e20a714163e3a88eefb86381e53673d6efe57288/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d623163646335383163343734663638312e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></p><h3 id="维度灾难的问题"><a href="#维度灾难的问题" class="headerlink" title="维度灾难的问题"></a>维度灾难的问题</h3><p>距离度量随着空间的维度d的不断增加，计算量复杂也逐增，另外在高维空间下，在维度越高的情况下，任意样本之间的距离越趋于相等（样本间最大与最小欧氏距离之间的相对差距就趋近于0），也就是维度灾难的问题，如下式结论：</p><p><a href="https://camo.githubusercontent.com/29dcd66a01c383eb0639a8b961865fb5b1583aa00d07144021780bde0014d287/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d363961623461613263656638343563392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/29dcd66a01c383eb0639a8b961865fb5b1583aa00d07144021780bde0014d287/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d363961623461613263656638343563392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><p>对于维度灾难的问题，常用的有PCA方法进行降维计算。</p><h3 id="量纲差异问题"><a href="#量纲差异问题" class="headerlink" title="量纲差异问题"></a>量纲差异问题</h3><p>假设各样本有年龄，工资两个变量，计算欧氏距离（p&#x3D;2）的时候，(年龄1-年龄2)² 的值要远小于(工资1-工资2)² ，这意味着在不使用特征缩放的情况下，距离会被工资变量（大的数值）主导, 特别当p越大，单一维度的差值对整体的影响就越大。因此，我们需要使用特征缩放来将全部的数值统一到一个量级上来解决此问题。基本的解决方法可以对数据进行“标准化”和“归一化”。</p><p><a href="https://camo.githubusercontent.com/117e3aa79041a5a08a1f9d11225c95101c0b442b1a606bd70939414d073730bd/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d376166376436353939626663343731302e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/117e3aa79041a5a08a1f9d11225c95101c0b442b1a606bd70939414d073730bd/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d376166376436353939626663343731302e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><p>另外可以使用马氏距离（协方差距离），与欧式距离不同其考虑到各种特性之间的联系是（量纲）尺度无关 (Scale Invariant) 的，可以排除变量之间的相关性的干扰，缺点是夸大了变化微小的变量的作用。马氏距离定义为：</p><p><a href="https://camo.githubusercontent.com/5307b4ca8788ed4bc62b3c7b1f979f723ae1c1c3af7882a32495b7e0817995fe/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d333736356331613736376431376662632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/5307b4ca8788ed4bc62b3c7b1f979f723ae1c1c3af7882a32495b7e0817995fe/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d333736356331613736376431376662632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><br>马氏距离原理是使用矩阵对两两向量进行投影后，再通过常规的欧几里得距离度量两对象间的距离。当协方差矩阵为单位矩阵，马氏距离就简化为欧氏距离；如果协方差矩阵为对角阵，其也可称为正规化的欧氏距离。</p><h2 id="二、相似度（Similarity"><a href="#二、相似度（Similarity" class="headerlink" title="二、相似度（Similarity)"></a>二、相似度（Similarity)</h2><ul><li><h2 id="余弦相似度-Cosine-Similarity"><a href="#余弦相似度-Cosine-Similarity" class="headerlink" title="余弦相似度 (Cosine Similarity)"></a>余弦相似度 (Cosine Similarity)</h2></li></ul><p>根据向量x,y的点积公式：</p><p><a href="https://camo.githubusercontent.com/97312fbe79e84ebdf0d9e26600b2be9d2aa2fca3a960545bc5c81777fef8b885/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d616531383266336535386263313639392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/97312fbe79e84ebdf0d9e26600b2be9d2aa2fca3a960545bc5c81777fef8b885/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d616531383266336535386263313639392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><p>我们可以利用向量间夹角的cos值作为向量相似度[1]：<br><a href="https://camo.githubusercontent.com/776b7fdb349dcee3c1f5b2ed112ad41734de0fcd091346f8fe9c21d5767614c0/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d356365623632316437623031323166392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/776b7fdb349dcee3c1f5b2ed112ad41734de0fcd091346f8fe9c21d5767614c0/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d356365623632316437623031323166392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><br>余弦相似度的取值范围为：-1~1，1 表示两者完全正相关，-1 表示两者完全负相关，0 表示两者之间独立。余弦相似度与向量的长度无关，只与向量的方向有关，但余弦相似度会受到向量平移的影响（上式如果将 x 平移到 x+1, 余弦值就会改变）。</p><p>另外，归一化后计算欧氏距离，等价于余弦值：两个向量x,y, 夹角为A，欧氏距离D&#x3D;(x-y)^2 &#x3D; x^2+y^2-2|x||y|cosA &#x3D; 2-2cosA</p><ul><li><h2 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h2></li></ul><p>协方差是衡量多维数据集中，变量之间相关性的统计量。如下公式X，Y的协方差即是，X减去其均值 乘以 Y减去其均值，所得每一组数值的期望（平均值）。<br><a href="https://camo.githubusercontent.com/e9c0245a31394f581649c72447cc489e1768e8605dba1ec06e5eecaf98a144f9/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633165613066666637373132613162392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/e9c0245a31394f581649c72447cc489e1768e8605dba1ec06e5eecaf98a144f9/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633165613066666637373132613162392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><br>如果两个变量之间的协方差为正值，则这两个变量之间存在正相关，若为负值，则为负相关。</p><ul><li><h2 id="皮尔逊相关系数-Pearson-Correlation"><a href="#皮尔逊相关系数-Pearson-Correlation" class="headerlink" title="皮尔逊相关系数 (Pearson Correlation)"></a>皮尔逊相关系数 (Pearson Correlation)</h2></li></ul><p>皮尔逊相关系数数值范围也是[-1，1]。皮尔逊相关系数可看作是在余弦相似度或协方差基础上做了优化（变量的协方差除以标准差）。它消除每个分量标准不同（分数膨胀）的影响，具有平移不变性和尺度不变性。<br><a href="https://camo.githubusercontent.com/d2c73bf5409d45a7ff8e511db1a91f61d8ee329a49de9bb2cc2857ee1e33b943/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d333039306462633763633066646162372e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/d2c73bf5409d45a7ff8e511db1a91f61d8ee329a49de9bb2cc2857ee1e33b943/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d333039306462633763633066646162372e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><ul><li><h2 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h2></li></ul><p>卡方检验X2，主要是比较两个分类变量的关联性、独立性分析。如下公式，A代表实际频数；E代表期望频数：<br><a href="https://camo.githubusercontent.com/187c4caa4617f742db957740ebaecbdc112bffad74664c845526736e50acbd91/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303137356136626432663937626139642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/187c4caa4617f742db957740ebaecbdc112bffad74664c845526736e50acbd91/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303137356136626432663937626139642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><h2 id="三、字符串距离-Distance-of-Strings"><a href="#三、字符串距离-Distance-of-Strings" class="headerlink" title="三、字符串距离(Distance of Strings)"></a>三、字符串距离(Distance of Strings)</h2><ul><li><h2 id="Levenshtein-距离"><a href="#Levenshtein-距离" class="headerlink" title="Levenshtein 距离"></a>Levenshtein 距离</h2></li></ul><p>Levenshtein 距离是 编辑距离 (Editor Distance) 的一种，指两个字串之间，由一个转成另一个所需的最少编辑操作次数。允许的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。<br>像hallo与hello两个字符串编辑距离就是1，我们通过替换”a“ 为 ”e“，就可以完成转换。<br><a href="https://camo.githubusercontent.com/22b9d5369d4ec6667d4339a72f19784bbbc0fd7f825a7c852c2e898fe0107886/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303431386563333736633134393365382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/22b9d5369d4ec6667d4339a72f19784bbbc0fd7f825a7c852c2e898fe0107886/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303431386563333736633134393365382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><ul><li><h2 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h2></li></ul><p>汉明距离为两个等长字符串对应位置的不同字符的个数，也就是将一个字符串变换成另外一个字符串所需要替换的字符个数。例如：1011101 与 1001001 之间的汉明距离是 2，“toned” 与 “roses” 之间的汉明距离是 3</p><ul><li><h2 id="带权重的字符串距离"><a href="#带权重的字符串距离" class="headerlink" title="带权重的字符串距离"></a>带权重的字符串距离</h2></li></ul><p>另外的，对于字符串距离来说，不同字符所占的份量是不一样的。比如”我乐了“ 与【“我怒了”，”我乐了啊” 】的Levenshtein 距离都是1，但其实两者差异还是很大的，因为像“啊”这种语气词的重要性明显不如“乐”，考虑字符（特征）权重的相似度方法有：TF-IDF、BM25、WMD算法。<br><a href="https://camo.githubusercontent.com/2437325d98902f36565cf374ddf01cebc4488b22b6df027e4e57dbf371a239ae/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d333435343637323635633331376334642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/2437325d98902f36565cf374ddf01cebc4488b22b6df027e4e57dbf371a239ae/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d333435343637323635633331376334642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><h2 id="四、集合距离-Distance-of-Sets"><a href="#四、集合距离-Distance-of-Sets" class="headerlink" title="四、集合距离 (Distance of Sets)"></a>四、集合距离 (Distance of Sets)</h2><ul><li><h2 id="Jaccard-系数"><a href="#Jaccard-系数" class="headerlink" title="Jaccard 系数"></a>Jaccard 系数</h2></li></ul><p><a href="https://camo.githubusercontent.com/9afc487f7cb49790cc6d03c3f9b7b0d31f81d1d876c101d65c3cac9d0e3ec6f9/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626237326265306463663533303236342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/9afc487f7cb49790cc6d03c3f9b7b0d31f81d1d876c101d65c3cac9d0e3ec6f9/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626237326265306463663533303236342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><br>Jaccard 取值范围为0~1，0 表示两个集合没有重合，1 表示两个集合完全重合。</p><ul><li><h2 id="Dice-系数"><a href="#Dice-系数" class="headerlink" title="Dice 系数"></a>Dice 系数</h2><p><img src="https://camo.githubusercontent.com/27b14a247c29f2e7dd158d1b064eac1fbcea818144dae3487baa6fa5f49d0182/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d326636386532346337623134613833652e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></p><p>Dice 系数取值范围为0~1，与Jaccard系数可以相互转换。</p><p><img src="https://camo.githubusercontent.com/2193c8a569ab5424af3a075c28bdbb91d95da9efe82cc79812dc14c03d11797c/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d646336376563636166343236643962652e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></p></li></ul><p>但Dice不满足距离函数的三角不等式，不是一个合适的距离度量。</p><ul><li><h2 id="Tversky-系数"><a href="#Tversky-系数" class="headerlink" title="Tversky 系数"></a>Tversky 系数</h2><p><img src="https://camo.githubusercontent.com/d8958e5a6466bac2522b70b629f86d199070ad375f704fc00b743479e393c034/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d336361633866373239303338643233312e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></p><p>Tversky 系数可以理解为 Jaccard 系数和 Dice 系数的一般化，当 α，β为1时为 Jaccard 系数，当 α，β为0.5时为 Dice 系数（X\Y表示集合的相对补集）。</p></li></ul><h2 id="五、信息论距离-Information-Theory-measures"><a href="#五、信息论距离-Information-Theory-measures" class="headerlink" title="五、信息论距离 (Information Theory measures)"></a>五、信息论距离 (Information Theory measures)</h2><p>基础地介绍下信息熵，用来衡量一个随机变量的不确定性程度。对于一个随机变量 X，其概率分布为：<br><a href="https://camo.githubusercontent.com/b07060aafe0bc455ba2392759fdefa98b1c00bb60fe731517ac717c8364faedb/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d653862663461313262623165616162392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/b07060aafe0bc455ba2392759fdefa98b1c00bb60fe731517ac717c8364faedb/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d653862663461313262623165616162392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><ul><li><h2 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h2></li></ul><p>互信息用于衡量两个变量之间的关联程度，衡量了知道这两个变量其中一个，对另一个不确定度减少的程度。公式为：<br><a href="https://camo.githubusercontent.com/9ca06bb08cb141c2652da402f55a587223503025944a9aaff5df4b4915424dc9/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626466333239663438396464356265352e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/9ca06bb08cb141c2652da402f55a587223503025944a9aaff5df4b4915424dc9/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626466333239663438396464356265352e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><br>如下图，条件熵表示已知随机变量X的情况下，随机变量Y的信息熵，因此互信息实际上也代表了已知随机变量X的情况下，随机变量Y的(信息熵)不确定性的减少程度。<br><a href="https://camo.githubusercontent.com/e40fa5dd1e7a67bd2946fbbcf7a14553570420990793dbcb2aef5638d0a3ce99/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633038313338303931396231356430612e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/e40fa5dd1e7a67bd2946fbbcf7a14553570420990793dbcb2aef5638d0a3ce99/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633038313338303931396231356430612e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><ul><li><p>相对熵 (Relative Entropy)相对熵又称之为 KL 散度 (Kullback-Leibler Divergence)，用于衡量两个分布之间的差异，定义为：<br><a href="https://camo.githubusercontent.com/54d0d864fbaf4e4706e35bda8b09192138cf2ebe14cb17b4bb30c852064d7c8d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d336362353938386162646265393132392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/54d0d864fbaf4e4706e35bda8b09192138cf2ebe14cb17b4bb30c852064d7c8d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d336362353938386162646265393132392e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p></li><li><h2 id="JS-散度-Jensen-Shannon-Divergence"><a href="#JS-散度-Jensen-Shannon-Divergence" class="headerlink" title="JS 散度 (Jensen-Shannon Divergence)"></a>JS 散度 (Jensen-Shannon Divergence)</h2></li></ul><p>JS 散度解决了 KL 散度不对称的问题，定义为：<br><a href="https://camo.githubusercontent.com/fccc2429956bd63d3265e7cf32152204547d661aa0b26276682c6f69aa59d17b/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d656435396431646639386366353239382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/fccc2429956bd63d3265e7cf32152204547d661aa0b26276682c6f69aa59d17b/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d656435396431646639386366353239382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><ul><li><h2 id="PSI"><a href="#PSI" class="headerlink" title="PSI"></a>PSI</h2></li></ul><p>群体稳定性指标（Population Stability Index，PSI）， 可以看做是解决KL散度非对称性的一个对称性度量指标，用于度量分布之间的差异（常用于风控领域的评估模型预测的稳定性）。</p><p>psi与JS散度的形式是非常类似的，如下公式：<br><a href="https://camo.githubusercontent.com/a9d4338383f7dc41b17e3c655c5bba7292cb4dcbb650e3731b7d84d3dac52325/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d666534643334666131613233376230322e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/a9d4338383f7dc41b17e3c655c5bba7292cb4dcbb650e3731b7d84d3dac52325/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d666534643334666131613233376230322e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><p>PSI的含义等同P与Q，Q与P之间的KL散度之和。</p><ul><li><h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p><img src="https://camo.githubusercontent.com/12ff93970f8c0964e989494ba9adeed40038dfb93240c85c953e99db00e32c7a/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d366331383261326539386237653864332e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></p><p>交叉熵常作为机器学习中的分类的损失函数，用于衡量模型预测分布和实际数据分布之间的差异性。</p></li></ul><h2 id="六、时间系列、图结构的距离"><a href="#六、时间系列、图结构的距离" class="headerlink" title="六、时间系列、图结构的距离"></a>六、时间系列、图结构的距离</h2><ul><li><h2 id="DTW-Dynamic-Time-Warping-距离"><a href="#DTW-Dynamic-Time-Warping-距离" class="headerlink" title="DTW (Dynamic Time Warping) 距离"></a>DTW (Dynamic Time Warping) 距离</h2></li></ul><p>DTW 距离用于衡量两个序列之间的相似性，适用于不同长度、不同节奏的时间序列。DTW采用了动态规划DP（dynamic programming）的方法来进行时间规整的计算，通过自动warping扭曲 时间序列（即在时间轴上进行局部的缩放），使得两个序列的形态尽可能的一致，得到最大可能的相似度。(具体可参考[5])<br><a href="https://camo.githubusercontent.com/98f65619a427cace4cb8e2ab94433f0e774c3cb69216cb722339bb4b65d8805d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d386633393262313536323433663730632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/98f65619a427cace4cb8e2ab94433f0e774c3cb69216cb722339bb4b65d8805d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d386633393262313536323433663730632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><ul><li><h2 id="图结构的距离"><a href="#图结构的距离" class="headerlink" title="图结构的距离"></a>图结构的距离</h2></li></ul><p>图结构间的相似度计算，有图同构、最大共同子图、图编辑距离、Graph Kernel 、图嵌入计算距离等方法（具体可参考[4][6]）。<br><a href="https://camo.githubusercontent.com/8c171587d8e066aa11898d874ff5180639bc9de4b2163c2fc25678352d6887df/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d636632343839336261373231643834642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/8c171587d8e066aa11898d874ff5180639bc9de4b2163c2fc25678352d6887df/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d636632343839336261373231643834642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><h2 id="七、度量学习-Metric-Learning"><a href="#七、度量学习-Metric-Learning" class="headerlink" title="七、度量学习(Metric Learning)"></a>七、度量学习(Metric Learning)</h2><p>度量学习的对象通常是样本特征向量的距离，度量学习的关键在于如何有效的度量样本间的距离，目的是通过训练和学习，减小或限制同类样本之间的距离，同时增大不同类别样本之间的距离，简单归类如下[2]：<br><a href="https://camo.githubusercontent.com/af46b27a597ce649bba5bdb12c8b6a4e84b734362b885566287cdf5eff023b2e/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d646661626366353865343161623766632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/af46b27a597ce649bba5bdb12c8b6a4e84b734362b885566287cdf5eff023b2e/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d646661626366353865343161623766632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><ul><li>基于降维的度量学习算法是学习一个到低维的映射矩阵，使得映射后的样本具有某些性质。包括无监督的PCA、有监督的LDA和ANMM。</li><li>基于Centroids的度量学习算法，即通过类中心进行分类的算法，而不是基于最近邻。</li><li>基于信息论推导的一些距离度量学习算法，比如ITML和MCML等通常是使用距离度量矩阵定义一个分布，然后推导出最小化两个分布的KL距离或者Jeffery距离等等。</li><li>基于深度度量学习：利用深度网络学习一个表示（Embedding），采用各种采样方法（Sampling），比如成对&#x2F;三元组训练样本（Triplet），计算一个带有Margin&#x2F;最近邻等分类或聚类算法的损失。</li></ul><h1 id="常用的度量方法汇总"><a href="#常用的度量方法汇总" class="headerlink" title="常用的度量方法汇总"></a>常用的度量方法汇总</h1><p>最后，附上常用的距离和相似度度量方法[3]：<br><a href="https://camo.githubusercontent.com/a3e2f227676e0ea3d6f51c30a2a502f163fd4b0b6610df6a07c0bc64d206aa26/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d663934353939333837313466666531652e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/a3e2f227676e0ea3d6f51c30a2a502f163fd4b0b6610df6a07c0bc64d206aa26/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d663934353939333837313466666531652e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><a href="https://camo.githubusercontent.com/f7a9a40869ba102730e779c8ebebe47b74d5b59eb51a1c2c73c359ce0c9ba210/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d373438623532613234393330653131352e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/f7a9a40869ba102730e779c8ebebe47b74d5b59eb51a1c2c73c359ce0c9ba210/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d373438623532613234393330653131352e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><a href="https://camo.githubusercontent.com/e8c868e9f06634bcb19997096b1bd2c0023dd398e615f7255ff9e33bdf4b8339/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d343161366230306361343434373636362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/e8c868e9f06634bcb19997096b1bd2c0023dd398e615f7255ff9e33bdf4b8339/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d343161366230306361343434373636362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a><a href="https://camo.githubusercontent.com/75da575a496f7db2633dde5db151155680cbfeda513127a5389cdf7785aaac50/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d613834386531383664353130666232382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"><img src="https://camo.githubusercontent.com/75da575a496f7db2633dde5db151155680cbfeda513127a5389cdf7785aaac50/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d613834386531383664353130666232382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="img"></a></p><blockquote><p>参考资料<br>[1] <a href="https://kexue.fm/archives/7388">https://kexue.fm/archives/7388</a><br>[2] <a href="https://zhuanlan.zhihu.com/p/80656461">https://zhuanlan.zhihu.com/p/80656461</a><br>[3] <a href="https://www.pianshen.com/article/70261312162/">https://www.pianshen.com/article/70261312162/</a><br>[4] <a href="https://arxiv.org/pdf/2002.07420.pdf">https://arxiv.org/pdf/2002.07420.pdf</a><br>[5] <a href="https://zhuanlan.zhihu.com/p/32849741">https://zhuanlan.zhihu.com/p/32849741</a><br>[6] <a href="https://github.com/ysig/GraKeL">https://github.com/ysig/GraKeL</a></p><p>转载于：<a href="https://github.com/aialgorithm/Blog/issues/36">https://github.com/aialgorithm/Blog/issues/36</a> 或访问 <a href="https://mp.weixin.qq.com/s/GXASBq_lujZM8HdU23CCMAASBq_lujZM8HdU23CCMA">https://mp.weixin.qq.com/s/GXASBq_lujZM8HdU23CCMAASBq_lujZM8HdU23CCMA</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——MultiLayer Perceptron</title>
    <link href="/2022/08/01/MachineLearning/MLP/"/>
    <url>/2022/08/01/MachineLearning/MLP/</url>
    
    <content type="html"><![CDATA[<h1 id="MultiLayer-Perceptron"><a href="#MultiLayer-Perceptron" class="headerlink" title="MultiLayer Perceptron"></a>MultiLayer Perceptron</h1><p>多层感知机可以简单的认为是多个perceptron组成的，拼接在一起。</p><p>多层感知机的构成很简单，一个输入层，若干个隐藏层，一个输出层。我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前$L_1$层看作表示，把最后一层看作线性预测器。</p><p><img src="/src/image-20220115191256116.png" alt="image-20220115191256116"></p><h2 id="通用近似定理"><a href="#通用近似定理" class="headerlink" title="通用近似定理"></a>通用近似定理</h2><p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。 例如，在一对输入上进行基本逻辑操作，多层感知机是通用近似器。 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。 你可能认为神经网络有点像C语言。 C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。 但实际上，想出一个符合规范的程序才是最困难的部分。��</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——KL散度</title>
    <link href="/2022/08/01/MachineLearning/KL%E6%95%A3%E5%BA%A6/"/>
    <url>/2022/08/01/MachineLearning/KL%E6%95%A3%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="如何理解K-L散度（相对熵）"><a href="#如何理解K-L散度（相对熵）" class="headerlink" title="如何理解K-L散度（相对熵）"></a>如何理解K-L散度（相对熵）</h1><p><code>Kullback-Leibler Divergence</code>，即<code>K-L散度</code>，是一种<strong>量化两种概率分布P和Q之间差异</strong>的方式，又叫<code>相对熵</code>。在概率学和统计学上，我们经常会使用一种<code>更简单的、近似的分布</code>来替代<code>观察数据</code>或<code>太复杂的分布</code>。K-L散度能帮助我们度量使用一个分布来近似另一个分布时所损失的信息量。</p><p>K-L散度定义见文末附录1。另外在附录5中解释了为什么在深度学习中，训练模型时使用的是<code>Cross Entropy</code> 而非<code>K-L Divergence</code>。</p><p>我们从下面这个问题出发思考K-L散度。</p><blockquote><p>假设我们是一群太空科学家，经过遥远的旅行，来到了一颗新发现的星球。在这个星球上，生存着一种长有牙齿的蠕虫，引起了我们的研究兴趣。我们发现这种蠕虫生有10颗牙齿，但是因为不注意口腔卫生，又喜欢嚼东西，许多蠕虫会掉牙。收集大量样本之后，我们得到关于蠕虫牙齿数量的经验分布，如下图所示</p></blockquote><p><img src="/src/webp.webp" alt="img"></p><p>牙齿数量分布</p><p><img src="/src/webp-165590384367879.webp" alt="img"></p><p>会掉牙的外星蠕虫</p><p>这些数据很有价值，但是也有点问题。我们距离地球🌍太远了，把这些概率分布数据发送回地球过于昂贵。还好我们是一群聪明的科学家，用一个只有一两个参数的简单模型来近似原始数据会减小数据传送量。最简单的近似模型是<code>均分布</code>，因为蠕虫牙齿不会超过10颗，所以有11个可能值，那蠕虫的牙齿数量概率都为 <code>1/11</code>。分布图如下：</p><p><img src="/src/webp-165590384766882.webp" alt="img"></p><p>uniform distribution</p><p>显然我们的原始数据并非均分布的，但也不是我们已知的分布，至少不是常见的分布。作为备选，我们想到的另一种简单模型是<code>二项式分布binomial distribution</code>。蠕虫嘴里面共有<code>n=10</code>个牙槽，每个牙槽出现牙齿与否为独立事件，且概率均为<code>p</code>。则蠕虫牙齿数量即为期望值<code>E[x]=n*p</code>，真实期望值即为观察数据的平均值，比如说<code>5.7</code>，则<code>p=0.57</code>，得到如下图所示的二项式分布：</p><p><img src="/src/webp-165590386927085.webp" alt="img"></p><p>binomial</p><p>对比一下原始数据，可以看出均分布和二项分布都不能完全描述原始分布。</p><p><img src="/src/75110-b049f7fc560bd8d8.png" alt="img"></p><p>all distributions</p><p>可是，我们不禁要问，哪一种分布更加接近原始分布呢？<br>已经有许多度量误差的方式存在，但是我们所要考虑的是减小发送的信息量。上面讨论的均分布和二项式分布都把问题规约到只需要两个参数，牙齿数量和概率值（均分布只需要牙齿数量即可）。那么哪个分布保留了更多的原始数据分布的信息呢？这个时候就需要K-L散度登场了。</p><h2 id="数据的熵"><a href="#数据的熵" class="headerlink" title="数据的熵"></a>数据的熵</h2><p>K-L散度源于信息论。信息论主要研究如何量化数据中的信息。最重要的信息度量单位是<code>熵</code>Entropy，一般用<code>H</code>表示。分布的熵的公式如下：</p><p><img src="/src/webp.webp" alt="img"></p><p>Entropy</p><p>上面对数没有确定底数，可以是<code>2</code>、<code>e</code>或<code>10</code>，等等。如果我们使用以<code>2</code>为底的对数计算H值的话，可以把这个值看作是编码信息所需要的最少二进制位个数bits。上面空间蠕虫的例子中，信息指的是根据观察所得的经验分布给出的蠕虫牙齿数量。计算可以得到原始数据概率分布的熵值为<code>3.12 bits</code>。这个值只是告诉我们编码蠕虫牙齿数量概率的信息需要的二进制位<code>bit</code>的位数。</p><p>可是熵值并没有给出压缩数据到最小熵值的方法，即如何编码数据才能达到最优（存储空间最优）。优化信息编码是一个非常有意思的主题，但并不是理解K-L散度所必须的。熵的主要作用是告诉我们最优编码信息方案的理论下界（存储空间），以及度量数据的信息量的一种方式。理解了熵，我们就知道有多少信息蕴含在数据之中，现在我们就可以计算当我们用一个带参数的概率分布来近似替代原始数据分布的时候，到底损失了多少信息。请继续看下节内容。↓↓↓</p><h2 id="K-L散度度量信息损失"><a href="#K-L散度度量信息损失" class="headerlink" title="K-L散度度量信息损失"></a>K-L散度度量信息损失</h2><p>只需要稍加修改<code>熵H</code>的计算公式就能得到<code>K-L散度</code>的计算公式。设<code>p</code>为观察得到的概率分布，<code>q</code>为另一分布来近似<code>p</code>，则<code>p</code>、<code>q</code>的<code>K-L散度</code>为：</p><p><img src="/src/webp-165694178715996.webp" alt="img"></p><p>entropy-p-q</p><p>显然，根据上面的公式，K-L散度其实是数据的原始分布p和近似分布q之间的对数差值的期望。如果继续用<code>2</code>为底的对数计算，则<strong>K-L散度值表示信息损失的二进制位数</strong>。下面公式以期望表达K-L散度：</p><p><img src="/src/webp-165694178859698.webp" alt="img"></p><p>DKL1</p><p>一般，K-L散度以下面的书写方式更常见：</p><p><img src="/src/webp-1656941790199100.webp" alt="img"></p><p>DKL2</p><p>注：<code>log a - log b = log (a/b)</code></p><p>OK，现在我们知道当用一个分布来近似另一个分布时如何计算信息损失量了。接下来，让我们重新回到最开始的蠕虫牙齿数量概率分布的问题。</p><h2 id="对比两种分布"><a href="#对比两种分布" class="headerlink" title="对比两种分布"></a>对比两种分布</h2><p>首先是用均分布来近似原始分布的K-L散度：</p><p><img src="/src/75110-f682f83675fa42ee.png" alt="img"></p><p>DKL-uniform</p><p>接下来计算用二项式分布近似原始分布的K-L散度：</p><p><img src="/src/webp-1656941798226108.webp" alt="img"></p><p>DKL-binomial</p><p>通过上面的计算可以看出，使用均分布近似原始分布的信息损失要比用二项式分布近似小。所以，如果要从均分布和二项式分布中选择一个的话，均分布更好些。</p><h2 id="散度并非距离"><a href="#散度并非距离" class="headerlink" title="散度并非距离"></a>散度并非距离</h2><p>很自然地，一些同学把K-L散度看作是不同分布之间距离的度量。这是不对的，因为从K-L散度的计算公式就可以看出它不符合对称性（距离度量应该满足对称性）。如果用我们上面观察的数据分布来近似二项式分布，得到如下结果：</p><p><img src="/src/webp-1656941800646111.webp" alt="img"></p><p>DKL-notdistance</p><p>所以，<code>Dkl (Observed || Binomial) != Dkl (Binomial || Observed)</code>。<br>也就是说，用<code>p</code>近似<code>q</code>和用<code>q</code>近似<code>p</code>，二者所损失的信息并不是一样的。</p><h2 id="使用K-L散度优化模型"><a href="#使用K-L散度优化模型" class="headerlink" title="使用K-L散度优化模型"></a>使用K-L散度优化模型</h2><p>前面使用的二项式分布的参数是概率 <code>p=0.57</code>，是原始数据的均值。<code>p</code>的值域在 [0, 1] 之间，我们要选择一个<code>p</code>值，建立二项式分布，目的是最小化近似误差，即K-L散度。那么<code>0.57</code>是最优的吗？<br>下图是原始数据分布和二项式分布的K-L散度变化随二项式分布参数<code>p</code>变化情况：</p><p><img src="/img/webp-165590387792790.webp" alt="img"></p><p>二项分布K-L值变化曲线</p><p>通过上面的曲线图可以看出，K-L散度值在圆点处最小，即<code>p=0.57</code>。所以我们之前的二项式分布模型已经是最优的二项式模型了。注意，我已经说了，是而像是模型，这里只限定在二项式模型范围内。</p><p>前面只考虑了均分布模型和二项式分布模型，接下来我们考虑另外一种模型来近似原始数据。首先把原始数据分成两部分，1）0-5颗牙齿的概率和 2）6-10颗牙齿的概率。概率值如下：</p><p><img src="/src/webp-1656941803774114.webp" alt="img"></p><p>ad hoc model</p><p>即，一只蠕虫的牙齿数量<code>x=i</code>的概率为<code>p/5</code>; <code>x=j</code>的概率为<code>(1-p) / 6</code>，<code>i=0,1,2,3,4,5</code>; <code>j=6,7,8,9,10</code>。<br>Aha，我们自己建立了一个新的（奇怪的）模型来近似原始的分布，模型只有一个参数<code>p</code>，像前面那样优化二项式分布的时候所做的一样，让我们画出K-L散度值随<code>p</code>变化的情况：</p><p><img src="/src/webp-165590388157393.webp" alt="img"></p><p>finding an optimal parameter value for our ad hoc model</p><p>当<code>p=0.47</code>时，K-L值取最小值<code>0.338</code>。似曾相识吗？对，这个值和使用均分布的K-L散度值是一样的（这并不能说明什么）！下面我们继续画出这个奇怪模型的概率分布图，看起来确实和均分布的概率分布图相似：</p><p><img src="/src/75110-857b751b045a3aa2.png" alt="img"></p><p>ad hoc model distribution</p><p>我们自己都说了，这是个奇怪的模型，在K-L值相同的情况下，更倾向于使用更常见的、更简单的均分布模型。</p><p>回头看，我们在这一小节中使用K-L散度作为目标方程，分别找到了二项式分布模型的参数<code>p=0.57</code>和上面这个随手建立的模型的参数<code>p=0.47</code>。是的，这就是本节的重点：<strong>使用K-L散度作为目标方程来优化模型</strong>。当然，本节中的模型都只有一个参数，也可以拓展到有更多参数的高维模型中。</p><h2 id="变分自编码器VAEs和变分贝叶斯法"><a href="#变分自编码器VAEs和变分贝叶斯法" class="headerlink" title="变分自编码器VAEs和变分贝叶斯法"></a>变分自编码器VAEs和变分贝叶斯法</h2><p>如果你熟悉神经网络，你肯能已经猜到我们接下来要学习的内容。除去神经网络结构的细节信息不谈，整个神经网络模型其实是在构造一个参数数量巨大的函数（百万级，甚至更多），不妨记为<code>f(x)</code>，通过设定目标函数，可以训练神经网络逼近非常复杂的真实函数<code>g(x)</code>。训练的关键是要设定目标函数，反馈给神经网络当前的表现如何。训练过程就是不断减小目标函数值的过程。</p><p>我们已经知道K-L散度用来度量在逼近一个分布时的信息损失量。K-L散度能够赋予神经网络近似表达非常复杂数据分布的能力。变分自编码器（Variational Autoencoders，VAEs）是一种能够学习最佳近似数据集中信息的常用方法，<a href="https://links.jianshu.com/go?to=https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders 2016</a>是一篇关于VAEs的非常不错的教程，里面讲述了如何构建VAE的细节。 <a href="https://links.jianshu.com/go?to=https://medium.com/@dmonn/what-are-variational-autoencoders-a-simple-explanation-ea7dccafb0e3">What are Variational Autoencoders? A simple explanation</a>简单介绍了VAEs，<a href="https://links.jianshu.com/go?to=https://blog.keras.io/building-autoencoders-in-keras.html">Building Autoencoders in Keras</a>介绍了如何利用Keras库实现几种自编码器。</p><p>变分贝叶斯方法（Variational Bayesian Methods）是一种更常见的方法。<a href="https://links.jianshu.com/go?to=https://www.countbayesie.com/blog/2015/3/3/6-amazing-trick-with-monte-carlo-simulations">这篇文章</a>介绍了强大的蒙特卡洛模拟方法能够解决很多概率问题。蒙特卡洛模拟能够帮助解决许多贝叶斯推理问题中的棘手积分问题，尽管计算开销很大。包括VAE在内的变分贝叶斯方法，都能用K-L散度生成优化的近似分布，这种方法对棘手积分问题能进行更高效的推理。更多变分推理（Variational Inference）的知识可以访问<a href="https://links.jianshu.com/go?to=http://edwardlib.org/">Edward library for python</a>。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ol><li><p>K-L 散度的定义</p><p><img src="/src/75110-5d773218b2511d9a.png" alt="img"></p><p>define K-L divergence</p></li><li><p>计算K-L的注意事项</p><p><img src="/src/75110-f55d663d60503fa4.png" alt="img"></p><p>notice</p></li><li><p>遇到<code>log 0</code>时怎么办</p><p><img src="/src/webp-1656941818267121.webp" alt="img"></p><p>example for K-L smoothing</p></li><li><p>信息熵、交叉熵、相对熵</p></li></ol><ul><li>信息熵，即熵，香浓熵。编码方案完美时，最短平均编码长度。</li><li>交叉熵，cross-entropy。编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度。是神经网络常用的损失函数。</li><li>相对熵，即K-L散度，relative entropy。编码方案不一定完美时，平均编码长度相对于最小值的增加值。<br>更详细对比，见知乎<a href="https://links.jianshu.com/go?to=https://www.zhihu.com/question/41252833">如何通俗的解释交叉熵与相对熵?</a></li></ul><ol><li>为什么在神经网络中使用交叉熵损失函数，而不是K-L散度？<br>K-L散度&#x3D;交叉熵-熵，即 <code>DKL( p||q )=H(p,q)−H(p)</code>。<br>在神经网络所涉及到的范围内，<code>H(p)</code>不变，则<code>DKL( p||q )</code>等价<code>H(p,q)</code>。<br>更多讨论见<a href="https://links.jianshu.com/go?to=https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne">Why do we use Kullback-Leibler divergence rather than cross entropy in the t-SNE objective function?</a>和<a href="https://links.jianshu.com/go?to=https://www.reddit.com/r/MachineLearning/comments/4mebvf/why_train_with_crossentropy_instead_of_kl/">Why train with cross-entropy instead of KL divergence in classification?</a>in_with_crossentropy_instead_of_kl%2F)</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——机器学习引言</title>
    <link href="/2022/08/01/MachineLearning/introduce/"/>
    <url>/2022/08/01/MachineLearning/introduce/</url>
    
    <content type="html"><![CDATA[<h1 id="机器学习定义"><a href="#机器学习定义" class="headerlink" title="机器学习定义"></a>机器学习定义</h1><ul><li>张志华教授定义为： A field that bridge computation and statistics with ties to information, signal process, algorithm control theory and optimization theory.</li><li>AI教父Mike Jordan的定义为：Machine Learning &#x3D; Matrix + Optimization + Algorithm + Statistics </li><li>Mitchell则给出了一个更加形象的定义：假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中的任务上获得了性能改善，则我们就说关于T和P，改程序对E进行了学习。</li></ul><p>机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能，从而在计算机上从数据中产生“模型”， 用于对新的情况给出判断。</p><pre><code class=" mermaid">graph LR;     观察--&gt;学习--&gt; 技能;     数据--&gt;机器学习--&gt; 提高某种性能指标</code></pre><h1 id="机器学习发展过程"><a href="#机器学习发展过程" class="headerlink" title="机器学习发展过程"></a>机器学习发展过程</h1><p>![屏幕截图 2022-01-03 165301](&#x2F;src&#x2F;屏幕截图 2022-01-03 165301.png)</p><h2 id="逻辑推理阶段——人工智能发展的早期阶段"><a href="#逻辑推理阶段——人工智能发展的早期阶段" class="headerlink" title="逻辑推理阶段——人工智能发展的早期阶段"></a>逻辑推理阶段——人工智能发展的早期阶段</h2><p><strong>思想</strong>： 普遍认为实现人工智能的关键技术在于自动逻辑推理，只要机器被赋予逻辑推理能力就可以实现人工智能<br><strong>成果</strong>： 主要通过谓词逻辑演算模拟人类智能。这个阶段的人工智能的主流核心技术是符号逻辑计算，在数学定理自动证明等领域获得了一定成功 </p><h2 id="专家系统——以知识工程为核心技术"><a href="#专家系统——以知识工程为核心技术" class="headerlink" title="专家系统——以知识工程为核心技术"></a>专家系统——以知识工程为核心技术</h2><p><strong>提出原因</strong>： 如果没有一定数量专业领域知识支撑，则很难实现对复杂实际问题的逻辑推理<br><strong>成果</strong>： 专家系统使用基于专家知识库的知识推理取代纯粹的符号逻辑计算，在故障诊断、游戏博弈等领域取得了巨大成功<br><strong>方法</strong>： 专家系统需要针对具体问题的专业领域特点建立相应的专家知识库，利用这些知识来完成推理和决策<br><strong>缺陷</strong>： 将专家知识总结出来并以适当的方式告诉计算机程序有时非常困难，通常需要针对每个具体任务手工建立相应的知识库<br><strong>总结</strong>： 专家知识的人工获取和表示方式严重制约了人工智能的进一步发展  </p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p><strong>发展</strong>： 20世纪90年代中期以来，机器学习得到迅速发展并逐步取代传统专家系统成为人工智能的主流核心技术，使得人工智能逐步进入机器学习时代。目前，以机器学习为主流核心技术的人工智能在多个领域取得的巨大成功已使其成为社会各界关注的焦点和引领社会未来的战略性技术  （如下图所示）</p><p><img src="/src/image-20220103165937826.png" alt="image-20220103165937826"></p><h3 id="机器学习早期各时期盛行的技术、方法"><a href="#机器学习早期各时期盛行的技术、方法" class="headerlink" title="机器学习早期各时期盛行的技术、方法"></a>机器学习早期各时期盛行的技术、方法</h3><p><img src="/src/image-20220103170040969.png" alt="image-20220103170040969"></p><pre><code class=" mermaid">graph LR;    1950s--&gt;基于神经网络的连接主义学习 --&gt; 感知机Perceptron    1970s--&gt;基于逻辑表示的符号主义学习 --&gt; 以决策理论为基础的学习技术    基于逻辑表示的符号主义学习 --&gt; 强化学习技术的发展    1980s--&gt; 从样本中学习--&gt; 符号主义学习 --&gt; 决策树DecisionTree    符号主义学习 --&gt; 基于逻辑的学习    1990s --&gt; 统计学习 --&gt; 支持向量机SVM和核方法    21stCentury --&gt; 深度学习</code></pre><h3 id="机器学习和其他领域之间的关系："><a href="#机器学习和其他领域之间的关系：" class="headerlink" title="机器学习和其他领域之间的关系："></a>机器学习和其他领域之间的关系：</h3><p><img src="/src/image-20220103172411863.png" alt="image-20220103172411863"></p><pre><code class=" mermaid">graph TD;    机器学习 --数据分析技术 --&gt; 数据挖掘    数据库 --数据管理技术 --&gt; 数据挖掘</code></pre><h1 id="机器学习的分类"><a href="#机器学习的分类" class="headerlink" title="机器学习的分类"></a>机器学习的分类</h1><ul><li><p>有监督学习：利用一组带标注样本调整模型参数，提升模型性能的学习方式。 基本思想是通过标注值告诉模型在给定输入的情况下应该输出什么值，由此获得尽可能接近真实映射方式的优化模型 <img src="/src/image-20220103173117302.png" alt="image-20220103173117302"><br><strong>有标记、有反馈、预测结果</strong></p></li><li><p>弱监督学习：利用标注数据和未标注数据学习预测，预测模型的机器学习问题。通常是有少量标注标签、大量未标注数据，因为标注数据的构建往往需要人工，成本较高，未标注数据的收集不需要太多成本。</p><ol><li>不充分学习：半监督学习、PU学习</li><li>不准确学习：多示例学习、偏标记学习</li><li>不精确学习：标签噪声学习</li></ol></li><li><p>无监督学习：通过比较样本之间的某种联系实现对样本的数据分析。 最大特点是学习算法的输入是无标记样本<br><strong>无标记、无反馈、挖掘内在结构</strong></p></li><li><p>强化学习：根据反馈信息来调整机器行为以实现自动决策的一种机器学习方式。 强化学习主要由智能体和环境两个部分组成。智能体是行为的实施者，由基于环境信息的评价函数对智能体的行为做出评价，若智能体的行为正确，则由相应的回报函数给予智能体正向反馈信息以示奖励，反之则给予智能体负向反馈信息以示惩罚<br><img src="/src/image-20220103173205080.png" alt="image-20220103173205080"><br><strong>决策过程、奖励机制、学习一系列动作</strong></p></li><li><p>主动学习：指机器不断主动给出实例让人进行标注，然后利用标注数据学习预测模型的机器学习问题。通常的监督学习使用给定的标注数据，往往是随机得到的，可以看作是“被动学习”，主动学习的目标是找出对学习最有帮助的实例让人标注，以较小的标注代价，达到较好的效果。</p></li></ul><h1 id="机器学习的任务"><a href="#机器学习的任务" class="headerlink" title="机器学习的任务"></a>机器学习的任务</h1><p>一般来说，机器学习面向的任务有三类：回归问题、分类问题、聚类问题</p><p>他们往往都是需要构建一个模型来确定，利用各种机器学习方法提升模型的泛化能力（即模型面对未知数据的处理能力）</p><h1 id="机器学习的归纳偏好"><a href="#机器学习的归纳偏好" class="headerlink" title="机器学习的归纳偏好"></a>机器学习的归纳偏好</h1><p>机器学习的归纳偏好一般有两种：</p><ul><li>No Free Lunch Theorem：一个算法$\xi_a$如果在某些问题上比另一个算法$\xi_b$ 好，必然存在另一些问题,$\xi_b$比$\xi_a$好</li><li>Ocam’s razor: 若多个假设与观察一致，则选择最简单的那个</li></ul><p>归纳偏好对应学习算法本身所作出的关于“什么样的模型更好”的假设。在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多时候直接决定了算法能都取得好的性能。</p><h1 id="损失函数和验证方法"><a href="#损失函数和验证方法" class="headerlink" title="损失函数和验证方法"></a>损失函数和验证方法</h1><p>可以参考另一篇博客</p><h1 id="结构风险最小化与经验风险最小化"><a href="#结构风险最小化与经验风险最小化" class="headerlink" title="结构风险最小化与经验风险最小化"></a>结构风险最小化与经验风险最小化</h1><p>经验风险最小化（Empirical Risk Minimization）的策略认为，经验风险最小的模型是最优的模型。根据这一策略，安装经验风险最小化求最优模型就是求解最优化问题<br>$<br>\min_{f\in \mathcal{F}}\frac{1}{N}\sum_{i&#x3D;1}^NL(y_i,f(x_i)),\qquad \mathcal{F}是假设空间<br>$当样本容量足够大，经验风险最小化能保证有很好的学习效果。（有足够的数据，包括所有发生的可能，全部给模型学习，模型的性能就会很准确），在现实中国被广泛采用。例如极大似然估计（Maximum Likehood Estimate）</p><p>但是当样本数量不够大的时候，经验风险最小化的效果就不是很好了，易产生过拟合。结构风险最小化（Structural Risk Minimization）是为了防止过拟合而提出的策略。结构风险最小化等价于正则化。结构风险在经验风险上加上了表示模型复杂度的正则化项（亦称惩罚项）。<br>$<br>\min_{f\in \mathcal{F}}\frac{1}{N}\sum_{i&#x3D;1}^NL(y_i,f(x_i))+\lambda\mathcal{J}(f),\qquad \mathcal{F}是假设空间,\ \mathcal{J}是模型复杂度<br>$结构风险最小化的应用即是最大后验概率（Maximum Posterior Probability Estimate） Estimate）</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——4.MapReduce介绍</title>
    <link href="/2022/08/01/BigData&amp;Linux/4.MapReduce/"/>
    <url>/2022/08/01/BigData&amp;Linux/4.MapReduce/</url>
    
    <content type="html"><![CDATA[<h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><p><img src="/../src/image-20211023183642227.png" alt="image-20211023183642227"></p><p>上图是MapReduce的任务处理过程</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>MapReduce是一种分布式计算模型，由Google提出，主要用于搜索领域，解决海量数据的计算问题.<br>MapReduce是分布式运行的，由两个阶段组成：Map和Reduce，Map阶段是一个独立的程序，有很多个节点同时运行，每个节点处理一部分数据。Reduce阶段是一个独立的程序，有很多个节点同时运行，每个节点处理一部分数据【在这先把reduce理解为一个单独的聚合程序即可】。<br>MapReduce框架都有默认实现，用户只需要覆盖map()和reduce()两个函数，即可实现分布式计算，非常简单。<br>这两个函数的形参和返回值都是&lt;key、value&gt;，使用的时候一定要注意构造&lt;k,v&gt;。</p><h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><ol><li>框架使用InputFormat类的子类把输入文件(夹)划分为很多InputSplit，默认，每个HDFS的block对应一个InputSplit。通过RecordReader类，把每个InputSplit解析成一个个&lt;k1,v1&gt;。默认，框架对每个InputSplit中的每一行，解析成一个&lt;k1,v1&gt;。</li><li>框架调用Mapper类中的map(…)函数，map函数的形参是&lt;k1,v1&gt;对，输出是&lt;k2,v2&gt;对。一个InputSplit对应一个map task。程序员可以覆盖map函数，实现自己的逻辑。</li><li>(假设reduce存在)框架对map输出的&lt;k2,v2&gt;进行分区。不同的分区中的&lt;k2,v2&gt;由不同的reduce task处理。默认只有1个分区。<br> (假设reduce不存在)框架对map结果直接输出到HDFS中。</li><li>(假设reduce存在)框架对每个分区中的数据，按照k2进行排序、分组。分组指的是相同k2的v2分成一个组。注意：分组不会减少&lt;k2,v2&gt;数量。</li><li>(假设reduce存在，可选)在map节点，框架可以执行reduce归约。</li><li>(假设reduce存在)框架会对map task输出的&lt;k2,v2&gt;写入到linux 的磁盘文件中。</li></ol><h2 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h2><ol><li>框架对多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点。这个过程称作shuffle。</li><li>框架对reduce端接收的[map任务输出的]相同分区的&lt;k2,v2&gt;数据进行合并、排序、分组。</li><li>框架调用Reducer类中的reduce方法，reduce方法的形参是&lt;k2,{v2…}&gt;，输出是&lt;k3,v3&gt;。一个&lt;k2,{v2…}&gt;调用一次reduce函数。程序员可以覆盖reduce函数，实现自己的逻辑。</li><li>框架把reduce的输出保存到HDFS中。</li></ol><h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><ol><li>每个map有一个环形内存缓冲区，用于存储map的输出。默认大小100MB（io.sort.mb属性），一旦达到阀值0.8（io.sort.spill.percent）,一个后台线程把内容溢写到(spill)磁盘的指定目录（mapred.local.dir）下的一个新建文件中。</li><li>写磁盘前，要partition,sort。如果有combiner，combine排序后数据。</li><li>等最后记录写完，合并全部文件为一个分区且排序的文件。</li></ol><hr><ol><li>Reducer通过Http方式得到输出文件的特定分区的数据。</li><li>排序阶段合并map输出。然后走Reduce阶段。</li><li>reduce执行完之后，写入到HDFS中。</li></ol><table><thead><tr><th>Java基本类型</th><th>Writable</th><th>序列化大小(字节)</th></tr></thead><tbody><tr><td>布尔型（boolean）</td><td>BooleanWritable</td><td>1</td></tr><tr><td>字节型（byte）</td><td>ByteWritable</td><td>1</td></tr><tr><td>整型（int）</td><td>IntWritable</td><td>4</td></tr><tr><td></td><td>VIntWritable</td><td>1-5</td></tr><tr><td>浮点型（float）</td><td>FloatWritable</td><td>4</td></tr><tr><td>长整型（long）</td><td>LongWritable</td><td>8</td></tr><tr><td>双精度浮点型（double）</td><td>VLongWritable</td><td>1-9</td></tr><tr><td></td><td>DoubleWritable</td><td>8</td></tr></tbody></table><h2 id="Demo-WordCount"><a href="#Demo-WordCount" class="headerlink" title="Demo:WordCount"></a>Demo:WordCount</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCount</span> &#123;<br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 该main方法是该mapreduce程序运行的入口，其中用一个Job类对象来管理程序运行时所需要的很多参数：</span><br><span class="hljs-comment">     * 比如，指定用哪个组件作为数据读取器、数据结果输出器 指定用哪个类作为map阶段的业务逻辑类，哪个类作为reduce阶段的业务逻辑类</span><br><span class="hljs-comment">     * 指定wordcount job程序的jar包所在路径 .... 以及其他各种需要的参数</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>        <span class="hljs-comment">// 指定hdfs相关的参数</span><br>        <span class="hljs-type">Configuration</span> <span class="hljs-variable">conf</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br><br>        <span class="hljs-comment">// 手动设置，该MapReduce程序读取的数据来自于HDFS集群</span><br>        conf.set(<span class="hljs-string">&quot;fs.defaultFS&quot;</span>, <span class="hljs-string">&quot;hdfs://master:9000&quot;</span>);<br>        <span class="hljs-comment">// 设置运行程序的用户是hadoop用户，就是安装hadoop集群的用户。如果该程序在Hadoop集群中使用hadoop用户进行运行，则可以去掉</span><br>        System.setProperty(<span class="hljs-string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="hljs-string">&quot;root&quot;</span>);<br><br>        <span class="hljs-comment">/**</span><br><span class="hljs-comment">         * 以上的配置信息，事实上，在实际企业生产环境中，也可以使用conf.addResource方法进行加载。</span><br><span class="hljs-comment">         * 当然如果配置文件的名字是core/hdfs/yarn/mapred-site/default.xml的话。 那么会自动加载的。</span><br><span class="hljs-comment">         */</span><br><span class="hljs-comment">//        conf.addResource(&quot;hadoop_config/core-site.xml&quot;);</span><br><span class="hljs-comment">//        conf.addResource(&quot;hadoop_config/hdfs-site.xml&quot;);</span><br><br>        <span class="hljs-comment">// 如果想让MR程序运行在特定的YARN集群之上，则可以使用一下代码，然后，这两信息，在安装集群的配置文件中都有配置的</span><br>        <span class="hljs-comment">// conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;);</span><br>        <span class="hljs-comment">// conf.set(&quot;yarn.resourcemanager.hostname&quot;, &quot;node1&quot;);</span><br><br>        <span class="hljs-comment">// 通过Configuration对象获取Job对象，该job对象会组织所有的该MapReduce程序所有的各种组件</span><br>        <span class="hljs-type">Job</span> <span class="hljs-variable">job</span> <span class="hljs-operator">=</span> Job.getInstance(conf);<br><br>        <span class="hljs-comment">// 设置jar包所在路径</span><br>        job.setJarByClass(WordCount.class);<br><br>        <span class="hljs-comment">// 指定mapper类和reducer类</span><br>        job.setMapperClass(WordCountMapper.class);<br>        job.setReducerClass(WordCountReducer.class);<br><br>        <span class="hljs-comment">/**</span><br><span class="hljs-comment">         * 指定maptask的输出类型</span><br><span class="hljs-comment">         * Mapper的输入key-value类型，由MapReduce框架决定， 默认情况下就是 LongWritable和Text类型</span><br><span class="hljs-comment">         * </span><br><span class="hljs-comment">         * 假如 mapTask的输出key-value类型，跟reduceTask的输出key-value类型一致，那么，以上两句代码可以不用设置</span><br><span class="hljs-comment">         */</span><br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(IntWritable.class);<br><br>        <span class="hljs-comment">/**</span><br><span class="hljs-comment">         * 指定reducetask的输出类型</span><br><span class="hljs-comment">         * 如果reduceTask的输入key-value类型就是 mapTask的输出key-value类型。可以不需要指定</span><br><span class="hljs-comment">         */</span><br>        job.setOutputKeyClass(Text.class);<br>        job.setOutputValueClass(IntWritable.class);<br><br>        <span class="hljs-comment">// 为job指定输入数据的组件和输出数据的组件，以下两个参数是默认的，所以不指定也是OK的</span><br>        job.setInputFormatClass(TextInputFormat.class);<br>        job.setOutputFormatClass(TextOutputFormat.class);<br><br>        <span class="hljs-comment">// 为该mapreduce程序制定默认的数据分区组件。默认是 HashPartitioner.class</span><br>        job.setPartitionerClass(HashPartitioner.class);<br><br>        <span class="hljs-comment">/**</span><br><span class="hljs-comment">         * 指定该mapreduce程序数据的输入和输出路径:</span><br><span class="hljs-comment">         * inputPath目录可以是文件，也可以是目录</span><br><span class="hljs-comment">         * outputPath路径必须是不存在的目录</span><br><span class="hljs-comment">         */</span><br>        <span class="hljs-type">Path</span> <span class="hljs-variable">inputPath</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;/bigdata/data.txt&quot;</span>);<br>        <span class="hljs-type">Path</span> <span class="hljs-variable">outputPath</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;/output/wc&quot;</span>);<br><br>        <span class="hljs-comment">// 设置该MapReduce程序的ReduceTask的个数</span><br>        <span class="hljs-comment">// job.setNumReduceTasks(3);</span><br><br>        <span class="hljs-comment">// 该段代码是用来判断输出路径存在不存在，存在就删除，虽然方便操作，但请谨慎</span><br>        <span class="hljs-type">FileSystem</span> <span class="hljs-variable">fs</span> <span class="hljs-operator">=</span> FileSystem.get(conf);<br>        <span class="hljs-keyword">if</span> (fs.exists(outputPath)) &#123;<br>            fs.delete(outputPath, <span class="hljs-literal">true</span>);<br>        &#125;<br><br>        <span class="hljs-comment">// 设置wordcount程序的输入路径</span><br>        FileInputFormat.setInputPaths(job, inputPath);<br>        <span class="hljs-comment">// 设置wordcount程序的输出路径</span><br>        FileOutputFormat.setOutputPath(job, outputPath);<br><br>        <span class="hljs-comment">// job.submit();</span><br>        <span class="hljs-comment">// 最后提交任务(verbose布尔值 决定要不要将运行进度信息输出给用户)</span><br>        <span class="hljs-type">boolean</span> <span class="hljs-variable">waitForCompletion</span> <span class="hljs-operator">=</span> job.waitForCompletion(<span class="hljs-literal">true</span>);<br>        <span class="hljs-comment">// 主线程程序根据MapReduce程序的运行结果成功与否退出。</span><br>        System.exit(waitForCompletion ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;</span><br><span class="hljs-comment">     * </span><br><span class="hljs-comment">     * KEYIN 是指框架读取到的数据的key的类型，在默认的InputFormat下，读到的key是一行文本的起始偏移量，所以key的类型是Long</span><br><span class="hljs-comment">     * VALUEIN 是指框架读取到的数据的value的类型,在默认的InputFormat下，读到的value是一行文本的内容，所以value的类型是String</span><br><span class="hljs-comment">     * KEYOUT 是指用户自定义逻辑方法返回的数据中key的类型，由用户业务逻辑决定，在此wordcount程序中，我们输出的key是单词，所以是String</span><br><span class="hljs-comment">     * VALUEOUT 是指用户自定义逻辑方法返回的数据中value的类型，由用户业务逻辑决定,在此wordcount程序中，我们输出的value是单词的数量，所以是Integer</span><br><span class="hljs-comment">     * </span><br><span class="hljs-comment">     * 但是，String ，Long等jdk中自带的数据类型，在序列化时，效率比较低，hadoop为了提高序列化效率，自定义了一套序列化框架</span><br><span class="hljs-comment">     * 所以，在hadoop的程序中，如果该数据需要进行序列化（写磁盘，或者网络传输），就一定要用实现了hadoop序列化框架的数据类型</span><br><span class="hljs-comment">     * </span><br><span class="hljs-comment">     * Long ----&gt; LongWritable </span><br><span class="hljs-comment">     * String ----&gt; Text </span><br><span class="hljs-comment">     * Integer ----&gt; IntWritable </span><br><span class="hljs-comment">     * Null ----&gt; NullWritable</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;<br><br>        <span class="hljs-comment">/**</span><br><span class="hljs-comment">         * LongWritable key : 该key就是value该行文本的在文件当中的起始偏移量</span><br><span class="hljs-comment">         * Text value ： 就是MapReduce框架默认的数据读取组件TextInputFormat读取文件当中的一行文本</span><br><span class="hljs-comment">         */</span><br>        <span class="hljs-meta">@Override</span><br>        <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br><br>            <span class="hljs-comment">// 切分单词</span><br>            String[] words = value.toString().split(<span class="hljs-string">&quot;,&quot;</span>);<br>            <span class="hljs-keyword">for</span> (String word : words) &#123;<br>                <span class="hljs-comment">// 每个单词计数一次，也就是把单词组织成&lt;hello,1&gt;这样的key-value对往外写出</span><br>                context.write(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(word), <span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>(<span class="hljs-number">1</span>));<br>            &#125;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 首先，和前面一样，Reducer类也有输入和输出，输入就是Map阶段的处理结果，输出就是Reduce最后的输出</span><br><span class="hljs-comment">     * reducetask在调我们写的reduce方法,reducetask应该收到了前一阶段（map阶段）中所有maptask输出的数据中的一部分</span><br><span class="hljs-comment">     * （数据的key.hashcode%reducetask数==本reductask号），所以reducetaks的输入类型必须和maptask的输出类型一样</span><br><span class="hljs-comment">     * </span><br><span class="hljs-comment">     * reducetask将这些收到kv数据拿来处理时，是这样调用我们的reduce方法的： 先将自己收到的所有的kv对按照k分组（根据k是否相同）</span><br><span class="hljs-comment">     * 将某一组kv中的第一个kv中的k传给reduce方法的key变量，把这一组kv中所有的v用一个迭代器传给reduce方法的变量values</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;<br><br>        <span class="hljs-comment">/**</span><br><span class="hljs-comment">         * Text key : mapTask输出的key值</span><br><span class="hljs-comment">         * Iterable&lt;IntWritable&gt; values ： key对应的value的集合（该key只是相同的一个key）</span><br><span class="hljs-comment">         * </span><br><span class="hljs-comment">         * reduce方法接收key值相同的一组key-value进行汇总计算</span><br><span class="hljs-comment">         */</span><br>        <span class="hljs-meta">@Override</span><br>        <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br><br>            <span class="hljs-comment">// 结果汇总</span><br>            <span class="hljs-type">int</span> <span class="hljs-variable">sum</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>            <span class="hljs-keyword">for</span> (IntWritable v : values) &#123;<br>                sum += v.get();<br>            &#125;<br>            <span class="hljs-comment">// 汇总的结果往外输出</span><br>            context.write(key, <span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>(sum));<br>        &#125;<br>    &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>打完jar包传到虚拟机以后，这里需要注意的是，你在代码中是如何书写关于路径问题的，如果是直接写路径，并且jar中指出来主类，那么运行jar不需要任何参数。</p><p>如果路径的设置是以 args【1】或者args【2】输入的话，就需要你在运行jar时手动追加输入路径。此时输出路径需注意是个空文件夹或者未生成，不然就会提醒报错。（同样也可以在代码中利用api写删除路径）如果是idea的maven打包的话，还需要注意声明jar 的主方法。</p><p>hadoop 任务 提交命令（利用参数提交）：<br>hadoop jar WCdemo.jar &#x2F;data&#x2F;wc.txt &#x2F;out1</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>MapReduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——安全半监督</title>
    <link href="/2022/07/31/Essay/Safe-Semi-Supervised/"/>
    <url>/2022/07/31/Essay/Safe-Semi-Supervised/</url>
    
    <content type="html"><![CDATA[<h1 id="Safe-Semi-Supervised"><a href="#Safe-Semi-Supervised" class="headerlink" title="Safe Semi-Supervised"></a>Safe Semi-Supervised</h1><p>半监督学习的出现是为了改善实际中标记工作昂贵代价问题。意图使用一些标记的数据和大量未标记的数据来对模型训练达到一个好的表现。半监督工作可主要分为这四类：半监督聚类，半监督分类，半监督降维，半监督回归。</p><p>安全半监督是针对在半监督学习中出现的模型性能退化问题的新的研究方向。其实不仅在半监督领域，在整个弱监督领域，安全性保证模型性能问题都是十分关键的</p><h2 id="机器学习安全半监督"><a href="#机器学习安全半监督" class="headerlink" title="机器学习安全半监督"></a>机器学习安全半监督</h2><p>南大LAMDA 周志华老师团队李宇峰老师的工作方向中就有这个</p><blockquote><p>[1] T. Wei, L.-Z. Guo, Y.-F. Li和W. Gao, 《Learning safe multi-label prediction for weakly labeled data》, <em>Mach. Learn.</em>, 2018<br>[2] Y.-F. Li和D.-M. Liang, 《Safe semi-supervised learning: a brief introduction》, <em>Frontiers Comput. Sci.</em>, 2019<br>[3] Y.-F. Li, L.-Z. Guo和Z.-H. Zhou, 《Towards Safe Weakly Supervised Learning》, <em>IEEE Trans. Pattern Anal. Mach. Intell.</em>, 2021<br>[4] S4VM: Safe Semi-Supervised Support Vector Machine. CoRR abs&#x2F;1005.1545, 2010</p></blockquote><p><img src="/src/image-20220724155916151.png" alt="image-20220724155916151"></p><p>这里小小的借用一下老师文章中的图，图中很清楚的描述了在半监督学习过程中，我们可能遇见的问题。老师同时也给出了一些解决的方式</p><pre><code class=" mermaid">graph LR;WeaklySupervised --&gt; InexactSupervisedWeaklySupervised --&gt; SemiSupervisedWeaklySupervised --&gt; InaccurateSupervisedSemiSupervised --&gt; GenerativeSemiSupervised --&gt; Semi-SVMsSemiSupervised --&gt; GraphSemiSupervised --&gt; Disagreement</code></pre><h3 id="基于生成式模型的方法"><a href="#基于生成式模型的方法" class="headerlink" title="基于生成式模型的方法"></a>基于生成式模型的方法</h3><h4 id="密度（概率）估计："><a href="#密度（概率）估计：" class="headerlink" title="密度（概率）估计："></a>密度（概率）估计：</h4><p>在不了解事件概率分布的情况下，先假设随机分布，然后通过数据观测来确定真正的数据分布是怎样的。这个假设使得我们通过潜在分布的参数将未标记的数据与学习目标练习起来，而未标记的数据的标记则可看作分布的缺失参数，通常可基于EM算法进行极大似然估计。不同的分布假设将产生不同的方法。</p><p>优点：此类方法简单易于实现，在有标记数据极少的情况下往往比其他方法表现更好。<br>缺点：必须保证假设模型的准确，否则利用未标记数据会降低泛化性能。在实际情况中，没有专业知识的情况下很难预先做出准确的假设模型。</p><h4 id="样本生成："><a href="#样本生成：" class="headerlink" title="样本生成："></a>样本生成：</h4><p>有少量训练数据，通过训练后的模型来生成类似的样本。<br>我们有时会面临数据缺乏数据的情况，我们可以通过生成模型来补足。<br>在大多数生成模型中，GAN是目前唯一一种直接从数据观测的一步到位的生成模型。</p><h3 id="半监督SVM方法"><a href="#半监督SVM方法" class="headerlink" title="半监督SVM方法"></a>半监督SVM方法</h3><p>使得SVM在所有训练数据上最大化间隔（穿越数据的低密度区域划分超平面）。TSVM是用有标签数据预训练一个SVM，再利用这个SVM给无标注数据进行标注，当所有数据有标注后又成为了一个典型的SVM问题。这个过程中可能给未标注数据附上错误的标签，而搜寻标记指派可能出错的每一对为标记样本进行调整，是一个涉及巨大计算开销的优化问题。样本的不平衡问题也可能使SVM受到困扰，解决方案是在损失函数中对未标注数据项的正负两个类别赋予不同的权重。</p><h3 id="基于图的方法"><a href="#基于图的方法" class="headerlink" title="基于图的方法"></a>基于图的方法</h3><p>使用数据结构中的图结构结合不同的边权重算法传递标签将数据集完成分类。<br>图半监督学习会耗费大量的存储空间，而且再加入新样本后，不易确定新样本在图中的位置，需要使用其他方法来更新图。</p><h3 id="基于分歧的方法："><a href="#基于分歧的方法：" class="headerlink" title="基于分歧的方法："></a>基于分歧的方法：</h3><p>通过使用多个学习器来对未标记数据进行利用，在学习过程中将未标记数据作为多学习器间信息交互的平台。李老师提出的S4VM也是基于这个方法的。</p><h2 id="深度学习半监督"><a href="#深度学习半监督" class="headerlink" title="深度学习半监督"></a>深度学习半监督</h2><p>深度SSL方法主要包括三类：一致性正则化方法、伪标记方法和混合方法。一致性正则化方法通过强制原始未标记数据和扰动未标记数据的输出相似来利用未标记数据。伪标记方法利用模型本身获得未标记数据的伪标记。混合方法同时结合了一致性正则化、伪标记和数据增强。然而，这些方法的成功是基于所有标记和未标记数据均来自相同分布的假设。一旦不满足这一假设，这些SSL方法的性能就会下降，甚至低于仅使用标记数据训练的监督学习方法的性能。</p><h2 id="深度学习安全半监督"><a href="#深度学习安全半监督" class="headerlink" title="深度学习安全半监督"></a>深度学习安全半监督</h2><p>近期看到一篇论文是关于深度学习安全半监督方面的，感觉做的很有意思。R. He, Z. Han, Y. Yang&amp;Y. Yin,  Not All Parameters Should Be Treated Equally: Deep Safe Semi-supervised Learning under Class Distribution Mismatch, AAAI 2022</p><p>这边文章中对于周志华老师提出的SDU问题(Safe Deep Semi-supervised Learning and Unseen-class Unlabeled data, ICML 2020)，进行另一个方向的解答。之前的工作总是意图在数据层面进行改进，无论是学习数据分布，还是判断数据可信度，对数据进行删除或者赋予低的权重，感觉都是有一些治标不治本的感觉。对于当下的动态环境，静态的数据分布往往是学习不过来的，所以本文提出了在模型参数方面的修正，感觉是一个可以深究的项目。</p><p><img src="/src/image-20220731113241884.png" alt="image-20220731113241884"></p><p>给人的感觉好像就是一个树的修剪，然后放到了不同的领域。具体的内容还请移步论文阅读。<br>��</p>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——LOF小记</title>
    <link href="/2022/07/22/Algorithms/LocalOutlierFactor/"/>
    <url>/2022/07/22/Algorithms/LocalOutlierFactor/</url>
    
    <content type="html"><![CDATA[<p>LOF is short for the Local Outlier Factor. LOF computes the degree of an object is an outlier and the degree depends on how isolated the object is with respect to its surrounding neighborhood. Note that LOF only</p><p><strong>定义</strong></p><p>首先，基于密度的离群点检测方法有一个基本假设：<strong>非离群点对象周围的密度与其邻域周围的密度类似，而离群点对象周围的密度显著不同于其邻域周围的密度。</strong></p><p>什么意思呢？看下面图片感受下。</p><p><img src="/src/640-1640416354808-16579722103081.webp" alt="图片"></p><p>集群 <code>C1</code> 包含了 400 多个点，集群 <code>C2</code> 包含 100 个点。<code>C1</code> 和 <code>C2</code> 都是一类集群点，区别是 <code>C1</code> 位置比较集中，或者说密度比较大。而像 <code>o1</code>、<code>o2</code>点均为异常点，因为基于我们的假设，这两个点周围的密度显著不同于周围点的密度。</p><p>LOF 就是基于密度来判断异常点的，通过给每个数据点都分配一个依赖于邻域密度的离群因子 LOF，进而判断该数据点是否为离群点。 如果 ，则该点为离群点，如果 ，则该点为正常数据点。</p><p><strong>LOF分数计算</strong></p><ol><li><p>K近邻距离。在距离数据点P最近的几个点中，第k个最近的点跟点P之间的距离称为点P的 K-邻近距离，记为 k-distance (p)，公式如下：<br>$$<br>d_k(p)&#x3D;d(P,O)<br>$$</p></li><li><p>K近邻领域。以点P的圆心，以K近邻距离$d_k(P)$为半径画圆，这个圈以内的范围就是K距离领域<br>$$<br>N_k(P)&#x3D;{ d(P,O’) \le d_k({P}) }<br>$$</p></li><li><p>可达距离。点P到点O的第K可达距离<br>$$<br>reach_dist_k(O,P)&#x3D;max{d_k(O),d(O,P) }<br>$$<br><img src="/src/image-20220316145612429-165693987619524.png" alt="image-20220316145612429"><br>$p_2$距离$o$点远，那么可达距离就是他们的实际距离。但是$p_1$离$o$点的距离近，小于K近邻距离，那么实际距离就会被K近邻距离所代替 </p></li><li><p><strong>局部可达密度</strong>。数据点P的局部可达密度就是基于P的最近邻的平均可达距离的倒数。距离越大，密度越小。<br>$$<br>lrd_k(P)&#x3D;\frac{1}{\frac{\sum_{O \ni N_k(P)}reach_dist_k(P,O)}{|N_k(P)|}}<br>$$</p></li><li><p><strong>局部异常因子</strong>。</p></li></ol><p><strong>实现流程</strong></p><p>了解了 LOF 的定义以后，整个算法也就显而易见了：</p><ol><li>对于每个数据点，计算它与其它所有点的距离，并按从近到远排序；</li><li>对于每个数据点，找到它的 k-nearest-neighbor，计算 LOF 得分;</li><li>如果LOF值越大，说明越异常，反之如果越小，说明越趋于正常。</li></ol><p><img src="/src/image-20211225151420414-165693987964027.png" alt="image-20211225151420414"></p><p><strong>优缺点</strong></p><p><strong>优点</strong></p><p>LOF 的一个优点是它同时考虑了数据集的局部和全局属性。异常值不是按绝对值确定的，而是相对于它们的邻域点密度确定的。当数据集中存在不同密度的不同集群时，LOF表现良好，比较适用于中等高维的数据集。</p><p><strong>缺点</strong></p><p>LOF算法中关于局部可达密度的定义其实暗含了一个假设，即：<strong>不存在大于等于 k 个重复的点。</strong></p><p>当这样的重复点存在的时候，这些点的平均可达距离为零，局部可达密度就变为无穷大，会给计算带来一些麻烦。在实际应用时，为了避免这样的情况出现，可以把 k-distance 改为 k-distinct-distance，不考虑重复的情况。或者，还可以考虑给可达距离都加一个很小的值，避免可达距离等于零。</p><p>另外，LOF 算法需要计算数据点两两之间的距离，造成整个算法时间复杂度为 。为了提高算法效率，后续有算法尝试改进。FastLOF （Goldstein，2012）先将整个数据随机的分成多个子集，然后在每个子集里计算 LOF 值。对于那些 LOF 异常得分小于等于 1 的，从数据集里剔除，剩下的在下一轮寻找更合适的 nearest-neighbor，并更新 LOF 值。</p><p><strong>代码实现</strong></p><p>有两个库可以计算LOF，分别是<code>PyOD</code>和<code>Sklearn</code>，下面分别介绍。</p><p>使用<code>pyod</code>自带的方法生成200个训练样本和100个测试样本的数据集。正态样本由多元高斯分布生成，异常样本是使用均匀分布生成的。</p><p>训练和测试数据集都有 5 个特征，10% 的行被标记为异常。并且在数据中添加了一些随机噪声，让完美分离正常点和异常点变得稍微困难一些。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyod.utils.data <span class="hljs-keyword">import</span> generate_data<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>X_train, y_train, X_test, y_test = \<br>        generate_data(n_train=<span class="hljs-number">200</span>,<br>                      n_test=<span class="hljs-number">100</span>,<br>                      n_features=<span class="hljs-number">5</span>,<br>                      contamination=<span class="hljs-number">0.1</span>,<br>                      random_state=<span class="hljs-number">3</span>) <br>X_train = X_train * np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=X_train.shape)<br>X_test = X_test * np.random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>, size=X_test.shape)<br></code></pre></td></tr></table></figure><p><strong>PyOD</strong></p><p>下面将训练数据拟合了 LOF 模型并将其应用于合成测试数据。</p><p>在 <code>PyOD</code> 中，有两个关键方法：<code>decision_function</code> 和 <code>predict</code>。</p><ul><li>decision_function：返回每一行的异常分数</li><li>predict：返回一个由 0 和 1 组成的数组，指示每一行被预测为正常 (0) 还是异常值 (1)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyod.models.lof <span class="hljs-keyword">import</span> LOF<br>clf_name = <span class="hljs-string">&#x27;LOF&#x27;</span><br>clf = LOF()<br>clf.fit(X_train)<br><br>test_scores = clf.decision_function(X_test)<br><br>roc = <span class="hljs-built_in">round</span>(roc_auc_score(y_test, test_scores), ndigits=<span class="hljs-number">4</span>)<br>prn = <span class="hljs-built_in">round</span>(precision_n_scores(y_test, test_scores), ndigits=<span class="hljs-number">4</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;clf_name&#125;</span> ROC:<span class="hljs-subst">&#123;roc&#125;</span>, precision @ rank n:<span class="hljs-subst">&#123;prn&#125;</span>&#x27;</span>)<br>&gt;&gt; LOF ROC:<span class="hljs-number">0.9656</span>, precision @ rank n:<span class="hljs-number">0.8</span><br></code></pre></td></tr></table></figure><p>可以通过 LOF 模型方法查看 LOF 分数的分布。在下图中看到正常数据（蓝色）的分数聚集在 1.0 左右。离群数据点（橙色）的得分均大于 1.0，一般高于正常数据。</p><p><img src="/src/640-1640416819808-165693988344330.webp" alt="图片"></p><p><strong>Sklearn</strong></p><p>在<code>scikit-learn</code>中实现 <code>LOF</code> 进行异常检测时，有两种模式选择：异常检测模式 <code>(novelty=False)</code> 和 novelty检测模式 <code>(novelty=True)</code>。</p><p>在异常检测模式下，只有<code>fit_predict</code>生成离群点预测的方法可用。可以使用<code>negative_outlier_factor_</code>属性检索训练数据的异常值分数，但无法为未见过的数据生成分数。模型会根据<code>contamination</code>参数（默认值为 0.1）自动选择异常值的阈值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>detector = LOF()<br>scores = detector.fit(X_train).decision_function(X_test)<br><br>sns.distplot(scores[y_test==<span class="hljs-number">0</span>], label=<span class="hljs-string">&quot;inlier scores&quot;</span>)<br>sns.distplot(scores[y_test==<span class="hljs-number">1</span>], label=<span class="hljs-string">&quot;outlier scores&quot;</span>).set_title(<span class="hljs-string">&quot;Distribution of Outlier Scores from LOF Detector&quot;</span>)<br>plt.legend()<br>plt.xlabel(<span class="hljs-string">&quot;Outlier score&quot;</span>)<br></code></pre></td></tr></table></figure><p>在novelty检测模式下，只有<code>decision_function</code>用于生成异常值可用。<code>fit_predict</code>方法不可用，但<code>predict</code>方法可用于生成异常值预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">clf = LocalOutlierFactor(novelty=<span class="hljs-literal">True</span>)<br>clf = clf.fit(X_train)<br>test_scores = clf.decision_function(X_test)<br><br>test_scores = -<span class="hljs-number">1</span>*test_scores<br><br>roc = <span class="hljs-built_in">round</span>(roc_auc_score(y_test, test_scores), ndigits=<span class="hljs-number">4</span>)<br>prn = <span class="hljs-built_in">round</span>(precision_n_scores(y_test, test_scores), ndigits=<span class="hljs-number">4</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;clf_name&#125;</span> ROC:<span class="hljs-subst">&#123;roc&#125;</span>, precision @ rank n:<span class="hljs-subst">&#123;prn&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>该模式下模型的异常值分数被反转，异常值的分数低于正常值。</p><p><img src="/src/640-1640416819835-165693988642633.webp" alt="图片">2633.webp)2633.webp)</p>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——元学习小记</title>
    <link href="/2022/07/22/Essay/MetaLearning/"/>
    <url>/2022/07/22/Essay/MetaLearning/</url>
    
    <content type="html"><![CDATA[<h1 id="Meta-Learing"><a href="#Meta-Learing" class="headerlink" title="Meta-Learing"></a>Meta-Learing</h1><p>元学习（Meta-Learing），又称“学会学习“（Learning to learn）, 即利用以往的知识经验来指导新任务的学习，使网络具备学会学习的能力，是解决小样本问题（Few-shot Learning）常用的方法之一</p><blockquote><p>以下信息摘自wikipedia</p><p>​元学习是机器学习的一个子领域，自动学习算法被应用于机器学习实验的元数据。主要目标是利用这种元数据来理解自动学习如何在解决学习问题时变得灵活，从而提高现有学习算法的性能，或者学习（诱导）学习算法本身，因此有了learning to learn的替代术语。<br>​<strong>灵活性</strong>很重要，因为每个学习算法都是基于一组关于数据的假设，即它的归纳偏向。这意味着只有当偏向与学习问题相匹配时，它才能学得好。一个学习算法可能在一个领域表现得非常好，但在下一个领域就不行了。这给机器学习或数据挖掘技术的使用带来了很大的限制，因为学习问题（通常是某种数据库）和不同学习算法的有效性之间的关系还没有被理解。<br>​通过使用不同类型的元数据，如学习问题的属性、算法属性（如性能度量）或先前从数据中派生的模式，可以学习、选择、更改或组合不同的学习算法以有效地解决给定的学习问题问题。<a href="https://en.wikipedia.org/wiki/Metaheuristic">元学习方法的批评与元启发式</a>的批评非常相似，这是一个可能相关的问题。</p></blockquote><h2 id="元学习中的元是什么意思？"><a href="#元学习中的元是什么意思？" class="headerlink" title="元学习中的元是什么意思？"></a>元学习中的元是什么意思？</h2><p>​元学习的本质是增加学习器在多任务的泛化能力，元学习对于任务和数据都需要采样，因此学习到的F(x)可以在未出现的任务中迅速（依赖很少的样本）建立起mapping。因此”元“体现在网络对于每个任务的学习，通过不断的适应每个具体任务，使网络具备了一种抽象的学习能力。</p><h2 id="元学习中的训练和测试？"><a href="#元学习中的训练和测试？" class="headerlink" title="元学习中的训练和测试？"></a>元学习中的训练和测试？</h2><p>​Meta-learning中为了区别概念，将训练过程定义为”Meta-training“、测试过程定义为”Meta-testing“, 如下图所示：</p><pre><code class=" mermaid">graph TD;MetaLearning --&gt; MetaTrainingMetaLearning --&gt; MetaTestingMetaTraining --&gt; SupportSet:SMetaTraining --&gt; QuerySet:QMetaTesting --&gt; SupportSet:S&#x27;MetaTesting --&gt; QuerySet:Q&#x27;</code></pre><p>​如上图，区别于一般神经网络端到端的训练方式，元学习的训练过程和测试过程各需要两类数据集(Support&#x2F;Query set)。划分方式如下：<img src="/src/v2-a4a973582683ba3f7008b69da1996c4e_720w.jpg" alt="img"></p><p>​其中<code>Base class</code>是Meta Training阶段借助的源域数据；<code>Novel class</code>是Meta Testing阶段要学习的目标域数据，其中<code>Base Class</code>和<code>Novel Class</code>没有交集。可以明显发现，这个任务涉及到<code>N-way k-shot</code>。</p><p>​在广义上来讲N代表类别数量，K代表每一类别中样本数量，但是类别数量N和每一类样本数量K分别指的是<code>Base Cass</code>还是 <code>Novel Class</code>中的Support Set还是Query Set，目前还没有看到一个明确的说明。但是由于深度学习是比较依赖数据集的，所以在<code>Novel Class</code>这个小数据中没有办法直接建模训练，因此只能借助<code>Base Class</code>来进行学习。但是两个数据集的不同，即使在<code>Base Class</code>上训练的很好，在<code>Novel Class</code>上进行微调的时候，结果也会出现偏差。所以个人认为这里的N和K是Meta Testing阶段<code>Novel Class</code>中Support Det下样本类别数量和每一类的样本量。通常为了保持Meta Training阶段和Meta Testing阶段的一致性，Meta Training阶段中<code>Base Class</code>的Support Set也会相同的设置N和K。</p><p>​在论文<a href="https://arxiv.org/abs/1903.03096v4">Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples</a> 中研究了N和K对实验的影响效果：<img src="/src/image-20220722105602314.png" alt="image-20220722105602314"></p><p>​可以发现，随着N的增加，会增加分类任务的难度，从而使预测性能下降；随着K的增加，对于度量学习来说，相当于减小了有偏，降低了期望风险。对于MAML和Reptile等优化二阶梯度问题而言，相当于降低了每个episodes的过拟合程度，都会提高预测的准确率。<br>​然而每种模型受K增加的影响情况不同，比如：Prototypical Networks和fo-Proto-MAML在低的K值上表现很好，但随着K的变大，性能但很快就饱和了，而Finetune baseline，Matching Networks和fo-MAML则以更快的速度提升其效果。当K值超过20后，Finetune baseline成了表现最优秀的。</p><p><strong>总结：当K值小时，使用元学习方法；K值增大，Finetune的结果更好</strong>。</p><p>顺便第一句，关于<code>Query Set</code>这方面貌似没有什么特殊的设置，暂且没关注到有这方面的工作。一般是根据目标数据集来设定的，基本上能包括目标域（个人认为就可以了）</p><h2 id="元学习和迁移学习的区别和联系？"><a href="#元学习和迁移学习的区别和联系？" class="headerlink" title="元学习和迁移学习的区别和联系？"></a>元学习和迁移学习的区别和联系？</h2><p>​从目标上看，元学习和迁移学习的本质都是增加学习器在多任务的范化能力，但元学习更偏重于任务和数据的双重采样，即任务和数据一样是需要采样的，具体来说对于一个10分类任务，元学习通过可能只会建立起一个5分类器，每个训练的episode都可以看成是一个子任务，而学习到的<img src="https://www.zhihu.com/equation?tex=F%5Cleft(+x+%5Cright)" alt="[公式]">可以帮助在未见过的任务里迅速建立mapping。而迁移学习更多是指从一个任务到其它任务的能力迁移，不太强调任务空间的概念。</p><hr><blockquote><p>参考：</p><ol><li><a href="https://en.wikipedia.org/wiki/Meta_learning_(computer_science)">https://en.wikipedia.org/wiki/Meta_learning_(computer_science)</a></li><li><a href="https://zhuanlan.zhihu.com/p/289043310">https://zhuanlan.zhihu.com/p/289043310</a></li><li><a href="https://zhuanlan.zhihu.com/p/108503451">https://zhuanlan.zhihu.com/p/108503451</a></li><li><a href="https://zhuanlan.zhihu.com/p/146804329">https://zhuanlan.zhihu.com/p/146804329</a></li><li><a href="https://zhuanlan.zhihu.com/p/151311431">https://zhuanlan.zhihu.com/p/151311431</a></li><li><a href="https://blog.csdn.net/bashendixie5/article/details/124277165">https://blog.csdn.net/bashendixie5/article/details/124277165</a></li><li><a href="https://easyai.tech/ai-definition/meta-learning/">https://easyai.tech/ai-definition/meta-learning/</a></li><li>Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples, ICLR 2020</li><li>Generalizing from a Few Examples: A Survey on Few-shot Learning，Computer Science 2019</li></ol></blockquote>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——异常检测小记</title>
    <link href="/2022/07/21/Essay/Anomaly%20Detection/"/>
    <url>/2022/07/21/Essay/Anomaly%20Detection/</url>
    
    <content type="html"><![CDATA[<h1 id="What-is-Anomaly-Detection"><a href="#What-is-Anomaly-Detection" class="headerlink" title="What is Anomaly Detection"></a>What is Anomaly Detection</h1><p>在数据分析中，异常检测（也称为异常值检测）是识别与大多数数据显着不同而引起怀疑的稀有项目、事件或观察结果。通常，异常项目会转化为某种问题，例如银行欺诈、结构缺陷、医疗问题或文本错误。异常也被称为异常值、新奇事物、噪音、偏差和异常。</p><p>特别是在滥用和网络入侵检测的背景下，感兴趣的对象往往不是稀有对象，而是突发的活动。这种模式不符合作为稀有对象的异常值的常见统计定义，并且许多异常值检测方法（特别是无监督方法）在此类数据上将失败，除非对其进行了适当的聚合。相反，聚类分析算法可能能够检测由这些模式形成的微聚类。</p><p>存在三大类异常检测技术。 无监督异常检测技术在假设数据集中的大多数实例是正常的情况下通过寻找似乎最不适合数据集其余部分的实例来检测未标记测试数据集中的异常。有监督的异常检测技术需要一个被标记为“正常”和“异常”的数据集，并涉及训练分类器（与许多其他统计分类问题的主要区别在于异常值检测的固有不平衡性质）。半监督异常检测技术构建一个模型，表示从给定的正常行为正常训练数据集，然后测试使用模型生成测试实例的可能性。</p><p>对于异常值的解释：样本中的正常值应该属于同一总体；而异常值有两种情况：第一种情况异常值不属于该总体，抽样抽错了，从另外一个总体抽出一个数据，其值与总体平均值相差较大；第二种情况异常值虽属于该总体，但是肯呢个是该总体固有随机变异性的极端表现，比如说超过$3\sigma$的数据，出现的概率很小但是依旧存在。那么对于异常值来说一般会出一下几种情况</p><ul><li>将本来不属于该总体的、第一种情况的异常值判断出来舍去  √</li><li>将本来属于该总体的、出现概率小的，第二种情况的异常值判断出来舍去  ×</li><li>不属于该总体但数值和该总体的平均值接近的数据被抽样抽出来，这有判断不出来是一个异常值。  ×</li></ul><h1 id="Anomaly-Detections’-Application"><a href="#Anomaly-Detections’-Application" class="headerlink" title="Anomaly Detections’ Application"></a>Anomaly Detections’ Application</h1><p>数据需求</p><ul><li>无标签或者类别极不平均</li><li>异常数据和样本大多数数据的差异性很大</li><li>异常数据在总体数据样本中所占的比例很大</li></ul><p>如果数据存在标签，而且异常数据与正常数据的比值是均衡的，且每个数据保持$i.i.d$。那么异常检测完全就没有必要，使用SVM或者随机森林就可以解决问题。</p><p>场景：</p><ol><li>金融领域：从金融数据中识别”欺诈用户“，如识别信用卡申请欺诈、信用卡盗刷、信贷欺诈等；</li><li>安全领域：判断流量数据波动以及是否受到攻击等等；</li><li>电商领域：从交易等数据中识别”恶意买家“，如羊毛党、恶意刷屏团伙；</li><li>生态灾难预警：基于天气指标数据，判断未来可能出现的极端天气；</li><li>医疗监控：从医疗设备数据，发现可能会显示疾病状况的异常数据；</li></ol><p>挑战：因为异常本身存在的未知性，异质性，特殊性及多样性等复杂情况</p><ul><li>1）最具挑战性的问题之一是难以实现高异常检测召回率。由于异常非常罕见且具有异质性，因此很难识别所有异常。</li><li>2）异常检测模型要提高精确度（precision）往往要深度结合业务特征，否则效果不佳，且容易导致对少数群体产生算法偏见。</li><li>3）<strong>未知性</strong>：异常与许多未知因素有关，例如，具有未知的突发行为、数据结构和分布的实例。它们直到真正发生时才为人所知，比如恐怖袭击、诈骗和网络入侵等应用；</li><li>4）异常类的<strong>异构性</strong>： 异常是不规则的，一类异常可能表现出与另一类异常完全不同的异常特征。例如，在视频监控中，抢劫、交通事故和盗窃等异常事件在视觉上有很大差异；</li><li>5）<strong>类别不均衡</strong>：异常通常是罕见的数据实例，而正常实例通常占数据的绝大部分。因此，收集大量标了标签的异常实例是困难的，甚至是不可能的。这导致在大多数应用程序中无法获得大规模的标记数据。</li></ul><h1 id="Anomaly-Detection-Data-Type"><a href="#Anomaly-Detection-Data-Type" class="headerlink" title="Anomaly Detection Data Type"></a>Anomaly Detection Data Type</h1><ul><li><p>统计性数据 static data （文本、网络流）</p></li><li><p>序列型数据 sequential data </p></li><li><p>空间性数据 spatial data（图像、视频）</p></li></ul><h1 id="Anomaly-Detections’-Classification"><a href="#Anomaly-Detections’-Classification" class="headerlink" title="Anomaly Detections’ Classification"></a>Anomaly Detections’ Classification</h1><p>按照训练集是否包含异常值可以划分为异常值检测（outlier detection）及新颖点检测（novelty detection），新颖点检测的代表方法如One-Class SVM。</p><p>按照异常类别的不同，异常检测可划分为：<strong>异常点检测</strong>(如异常消费用户)，<strong>上下文异常检测</strong>，又称条件异常（如时间序列异常，空间区域、图形、时序、客户段等都是上下文的范围），<strong>组异常检测</strong>（如异常团伙。相关数据实例集合对于整个数据集异常，并不仅仅是单个值。有序的例如：心电图中的节律紊乱；无序的：购买大量昂贵的物品）。</p><p>按照学习方式的不同，异常检测可划分为：</p><ul><li><p><strong>有监督异常检测</strong>（Supervised Anomaly Detection）在训练集中的正常实例和异常实例都有标签，这类方法的缺点在于数据标签难以获得或数据不均衡（正常样本数量远大于异常样本数量）。</p></li><li><p><strong>半监督异常检测</strong>（Semi-Supervised Anomaly Detection）在训练集中只有单一类别（正常实例）的实例，没有异常实例参与训练，目前很多异常检测研究都集中在半监督方法上。</p></li><li><p><strong>无监督异常检测</strong>（Unsupervised Anomaly Detection）在训练集中既有正常实例也可能存在异常实例，但假设数据的比例是正常实例远大于异常实例，模型训练过程中没有标签进行校正。</p></li><li><p><strong>弱监督异常检测</strong>（Weakly Anomaly Detection）。主要是针对异常实例不完全、粗粒度标签、部分实例标签错误等情况进行算法设计。</p><p>现实情况的异常检测问题，由于收集异常标签样本的难度大，往往是没有标签的，所以无监督异常检测应用最为广泛。</p></li></ul><p>根据数据的维度来划分：<strong>单维度数据集</strong>下，可以使用的是3-sigma原理以及他的一些变种形式。例如 Grubbs 算法，T Test 等等。在高维度领域，通常来说有两种处理方式。一种是降低到单维度，然后使用 3-sigma 之类的算法判断新的序列是否异常，从而推断出原始序列是否异常。另外一种方法是在高维度上直接建立模型，然后判断。在<strong>多维数据检测</strong>的话，方法就比较多可以参考其他部分。</p><p><strong>3-sigma原理</strong>：前提：数据只含有随机误差。当质量特性呈现正态分布的时候（实际上，当样本足够大的时候，泊松分布、二项分布等均趋近于正态分布的）。3-sigma水平代表了99.73%的合格率。</p><p><img src="/src/7af40ad162d9f2d35daa1e0aa4ec8a136227cc9f-16569395567949.png" alt="img"></p><p><strong>Grubbs原理</strong>：计算可疑值的G值($G_i&#x3D;\frac{x_i-x^-}{s}$)，将其与格拉布斯表匹配，判断临界值的大小。若大于，则是异常值舍去。若小于则不是异常值应该保留。</p><p><img src="/src/image-20211222150026654-165693956096012.png" alt="image-20211222150026654"></p><h2 id="静态规则方法（最基础）"><a href="#静态规则方法（最基础）" class="headerlink" title="静态规则方法（最基础）"></a>静态规则方法（最基础）</h2><p>想法是识别已知异常的列表，然后编写规则来检测这些异常。规则识别是由领域专家通过模式挖掘技术或者通过两者的组合来完成。</p><p>静态规则用于假设异常遵循80&#x2F;20规则，其中大多数的异常事件属于少数异常类型。如果假设为真，那么我们可以通过找到描述这些异常的少数规则来检测大多数异常。</p><p>可以使用一下三种方法之一来实现这些规则：</p><ul><li>如果他们很简单并且不需要推理，可以使用任意编程语言对其编码</li><li>如果决策需要推理，那么可以使用基于规则或专家系统</li><li>如果决策有时间约束，可以使用复杂事件处理系统</li></ul><p>虽然简单，但是基于静态规则的系统往往脆弱而复杂。此外，识别这些规则通常是一项复杂且主观的任务。<em><strong>因此，自动学习一般规则的基于统计或机器学习的方法要优于静态规则。</strong></em></p><h2 id="有训练数据（有监督）"><a href="#有训练数据（有监督）" class="headerlink" title="有训练数据（有监督）"></a>有训练数据（有监督）</h2><p>大多数情况下，异常检测是很难获取到训练数据的。对于一个数据集中，你没有办法保证所有的数据都是正常的，哪怕他已经被打上了一个“正常”的标签。所以对于这些可用的训练数据集，在几百万的常规数据中仍是会存在几个或者几十个异常也是个正常现象。即使如此，利用SVM或者随机森林这些分类算法的准确路仍然是可以到达99.8以上。</p><p>通常我们的想法可使利用多次重采样数据构建的集成方式来解决不平衡问题。这个想法首先通过获取所有异常的数据点并添加正常数据点的子集（例如作为异常数据点的4倍）来创建一个新的数据集。然后利用SVM等算法为每一个数据集构建一个分类器，最后使用集成学习组合这些分类器。这种方法运作良好并产生了非常好的效果</p><p>但是，如果数据点彼此关联，那么多次重采样就会出现问题，丢失了数据之间的联系。简单的分类器就没有办法正常的工作。这时候可以使用时间序列技术或者循环神经网络来处理</p><h2 id="无训练数据（点异常-无监督或半监督）"><a href="#无训练数据（点异常-无监督或半监督）" class="headerlink" title="无训练数据（点异常  无监督或半监督）"></a>无训练数据（点异常  无监督或半监督）</h2><p>点异常在数据集中往往就是一个记录中的某个字段。所以我们通常可以使用3-sigma原则这样的百分位数方式来检测数据异常。用直方图来检测分类数据中的检测点异常。无论那种情况，我们都可以从数据中获得罕见的数据范围或字段值，并在再次发生时将其预测为异常。在构建模型时，我们通常会尽可能使用移动平均值而不是点值，因为他们对噪声更稳定。</p><h2 id="无训练数据（单变量集体异常）"><a href="#无训练数据（单变量集体异常）" class="headerlink" title="无训练数据（单变量集体异常）"></a>无训练数据（单变量集体异常）</h2><p>例如时间序列数据是单变量数据集中集体异常值的最佳案例。在这种情况下，异常值的发生是因为值以意外的顺序出现。例如如下图：</p><p><img src="/src/heart-165693956441115.jpg" alt="心"></p><p>第三次心跳可能是异常的，不是因为值超出范围，而是因为他们以错误的顺序发生。</p><p>一般处理这种情况的方法：</p><ol><li><p>构建一个预测器并使用残差寻找异常值：这是基于启发式的，即模型未解释的值是异常的。因此我们可以建立一个模型来预测下一个值，然后如前所述对误差（预测值-实际值）应用百分位数。该模型可以使用回归、时间序列模型或者循环神经网络构建</p></li><li><p>Markov Chain或者Hidden Markov Chain可以衡量一系列事件发生的概率。当一系列事件发生的时候，我们可以使用Markov Chain来衡量该序列发生的概率，并用他来检测任何稀有序列。<br><img src="/src/fraud-165693956683718.png" alt="Fraud"></p><p>例如，让我们考虑信用卡交易。未来使用MarkovChain对整个事件进行建模，让我们使用两个值来表示每个交易：交易价值$(L,H)$和自上次交易以来的时间$(L,H)$。由于MarkovChain的状态必须是有限的，我们将选择两个值$Low(L),High(H)$来表示变量值。然后MarkovChain将用状态LL、LH、HL、HH表示。每笔交易都是从一个状态到另一个状态的转换。我们可以使用历史数据构建MarkovChain，并用该链来计算序列概率。然后，我们可以找到任何新序列发生的概率，然后将稀有序列标记为异常。</p></li></ol><h2 id="无训练数据（多元集体异常值-无序）"><a href="#无训练数据（多元集体异常值-无序）" class="headerlink" title="无训练数据（多元集体异常值 无序）"></a>无训练数据（多元集体异常值 无序）</h2><p>这里的数据一般都是多次读写，数据本身没有顺序。例如，从许多人哪里收集的生命体征就是一个多变量但是没有排序的数据集。例如，即使温度和心跳本身都在正常范围内，但较高的温度和缓慢的心跳都是异常的情况。</p><p>一般的处理方法有：</p><ol><li>聚类</li><li>最紧邻技术</li></ol><h2 id="无训练数据（多元集体异常值-有序）"><a href="#无训练数据（多元集体异常值-有序）" class="headerlink" title="无训练数据（多元集体异常值 有序）"></a>无训练数据（多元集体异常值 有序）</h2><p>此类情况应当是最常见的，需要考虑排序和值的组合。例如，考虑取自同一患者的一系列重要读书。某些读数可能在组合中是正常的，但是由于组合以错误的顺序发生而异常。例如，给一个包含血压、体温和心跳频率的读书，每个读数本身可能都是正常的，但是如果在短时间震荡太快，则不正常了。</p><p>处理这类的方法可以使用MarkovChain和聚类结合。首先对数据进行聚类没然后使用簇作为MarkovChain中的状态并构建MarkovChain。聚类将捕获常见的值组合，而MarkovChain将捕获他的顺序。当然也存在深层网络的方法。</p><h1 id="Anomaly-Detections’-Method"><a href="#Anomaly-Detections’-Method" class="headerlink" title="Anomaly Detections’ Method"></a>Anomaly Detections’ Method</h1><p>对于任意现实任务来说，异常检测都是至关重要的一部分。它可以保障模型运行的正常，模型处理更高效。异常检测的广泛应用，印证了其多样的方法设计。监督方法到无监督方法，静态方法到动态方法，非深度方法到深度方法。由于异常自身的特性，极其稀缺，且无法准确分类。所以异常检测方法大多以无监督的方法为主。</p><h2 id="Supervised-learning-method"><a href="#Supervised-learning-method" class="headerlink" title="Supervised learning method"></a>Supervised learning method</h2><p>EGADS：使用 AdaBoost 选择异常，其架构主要包括三个组件：预测、异常检测和警报。 </p><p>Opprentice ：集成了 14 种广泛使用的统计算法来提取数据点的特征，然后使用随机森林训练分类器。</p><p>这种方法需要一个完全标签的数据集。但是，对所有数据进行手动标记是不可行的</p><h2 id="Unsupervised-learning-method"><a href="#Unsupervised-learning-method" class="headerlink" title="Unsupervised learning method"></a>Unsupervised learning method</h2><ul><li>非深度学习<ol><li>矩阵分解（matrix factorization, MF）方法：Radar、ANOMALOUS、ITAD(对分解张量进行聚类)、.</li><li>基于密度的聚类方法：<a href="/Algorithms/LocalOutlierFactor.md">Local Outlier Factor</a>、COF(连通性离群值因子)、DAGMM、MPPCACD(整合高斯混合模型以估计表示的密度)、<a href="/Algorithms/DBSCAN.md">DBSCAN</a>、<a href="/Algorithms/SVDD.md">SVDD</a>、Deep SVDD(整合高斯混合模型以估计表示的密度)、THOC(通过层次聚类机制融合中间层的多尺度时间特征，并通过多层距离检测异常)、SCAN、Fraudar、.</li><li>关系学习方法：SPEAGLE、.</li><li>其他：OC-SVM、<a href="/Algorithms/IsoForest.md">Isolation Forest(孤立森林)</a>、<a href="/Algorithms/GrubbsTest.md">统计假设检验(Grubbs Test)</a>、[Numeric Oulier（IQR)](&#x2F;Algorithms&#x2F;Numeric Oulier–IQR.md)、[Winsorization Method](&#x2F;Algorithms&#x2F;Winsorization Method.md)、.</li></ol></li><li>深度学习<ol><li>自编码结构方法：InterFusion、LSTM-VAE、OmniAbnormaly、Donut、Buzz 、Bagel、Dominant、Anomalydae、GUIDE、.</li><li>图神经网络方法：OCGNN、PAMFUL、.</li><li>生成对抗网络方法：BeatGAN、GAAN、.</li></ol></li></ul><p><a href="/Algorithms/IsoForest.md">Isolation Forest(孤立森林)</a>：是一种用于异常值检测的经典树集成方法。它构建一个基树集合来隔离数据点，并将决策边界定义为单个实例与树的根节点的接近程度。它只使用数据的节点属性。模型假设异常很少且不同，对于点，越接近树的根，越异常。</p><p>Donut、Buzz  和 Bagel 是基于 VAE 的模型，它们专注于学习正常数据的模式而不是异常并计算重建概率以进行检测。 虽然这些算法不需要数据标签，但它们并不能达到令人满意的效果。</p><h2 id="Semi-supervised-learning-method"><a href="#Semi-supervised-learning-method" class="headerlink" title="Semi-supervised learning method"></a>Semi-supervised learning method</h2><p>ADS 使用聚类和 CPLE（对比悲观似然估计）来检测时间序列中的异常。</p><p>半监督学习方法使用未标记的数据来修改仅从标记数据中获得的参数或模型，以最大化学习性能。它们确实降低了手动标记的成本，但是，操作员仍然难以标记和检查那些指定时间序列段中的所有异常。</p><p>对大多数实际场景下，我们面临的既不是完全无样本标签的数据，也不是完备样本标签的数据。大多数公司一般都有一些人工标记的样本，剩下一堆无标签样本，因此半监督学习方法可能更贴近实际应用的场景。</p><p>蚂蚁金服在WWW大会投中过类似的paper</p><p>这个算法ADOA适用的场景是：仅有部分样本有标签，剩下大部分样本都无标签；异常样本的类型不止一种，往往是多种异常情形同时存在；有标签的异常样本可能并没有区分出不同的异常类型；</p><p>这个算法的主要流程分成两个阶段。在第一阶段，对已知异常样本进行聚类，并从未标记样本中挖掘潜在异常样本以及可靠正常样本；第二阶段，基于以上的样本，构建带权重的多分类模型。</p><p><img src="/src/v2-62be70e77861d1b33dcbb83e002d5b26_720w-1640143042828-165693957305421.jpg" alt="img"></p><p><strong>问题</strong>:这个算法的假设情形很适合国内的风控业务</p><h1 id="Anomaly-Detection-SOTA-Model："><a href="#Anomaly-Detection-SOTA-Model：" class="headerlink" title="Anomaly Detection SOTA Model："></a>Anomaly Detection SOTA Model：</h1><ol><li>LSTM-NDT <a href="https://github.com/khundman/telemanom">https://github.com/khundman/telemanom</a></li><li>openGauss <a href="https://gitee.com/opengauss/openGauss-AI">https://gitee.com/opengauss/openGauss-AI</a></li><li>DAGMM <a href="https://github.com/tnakae/DAGMM">https://github.com/tnakae/DAGMM</a></li><li>OmniAnomaly <a href="https://github.com/NetManAIOps/OmniAnomaly">https://github.com/NetManAIOps/OmniAnomaly</a></li><li>MSCRED <a href="https://github.com/7fantasysz/MSCRED">https://github.com/7fantasysz/MSCRED</a></li><li>MAD-GAN <a href="https://github.com/LiDan456/MAD-GANs">https://github.com/LiDan456/MAD-GANs</a></li></ol><h1 id="Anomaly-Detection-Datasets："><a href="#Anomaly-Detection-Datasets：" class="headerlink" title="Anomaly Detection Datasets："></a>Anomaly Detection Datasets：</h1><ol><li>Numenta Aboration  Benchmark（<strong>NAB</strong>）：是一个包含多个真实世界数据跟踪的数据集，包括来自温度传感器的读数、云机器的CPU利用率、服务请求延迟和纽约市的出租车需求。然而，已知该数据集的序列带有不正确的异常标签，例如我们在实验中排除的纽约出租车追踪。 </li><li>HexagonML（<strong>UCR</strong>）数据集：是KDD 2021  cup中使用的多个单变量时间序列的数据集（仅为完整性而包括）。我们只包括从自然来源获得的数据集（内出血和ECG数据集），而忽略了合成序列。 </li><li>MIT-BIH室上性心律失常数据库（MBA）：是四名患者的心电图记录的集合，包含两种不同类型异常的多个实例（室上收缩或早搏）。这是数据管理社区中流行的大型数据集。 </li><li>土壤水分主动-被动（<strong>SMAP</strong>）数据集：是美国宇航局（NASA）使用火星探测器收集土壤样本和遥测信息的数据集。 </li><li>火星科学实验室（MSL）数据集：是一个类似于SMAP的数据集，但与火星探测车本身的传感器和执行器数据相对应。然而，已知该数据集有许多琐碎的序列；因此，我们只考虑指出的三个非平凡的因素（A4、C2和T1）。 </li><li>安全水处理（<strong>SWaT</strong>）数据集：该数据集收集自现实世界中正常运行7天、异常运行4天的水处理厂。该数据集包括传感器值（水位、流速等）和执行机构操作（阀门和泵）。 </li><li>水分配（<strong>WADI</strong>）数据集：这是SWaT系统的扩展，但传感器和执行器的数量是SWaT模型的两倍多。在正常和攻击情况下，数据集的收集持续时间也更长，分别为14天和2天。 </li><li>服务器机器数据集（<strong>SMD</strong>）：这是一个为期五周的数据集，包含计算集群中28台机器的资源利用率的叠加跟踪。与MSL类似，我们在该数据集中使用了非平凡序列，特别是名为machine-1-1、2-1、3-2和3-7的跟踪</li><li>多源分布式系统（<strong>MSDS</strong>）数据集：这是一个最近的高质量多源数据集，由分布式跟踪、应用程序日志和来自复杂分布式系统的度量组成。该数据集专门为人工智能操作构建，包括自动异常检测、根本原因分析和补救。 </li><li>无监督节点异常检测基准(<strong>UNOD</strong>): 图离群点检测  Benchmarking Node Outlier Detection on Graphs, NeuraIPS2022</li></ol><h1 id="Deal-With-Anomaly"><a href="#Deal-With-Anomaly" class="headerlink" title="Deal With Anomaly"></a>Deal With Anomaly</h1><p><strong>异常值的危害：</strong></p><ul><li>异常值严重影响数据集的均值和标准差。这些可能在统计学上给出错误的结果。</li><li>它增加了误差方差并降低了统计检验的功效。</li><li>如果异常值是非随机分布的，则它们会降低正态性。</li><li>大多数机器学习算法在存在异常值的情况下不能很好地工作。因此，最好检测并删除异常值。</li><li>它们还会影响回归、方差分析和其他统计模型假设的基本假设。</li></ul><p>由于所有这些原因，我们必须小心异常值，并在构建统计&#x2F;机器学习模型之前对其进行处理。有一些技术用于处理异常值。</p><ol><li><p>删除观测值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">drop_outliers</span>(<span class="hljs-params">df, field_name</span>):<br>    iqr = <span class="hljs-number">1.5</span> * (np.percentile(df[field_name], <span class="hljs-number">75</span>) - np.percentile(df[field_name], <span class="hljs-number">25</span>))<br>    df.drop(df[df[field_name] &gt; (iqr + np.percentile(df[field_name], <span class="hljs-number">75</span>))].index, inplace=<span class="hljs-literal">True</span>)<br>    df.drop(df[df[field_name] &lt; (np.percentile(df[field_name], <span class="hljs-number">25</span>) - iqr)].index, inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></li><li><p>转变变量。</p><ol><li>Scalling</li><li>Log transformation</li><li>Cube Root Normalization</li><li>Box-Cox transformation</li></ol><p>这些技术将数据集中的值转换为较小的值。如果数据具有许多极值或偏斜，则此方法有助于使数据正常。但这些技术并不总是能给你最好的结果。这些方法不会丢失数据。 在所有这些方法中，Box-Cox变换给出了最好的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#Scalling 归一化</span><br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing<br>scaler = preprocessing.StandardScaler()<br>train[<span class="hljs-string">&#x27;Cost of Living Index&#x27;</span>] = scaler.fit_transform(train[<span class="hljs-string">&#x27;Cost of Living Index&#x27;</span>].values.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><br><span class="hljs-comment">#Log transformation</span><br>train[<span class="hljs-string">&#x27;Cost of Living Index&#x27;</span>] = np.log(train[<span class="hljs-string">&#x27;Cost of Living Index&#x27;</span>])<br><br><span class="hljs-comment">#Cube Root Normalization</span><br>train[<span class="hljs-string">&#x27;Age&#x27;</span>] = (train[<span class="hljs-string">&#x27;Age&#x27;</span>]**(<span class="hljs-number">1</span>/<span class="hljs-number">3</span>))<br><br><span class="hljs-comment">#Box-Cox transformation</span><br><span class="hljs-keyword">import</span> scipy<br>train[<span class="hljs-string">&#x27;Rent Index&#x27;</span>],fitted_lambda= scipy.stats.boxcox(train[<span class="hljs-string">&#x27;Rent Index&#x27;</span>] ,lmbda=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure></li><li><p>归因。<br>与缺失值的插补一样，我们也可以插补异常值。我们可以在此方法中使用均值、中位数、零值。由于我们插补，因此不会丢失数据。此处的中位数是合适的，因为它不受异常值的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">q1 = train[<span class="hljs-string">&#x27;Age&#x27;</span>].quantile(<span class="hljs-number">0.25</span>)<br>q3 = train[<span class="hljs-string">&#x27;Age&#x27;</span>].quantile(<span class="hljs-number">0.75</span>)<br>iqr = q3-q1<br>Lower_tail = q1 - <span class="hljs-number">1.5</span> * iqr<br>Upper_tail = q3 + <span class="hljs-number">1.5</span> * iqr<br>m = np.mean(train[<span class="hljs-string">&#x27;Age&#x27;</span>])<br><span class="hljs-comment">#med = np.median(train[&#x27;Age&#x27;])</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> train[<span class="hljs-string">&#x27;Age&#x27;</span>]:<br>    <span class="hljs-keyword">if</span> i &gt; Upper_tail <span class="hljs-keyword">or</span> i &lt; Lower_tail:<br>            train[<span class="hljs-string">&#x27;Age&#x27;</span>] = train[<span class="hljs-string">&#x27;Age&#x27;</span>].replace(i, m)<br>            <span class="hljs-comment">#train[&#x27;Age&#x27;] = train[&#x27;Age&#x27;].replace(i, med)</span><br>            <span class="hljs-comment">#train[&#x27;Age&#x27;] = train[&#x27;Age&#x27;].replace(i, 0)</span><br></code></pre></td></tr></table></figure></li><li><p>单独处理.<br>如果异常值数量很大，数据集很小，我们应该在统计模型中分别处理它们。其中一种方法是将两个组视为两个不同的组，并为两个组构建单独的模型，然后合并输出。但是，当数据集很大时，这种技术很繁琐。</p></li></ol><h1 id="Lessons"><a href="#Lessons" class="headerlink" title="Lessons"></a>Lessons</h1><p>1.Perfect Density Models cannot Guarantee Anomly Detection</p><p>​    lesson: </p><ul><li><p>The repersentational space matter</p></li><li><p>We want to apply density estimation in a meaningful space</p><ul><li>This is not the input image&#x2F;pixal space</li><li>We do not want to compute $PR[x \in V_x] &#x3D; \int_{x \in V}P(x)dx$</li></ul></li><li><p>We want to learn a good latent space Z such that images of similar contents(same objects, same label, etc) are close together</p></li><li><p>Then apply density estimation in that space</p></li></ul><p>2.可以用画图的方式，显性的表示出异常点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Box_plots</span>(<span class="hljs-params">df</span>):<br>    plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))<br>    plt.title(<span class="hljs-string">&quot;Box Plot&quot;</span>)<br>    sns.boxplot(df)<br>    plt.show()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">hist_plots</span>(<span class="hljs-params">df</span>):<br>    plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))<br>    plt.hist(df)<br>    plt.title(<span class="hljs-string">&quot;Histogram Plot&quot;</span>)<br>    plt.show()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">scatter_plots</span>(<span class="hljs-params">df1,df2</span>):<br>    fig, ax = plt.subplots(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">4</span>))<br>    ax.scatter(df1,df2)<br>    ax.set_xlabel(<span class="hljs-string">&#x27;Age&#x27;</span>)<br>    ax.set_ylabel(<span class="hljs-string">&#x27;Fare&#x27;</span>)<br>    plt.title(<span class="hljs-string">&quot;Scatter Plot&quot;</span>)<br>    plt.show()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dist_plots</span>(<span class="hljs-params">df</span>):<br>    plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))<br>    sns.distplot(df)<br>    plt.title(<span class="hljs-string">&quot;Distribution plot&quot;</span>)<br>    sns.despine()<br>    plt.show()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">qq_plots</span>(<span class="hljs-params">df</span>):<br>    plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))<br>    qqplot(df,line=<span class="hljs-string">&#x27;s&#x27;</span>)<br>    plt.title(<span class="hljs-string">&quot;Normal QQPlot&quot;</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure><ol start="3"><li></li></ol><ul><li>中位数是数据具有异常值或偏斜时中心趋势的最佳度量。</li><li>Winsorization 方法或百分位封顶是比其他方法更好的异常值检测技术。</li><li>中位数插补完全消除异常值。</li></ul><blockquote><p>参考：</p><ol><li><a href="https://github.com/XIANG64Young/anomaly-detection-resources#2-coursesseminarsvideos">https://github.com/XIANG64Young/anomaly-detection-resources#2-coursesseminarsvideos</a></li></ol></blockquote><blockquote><p>论文参考：</p><ol><li>Radar：Residual analysis for anomaly detection in attributed networks.  IJCAI,  2017. </li><li>ANOMALOUS： A joint modeling approach for anomaly detection on attributed networks.  IJCAI, 2018.</li><li>LOF: identifying density-based local outliers.  SIGMOD, 2000</li><li>DAGMM:  Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection. ICLR, 2018</li><li>DBSCAN: A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. KDD 1996</li><li>SVDD: Support Vector Data Description. Machine Learning, 2004</li><li>Deep SVDD: Deep Support Vector Data Description. ICML 2018</li><li>THOC: Timeseries Anomaly Detection using Temporal Hierarchical One-Class Network. NeurIPS 2020</li><li>SCAN:  Scan: a structural clustering algorithm for networks. KDD, 2007</li><li>Fraudar：Bounding graph fraud in the face of camouflage. KDD, 2016.</li><li>SPEAGLE: Collective Opinion Spam Detection: Bridging Review Networks and Metadata. KDD, 2015</li><li>OC-SVM: Support Vector Method for Novelty Detection.  NIPS , 1999</li><li>InterFusion: Multivariate Time Series Anomaly Detection and InterPretation using Hierarchical Inter-Metirc and Temporal Embedding. KDD, 2021</li><li>LSTM-VAE: Improved Variational Autoencoders for Text Modeling using Dilated Convolutions. Arxiv, 2017</li><li>OmniAbnormaly: Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network. KDD, 2019</li><li>Donut: Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications. WWW, 2018</li><li>Buzz: Unsupervised Anomaly Detection for Intricate KPIs via Adversarial Training of VAE. INFOCOM, 2019</li><li>Bagel: Robust and Unsupervised KPI Anomaly Detection Based on Conditional Variational Autoencoder. IPCCC 2018</li><li>Dominant:　Deep Anomaly Detection on Attributed Networks. SDM, 2019</li><li>Anomalydae: Dual autoencoder for anomaly detection on attributed networks. ICASSP, 2000</li><li>GUIDE: Higher-order Structure Based Anomaly Detection on Attributed Networks. BIG DATA, 2021</li><li>OCGNN: One-class graph neural networks for anomaly detection in attributed networks. Neural computing and applications, 2021</li><li>PAMFUL: A Synergistic Approach for Graph Anomaly Detection With Pattern Mining and Feature Learning.IEEE Transactions on Neural Networks and Learning Systems, 2021</li><li>BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time Series. IJCAI, 2019</li><li>GAAN: Generative Adversarial Attributed Network Anomaly Detection. CIKM, 2020 </li><li>ADOA: Anomaly Detection with Partially Observed Anomlies. WWW, 2018</li><li>UNOD： Benchmarking Node Outlier Detection on Graphs, NeurIPS, 2022 </li><li>Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series, Proc. VLDB Endow. ,2020<br>Time Series, Proc. VLDB Endow. ,2020</li></ol></blockquote>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python Note</title>
    <link href="/2022/07/21/Python/PythonNote/"/>
    <url>/2022/07/21/Python/PythonNote/</url>
    
    <content type="html"><![CDATA[<h3 id="Map与lambda"><a href="#Map与lambda" class="headerlink" title="Map与lambda"></a>Map与lambda</h3><p>Python 通过许多内置功能支持函数式编程。最有用的map()功能之一是函数——尤其是与lambda函数结合使用时。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>] <br>y = <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x : x + <span class="hljs-number">1</span>, x)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(y)<br>[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><p>在上面的示例中，<code>map()</code>将一个简单的 <code>lambda</code> 函数应用于<code>x</code>. 它返回一个映射对象，该对象可以转换为一些可迭代对象，例如列表或元组。</p><h3 id="If-else"><a href="#If-else" class="headerlink" title="If-else"></a>If-else</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">age = <span class="hljs-number">18</span><br>valid = <span class="hljs-string">&quot;You&#x27;re an adult&quot;</span><br>invalid = <span class="hljs-string">&quot;You&#x27;re NOT an adult&quot;</span><br><span class="hljs-built_in">print</span>(valid) <span class="hljs-keyword">if</span> age &gt;= <span class="hljs-number">18</span> <span class="hljs-keyword">else</span> <span class="hljs-built_in">print</span>(invalid)<br></code></pre></td></tr></table></figure><h3 id="根据现有列表创建新列表"><a href="#根据现有列表创建新列表" class="headerlink" title="根据现有列表创建新列表"></a><strong>根据现有列表创建新列表</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">words = [<span class="hljs-string">&#x27;united states&#x27;</span>, <span class="hljs-string">&#x27;brazil&#x27;</span>, <span class="hljs-string">&#x27;united kingdom&#x27;</span>]<br><br>capitalized = [word.title() <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words]<br><span class="hljs-meta">&gt;&gt;&gt; </span>capitalized<br>[<span class="hljs-string">&#x27;United States&#x27;</span>, <span class="hljs-string">&#x27;Brazil&#x27;</span>, <span class="hljs-string">&#x27;United Kingdom&#x27;</span>]<br></code></pre></td></tr></table></figure><p>Grammar：<code>[expression for item in list]</code><br>上面的代码确实看起来更好！但是要记住，我们应该保持代码对用户友好，因此不推荐在一行代码中编写很长的列表推导式。</p><h3 id="字典推导"><a href="#字典推导" class="headerlink" title="字典推导"></a><strong>字典推导</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">dict_numbers = &#123;x:x*x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>) &#125;<br><span class="hljs-meta">&gt;&gt;&gt; </span>dict_numbers<br>&#123;<span class="hljs-number">1</span>: <span class="hljs-number">1</span>, <span class="hljs-number">2</span>: <span class="hljs-number">4</span>, <span class="hljs-number">3</span>: <span class="hljs-number">9</span>, <span class="hljs-number">4</span>: <span class="hljs-number">16</span>, <span class="hljs-number">5</span>:<span class="hljs-number">25</span>&#125;<br></code></pre></td></tr></table></figure><p>Grammar：<code>&#123;key: value for key, value in iterable&#125;</code></p><h3 id="合并词典"><a href="#合并词典" class="headerlink" title="合并词典"></a><strong>合并词典</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">dict_1 = &#123;<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">2</span>&#125;<br>dict_2 = &#123;<span class="hljs-string">&#x27;c&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;d&#x27;</span>: <span class="hljs-number">4</span>&#125;<br>merged_dict = &#123;**dict_1, **dict_2&#125;<br><span class="hljs-meta">&gt;&gt;&gt; </span>merged_dict<br>&#123;<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;c&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;d&#x27;</span>: <span class="hljs-number">4</span>&#125;<br></code></pre></td></tr></table></figure><p>在我们将 <code>**</code> 运算符应用于字典后，两者都将扩展其内容并合并以创建一个新字典。</p><h3 id="从列表中过滤值"><a href="#从列表中过滤值" class="headerlink" title="从列表中过滤值"></a><strong>从列表中过滤值</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">my_list = [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x%<span class="hljs-number">2</span> == <span class="hljs-number">0</span>, my_list ))<br>[<span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">14</span>]<br></code></pre></td></tr></table></figure><p>Grammar：<code>filter(function, iterable)</code></p><h3 id="按键、按值排序字典"><a href="#按键、按值排序字典" class="headerlink" title="按键、按值排序字典"></a><strong>按键、按值排序字典</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">product_prices = &#123;<span class="hljs-string">&#x27;Z&#x27;</span>: <span class="hljs-number">9.99</span>, <span class="hljs-string">&#x27;Y&#x27;</span>: <span class="hljs-number">9.99</span>, <span class="hljs-string">&#x27;X&#x27;</span>: <span class="hljs-number">9.99</span>&#125;<br>population = &#123;<span class="hljs-string">&#x27;USA&#x27;</span>:<span class="hljs-number">329.5</span>, <span class="hljs-string">&#x27;Brazil&#x27;</span>: <span class="hljs-number">212.6</span>, <span class="hljs-string">&#x27;UK&#x27;</span>: <span class="hljs-number">67.2</span>&#125;<br><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">sorted</span>(population.items(), key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>])  <span class="hljs-comment">#by value to list</span><br>[(<span class="hljs-string">&#x27;UK&#x27;</span>, <span class="hljs-number">67.2</span>), (<span class="hljs-string">&#x27;Brazil&#x27;</span>, <span class="hljs-number">212.6</span>), (<span class="hljs-string">&#x27;USA&#x27;</span>, <span class="hljs-number">329.5</span>)]<br>population = &#123;<span class="hljs-string">&#x27;USA&#x27;</span>:<span class="hljs-number">329.5</span>, <span class="hljs-string">&#x27;Brazil&#x27;</span>: <span class="hljs-number">212.6</span>, <span class="hljs-string">&#x27;UK&#x27;</span>: <span class="hljs-number">67.2</span>&#125;<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>&#123;k:v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(population.items(), key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>])&#125; <span class="hljs-comment">#by value to dict</span><br>&#123;<span class="hljs-string">&#x27;UK&#x27;</span>: <span class="hljs-number">67.2</span>, <span class="hljs-string">&#x27;Brazil&#x27;</span>: <span class="hljs-number">212.6</span>, <span class="hljs-string">&#x27;USA&#x27;</span>: <span class="hljs-number">329.5</span>&#125;<br><br>&gt;&gt;&#123;key:product_prices[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(product_prices.keys())&#125; <span class="hljs-comment"># by key</span><br>&#123;<span class="hljs-string">&#x27;X&#x27;</span>: <span class="hljs-number">9.99</span>, <span class="hljs-string">&#x27;Y&#x27;</span>: <span class="hljs-number">9.99</span>, <span class="hljs-string">&#x27;Z&#x27;</span>: <span class="hljs-number">9.99</span>&#125;<br></code></pre></td></tr></table></figure><p>Grammar：<code>sorted(iterable, key=None, reverse=False)</code></p><h3 id="密码（输入信息）不可见"><a href="#密码（输入信息）不可见" class="headerlink" title="密码（输入信息）不可见"></a><strong>密码（输入信息）不可见</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> getpass<br>pwd=getpass.getpass(<span class="hljs-string">&quot;请输入密码:&quot;</span>)<br><span class="hljs-built_in">print</span>(pwd)<br></code></pre></td></tr></table></figure><h3 id="进度条"><a href="#进度条" class="headerlink" title="进度条"></a><strong>进度条</strong></h3><p>或者利用print的 flush &#x3D; True 参数实现更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    sys.stdout.write(<span class="hljs-string">&#x27;#&#x27;</span>)<br>    sys.stdout.flush()<br>    time.sleep(<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure><h3 id="三元运算"><a href="#三元运算" class="headerlink" title="三元运算"></a><strong>三元运算</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">d=a <span class="hljs-keyword">if</span> a&gt;b <span class="hljs-keyword">else</span> c<br></code></pre></td></tr></table></figure><h3 id="解码与编码"><a href="#解码与编码" class="headerlink" title="解码与编码"></a><strong>解码与编码</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">msg=<span class="hljs-string">&quot;我爱北京天安门&quot;</span><br><br><span class="hljs-built_in">print</span>(msg)<br><span class="hljs-built_in">print</span>(msg.encode(encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)) <span class="hljs-comment"># str转bytes,编码</span><br><span class="hljs-built_in">print</span>(msg.encode(encoding=<span class="hljs-string">&quot;utf-8&quot;</span>).decode(encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)) <span class="hljs-comment"># bytes转str,解码</span><br></code></pre></td></tr></table></figure><p>映射函数 map(function, data)用作变换</p><p>过滤函数 filter(function, data)用作数据筛选</p><p>生成器 yield 相当于print的变种 带有 yield 的函数在 Python 中被称之为 generator（生成器）</p><h3 id="正则化匹配"><a href="#正则化匹配" class="headerlink" title="正则化匹配"></a><strong>正则化匹配</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br>email = <span class="hljs-string">&quot;john@google.com elise@python.com&quot;</span><br>pattern = <span class="hljs-string">&quot;(\w+)@\w+.com&quot;</span><br>ans = re.findall(pattern, email)<br><span class="hljs-built_in">print</span>(ans)<br></code></pre></td></tr></table></figure><h3 id="可以完美搭配yield-写一些比较好玩的东西"><a href="#可以完美搭配yield-写一些比较好玩的东西" class="headerlink" title="可以完美搭配yield  写一些比较好玩的东西"></a><strong>可以完美搭配yield  写一些比较好玩的东西</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">n = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>())<br><span class="hljs-built_in">print</span>(*generate(n),sep=<span class="hljs-string">&#x27; &#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="压缩和解压字符"><a href="#压缩和解压字符" class="headerlink" title="压缩和解压字符"></a><strong>压缩和解压字符</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> zlib<br><br>t = zlib.compress(<span class="hljs-string">b&#x27;hello world!hello world!hello world!hello world!&#x27;</span>)<br><span class="hljs-built_in">print</span>(t)<br><span class="hljs-built_in">print</span>(zlib.decompress(t))<br></code></pre></td></tr></table></figure><h3 id="创建并读写文件"><a href="#创建并读写文件" class="headerlink" title="创建并读写文件"></a><strong>创建并读写文件</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br>os.makedirs(os.path.join(<span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span>), exist_ok=<span class="hljs-literal">True</span>)<br>data_file = os.path.join(<span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-string">&#x27;house_tiny.csv&#x27;</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(data_file, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(<span class="hljs-string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="hljs-comment"># 列名</span><br>    f.write(<span class="hljs-string">&#x27;NA,Pave,127500\n&#x27;</span>)  <span class="hljs-comment"># 每行表示一个数据样本</span><br>    f.write(<span class="hljs-string">&#x27;2,NA,106000\n&#x27;</span>)<br>    f.write(<span class="hljs-string">&#x27;4,NA,178100\n&#x27;</span>)<br>    f.write(<span class="hljs-string">&#x27;NA,NA,140000\n&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a><strong>Notes</strong></h3><p>打印list中的元素可以直接 print(*list)调用 指针 使数据变成一个参数集(C的解释)</p><p>python的解释是 $*$是将所以参数以<strong>元组(tuple)<strong>的形式导入  $</strong>$将参数以字典的形式导入<br>一般$*$是引入非关键词参数，$</strong>$是关键词参数</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——DBSCAN</title>
    <link href="/2022/07/01/Algorithms/DBSCAN/"/>
    <url>/2022/07/01/Algorithms/DBSCAN/</url>
    
    <content type="html"><![CDATA[<p>Density-Based Spatial Clustering of Applications with Noise</p><p>该技术基于<a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/DBSCAN">DBSCAN聚类方法</a>，DBSCAN是一维或多维特征空间中的非参数，基于密度的离群值检测方法。</p><p>在DBSCAN聚类技术中，所有数据点都被定义为核心点（Core Points）、边界点（Border Points）或噪声点（Noise Points）。它将数据集划分为高密度区域的子组，并将高密度区域聚类标识为异常值。此处，聚类 -1 表示聚类包含异常值，其余聚类没有异常值。此 approch 类似于 K 均值聚类。DBSCAN 需要两个参数。DBSCAN 为多变量异常值检测提供了最佳结果。</p><ol><li><p>epsilon：一个距离参数，用于定义搜索附近邻居的半径。</p></li><li><p>minPts:  形成集群所需的最低点数。</p></li></ol><p>使用 epsilon 和 minPts，我们可以将每个数据点分类为：</p><ul><li>核心点是在距离ℇ内至少具有最小包含点数（minPTs）的数据点；</li><li>边界点是核心点的距离ℇ内邻近点，但包含的点数小于最小包含点数（minPTs）；</li><li>所有的其他数据点都是噪声点，也被标识为异常值；</li></ul><p>从而，异常检测取决于所要求的最小包含点数、距离ℇ和所选择的距离度量，比如<a href="https://www.zhihu.com/search?q=%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:644680558%7D">欧几里得</a>或曼哈顿距离。该技术是使用KNIME工作流中的<a href="https://www.zhihu.com/search?q=DBSCAN%E8%8A%82%E7%82%B9&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:644680558%7D">DBSCAN节点</a>实现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN<br>train = pd.read_csv(<span class="hljs-string">&#x27;/input/titanic/train.csv&#x27;</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">DB_outliers</span>(<span class="hljs-params">df</span>):<br>    outlier_detection = DBSCAN(eps = <span class="hljs-number">2</span>, metric=<span class="hljs-string">&#x27;euclidean&#x27;</span>, min_samples = <span class="hljs-number">5</span>)<br>    clusters = outlier_detection.fit_predict(df.values.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>    data = pd.DataFrame()<br>    data[<span class="hljs-string">&#x27;cluster&#x27;</span>] = clusters<br>    <span class="hljs-built_in">print</span>(data[<span class="hljs-string">&#x27;cluster&#x27;</span>].value_counts().sort_values(ascending=<span class="hljs-literal">False</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——Drain算法</title>
    <link href="/2022/07/01/Algorithms/Drain%E7%AE%97%E6%B3%95/"/>
    <url>/2022/07/01/Algorithms/Drain%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>论文<br>《Drain: An Online Log Parsing Approach with Fixed Depth Tree》</p><h2 id="1-算法的作用"><a href="#1-算法的作用" class="headerlink" title="1 算法的作用"></a>1 算法的作用</h2><p>  Drain是一种基于固定深度树的在线日志解析（日志解析的目标是将原始日志信息转换为结构化的日志消息，如下图所示，结构化的日志分为常量部分例如src、dest和变量部分例如blk_3587）方法。当新的原始日志消息到达时，Drain将根据领域知识通过简单的正则表达式对其进行预处理。然后，Drain按照树内部节点中编码的特殊设计规则搜索日志组（即树的叶子节点）。如果找到合适的日志组，则日志消息将与存储在该日志组中的日志事件相匹配。否则，将根据日志信息创建新的日志组</p><p><img src="/src/image-20220404152753608-165694028143163.png" alt="image-20220404152753608"></p><p>本质上来讲，Drain就是将不同类型的日志区分开来，进行一个聚类</p><h2 id="2-算法原理"><a href="#2-算法原理" class="headerlink" title="2 算法原理"></a>2 算法原理</h2><h3 id="0-树结构"><a href="#0-树结构" class="headerlink" title="0 树结构"></a>0 树结构</h3><p>  当原始日志消息到达时，在线日志解析器需要为它搜索最合适的日志组，或者创建一个新的日志组。</p><p>  Drain的解析树格式如下图所示</p><p><img src="/src/image-20220404152807697-165694028378266.png" alt="image-20220404152807697"></p><p>最顶层是解析树的<strong>根节点</strong><br>其他层是解析树的<strong>内部节点</strong><br>最底层是解析树的<strong>叶子节点</strong><br>根节点和内部节点编码专门设计的规则来指导搜索过程，它们不存储日志组</p><p>  叶子节点存储一个日志组列表，每个日志组由两部分组成：日志事件和日志id。日志事件是最好地描述该组中的日志消息的模板，该组由日志消息的常量部分组成；日志id记录该组日志消息的id。</p><p>  解析树的一个特殊设计是，所有叶节点的深度都是相同的，并由一个预定义的参数$depth$固定，该参数限制了搜索过程中节点Drain访问的数量，提高搜索效率。</p><p>  此外，为了避免树枝爆炸，使用参数$maxChild$来限制一个节点的最大子节点数.</p><p>  Drain将第$n$层节点定义为深度为$n$的节点，后续的算法流程以上图为参照。</p><h3 id="1-算法第一步：通过专业知识对消息进行预处理"><a href="#1-算法第一步：通过专业知识对消息进行预处理" class="headerlink" title="1 算法第一步：通过专业知识对消息进行预处理"></a>1 算法第一步：通过专业知识对消息进行预处理</h3><p>  Drain允许用户根据表示常用变量（如IP地址和块ID）的专业知识提供简单的正则表达式，然后Drain会从原始日志消息中删除这些正则表达式匹配到的token（例如最上面图中的块ID会被blk_[0-9]+移除）。一个数据集通常只需要几个这样的正则表达式。</p><h3 id="2-算法第二步：通过日志消息长度进行搜索"><a href="#2-算法第二步：通过日志消息长度进行搜索" class="headerlink" title="2 算法第二步：通过日志消息长度进行搜索"></a>2 算法第二步：通过日志消息长度进行搜索</h3><p>  Drain从带有预处理日志消息的解析树的根节点开始。解析树的第1层节点以日志消息的长度（token的数量）区分日志组，Drain根据预处理日志消息的日志消息长度选择到第1层节点的路径。（基于假设：具有相同日志事件的日志消息可能具有相同的日志消息长度）</p><h3 id="3-算法第三步：通过前面的tokens进行搜索"><a href="#3-算法第三步：通过前面的tokens进行搜索" class="headerlink" title="3 算法第三步：通过前面的tokens进行搜索"></a>3 算法第三步：通过前面的tokens进行搜索</h3><p>  Drain从步骤2中搜索的第1层节点遍历到叶节点。（此步骤基于这样一个假设：日志消息开始位置的token更有可能是常量）。</p><p>  Drain通过日志消息开始位置中的token选择下一个内部节点。例如，对于上图中节点Length&#x3D;4的日志消息Receive, Drain从第1层节点Length&#x3D;4遍历到第2层节点Receive，因为日志消息的第一个位置的token是Receive。在这个实例下，内部节点的数量是$depth-2$，因此有$depth-2$个层将日志消息的前$depth−2$个token当做搜索规则。</p><p>  在某些情况下，日志消息可能以一个参数开始，例如，120 bytes received。这类日志消息可能导致解析树中的分支爆炸，因为每个参数将被编码在一个内部节点中。为了避免分支爆炸，在这一步只考虑不包含数字的token。</p><p>  如果一个token包含数字，它将匹配一个特殊的内部节点<em>。例如，对于上面的日志消息”120 bytes received”，Drain将遍历到内部节点</em>而不是120。此外参数$maxChild$限制了节点的最大子节点数。如果一个节点已经有$maxChild$子节点，那么任何不匹配的标记都将在其所有子节点中匹配特殊的内部节点。</p><h3 id="4-算法第四步：通过token相似度搜索"><a href="#4-算法第四步：通过token相似度搜索" class="headerlink" title="4 算法第四步：通过token相似度搜索"></a>4 算法第四步：通过token相似度搜索</h3><p>  在此步骤之前，Drain已经遍历了一个叶节点，其中包含一个日志组列表。这些日志组中的日志消息遵循路径内部节点中编码的规则。例如，图2中的日志组中有一个日志消息包含4个令牌，并以令牌Receive开始。</p><p>  在此步骤中，Drain从日志组列表中选择最合适的日志组。通过计算日志消息和每个日志组的日志事件之间的$simSeq$相似性：<br>$$<br>sinSeq&#x3D;\frac{\sum_{i&#x3D;1}^nequ(seq_1(i),seq_2(i))}{n}<br>$$<br>其中$seq_1(i),seq_2(i)$分别代表日志消息和日志事件，$Seq(i)$为序列的第$i$个token;$N$为序列的日志消息长度，$equ$函数定义如下：<br>$$<br>equ(t_1,t_2)&#x3D;\left{\begin{array}{ll}<br>         1 &amp; {\text { if } t_1 &#x3D;&#x3D; t_2} \<br>        0 &amp; {\text { otherwise }}<br>        \end{array}\right.<br>$$<br>在找到$simSeq$最大的日志组后，将其与预定义的相似度阈值$st$进行比较，如果$simSeq \ge st$，那么Drain就会返回该组作为最佳匹配，否则返回一个标志位表示没有合适的。</p><h3 id="5-算法第五部：更新解析树"><a href="#5-算法第五部：更新解析树" class="headerlink" title="5 算法第五部：更新解析树"></a>5 算法第五部：更新解析树</h3><p>  如果在步骤4中返回了合适的日志组，则Drain将当前日志消息的日志ID添加到返回的日志组中的日志ID中。此外，将更新返回日志组中的日志事件。</p><p>  Drain扫描日志消息和日志事件相同位置的token，如果两个token相同，则不修改该token位置上的token。否则，在日志事件中通过通配符*更新该token位置上的token</p><p>  如果无法找到合适的日志组，则根据当前日志消息创建一个新的日志组，其中日志ID只包含日志消息的ID，日志事件就是这个日志消息。然后，Drain将用新的日志组更新解析树。</p><p>  直观地，Drain从根节点遍历到应该包含新日志组的叶节点，并相应地沿着这条路径添加确实的内部节点和叶子节点。下图展示了这个过程</p><p><img src="/src/image-20220404153822780-165694028724769-165694034590372.png" alt="image-20220404153822780"></p><p>可以看到接受消息 Receive 120 bytes，在解析树中被编码成了最右边这条路径，第三层的内部节点被编码为了通配符*，因为120是数字。该树$depth&#x3D;4$<br>h&#x3D;4$<br>h&#x3D;4$</p>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——吉布斯检测</title>
    <link href="/2022/07/01/Algorithms/GrubbsTest/"/>
    <url>/2022/07/01/Algorithms/GrubbsTest/</url>
    
    <content type="html"><![CDATA[<h1 id="Grubbs-Test"><a href="#Grubbs-Test" class="headerlink" title="Grubbs Test"></a>Grubbs Test</h1><p>统计检验是最直观也最容易的一个方法，通常来说就是：假设原数据服从某个分布（如<a href="https://www.zhihu.com/search?q=%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:417665007%7D">高斯分布</a>），然后计算 $\mu$ 和 $\sigma $，再计算$\mu \pm 3\sigma $的区间，最后落在区间之外的数据点就被认为是异常值。暗含的思想是，落在尾部分布的数据概率很小了，几乎不可能出现；但是出现了，所以是异常的（好想吐槽……）</p><p><img src="/src/v2-e330b934a6d1f401b6d537166594a8be_720w-165693999418640.jpg" alt="img"></p><p>一种近似的统计检验叫Grubbs Test。假设原数据服从正态分布，然后定义零假设与备择假设：<br>H0 :There are no outliers in the data set；<br>H1 :There is at least one outlier in the data set；<br>接下来构建Grubbs test统计量：<br>$C &#x3D; \frac{\max_t\vert x_t - \bar{x}\vert}{s}$</p><p>给定显著性水平 $\alpha$下，当$C&gt;\frac{N-1}{\sqrt{N}}\sqrt{\frac{(t_{\alpha&#x2F;(2N),N-2})^2}{N-2+(t_{\alpha&#x2F;(2N),N-2})^2}}$时，拒绝原假设。这时候比$C$大的$x_t$中最大的那个就是异常值。然后剔除这个值，再重复计算$C$，直到找不出异常值或者剩下的数据量&lt;&#x3D; 6时停止。</p><p>$( G &gt; \frac{(N-1)} {\sqrt{N}} \sqrt{\frac{(t_{\alpha&#x2F;(2N),N-2})^2} {N-2+(t_{\alpha&#x2F;(2N),N-2})^2}} )<br>$<br><strong>适用范围</strong><br>统计检验方法适用于一维数据（一元数据）。比如在反欺诈领域，用户支付金额、支付频次、购买特定商品次数等等，都适用于上述方法。实际上我在某团某评工作时，就用上述方法去识别了某个特定支付渠道的欺诈交易。</p><p><strong>问题</strong></p><ol><li>用既有数据计算$\mu$和$\sigma$时，实际上你是把正常数据与异常数据混合计算了。对欺诈特征异常明显的数据（如下图）来说，上述方法的确有效；但当你排除了异常值或者风控上线策略规避了这些异常交易后，剩下的新数据集总是能再计算出一对新的$\mu$和$\sigma$ ，总是能再找到尾部分布的数值。这时候，你有足够的理由认为这些尾部分布的交易一定是异常的或者有风险的吗？</li><li>只适用于<a href="https://www.zhihu.com/search?q=%E4%B8%80%E7%BB%B4%E6%95%B0%E6%8D%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:417665007%7D">一维数据</a>。但单纯从<strong>一维数据</strong>上进行风险判别本身就不太靠谱。用户大额支付不一定是风险，可能是买贵的商品，所以需要结合其他特征进行综合判断，统计方法就不再适用；</li><li>对于存量数据，通过统计假设检验可以一次性找出其中的高风险交易。但是对于风控从业人员来讲，业务和指标上并不允许你有足够的<strong>时间和容忍度</strong>直到高风险金额累积到显著之后再进行判别，除非你KPI不想要了: )</li></ol><p><img src="/src/v2-20306ceee65981c9c31c92b8adf801b4_720w-165693999770243.jpg" alt="img"></p><center>模拟交易金额分布</center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scipy.stats <span class="hljs-keyword">as</span> stats<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">grubbs_test</span>(<span class="hljs-params">x</span>):<br>    n = <span class="hljs-built_in">len</span>(x)<br>    mean_x = np.mean(x)<br>    sd_x = np.std(x)<br>    numerator = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">abs</span>(x-mean_x))<br>    g_calculated = numerator/sd_x<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Grubbs Calculated Value:&quot;</span>,g_calculated)<br>    t_value = stats.t.ppf(<span class="hljs-number">1</span> - <span class="hljs-number">0.05</span> / (<span class="hljs-number">2</span> * n), n - <span class="hljs-number">2</span>)<br>    g_critical = ((n - <span class="hljs-number">1</span>) * np.sqrt(np.square(t_value))) / (np.sqrt(n) * np.sqrt(n - <span class="hljs-number">2</span> + np.square(t_value)))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Grubbs Critical Value:&quot;</span>,g_critical)<br>    <span class="hljs-keyword">if</span> g_critical &gt; g_calculated:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accept null hypothesis and conclude that there is no outliers\n&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Reject null hypothesis and conclude that there is n outliers\n&quot;</span>)<br></code></pre></td></tr></table></figure><h1 id="Z-score"><a href="#Z-score" class="headerlink" title="Z-score"></a>Z-score</h1><p>Figure in the left shows area under normal curve and how much area that standard deviation covers.</p><ul><li>68% of the data points lie between + or - 1 standard deviation.</li><li>95% of the data points lie between + or - 2 standard deviation</li><li>99.7% of the data points lie between + or - 3 standard deviation</li></ul><p><img src="/%5Csrc%5Cv2-e330b934a6d1f401b6d537166594a8be_720w.jpg" alt="img"></p><p>Z-score是一维或低维特征空间中的参数异常检测方法。该技术假定数据是高斯分布，异常值是分布尾部的数据点，因此远离数据的平均值。距离的远近取决于使用公式计算的归一化数据点$Z_{score}$的设定阈值$Z_{thr}$：</p><p>$Z_{score}&#x3D;\frac{x_i-Mean}{StandardDeviation}$</p><p>然后经过标准化处理后，异常值也进行标准化处理，其绝对值大于$Z_{thr}$：<br>$\vert Z_{score} \vert \gt Z_{thr}$</p><p>$Z_{thr}$值一般设置为2.5、3.0和3.5。该技术是使用KNIME工作流中的行过滤器节点实现的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Zscore_outlier</span>(<span class="hljs-params">df</span>):<br>    m = np.mean(df)<br>    sd = np.std(df)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> df: <br>        z = (i-m)/sd<br>        <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">abs</span>(z) &gt; <span class="hljs-number">3</span>: <br>            out.append(i)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Outliers:&quot;</span>,out)<br></code></pre></td></tr></table></figure><hr><h1 id="Robust-Z-score"><a href="#Robust-Z-score" class="headerlink" title="Robust Z-score"></a>Robust Z-score</h1><p>它类似于 Z 评分方法，但参数发生了一些变化。由于平均值和标准偏差受异常值的严重影响，因此更改此参数时，我们使用中位数和中位数的绝对偏差。</p><p>$R.Z_{score}&#x3D;\frac{0.6745*(x_i-Median)}{MAD}\<br>MAD&#x3D; median(\vert X-median\vert)$</p><p>假设 x 遵循标准正态分布。MAD 将收敛到半正态分布的中位数，即正态分布的 75% 百分位数，以及 N（0.75）≃0.6745。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ZRscore_outlier</span>(<span class="hljs-params">df</span>):<br>    med = np.median(df)<br>    ma = stats.median_absolute_deviation(df)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> df: <br>        z = (<span class="hljs-number">0.6745</span>*(i-med))/ (np.median(ma))<br>        <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">abs</span>(z) &gt; <span class="hljs-number">3</span>: <br>            out.append(i)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Outliers:&quot;</span>,out)<br></code></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs"><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——ISO Forest</title>
    <link href="/2022/07/01/Algorithms/IsoForest/"/>
    <url>/2022/07/01/Algorithms/IsoForest/</url>
    
    <content type="html"><![CDATA[<h1 id="ISO-Forest"><a href="#ISO-Forest" class="headerlink" title="ISO Forest"></a>ISO Forest</h1><p>无监督算法，通过隔离数据中的离群值识别异常。</p><p>孤立森林是基于决策树的算法。从给定的特征集合中随机选择特征，然后在特征的最大值和最小值间随机选择一个分割值，来隔离离群值。这种特征的随机划分会使异常数据点在树中生成的路径更短，从而将它们和其他数据分开。</p><p>一般而言，异常检测的第一步是构造「正常」内容，然后报告任何不能视为正常的异常内容。但孤立森林算法不同于这一原理，首先它不会定义「正常」行为，而且也没有计算基于点的距离。</p><p>一如其名，孤立森林不通过显式地隔离异常，它隔离了数据集中的异常点。</p><p>孤立森林的原理是：异常值是少量且不同的观测值，因此更易于识别。孤立森林集成了孤立树，在给定的数据点中隔离异常值。</p><p>孤立森林通过随机选择特征，然后随机选择特征的分割值，递归地生成数据集的分区。和数据集中「正常」的点相比，要隔离的异常值所需的随机分区更少，因此异常值是树中路径更短的点，路径长度是从根节点经过的边数。</p><p>用孤立森林，不仅可以更快地检测异常，还需要更少的内存。</p><p>孤立森林算法是基于 Ensemble 的异常检测方法，因此具有线性的时间复杂度。且精准度较高，在处理大数据时速度快，所以目前在工业界的应用范围比较广。其基本思想是：通过树模型方法随机地切分样本空间，那些密度很高的簇要被切很多次才会停止切割（即每个点都单独存在于一个子空间内），但那些分布稀疏的点（即异常点），大都很早就停到一个子空间内了。算法步骤为：</p><ol><li><p>从训练数据中随机选择 Ψ 个样本，以此训练单棵树。</p></li><li><p>随机指定一个q维度（attribute），在当前节点数据中随机产生一个切割点p。p切割点产生于当前节点数据中指定q维度的最大值和最小值之间。</p></li><li><p>在此切割点的选取生成了一个超平面，将当前节点数据空间切分为2个子空间：把当前所选维度下小于 p 的点放在当前节点的左分支，把大于等于 p 的点放在当前节点的右分支；</p></li><li><p>在节点的左分支和右分支节点递归步骤 2、3，不断构造新的叶子节点，直到叶子节点上只有一个数据（无法再继续切割） 或树已经生长到了所设定的高度 。（设置单颗树的最大高度是因为异常数据记录都比较少，其路径长度也比较低，而我们也只需要把正常记录和异常记录区分开来，因此只需要关心低于平均高度的部分就好，这样算法效率更高。）</p></li><li><p>由于每颗树训练的切割特征空间过程是完全随机的，所以需要用 ensemble 的方法来使结果收敛，即多建立几棵树，然后综合计算每棵树切分结果的平均值。对于每个样本 x，通过下面的公式计算综合的异常得分s。</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-1640094926775.webp" alt="图片"></p><p>h(x) 为 x 在每棵树的高度，c(Ψ) 为给定样本数 Ψ 时路径长度的平均值，用来对样本 x 的路径长度 h(x) 进行标准化处理。</p></li></ol><p>问题</p><ul><li>工业应用时，作为一个纯粹的无监督算法，异常值占比多少，并没有一个很好的衡量标准（拍脑袋大法好）。因此，模型上线后，仍然需要投入人力进行样本标注，才能对模型进行迭代优化。</li><li>特征要求高。这个例子里特征只有28个，因此模型性能好；但对于高维特征，isolation tree表现似乎一般。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> IsolationForest<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Iso_outliers</span>(<span class="hljs-params">df</span>):<br>    iso = IsolationForest( behaviour = <span class="hljs-string">&#x27;new&#x27;</span>, random_state = <span class="hljs-number">1</span>, contamination= <span class="hljs-string">&#x27;auto&#x27;</span>)<br>    preds = iso.fit_predict(df.values.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>    data = pd.DataFrame()<br>    data[<span class="hljs-string">&#x27;cluster&#x27;</span>] = preds<br>    <span class="hljs-built_in">print</span>(data[<span class="hljs-string">&#x27;cluster&#x27;</span>].value_counts().sort_values(ascending=<span class="hljs-literal">False</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——IQP小记</title>
    <link href="/2022/07/01/Algorithms/Numeric%20Oulier--IQR/"/>
    <url>/2022/07/01/Algorithms/Numeric%20Oulier--IQR/</url>
    
    <content type="html"><![CDATA[<p>数字异常值方法是一维特征空间中最简单的非参数异常值检测方法，异常值是通过IQR（InterQuartile Range）计算得的。</p><p>计算第一和第三 四分位数（$Q_1$、$Q_3$），异常值是位于四分位数范围之外的数据点$x_i$：<br>$x_i \gt Q_3 + k(IQR) \quad or \quad x_i \lt Q_1-k(IQR),\where \ IQR &#x3D; Q_3-Q_1 \ and \ k \ge 0$</p><p>使用四分位数乘数值k&#x3D;1.5，范围限制是典型的上下晶须的盒子图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">iqr_outliers</span>(<span class="hljs-params">df</span>):<br>    q1 = df.quantile(<span class="hljs-number">0.25</span>)<br>    q3 = df.quantile(<span class="hljs-number">0.75</span>)<br>    iqr = q3-q1<br>    Lower_tail = q1 - <span class="hljs-number">1.5</span> * iqr<br>    Upper_tail = q3 + <span class="hljs-number">1.5</span> * iqr<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> df:<br>        <span class="hljs-keyword">if</span> i &gt; Upper_tail <span class="hljs-keyword">or</span> i &lt; Lower_tail:<br>            out.append(i)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Outliers:&quot;</span>,out)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——SVDD小记</title>
    <link href="/2022/07/01/Algorithms/SVDD/"/>
    <url>/2022/07/01/Algorithms/SVDD/</url>
    
    <content type="html"><![CDATA[<h1 id="Support-Vector-Data-Description，SVDD"><a href="#Support-Vector-Data-Description，SVDD" class="headerlink" title="Support Vector Data Description，SVDD"></a>Support Vector Data Description，<a href="https://github.com/iqiukp/SVDD-Python">SVDD</a></h1><p>支持向量数据描述，是一种单值分类算法，能够实现目标样本和非目标样本的区分，通常用于异常检测与入侵检测等领域。</p><p><img src="/src/IwtDFH.gif" alt="123456.gif"></p><p>SVDD是One-Class Classification的一种常用方法。在One-Class Classification中，训练集中只有target class的数据，要使用这些数据来训练一个判决器，判断新样本是不是属于target class。SVDD试图求得高维空间的一个超球面，以最小的半径将训练集中的数据包起来。新来的待测数据映射到高维空间后，如果落在这个超球面内，则认为它属于target class，否则就认为它是一个outlier。</p><p><img src="/src/v2-23599fa94cac6ec616ebc130d1e46d2d_720w-165692130417283.jpg" alt="img"></p><p><strong>SVDD其实通过一个非线性变化，把原始数据映射到特征空间，然后在特征空间中寻找一个体积最小的超球体。</strong></p><p>给定一组训练数据 $x_i\in \mathbb{R}^n,i&#x3D;1,\dots,l$, 求解如下的最优化问题：<br>$$<br>\min_{R,a,\xi} \ R^2+C\sum_{i&#x3D;1}^l\xi_{i} \<br>s.t. ||\phi(x_i)-a||^2\le R^2+\xi_i,\ \xi_i\ge 0\ ,i&#x3D;1,\dots,l,<br>$$<br>其中, $\phi$是将数据映射到高维空间的函数， $C\ge 0$是一个权衡超球体体积和误分率的惩罚参数。公式求解后得到一个由中心$a$和半径 $R$ 唯一确定的超球面,$\xi$是松弛因子。对于一个测试样本 $x$, 如果 $||\phi(x_i)-a||^2\ge R^2$ ，那么 $x$,就是一个outlier。</p><p>然而式 (1) 是一个非凸问题，因为$ ||\phi(x_i)-a||^2- R^2-\xi_i$对于$R$是凹的。可令 $\bar{R}&#x3D;R^2$ 将其转化为一个凸问题，<br>$$<br>\min_{R,a,\xi} \ \bar{R}+C\sum_{i&#x3D;1}^l\xi_{i} \<br>s.t. ||\phi(x_i)-a||^2\le \bar{R}+\xi_i,\ \xi_i\ge 0\ ,i&#x3D;1,\dots,l,\bar{R}\ge0<br>$$<br>上式的拉格朗日形式为<br>$$<br>\begin{align}<br>L(a,\bar{R},\xi,\alpha, \gamma) &amp;&#x3D;\bar{R}+C\sum_{i&#x3D;1}^l \xi_i-\sum_{i&#x3D;1}^l\alpha_i(\bar{R}+\xi_i-||\phi(x_i)-a||^2 )-\sum_{i&#x3D;1}^l\gamma_i\xi_i\<br>&amp;&#x3D;\bar{R}(1-\sum_{i&#x3D;1}^l\alpha_i)+\sum_{i&#x3D;1}^l\xi_i(C-\alpha_i-\gamma_i)+\sum_{i&#x3D;1}^l\alpha_i(||\phi(x_i)-a||^2)<br>\end{align}<br>$$</p><p>$\alpha$ 和 $\gamma$ 是拉格朗日乘子。</p><p>分别对 $\bar{R}$和 $\xi$求偏导，并另其为0，</p><p>$$<br>1-e^T\alpha&#x3D;0 \<br>C-\alpha_i-\gamma_i&#x3D;0, \qquad i&#x3D;1,\dots,l<br>$$<br>又因为$\gamma \ge0,\forall i$，SVDD的对偶问题为</p><p>$$<br>\max_{\alpha} (\inf_{\alpha}\sum_{i&#x3D;1}^l \alpha_i||\phi(x_i)-a||^2) \<br>s.t. 0\le\alpha_i\le C, i&#x3D;1,\dots,l,e^T\alpha&#x3D;1<br>$$<br>$\sum_{i&#x3D;1}^l \alpha_i||\phi(x_i)-a||^2$ 对于$ a$是凸的，其下界并不难求，对 $\alpha$求偏导，并令其为0，得</p><p>$$<br>a\sum_{i&#x3D;1}^l \alpha_i&#x3D;\sum_{i&#x3D;1}^l \alpha_i\phi(x_i)\<br>a &#x3D; \frac{\sum_{i&#x3D;1}^l \alpha_i\phi(x_i)}{e^T\alpha}&#x3D;\sum_{i&#x3D;1}^l \alpha_i\phi(x_i)<br>$$</p><p>将 $\alpha$ 代回式 (2)，那么对偶问题就可以化为</p><p>$$<br>\max_{\alpha} \sum_{i&#x3D;1}^l \alpha_iQ_{i,i}-\alpha^TQ\alpha\<br>s.t. 0\le\alpha_i\le C, i&#x3D;1,\dots,l,e^T\alpha&#x3D;1<br>$$<br>$Q$为核矩阵。上式可用MATLAB中的quadprog函数求解。</p><p>$$<br>||\phi(x_i)-a||^2-\bar{R}&#x3D;K(x,x)-2\sum_{i:\alpha_i\gt0}\alpha_iK(x,x_i)+\alpha^TQ\alpha-\bar{R}<br>$$<br>是否大于0，其中$K(\cdot,\cdot)$为核函数。�。</p>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——TF-IDF小记</title>
    <link href="/2022/07/01/Algorithms/TD-IDF/"/>
    <url>/2022/07/01/Algorithms/TD-IDF/</url>
    
    <content type="html"><![CDATA[<p>TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用加权技术。</p><p>TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p><h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><p>   TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p><p>（1）TF是词频(Term Frequency)<br>        词频（TF）表示词条（关键字）在文本中出现的频率。<br>        这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。<br>$$<br>tf_i &#x3D; \frac{n_{i,j}}{\sum_kn_{k,j}} \iff TF_w&#x3D;\frac{在某一类中词条w出现的次数}{该类中所有的词条数目}<br>$$<br>  其中 $n_{i,j}$是该词在文件$d_j$中出现的次数，分母则是文件$d_j$中所有词汇出现的次数总和；</p><p>（2） IDF是逆向文件频率(Inverse Document Frequency)<br>        逆向文件频率 (IDF) ：某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。<br>$$<br>idf_i&#x3D;log\frac{|D|}{|{j:t_i∈d_j}|} \iff IDF&#x3D;log(\frac{语料库的文档总数}{包含词条w的文档数+1})<br>$$<br>其中，$|D| $是语料库中的文件总数。$ |{j:t_i∈d_j}| $表示包含词语$ t_i$ 的文件数目（即 $n_{i,j}≠0$ 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用$ 1+|{j:t_i∈d_j}|$(Laplace修正)</p><p>（3）TF-IDF实际上是：TF * IDF<br>       某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。<br>$$<br>TF-IDF&#x3D;\ TF\ * IDF<br>$$</p><p>$$<br>W_{w,e}&#x3D;TF_{w,e}*IDF_e \qquad V_e&#x3D;\frac{1}{N_e}\sum_{w&#x3D;1}^{N_e}W_{w,e}*V_w<br>$$</p><h2 id="优缺"><a href="#优缺" class="headerlink" title="优缺"></a>优缺</h2><p>TF-IDF算法非常容易理解，并且很容易实现，但是其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。</p><p>TF-IDF 采用文本逆频率 IDF 对 TF 值加权取权值大的作为关键词，但 IDF 的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以 TF-IDF 算法的精度并不是很高，尤其是当文本集已经分类的情况下。</p><p>在本质上 IDF 是一种试图抑制噪音的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用。这对于大部分文本信息，并不是完全正确的。IDF 的简单结构并不能使提取的关键词， 十分有效地反映单词的重要程度和特征词的分布情 况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被盖。</p><p>TF-IDF算法实现简单快速，但是仍有许多不足之处：</p><p>（1）没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。</p><p>（2）按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。</p><p>（3）传统TF-IDF中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。</p><p>（4）对于文档中出现次数较少的重要人名、地名信息提取效果不佳。</p>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——Winsorization小记</title>
    <link href="/2022/07/01/Algorithms/Winsorization%20Method/"/>
    <url>/2022/07/01/Algorithms/Winsorization%20Method/</url>
    
    <content type="html"><![CDATA[<h1 id="Winsorization-Method"><a href="#Winsorization-Method" class="headerlink" title="Winsorization Method"></a>Winsorization Method</h1><p>如果某个值超过第 99 个百分位的值，并且低于给定值的第 1 个百分位，则将被视为异常值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Winsorization_outliers</span>(<span class="hljs-params">df</span>):<br>    q1 = np.percentile(df , <span class="hljs-number">1</span>)<br>    q3 = np.percentile(df , <span class="hljs-number">99</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> df:<br>        <span class="hljs-keyword">if</span> i &gt; q3 <span class="hljs-keyword">or</span> i &lt; q1:<br>            out.append(i)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Outliers:&quot;</span>,out)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——ROCKA——时间序列聚类算法</title>
    <link href="/2022/07/01/Algorithms/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94ROCKA/"/>
    <url>/2022/07/01/Algorithms/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94ROCKA/</url>
    
    <content type="html"><![CDATA[<h1 id="ROCKA——时间序列聚类算法"><a href="#ROCKA——时间序列聚类算法" class="headerlink" title="ROCKA——时间序列聚类算法"></a>ROCKA——时间序列聚类算法</h1><h3 id="KPI时间序列聚类面临的挑战"><a href="#KPI时间序列聚类面临的挑战" class="headerlink" title="KPI时间序列聚类面临的挑战"></a>KPI时间序列聚类面临的挑战</h3><ul><li>噪声和异常：噪声是指数据采集时候带来的较小的波动。异常是指相对于序列的平均水平波动很大的数值。噪声和异常都会影响到时间序列的聚类，无论是使用特征工程还是相关性的方式做聚类。</li><li>振幅：kpi数据因为是在不同机器上采集的数值有可能存在一定的差异。</li><li>偏移：各个时间序列因为在调用链上的位置不同，所以有可能存在延迟</li></ul><h3 id="提取baseline的算法流程"><a href="#提取baseline的算法流程" class="headerlink" title="提取baseline的算法流程"></a>提取baseline的算法流程</h3><ol><li><p>为了能解决上面提到的振幅不同的问题，所以需要进行标准化。也就是减均值除以标准差。</p></li><li><p>提取baseline</p></li><li><p>平滑极端值，也就是解决上面提到的异常的问题。平滑极端值的方法很简单，就是直接去掉相对平均值来说偏差最大的5%的数据，然后利用线性插值填充。因为经过标准化之后，序列的均值为0，方差为1，所以直接去掉绝对值最大的5%的数据，这个比例可以根据数据中异常值的数量进行改变。</p></li><li><p>提取baseline，可以通过简单的移动平均的方式进行提取。<br>T: 经过平滑极端值后的序列；W:the length of sliding window；B：Baseline；R:Residual<br>$$<br>T &#x3D; (x_1, x_2,\dots,x_m)\<br>x^*<em>t&#x3D;\frac{1}{W}\sum</em>{i&#x3D;1}^Wx_{t-i+1}\<br>B&#x3D;(x^*<em>W,x^*</em>{W+1},\dots,x^*_{m})\<br>R&#x3D;(x_W-x^*<em>W,\dots,x</em>{m}-x^*_{m})<br>$$<br><img src="/src/v2-31b28e24e34085401ee76767924a06a7_720w.jpg" alt="img"></p></li></ol><p>这个是论文中baseline、raw data、residuals的可视化，可以看到baseline清晰了很多。</p><h3 id="基于密度的聚类"><a href="#基于密度的聚类" class="headerlink" title="基于密度的聚类"></a>基于密度的聚类</h3><p>1.相似度的计算</p><p>进行聚类需要的一个距离的度量算法，论文中使用了SBD（shape-based distance）的方法进行度量。<br>$$<br>\vec{x}_{(s)}&#x3D;(x_1, x_2,\dots,x_m)\</p><p>\vec{y}<em>{(s)}&#x3D;<br>\left{\begin{array}{ll}<br>        {(\overbrace{0,\dots,0}^{|s|},y_1,y_2,\dots,y</em>{m-s})} &amp; {\text s \ge 0} \<br>        {(y_{1-s},\dots,y_{m-1},y_m,\underbrace{0,\dots,0}<em>{|s|})} &amp; {\text s \lt 0}<br>        \end{array}\right. \qquad s\in [-m+1,m-1]<br>$$<br>$x,y$都是一个时间序列，长度为$m$。$s$是一个偏移的校正系数，用于处理偏移问题。<br>$$<br>CC_s(\vec{x},\vec{y})&#x3D;<br>\left{\begin{array}{ll}<br>        {\sum</em>{i&#x3D;1}^{m-s}x_{s+i}·y_i} &amp; {\text s \ge 0} \<br>        {\sum_{i&#x3D;1}^{m+s}x_{i}·y_{i-s}} &amp; {\text s \lt 0}<br>        \end{array}\right. \qquad s\in [-m+1,m-1]\</p><p>NCC(\vec{x},\vec{y})&#x3D; \underset{s}{\max}(\frac{CC_s(\vec{x},\vec{y})}{||\vec{x}||_2·||\vec{y}||_2})  \quad \in [-1,1]\<br>SBD(\vec{x},\vec{y}) &#x3D; 1- NCC(\vec{x},\vec{y})\quad \in [0, 2]<br>$$<br>$NCC&#x3D;1$两个序列完全相同，$NCC&#x3D;-1$两个序列完全相反</p><p><img src="/src/v2-021e77545a805b925d4cd91f34a3ba2c_720w.jpg" alt="img"></p><p>左边是没经过baseline提取的SBD值和原始图，右边是经过baseline提取的SBD值和baseline的图片，可以看到SBD相差了10倍，说明提取的效果还是比较明显的。</p><p>2.基于密度的聚类算法的参数估计</p><p>论文中使用的聚类算法是DBSCAN，是基于密度的聚类算法，需要指定两个参数，一个minPts，一个radius。minPts用处就是如果在radius的范围内，有大于或者等于minPts个的点，就可以算做的是一个核心点（更多的细节可以查找DBSCAN的一些资料）。论文中minPts的值使用的是DBSCAN论文中建议使用的4。radius的设置很关键，论文中提出了一个搜索的方法。</p><p>论文中采用下面的规则获得中心序列。一般来说，两个序列的NCC的值小于0.8的话就可以说这个在形态上不相似。<br>$$<br>centroid&#x3D;\underset{\vec{x}\in cluster_i}{\arg min}\underset{\vec{y}\in cluster_i}{\sum}SBD(\vec{x},\vec{y})^2<br>$$<br>$$<br>$$</p>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithms——轮盘赌</title>
    <link href="/2022/07/01/Algorithms/%E8%BD%AE%E7%9B%98%E8%B5%8C/"/>
    <url>/2022/07/01/Algorithms/%E8%BD%AE%E7%9B%98%E8%B5%8C/</url>
    
    <content type="html"><![CDATA[<p>轮盘赌选择法（roulette wheel selection）是最简单也是最常用的选择方法，在该方法中，各个个体的选择概率和其适应度值成比例，适应度越大，选中概率也越大。但实际在进行轮盘赌选择时个体的选择往往不是依据个体的选择概率，而是根据“累积概率”来进行选择。</p><h2 id="轮盘赌选择法操作过程"><a href="#轮盘赌选择法操作过程" class="headerlink" title="轮盘赌选择法操作过程"></a>轮盘赌选择法操作过程</h2><p>以一个实例来讲述轮盘赌选择法的具体过程，现有一个抽奖轮盘如下：</p><pre><code class=" mermaid">pietitle 轮盘赌    &quot;一等奖&quot; : 5    &quot;二等奖&quot; : 15    &quot;三等奖&quot; : 20    &quot;幸运奖&quot; :  25    &quot;谢谢参与&quot;: 35</code></pre><p>对于这个游戏来说， 每一个奖项都有它概率。</p><table><thead><tr><th align="center">奖项</th><th align="center">谢谢参与</th><th align="center">幸运奖</th><th align="center">三等奖</th><th align="center">二等奖</th><th align="center">一等奖</th></tr></thead><tbody><tr><td align="center">编号</td><td align="center">1</td><td align="center">2</td><td align="center">3</td><td align="center">4</td><td align="center">5</td></tr><tr><td align="center">选中概率</td><td align="center">0.35</td><td align="center">0.25</td><td align="center">0.2</td><td align="center">0.15</td><td align="center">0.05</td></tr><tr><td align="center">累计概率</td><td align="center">0.35</td><td align="center">0.6</td><td align="center">0.8</td><td align="center">0.95</td><td align="center">1</td></tr></tbody></table><p>但是对于轮盘赌这个说法来说，并没有这一说，概率往往用适应度来表示，类似于发生次数。</p><p>$x_i$的适应度记为$f(x_i)$。其计算公式为$p(x_i)&#x3D;\frac{f(x_i)}{\sum_{j&#x3D;i}^Nf_j}$,很像softmax的计算公式。</p><p> 同样的我们利用这个累计概率和cdf画两个图出来。<img src="/%5Csrc%5Cimage-20220228202304041.png" alt="image-20220228202304041"></p><p><img src="/%5Csrc%5CB859BA4DA780E441279D2F48B6AF20CF.png" alt="B859BA4DA780E441279D2F48B6AF20CF"></p><p>这样我们随机从0-1分布内取数，就可以获得与原数据分布相同的数据 </p>]]></content>
    
    
    <categories>
      
      <category>Algorithms</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux——Operation</title>
    <link href="/2022/07/01/BigData&amp;Linux/Linux%20Operation/"/>
    <url>/2022/07/01/BigData&amp;Linux/Linux%20Operation/</url>
    
    <content type="html"><![CDATA[<h1 id="Linux基本目录结构"><a href="#Linux基本目录结构" class="headerlink" title="Linux基本目录结构"></a>Linux基本目录结构</h1><pre><code class=" mermaid">graph TD; / --&gt; bin     / --&gt; boot     / --&gt; dev     / --&gt; etc     / --&gt; home     / --&gt; lib     / --&gt; proc     / --&gt; usr     / --&gt; var     / --&gt; .</code></pre><p>Linux 文件系统是一个<strong>目录树的结构</strong>，文件系统结构从一个根目录开始，根目录下可以有任意多个文件和子目录，子目录中又可以有任意多个文件和子目录</p><ul><li><strong>bin 存放二进制可执行文件(ls,cat,mkdir等)</strong></li><li>boot 存放用于系统引导时使用的各种文件</li><li>dev 用于存放设备文件</li><li><strong>etc 存放系统配置文件</strong></li><li>home 存放所有用户文件的根目录</li><li>lib 存放跟文件系统中的程序运行所需要的共享库及内核模块</li><li>mnt 系统管理员安装临时文件系统的安装点</li><li><strong>opt 额外安装的可选应用程序包所放置的位置</strong></li><li>proc 虚拟文件系统，存放当前内存的映射</li><li><strong>root 超级用户目录</strong></li><li>sbin 存放二进制可执行文件，只有root才能访问</li><li>tmp 用于存放各种临时文件</li><li>usr 用于存放系统应用程序，比较重要的目录&#x2F;usr&#x2F;local 本地管理员软件安装目录</li><li>var 用于存放运行时需要改变数据的文件</li></ul><h1 id="命令基本格式"><a href="#命令基本格式" class="headerlink" title="命令基本格式"></a>命令基本格式</h1><p><code>cmd [options] [argument] </code>  options称为选项，arguments称为参数</p><p>选项和参数都作为Shell命令执行时的输入，它们<strong>之间用空格分隔开</strong>。要注意的是 Linux是<strong>区分大小</strong>写的</p><p>对于命令来说，你可以理解为 系统的各个函数</p><p>cmd命令就是一个函数接口，options就是一个函数的模式判断， argument就是这个函数接口携带的参数</p><p>举个例子<code>ls </code> 是显示文件或者目录信息，<code>ls -a</code> 是会显示文件下所有文件包括隐藏文件，<code>ls -l</code>则是会显示文件夹内文件的所有信息，这就是options的效果。<code>ls /lib</code> 就不会显示当前路径下的文件，而是会显示&#x2F;lib文件夹下的文件，这就是参数的效果</p><p>当然这个 可执行的文件大致有一下几种</p><ul><li><strong>内置命令</strong>：出于效率的考虑，将一些常用命令的解释程序<strong>构造在Shell内部</strong>。</li><li><strong>外置命令</strong>：存放在&#x2F;bin、&#x2F;sbin目录下的命令</li><li><strong>实用程序</strong>：存放在&#x2F;usr&#x2F;bin、&#x2F;usr&#x2F;sbin、&#x2F;usr&#x2F;share、&#x2F;usr&#x2F;local&#x2F;bin等目录下的实用程序</li><li><strong>用户程序</strong>：用户程序经过编译生成可执行文件后，可作为Shell命令运行</li><li><strong>Shell脚本</strong>：由Shell语言编写的批处理文件，可作为Shell命令运行</li></ul><p>忘了说了在Linux内其实也有通配符这一说，后面grep也有涉及到这一概念，但是两者还是有些不同的</p><ul><li><code>*</code>：匹配任何字符和任何数目的字符</li><li><code>?</code>：匹配单一数目的任何字符</li><li><code>[ ]</code>：匹配[ ]之内的任意一个字符</li></ul><h1 id="常见的文件、目录操作命令"><a href="#常见的文件、目录操作命令" class="headerlink" title="常见的文件、目录操作命令"></a>常见的文件、目录操作命令</h1><ul><li><p><strong>pwd</strong><strong>:</strong>   查看当前用户的当前目录</p></li><li><p><strong>cd：</strong>    用来切换目录</p><ol><li><p>. : 表示当前目录</p></li><li><p>: 表示当前目录的上一级目录</p></li><li><p>- : 表示用cd命令切换目录所在目录</p></li><li><p>~ ：表示用户主目录的绝对目录</p></li></ol></li><li><p><strong>ls :</strong>     显示文件或者目录信息</p></li><li><p><strong>mkdir：</strong> 当前目录下创新一个空的目录</p></li><li><p><strong>rmdir：</strong>  要求目录为空</p></li><li><p><strong>touch：</strong> 生成一个空文件或者更改文件的时间</p></li><li><p><strong>cp:</strong>      复制文件或目录</p></li><li><p><strong>mv:</strong>     移动文件或目录、文件或目录改名</p></li><li><p><strong>rm:</strong>     删除文件或目录</p></li><li><p><strong>ln:</strong>      建立链接文件</p></li><li><p><strong>find:</strong>    查找文件</p></li><li><p><strong>file&#x2F;stat:</strong> 查看文件类型或文件属性信息</p></li><li><p><strong>cat:</strong>     查看文本文件内容</p></li><li><p><strong>more:</strong>   分页查看文本文件内容</p></li><li><p><strong>less:</strong>    分页，搜索，回翻等操作查看文本文件内容</p></li><li><p><strong>tail -10:</strong>  查看文件的尾部的10行</p></li><li><p><strong>tail -f：</strong>  监控一个文件内容</p></li><li><p><strong>head -10:</strong> 查看文件的头部10行</p></li><li><p><strong>tar -zxvf</strong>：解压压缩包tar.gz</p></li><li><p><strong>gzip -d:</strong>  解压压缩包gz</p></li><li><p><strong>bzip2 -d:</strong>  解压压缩包bz2</p></li><li><p><strong>tar -cxvf</strong>：生成压缩包tar.gz</p></li><li><p><strong>gzip</strong>   : 生成压缩包gz</p></li><li><p><strong>bzip2</strong>  :  生成压缩包bz2</p></li><li><p><strong>echo:</strong>    把内容重定向到指定的文件中 ，有则打开，无则创建</p></li><li><p><strong>管道命令 | ：</strong>将前面的结果给后面的命令，例如：ps aux | grep mysql，将ps显示的进程按照mysql的分组显示</p></li><li><p><strong>重定向   &gt;覆盖模式   &gt;&gt;追加模式：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;l love you&quot; &gt; amazing.txt<br></code></pre></td></tr></table></figure><p>如上就是把l love you这句话写入amazing.txt并覆盖之前的内容</p></li></ul><hr><h1 id="Vim程序编辑器"><a href="#Vim程序编辑器" class="headerlink" title="Vim程序编辑器"></a>Vim程序编辑器</h1><p>所有的 Unix Like 系统都会内建 vi 文书编辑器。</p><p>Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。详情的可以参考<a href="https://www.vim.org/">vim的官方网站</a></p><p>基本上vim分为三种模式，分别是<strong>命令模式（Command mode）</strong>，<strong>输入模式（Insert mode）</strong>和<strong>底线命令模式（Last line mode）</strong>。</p><h2 id="命令模式（Command-mode）"><a href="#命令模式（Command-mode）" class="headerlink" title="命令模式（Command mode）"></a><strong>命令模式（Command mode）</strong></h2><p>用户刚刚启动 vi&#x2F;vim，便进入了命令模式。</p><p>此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。</p><p>以下是常用的几个命令：</p><ul><li><code>i</code> 切换到输入模式，以输入字符。</li><li><strong><code>x</code></strong> 删除当前光标所在处的字符。</li><li><code>: </code>切换到底线命令模式，以在最底一行输入命令。</li><li><code>G</code>用于直接跳转到文件尾</li><li><code>ZZ</code>用于存盘退出Vi</li><li><code>ZQ</code>用于不存盘退出Vi</li><li><code>/和？</code>用于查找字符串</li><li><code>n</code>继续查找下一个</li><li><code>yy</code>复制一行</li><li><code>p</code>粘帖在下一行，P粘贴在前一行</li><li><code>dd</code>删除一行文本</li><li><code>u</code>取消上一次编辑操作（undo）</li></ul><p>若想要编辑文本：启动Vim，进入了命令模式，按下i，切换到输入模式。</p><p>命令模式只有一些最基本的命令，因此仍要依靠底线命令模式输入更多命令。</p><h2 id="输入模式（Insert-mode）"><a href="#输入模式（Insert-mode）" class="headerlink" title="输入模式（Insert mode）"></a><strong>输入模式（Insert mode）</strong></h2><p>在命令模式下按下i就进入了输入模式。</p><p>在输入模式中，可以使用以下按键：</p><ul><li><strong>字符按键以及Shift组合</strong>，输入字符</li><li><strong>ENTER</strong>，回车键，换行</li><li><strong>BACK SPACE</strong>，退格键，删除光标前一个字符</li><li><strong>DEL</strong>，删除键，删除光标后一个字符</li><li><strong>方向键</strong>，在文本中移动光标</li><li><strong>HOME</strong>&#x2F;<strong>END</strong>，移动光标到行首&#x2F;行尾</li><li><strong>Page Up</strong>&#x2F;<strong>Page Down</strong>，上&#x2F;下翻页</li><li><strong>Insert</strong>，切换光标为输入&#x2F;替换模式，光标将变成竖线&#x2F;下划线</li><li><strong>ESC</strong>，退出输入模式，切换到命令模式</li><li><strong>Ctrl+n</strong>，自动补全</li></ul><h2 id="底线命令模式（Last-line-mode）"><a href="#底线命令模式（Last-line-mode）" class="headerlink" title="底线命令模式（Last line mode）"></a><strong>底线命令模式（Last line mode）</strong></h2><p>在命令模式下按下:（英文冒号）就进入了底线命令模式。</p><p>底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。</p><p>在底线命令模式中，基本的命令有）：</p><ul><li><code>:w</code> 保存当前编辑文件，但并不退出</li><li><code>:w</code> newfile 存为另外一个名为 “newfile” 的文件</li><li><code>:wq</code> 用于存盘退出Vi</li><li><code>:q!</code> 用于不存盘退出Vi</li><li><code>:q</code>用于直接退出Vi （未做修改）</li></ul><p><strong>设置Vi环境:</strong></p><ul><li>:set autoindent 缩进,常用于程序的编写</li><li>:set noautoindent 取消缩进</li><li>:set number 在编辑文件时显示行号</li><li>:set nonumber 不显示行号</li><li>:set tabstop&#x3D;value 设置显示制表符的空格字符个数</li><li>:set 显示设置的所有选项</li><li>:set all 显示所有可以设置的选项</li></ul><p>按ESC键可随时退出底线命令模式。</p><hr><h1 id="grep及正则表达式"><a href="#grep及正则表达式" class="headerlink" title="grep及正则表达式"></a>grep及正则表达式</h1><h2 id="grep——强大的文本搜索工具"><a href="#grep——强大的文本搜索工具" class="headerlink" title="grep——强大的文本搜索工具"></a>grep——<strong>强大的文本搜索工具</strong></h2><p>grep 可以使用正则表达式搜索文本，并把匹配的行打印出来。</p><p>　格式：**<code>grep [options] PATTERN [FILE.]</code>**</p><ul><li>PATTERN 是查找条件：<strong>可以是普通字符串、可以是正则表达式</strong>，通常用单引号将RE括起来。</li><li>FILE 是要查找的文件，可以是用空格间隔的多个文件，也可是使用Shell的通配符在多个文件中查找PATTERN，省略时表示在标准输入中查找。</li><li>grep命令不会对输入文件进行任何修改或影响，可以使用输出重定向将结果存为文件</li></ul><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><h3 id="1、基础内容"><a href="#1、基础内容" class="headerlink" title="1、基础内容"></a><strong>1、基础内容</strong></h3><p><strong>字符匹配</strong></p><ul><li>在正则表达式中，如果直接给出字符，就是精确匹配。</li><li>\d 匹配一个数字</li><li>\D 匹配一个非数字</li><li>\w 匹配一个字母、数字或下划线_</li><li>\W 匹配任何非单词字符,等价于“[^A-Za-z0-9_]”</li><li>\s 匹配任何空白字符，包括空格、制表符、换页符等等,等价于[ \f\n\r\t\v]</li><li>\S 匹配任何非空白字符</li><li>\n 匹配一个换行符</li><li>\r 匹配一个回车符</li><li>\t 匹配一个制表符</li></ul><p><strong>数量匹配</strong></p><ul><li>.匹配除“\n”之外的任何单个字符</li><li>*匹配前面的子表达式零次或多次</li><li>+匹配前面的子表达式一次或多次</li><li>?匹配前面的子表达式零次或一次</li><li>{n}，n是一个非负整数，匹配确定的n次</li><li>{n,m}，m和n均为非负整数，其中n&lt;&#x3D;m，最少匹配n次且最多匹配m次</li><li>{n,}，n是一个非负整数，至少匹配n次</li><li>{,m} 匹配前面的正则表达式最多m次</li></ul><p><strong>范围匹配</strong></p><ul><li>x|y 匹配x或y</li><li>[xyz] 字符集合,匹配所包含的任意一个字符</li><li>[^xyz] 负值字符集合,匹配未包含的任意字符</li><li>[a-z] 字符范围,匹配指定范围内的任意字符</li><li>[^a-z] 负值字符范围,匹配任何不在指定范围内的任意字符</li></ul><p>来看一个稍微复杂的例子：\d{3,4}\s+\d{3,8}<br>我们来从左到右解读一下：</p><ol><li>\d{3,4}表示匹配3到4个数字，例如’010’、’0755’；</li><li>\s可以匹配一个空格（也包括Tab等空白符），所以\s+表示至少有一个空格，例如匹配’ ‘，’ ‘等；</li><li>\d{3,8}表示3-8个数字例如’1234567’。</li></ol><p>综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。<br>如果要匹配’010-12345’、’0755-26776666’这样的号码呢？<br>由于’-‘是特殊字符，在正则表达式中，要用’&#39;转义，所以，上面的正则是\d{3,4}-\d{3,8}。<br>但是，仍然无法匹配’010 - 12345’，因为带有空格。所以我们需要更复杂的匹配方式。</p><h3 id="2、进阶内容"><a href="#2、进阶内容" class="headerlink" title="2、进阶内容"></a><strong>2、进阶内容</strong></h3><p>要做更精确地匹配，可以用[]表示范围，比如：</p><ul><li>[0-9a-zA-Z_]可以匹配一个数字、字母或者下划线；</li><li>[0-9a-zA-Z_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如’a100’，’0_Z’，’Py3000’等等；</li><li>[a-zA-Z_][0-9a-zA-Z_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量；</li><li>[a-zA-Z_][0-9a-zA-Z_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。</li><li>A|B可以匹配A或B，所以(P|p)ython可以匹配’Python’或者’python’。</li><li>^表示行的开头，^\d表示必须以数字开头。</li><li>$表示行的结束，$\d表示必须以数字结束。</li></ul><hr><h1 id="系统管理命令"><a href="#系统管理命令" class="headerlink" title="系统管理命令"></a>系统管理命令</h1><table><thead><tr><th>stat</th><th>显示指定文件的详细信息，比ls更详细</th></tr></thead><tbody><tr><td>who</td><td>显示在线登陆用户</td></tr><tr><td>whoami</td><td>显示当前操作用户</td></tr><tr><td>hostname</td><td>显示主机名</td></tr><tr><td>uname</td><td>显示系统信息</td></tr><tr><td>top</td><td>动态显示当前耗费资源最多进程信息</td></tr><tr><td>ps</td><td>显示瞬间进程状态</td></tr><tr><td>du</td><td>查看目录大小 du -h &#x2F;home带有单位显示目录信息</td></tr><tr><td>df</td><td>查看磁盘大小 df -h 带有单位显示磁盘信息</td></tr><tr><td>ifconfig</td><td>查看网络情况</td></tr><tr><td>ping</td><td>测试网络连通</td></tr><tr><td>netstat</td><td>显示网络状态信息</td></tr><tr><td>clear</td><td>清屏</td></tr><tr><td>alias</td><td>对命令重命名 如：alias showmeit&#x3D;”ps -aux” ，另外解除使用unaliax showmeit</td></tr><tr><td>kill</td><td>杀死进程，可以先用ps 或 top命令查看进程的id，然后再用kill命令杀死进程。</td></tr></tbody></table><hr><h1 id="系统服务管理"><a href="#系统服务管理" class="headerlink" title="系统服务管理"></a>系统服务管理</h1><table><thead><tr><th>任务</th><th>旧指令CentOS6</th><th>新指令CentOS7</th></tr></thead><tbody><tr><td>使某服务自动启动</td><td>chkconfig –level 3 httpd on</td><td>systemctl enable httpd.service</td></tr><tr><td>使某服务不自动启动</td><td>chkconfig –level 3 httpd off</td><td>systemctl disable httpd.service</td></tr><tr><td>检查服务状态</td><td>service httpd status</td><td>systemctl status httpd.service （服务详细信息） systemctl is-active  httpd.service （仅显示是否  Active)</td></tr><tr><td>显示所有已启动的服务</td><td>chkconfig –list</td><td>systemctl list-units –type&#x3D;service</td></tr><tr><td>启动某服务</td><td>service httpd start</td><td>systemctl start httpd.service</td></tr><tr><td>停止某服务</td><td>service httpd stop</td><td>systemctl stop httpd.service</td></tr><tr><td>重启某服务</td><td>service httpd restart</td><td>systemctl restart httpd.service</td></tr></tbody></table><p>例如在搭建hadoop集成环境的时候需要关闭防火墙和防火墙自启动：（在root权限下)</p><p>Centos6.5：service iptables stop</p><p>​          chkconfig iptables off</p><p>Centos 7 :  systemctl stop firewalld.service</p><p>​           systemctl disable firewalld.service</p><hr><h1 id="其他一些杂七杂八的东西"><a href="#其他一些杂七杂八的东西" class="headerlink" title="其他一些杂七杂八的东西"></a>其他一些杂七杂八的东西</h1><p>时间设置：date -s “2021-09-23 11:26:35”</p><p>终端显示：在&#x2F;etc&#x2F;profile这个环境变量里配置</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs swift"><span class="hljs-type">PS1</span><span class="hljs-operator">=</span><span class="hljs-string">&quot;\[<span class="hljs-subst">\0</span>33[1;36;40m\][\[<span class="hljs-subst">\0</span>33[0;32;40m\]\u@\h: \H \#  \d <span class="hljs-subst">\t</span> \[<span class="hljs-subst">\0</span>33[1;35;40m\]\w\[<span class="hljs-subst">\0</span>33[1;36;40m\]]\[<span class="hljs-subst">\0</span>33[1;33;40m\]<span class="hljs-subst">\\</span>$\[<span class="hljs-subst">\0</span>33[1;37;40m\]&quot;</span><br></code></pre></td></tr></table></figure><p>再source一下就可以显示更详细的终端信息信息</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux——多版本CUDA</title>
    <link href="/2022/07/01/BigData&amp;Linux/Linux%E5%AE%89%E8%A3%85%E5%A4%9A%E7%89%88%E6%9C%ACCUDA/"/>
    <url>/2022/07/01/BigData&amp;Linux/Linux%E5%AE%89%E8%A3%85%E5%A4%9A%E7%89%88%E6%9C%ACCUDA/</url>
    
    <content type="html"><![CDATA[<h1 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h1><p>同样的在运行不同代码的时候，需要的环境不同，所以需要不同的GPU环境，Cuda的需要自然也就发生改变 特别是关于tensorflow的运行中。</p><p>首先我们需要在nvidia的官网上找到我们所需要的<a href="https://developer.nvidia.com/cuda-toolkit-archive">cuda版本</a>，接下来可以根据各自的实际要求来选择</p><blockquote><p>当前环境配置：Ubuntu20.04  gcc-5， gcc-8， gcc-9， cuda 11.3</p><p>需求安装cuda 9 </p></blockquote><p><img src="https://gitee.com/xiang976young/note/raw/master/img/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202022-04-20%20103810.png" alt="屏幕截图 2022-04-20 103810"></p><p>然后运行安装这个runfile <code>sudo sh xxx.run</code></p><p>注意不要安装英伟达驱动 因为之前已经有一个cuda环境 安装过了</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20220420121744526.png" alt="image-20220420121744526"></p><p>然后修改cuda的环境变量 <code>vim ~/.bashrc</code>   （如果环境变量使用的是cuda软连接 就可以不用修改环境变量了）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> LD_LIBRARY_PATH=<span class="hljs-variable">$LD_LIBRARY_PATH</span>:/usr/local/cuda/lib64<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:/usr/local/cuda/bin<br><span class="hljs-built_in">export</span> CUDA_HOME=<span class="hljs-variable">$CUDA_HOME</span>:/usr/local/cuda<br></code></pre></td></tr></table></figure><p><code>source ~/.bashrc</code>保存环境变量，使配置生效</p><p>进入<code>cd /usr/local</code></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20220420121929018.png" alt="image-20220420121929018"></p><p>发现cuda其实一个软连接，所以以后我们需要切换cuda版本的时候，只需要修改这个软连接的指向就可以了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">rm</span> -rf cuda<br>sudo <span class="hljs-built_in">ln</span> -s /usr/local/cuda-11.5 /usr/local/cuda<br></code></pre></td></tr></table></figure><p>可以利用<code>nvcc -V</code>来查看当前CUDA的版本</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20220421104236667.png" alt="image-20220421104236667"></p><h1 id="安装Cudnn"><a href="#安装Cudnn" class="headerlink" title="安装Cudnn"></a>安装Cudnn</h1><p>当然有的时候在使用CUDA的时候，还需要使用Cudnn，所以我们需要在对应的CUDA环境内安装我们需要的Cudnn</p><p>Cudnn的<a href="https://developer.nvidia.com/rdp/cudnn-archive#a-collapse764-90">下载地址</a>，下载的时候需要先登陆一下</p><p>然后根据自己环境和CUDA来选择下载的版本 我这里选择是下方的版本</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202022-04-21%20102906.png" alt="屏幕截图 2022-04-21 102906"></p><p>対以下操作进行修改：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">tar -xzvf cudnn-9.0-linux-x64-v7.6.5.32.tgz<br>sudo <span class="hljs-built_in">cp</span> cuda-9.0/include/cudnn.h  /usr/local/cuda-9.0/include<br>sudo <span class="hljs-built_in">cp</span> cuda-9.0/lib64/lib*  /usr/local/cuda-9.0/lib64<br><br><br><br>sudo <span class="hljs-built_in">rm</span> -rf libcudnn.so libcudnn.so.7<br>sudo <span class="hljs-built_in">ln</span> -s libcudnn.so.7.6.5 libcudnn.so.7<br>sudo <span class="hljs-built_in">ln</span> -s libcudnn.so.7 libcudnn.so<br>sudo <span class="hljs-built_in">chmod</span> a+r /usr/local/cuda-9.0/include/cudnn.h   /usr/local/cuda-9.0/lib64/libcudnn*<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux——多版本GCC</title>
    <link href="/2022/07/01/BigData&amp;Linux/Linux%E5%AE%89%E8%A3%85%E5%A4%9A%E7%89%88%E6%9C%ACgcc/"/>
    <url>/2022/07/01/BigData&amp;Linux/Linux%E5%AE%89%E8%A3%85%E5%A4%9A%E7%89%88%E6%9C%ACgcc/</url>
    
    <content type="html"><![CDATA[<p>有的时候，不同环境要求的gcc编译器版本不同，需要安装多个gcc来随时切换</p><p>所以一开始安装的时候，利用ubuntu软件源包含一个软件包组”build-essential”，其中有GNU编辑器集合、GNU调试器、和其他编译软件所必需的开发库与工具</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt update<br>sudo apt install build-essential<br></code></pre></td></tr></table></figure><p>安装完以后，可以利用<code>gcc --version </code>或者<code>gcc -v</code>来查看gcc的版本情况</p><p>如果你需要安装多版本的gcc  那么你就可以利用以下命令</p><p><code>sudo apt install gcc-8 g++-8 gcc-9 g++-9 gcc-5 g++-5</code></p><blockquote><p>注意这里不同的Linux 或者 Ubuntu版本所能安装的gcc版本有限制，需要将相对应的软件源写到当前版本下</p><p>利用管理员权限打开 <code>sudo vim /etc/apt/sources.list</code></p><p>添加一下软件源</p><p><code>deb http://dk.archive.ubuntu.com/ubuntu/ xenial main</code><br><code>deb http://dk.archive.ubuntu.com/ubuntu/ xenial universe</code></p><p>更新一下apt  <code>sudo apt update</code></p></blockquote><p>为各个gcc配置一个版本，并设置优先级</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 100 --slave /usr/bin/g++ g++ /usr/bin/g++-9<br>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 90 --slave /usr/bin/g++ g++ /usr/bin/g++-8<br>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 5 --slave /usr/bin/g++ g++ /usr/bin/g++-5<br></code></pre></td></tr></table></figure><p>以后，如果你想修改默认的版本，使用<code>sudo update-alternatives --config gcc</code>命令：<img src="https://gitee.com/xiang976young/note/raw/master/img/image-20220420094340037.png" alt="image-20220420094340037"></p><p>你将会被展示一系列已经安装在你的 Ubuntu 系统上的 GCC 版本。输入你想设置为默认的 GCC 版本，并且按回车<code>Enter</code>。</p><p>这个命令将会创建符号链接到指定版本的 GCC 和 G++。</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux——查看linux版本</title>
    <link href="/2022/07/01/BigData&amp;Linux/Linux%E6%9F%A5%E7%9C%8B%E7%89%88%E6%9C%AC/"/>
    <url>/2022/07/01/BigData&amp;Linux/Linux%E6%9F%A5%E7%9C%8B%E7%89%88%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<p> <code>cat /proc/version</code>可以查看目录下”&#x2F;proc”下version的信息，也可以得到当前系统的内核版本号及系统名称 。</p><p><img src="/src/image-20220420085717764.png" alt="image-20220420085717764"></p><p>显示linux standard base的基础配置</p><p><img src="/src/image-20220420085829154.png" alt="image-20220420085829154">png)</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux——无法解析域名</title>
    <link href="/2022/07/01/BigData&amp;Linux/Linux%E6%9A%82%E4%B8%8D%E8%83%BD%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D/"/>
    <url>/2022/07/01/BigData&amp;Linux/Linux%E6%9A%82%E4%B8%8D%E8%83%BD%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="apt-get-操作过程中提示无法解析域名“cn-archive-ubuntu-com”-的解决"><a href="#apt-get-操作过程中提示无法解析域名“cn-archive-ubuntu-com”-的解决" class="headerlink" title="apt-get 操作过程中提示无法解析域名“cn.archive.ubuntu.com” 的解决"></a>apt-get 操作过程中提示无法解析域名“cn.archive.ubuntu.com” 的解决</h1><p>因为DNS服务器没有配置</p><p><code>sudo vim /etc/resolv.conf</code></p><p>添加nameserver 8.8.8.8 即可</p><p>但是使用该方法会发现重启电脑之后resolv.conf文件内容会还原</p><p>应该在<code>/etc/resolvconf/resolv.conf.d/base</code> 做同样的修改</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—CNN小记</title>
    <link href="/2022/07/01/DeepLearning/CNN/"/>
    <url>/2022/07/01/DeepLearning/CNN/</url>
    
    <content type="html"><![CDATA[<h1 id="局部连接-权值共享"><a href="#局部连接-权值共享" class="headerlink" title="局部连接+权值共享"></a><strong>局部连接+权值共享</strong></h1><p>全连接神经网络需要非常多的计算资源才能支撑它来做反向传播和前向传播，所以说全连接神经网络可以存储非常多的参数，如果你给它的样本如果没有达到它的量级的时候，它可以轻轻松松把你给他的样本全部都记下来，这会出现过拟合的情况。</p><p>所以我们应该把神经元和神经元之间的连接的权重个数降下来，但是降下来我们又不能保证它有较强的学习能力，所以这是一个纠结的地方，所以有一个方法就是<strong>局部连接+权值共享</strong>，局部连接+权值共享不仅权重参数降下来了，而且学习能力并没有实质的降低，除此之外还有其它的好处，下来看一下，下面的这几张图片：</p><p><img src="/%5Csrc%5C15388119347194e0d0f4b72.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>一个图像的不同表示方式</p><p>这几张图片描述的都是一个东西，但是有的大有的小，有的靠左边，有的靠右边，有的位置不同，但是我们构建的网络识别这些东西的时候应该是同一结果。为了能够达到这个目的，我们可以让图片的不同位置具有相同的权重（权值共享），也就是上面所有的图片，我们只需要在训练集中放一张，我们的神经网络就可以识别出上面所有的，这也是<strong>权值共享</strong>的好处。</p><p><strong>而卷积神经网络就是局部连接+权值共享的神经网络。</strong></p><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>现在我们对卷积神经网络有一个初步认识了，下面具体来讲解一下卷积神经网络，卷积神经网络依旧是层级结构，但层的功能和形式做了改变，卷积神经网络常用来处理图片数据，比如识别一辆汽车：</p><p><img src="/%5Csrc%5C1538811934930f8df8dfdce.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>卷积神经网络结构</p><p>其中数据输入的是一张图片（<strong>输入层</strong>），CONV表示<strong>卷积层</strong>，RELU表示<strong>激励层</strong>，POOL表示<strong>池化层</strong>，Fc表示<strong>全连接层</strong></p><h1 id="卷积神经网络之输入层"><a href="#卷积神经网络之输入层" class="headerlink" title="卷积神经网络之输入层"></a>卷积神经网络之输入层</h1><p>在图片输出到神经网络之前，常常先进行图像处理，有<strong>三种</strong>常见的图像的处理方式：</p><ol><li>均值化：把输入数据各个维度都中心化到0，所有样本求和求平均，然后用所有的样本减去这个均值样本就是去均值。</li><li>归一化：数据幅度归一化到同样的范围，对于每个特征而言，范围最好是[-1,1]</li><li>PCA&#x2F;白化：用PCA降维，让每个维度的相关度取消，特征和特征之间是相互独立的。白化是对数据每个特征轴上的幅度归一化</li></ol><p><img src="/%5Csrc%5C1538811927998ccf5c60eaa.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>均值化和归一化</p><p><img src="/%5Csrc%5C153881192817621655ba3c7.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>去相关和白化</p><h1 id="卷神网络之卷积层："><a href="#卷神网络之卷积层：" class="headerlink" title="卷神网络之卷积层："></a>卷神网络之卷积层：</h1><p>图片有一个性质叫做局部关联性质，一个图片的像素点影响最大的是它周边的像素点，而距离这个像素点比较远的像素点二者之间关系不大。这个性质意味着每一个神经元我们不用处理全局的图片了（和上一层全连接），我们的每一个神经元只需要和上一层局部连接，相当于每一个神经元扫描一小区域，然后许多神经元（这些神经元权值共享）合起来就相当于扫描了全局，这样就构成一个特征图，n个特征图就提取了这个图片的n维特征，每个特征图是由很多神经元来完成的。</p><p>在卷积神经网络中，我们先选择一个局部区域（filter），用这个局部区域（filter）去扫描整张图片。 局部区域所圈起来的所有节点会被连接到下一层的<strong>一个节点上</strong>。我们拿灰度图（只有一维）来举例：</p><p><img src="/%5C153881192835439d6e12fa9.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>局部区域</p><p>图片是矩阵式的，将这些以矩阵排列的节点展成了向量。就能更好的看出来卷积层和输入层之间的连接，并不是全连接的，我们将上图中的红色方框称为filter，它是2*2的，这是它的尺寸，这不是固定的，我们可以指定它的尺寸。</p><p>我们可以看出来当前filter是2*2的小窗口，这个小窗口会将图片矩阵从左上角滑到右下角，每滑一次就会一下子圈起来四个，连接到下一层的一个神经元，然后产生四个权重，这四个权重(w1、w2、w3、w4)构成的矩阵就叫做卷积核。</p><p>卷积核是算法自己学习得到的，它会和上一层计算，比如，第二层的0节点的数值就是局部区域的线性组合（w1<em>0+w2</em>1+w3<em>4+w4</em>5），即被圈中节点的数值乘以对应的权重后相加。</p><p><img src="/%5C153881192861205b0c14653.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>卷积核计算</p><p><img src="/%5Csrc%5C1538812170652003d236aef.gif" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>卷积操作</p><p>我们前面说过图片不用向量表示是为了保留图片平面结构的信息。 同样的，卷积后的输出若用上图的向量排列方式则丢失了平面结构信息。 所以我们依然用矩阵的方式排列它们，就得到了下图所展示的连接，每一个蓝色结点连接四个黄色的结点。</p><p><img src="/%5Csrc%5C1538811928873e8e0626a39.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>卷积层的连接方式</p><p>图片是一个矩阵然后卷积神经网络的下一层也是一个矩阵，我们用一个卷积核从图片矩阵左上角到右下角滑动，每滑动一次，当然被圈起来的神经元们就会连接下一层的一个神经元，形成参数矩阵这个就是卷积核，每次滑动虽然圈起来的神经元不同，连接下一层的神经元也不同，但是产生的参数矩阵确是一样的，这就是<strong>权值共享</strong>。</p><p>卷积核会和扫描的图片的那个局部矩阵作用产生一个值，比如第一次的时候，（w1<em>0+w2</em>1+w3<em>4+w4</em>5），所以，filter从左上到右下的这个过程中会得到一个矩阵（这就是下一层也是一个矩阵的原因），具体过程如下所示：</p><p><img src="/%5Csrc%5C1538812228698d0c184b223.gif" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>卷积计算过程</p><p>上图中左边是图矩阵，我们使用的filter的大小是3<em>3的，第一次滑动的时候，卷积核和图片矩阵作用（1</em>1+1<em>0+1</em>1+0<em>0+1</em>1+1<em>0+0</em>1+0<em>0+1</em>1）&#x3D;4，会产生一个值，这个值就是右边矩阵的第一个值，filter滑动9次之后，会产生9个值，也就是说下一层有9个神经元，这9个神经元产生的值就构成了一个矩阵，这矩阵叫做特征图，表示image的某一维度的特征，当然具体哪一维度可能并不知道，可能是这个图像的颜色，也有可能是这个图像的轮廓等等。</p><p><strong>单通道图片总结</strong>：以上就是单通道的图片的卷积处理，图片是一个矩阵，我们用指定大小的卷积核从左上角到右下角来滑动，每次滑动所圈起来的结点会和下一层的一个结点相连，连接之后就会形成局部连接，每一条连接都会产生权重，这些权重就是卷积核，所以每次滑动都会产生一个卷积核，因为权值共享，所以这些卷积核都是一样的。卷积核会不断和当时卷积核所圈起来的局部矩阵作用，每次产生的值就是下一层结点的值了，这样多次产生的值组合起来就是一个特征图，表示某一维度的特征。也就是从左上滑动到右下这一过程中会形成一个特征图矩阵（共享一个卷积核），再从左上滑动到右下又会形成另一个特征图矩阵（共享另一个卷积核），这些特征图都是表示特征的某一维度。</p><p><strong>三个通道的图片如何进行卷积操作？</strong></p><p>至此我们应该已经知道了单通道的灰度图是如何处理的，实际上我们的图片都是RGB的图像，有三个通道，那么此时图像是如何卷积的呢？</p><p><img src="/%5Csrc%5C1538811929377662c8664e0.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>彩色图像</p><p>filter窗口滑的时候，我们只是从width和height的角度来滑动的，并没有考虑depth，所以每滑动一次实际上是产生一个卷积核，共享这一个卷积核，而现在depth&#x3D;3了，所以每滑动一次实际上产生了具有三个通道的卷积核（它们分别作用于输入图片的蓝色、绿色、红色通道），卷积核的一个通道核蓝色的矩阵作用产生一个值，另一个和绿色的矩阵作用产生一个值，最后一个和红色的矩阵作用产生一个值，然后这些值加起来就是下一层结点的值，结果也是一个矩阵，也就是一张特征图。</p><p><img src="/%5Csrc%5C153881192952676b46cd98c.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>三通道的计算过程</p><p>要想有多张特征图的话，我们可以再用新的卷积核来进行左上到右下的滑动，这样就会形成<strong>新的特征图</strong>。</p><p><img src="/%5Csrc%5C153881225598237c4e148f2.gif" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>三通道图片的卷积过程</p><p>也就是说增加一个卷积核，就会产生一个特征图，总的来说就是输入图片有多少通道，我们的卷积核就需要对应多少通道，而本层中卷积核有多少个，就会产生多少个特征图。这样卷积后输出可以作为新的输入送入另一个卷积层中处理，有几个特征图那么depth就是几，那么下一层的每一个特征图就得用相应的通道的卷积核来对应处理，这个逻辑要清楚，我们需要先了解一下<strong>基本的概念：</strong></p><ol><li>深度depth（通道）：由上一层滤波器的个数决定</li><li>步长stride：每次滑动几步，步数越大得到的特征数越少，上面的例子中每次滑动1步。</li><li>填充值zero-padding：我们设置了步长之后，很有可能某些位置滑不到，为了避免了边缘信息被一步步舍弃的问题，我们需要设置填充值来解决这个问题。还有一个问题，4x4的图片被2x2的filter卷积后变成了3x3的图片，每次卷积后都会小一圈的话，经过若干层后岂不是变的越来越小？zero padding就可以在这时帮助控制Feature Map的输出尺寸，同时避免了边缘信息被一步步舍弃的问题。</li></ol><p><img src="/%5Csrc%5C1538811929789ebba8dc146.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>卷积计算的公式</p><p>4x4的图片在边缘Zero padding一圈后，再用3x3的filter卷积后，得到的Feature Map尺寸依然是4x4不变。</p><p><img src="/%5Csrc%5C15388119304907f29ae3d5f.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>填充</p><p>当然也可以使用5x5的filte和2的zero padding可以保持图片的原始尺寸，3x3的filter考虑到了像素与其距离为1以内的所有其他像素的关系，而5x5则是考虑像素与其距离为2以内的所有其他像素的关系。</p><p><strong>规律：</strong> Feature Map的尺寸等于</p><p><strong>(input_size + 2 * padding_size − filter_size)&#x2F;stride+1</strong></p><p>我们可以把卷积层的作用<strong>总结一点：</strong>卷积层其实就是在提取特征，卷积层中最重要的是卷积核（训练出来的），不同的卷积核可以探测特定的形状、颜色、对比度等，然后特征图保持了抓取后的空间结构，所以不同卷积核对应的特征图表示某一维度的特征，具体什么特征可能我们并不知道。特征图作为输入再被卷积的话，可以则可以由此探测到”更大”的形状概念，也就是说随着卷积神经网络层数的增加，特征提取的越来越具体化。</p><h1 id="卷积神经网络之激励层"><a href="#卷积神经网络之激励层" class="headerlink" title="卷积神经网络之激励层"></a>卷积神经网络之激励层</h1><p>下面讲解激励层的作用，激励层的作用可以理解为把卷积层的结果做<strong>非线性映射</strong>。</p><p><img src="/%5Csrc%5C15388119312787bf24b56c9.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>激励层</p><p>上图中的f表示激励函数，常用的激励函数几下几种：</p><p><img src="/%5Csrc%5C1538811931507ff23b8bbe8.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>常用的激励函数</p><p>我们先来看一下激励函数Sigmoid导数最小为0，最大为1&#x2F;4，</p><p><img src="/%5Csrc%5C15388119316439077e6c9cc.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>激励函数Sigmoid</p><p>Tanh激活函数：和sigmoid相似，它会关于x轴上下对应，不至于朝某一方面偏向</p><p><img src="/%5Csrc%5C15388119317859629ca6f60.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>Tanh激活函数</p><p>ReLU激活函数（修正线性单元)：收敛快，求梯度快，但较脆弱，左边的梯度为0</p><p><img src="/%5Csrc%5C15388119319277bd0446ed5.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>ReLU激活函数</p><p>Leaky ReLU激活函数：不会饱和或者挂掉，计算也很快，但是计算量比较大</p><p><img src="/%5Csrc%5C15388119322943b13cc43d7.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>Leaky ReLU激活函数</p><p><strong>一些激励函数的使用技巧</strong>：一般不要用sigmoid，首先试RELU，因为快，但要小心点，如果RELU失效，请用Leaky ReLU，某些情况下tanh倒是有不错的结果。</p><p>这就是卷积神经网络的激励层，它就是将卷积层的线性计算的结果进行了非线性映射。可以从下面的图中理解。它展示的是将非线性操作应用到一个特征图中。这里的输出特征图也可以看作是”修正”过的特征图。如下所示：</p><p><img src="/%5Csrc%5C1538811932782a875fdc5c0.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>非线性操作</p><h1 id="卷积神经网络之池化层"><a href="#卷积神经网络之池化层" class="headerlink" title="卷积神经网络之池化层"></a>卷积神经网络之池化层</h1><p>池化层：降低了各个特征图的维度，但可以保持大分重要的信息。池化层夹在连续的卷积层中间，压缩数据和参数的量，减小过拟合，池化层并没有参数，它只不过是把上层给它的结果做了一个下采样（数据压缩）。下采样有<strong>两种</strong>常用的方式：</p><p><strong>Max pooling</strong>：选取最大的，我们定义一个空间邻域（比如，2x2 的窗口），并从窗口内的修正特征图中取出最大的元素，最大池化被证明效果更好一些。</p><p><strong>Average pooling</strong>：平均的，我们定义一个空间邻域（比如，2x2 的窗口），并从窗口内的修正特征图算出平均值</p><p><img src="/%5Csrc%5C1538811933035bbde841477.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>Max pooling</p><p>我们要注意一点的是：pooling在不同的depth上是分开执行的，也就是depth&#x3D;5的话，pooling进行5次，产生5个池化后的矩阵，池化不需要参数控制。池化操作是分开应用到各个特征图的，我们可以从五个输入图中得到五个输出图。</p><p><img src="/%5Csrc%5C153881193345322115914d7.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>池化操作</p><p>无论是max pool还是average pool都有分信息被舍弃，那么部分信息被舍弃后会损坏识别结果吗？</p><p>因为卷积后的Feature Map中有对于识别物体不必要的冗余信息，我们下采样就是为了去掉这些冗余信息，所以并不会损坏识别结果。</p><p>我们来看一下卷积之后的冗余信息是怎么产生的？</p><p>我们知道卷积核就是为了找到特定维度的信息，比如说某个形状，但是图像中并不会任何地方都出现这个形状，但卷积核在卷积过程中没有出现特定形状的图片位置卷积也会产生一个值，但是这个值的意义就不是很大了，所以我们使用池化层的作用，将这个值去掉的话，自然也不会损害识别结果了。</p><p>比如下图中，假如卷积核探测”横折”这个形状。 卷积后得到3x3的Feature Map中，真正有用的就是数字为3的那个节点，其余数值对于这个任务而言都是无关的。 所以用3x3的Max pooling后，并没有对”横折”的探测产生影响。 试想在这里例子中如果不使用Max pooling，而让网络自己去学习。 网络也会去学习与Max pooling近似效果的权重。因为是近似效果，增加了更多的参数的代价，却还不如直接进行最大池化处理。</p><p><img src="/%5Csrc%5C1538811933889d5d5c7e17a.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>最大池化处理</p><h1 id="卷积神经网络之全连接层"><a href="#卷积神经网络之全连接层" class="headerlink" title="卷积神经网络之全连接层"></a>卷积神经网络之全连接层</h1><p>在全连接层中所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。当前面卷积层抓取到足以用来识别图片的特征后，接下来的就是如何进行分类。 通常卷积网络的最后会将末端得到的长方体平摊成一个长长的向量，并送入全连接层配合输出层进行分类。比如，在下面图中我们进行的图像分类为四分类问题，所以卷积神经网络的输出层就会有四个神经元。</p><p><img src="/%5Csrc%5C153881193397944bb2a1b9b.png" alt="从卷积层、激活层、池化层到全连接层深度解析卷积神经网络的原理"></p><p>四分类问题</p><p>我们从卷积神经网络的输入层、卷积层、激活层、池化层以及全连接层来讲解卷积神经网络，我们可以认为全连接层之间的在做特征提取，而全连接层在做分类，这就是卷积神经网络的核心。</p><p>��这就是卷积神经网络的核心。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux——切换Java版本</title>
    <link href="/2022/07/01/BigData&amp;Linux/%E5%88%87%E6%8D%A2java%E7%89%88%E6%9C%AC/"/>
    <url>/2022/07/01/BigData&amp;Linux/%E5%88%87%E6%8D%A2java%E7%89%88%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<p>首先需要安装一个新的jdk</p><p>然后修改环境变量里面的JAVA_HOME</p><p>然后复制新的jdk&#x2F;bin里面的java.exe&#x2F;javaw.exe&#x2F;javaws.exe到C:\Windows\System32</p><p>注意修改regedit中的注册表的信息HKEY_LOCAL_MACHINE\Software\JavaSoft\Java Runtime Environment<br>修改默认jdk的版本</p><p>注意要及时删除环境变量path中C:\ProgramData\Oracle\Java\javapath;<br>这是java1.8安装的时候默认自带的。</p><p>对于项目来说，直接右键jre包，切换编译环境就好了</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——12.Q&amp;A Notice1</title>
    <link href="/2022/07/01/BigData&amp;Linux/%E6%B3%A8%E6%84%8Fnotices/"/>
    <url>/2022/07/01/BigData&amp;Linux/%E6%B3%A8%E6%84%8Fnotices/</url>
    
    <content type="html"><![CDATA[<h1 id="大数据-注意小细节"><a href="#大数据-注意小细节" class="headerlink" title="大数据 注意小细节"></a>大数据 注意小细节</h1><h2 id="关于IP的配置问题"><a href="#关于IP的配置问题" class="headerlink" title="关于IP的配置问题"></a>关于IP的配置问题</h2><p>网络配置文件 &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts内</p><p>需要先查看网络配置的接口是多少再行修改</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">ifconfig</span><br></code></pre></td></tr></table></figure><p>网络接口内的配置</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">ONBOOT</span>=<span class="hljs-literal">yes</span><br><span class="hljs-attr">NM_CONTROLLED</span>=<span class="hljs-literal">yes</span><br><span class="hljs-attr">BOOTPROTO</span>=static<br><span class="hljs-attr">IPADDR</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">174.100</span>   &lt;看自己的NET的网段&gt;<br><span class="hljs-attr">NETMASK</span> = <span class="hljs-number">255.255</span>.<span class="hljs-number">255.0</span><br><span class="hljs-attr">GATEWAY</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">174.1</span>  &lt;看NET设置的网关地址 一般是.<span class="hljs-number">1</span>或者是.<span class="hljs-number">255</span>&gt;<br><span class="hljs-attr">DNS1</span>=<span class="hljs-number">114.114</span>.<span class="hljs-number">114.114</span><br></code></pre></td></tr></table></figure><p>网络服务操作：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">service network restart<span class="hljs-regexp">/start/</span>stop<span class="hljs-regexp">/reload/</span>status<br></code></pre></td></tr></table></figure><p>关闭防火墙：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">service iptables stop<br></code></pre></td></tr></table></figure><p>关闭自启动防火墙：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">chkconfig</span> iptables <span class="hljs-literal">off</span><br></code></pre></td></tr></table></figure><p>设置时间同步 </p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-built_in">date</span> -s <span class="hljs-string">&quot;2021-09-23 11:26:35&quot;</span><br></code></pre></td></tr></table></figure><p>环境变量一般都放在</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/etc/</span>profile<br></code></pre></td></tr></table></figure><h2 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h2><p>hdfs端口是50070<br>hdfs中输入的路径需要是绝对路径</p><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs dos">hadoop <span class="hljs-built_in">fs</span> -xx <br>hdfs dfs -xx<br></code></pre></td></tr></table></figure><p>查看hdfs目录：如查看根目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">ls</span> /<br></code></pre></td></tr></table></figure><p>创建目录命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">mkdir</span> /data<br></code></pre></td></tr></table></figure><p>删除目录或者文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">rm</span> -r /data<br></code></pre></td></tr></table></figure><p>hdfs 将linux本地文件上传到hdfs 的路径下</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-title">hadoop</span> fs -put dianxin_data.csv /<span class="hljs-class"><span class="hljs-keyword">data</span></span><br></code></pre></td></tr></table></figure><p>hdfs 将文件下载到本地。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">hadoop fs -get <span class="hljs-regexp">/data/</span>dianxin_data.csv <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/soft/</span><br></code></pre></td></tr></table></figure><p>hdfs 移动一个文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">mv</span> <br></code></pre></td></tr></table></figure><p>hdfs 查看文件内容 ：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">hadooop fs -cat <span class="hljs-regexp">/data/</span>dianxin_data.csv<br></code></pre></td></tr></table></figure><h2 id="Zookeeper配置"><a href="#Zookeeper配置" class="headerlink" title="Zookeeper配置"></a>Zookeeper配置</h2><p>zookeeper的启动 每台子机都要启动 客户端就在一台就可以了<br>已经设置了zookeeper自动处理日志 避免日志过大 同样也可以使用zkCleanup.sh脚本处理<br>如果子节点开不起来 删除data和log文件下的日志和数据</p><h2 id="Hbase配置"><a href="#Hbase配置" class="headerlink" title="Hbase配置"></a>Hbase配置</h2><p>关闭安全模式 </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">hadoop dfsadmin -safemode leave</span><br></code></pre></td></tr></table></figure><p>Hbase的端口是60010</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—李沐课程小记</title>
    <link href="/2022/07/01/DeepLearning/For%20%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/07/01/DeepLearning/For%20%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="For-动手学深度学习"><a href="#For-动手学深度学习" class="headerlink" title="For 动手学深度学习"></a>For 动手学深度学习</h1><p>物体检测和分割 github.com&#x2F;matterport&#x2F;Mask_RCNN</p><p>样式迁移  github.com&#x2F;zhanghang1989&#x2F;MXNET-Gluon-Style-Transfer</p><p>文字生成图片 openai.com&#x2F;blog&#x2F;dall-e</p><p>网络的输入：[Batch, Channels, Height, Width]</p><p><strong>深度学习&#x3D;表示学习+浅层学习</strong></p><p><img src="/%5Csrc%5Cimage-20220303134155078.png" alt="image-20220303134155078"></p><p>rnn的数据类型是（num_steps,  batch_size, vocab_size）<br>而Encoder的数据类型是（Batch_size, num_steps, vocab_size）<br>所以在Encoder中调用rnn的时候需要对数据进行permute</p><blockquote><p>在<code>nn.Sequential</code>的实例被函数<code>torch.jit.script</code>脚本化后，通过使用符号式编程提高了计算性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">net = get_net()<br><span class="hljs-keyword">with</span> Benchmark(<span class="hljs-string">&#x27;无torchscript&#x27;</span>):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>): net(x)<br><br>net = torch.jit.script(net)<br><span class="hljs-keyword">with</span> Benchmark(<span class="hljs-string">&#x27;有torchscript&#x27;</span>):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>): net(x)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5Cimage-20220310095053692.png" alt="image-20220310095053692"></p></blockquote><p>torch.cuda.synchronize()等待当前设备上所有流中的所有核心完成。 （通常用于异步）<br>��</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—Geo小记</title>
    <link href="/2022/07/01/DeepLearning/Geo/"/>
    <url>/2022/07/01/DeepLearning/Geo/</url>
    
    <content type="html"><![CDATA[<p>对于图结构来说，<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data">数据格式</a>为</p><p><img src="/src/image-20220710084309263.png" alt="image-20220710084309263"></p><p><code>x=[34,34]</code> 指的是样本的数量与维度大小，也就是样本的shape</p><p><code>edge_index</code>指的是一定是[2, n]。 是一个source与target的连接，n指的是边的个数，是稀疏的边，并不是密集的</p><p><code>y</code>根据任务不同(可能是图级别任务，也有可能是节点级任务)，然后就会有不同的任意大小。这里是34， 因为是node classification，所以是节点的数量。��</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—RNN小记</title>
    <link href="/2022/07/01/DeepLearning/RNN/"/>
    <url>/2022/07/01/DeepLearning/RNN/</url>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>我们介绍了$n$元语法模型，其中单词$x_t$在时间步$t$的条件概率仅取决于前面$n-1$个单词。对于时间步$t-(n-1)$之前的单词，如果我们想将其可能产生的影响合并到$x_t$上，需要增加$n$，然而模型参数的数量也会随之呈指数增长，因为词表$\mathcal{V}$需要存储$|\mathcal{V}|^n$个数字，因此与其将$P(x_t \mid x_{t-1}, \ldots, x_{t-n+1})$模型化，不如使用隐变量模型：</p><p>$$P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1}),$$</p><p>其中$h_{t-1}$是<em>隐状态</em>（hidden state），也称为<em>隐藏变量</em>（hidden variable），它存储了到时间步$t-1$的序列信息。<br>通常，我们可以基于当前输入$x_{t}$和先前隐状态$h_{t-1}$来计算时间步$t$处的任何时间的隐状态：</p><p>$$h_t &#x3D; f(x_{t}, h_{t-1}).$$</p><p>对于<code>eq_ht_xt</code>中的函数$f$，隐变量模型不是近似值。毕竟$h_t$是可以仅仅存储到目前为止观察到的所有数据，然而这样的操作可能会使计算和存储的代价都变得昂贵。</p><p>回想一下，我们在感知机中讨论过的具有隐藏单元的隐藏层。值得注意的是，隐藏层和隐状态指的是两个截然不同的概念。如上所述，隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的<em>输入</em>，并且这些状态只能通过先前时间步的数据来计算。</p><p><em>循环神经网络</em>（recurrent neural networks，RNNs）是具有隐状态的神经网络。在介绍循环神经网络模型之前，<br>我们首先回顾多层感知机模型。</p><h2 id="无隐状态的神经网络"><a href="#无隐状态的神经网络" class="headerlink" title="无隐状态的神经网络"></a>无隐状态的神经网络</h2><p>让我们来看一看只有单隐藏层的多层感知机。设隐藏层的激活函数为$\phi$，给定一个小批量样本$\mathbf{X} \in \mathbb{R}^{n \times d}$，其中批量大小为$n$，输入维度为$d$，则隐藏层的输出$\mathbf{H} \in \mathbb{R}^{n \times h}$通过下式计算：</p><p>$$\mathbf{H} &#x3D; \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}<em>h).$$<br>我们拥有的隐藏层权重参数为$\mathbf{W}</em>{xh} \in \mathbb{R}^{d \times h}$，偏置参数为$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及隐藏单元的数目为$h$。<br>因此求和时可以应用广播机制（见 :numref:<code>subsec_broadcasting</code>）。接下来，将隐藏变量$\mathbf{H}$用作输出层的输入。<br>输出层由下式给出：</p><p>$$\mathbf{O} &#x3D; \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,$$</p><p>其中，$\mathbf{O} \in \mathbb{R}^{n \times q}$是输出变量，$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$是权重参数，$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的偏置参数。如果是分类问题，我们可以用$\text{softmax}(\mathbf{O})$来计算输出类别的概率分布。</p><p>只要可以随机选择“特征-标签”对，并且通过自动微分和随机梯度下降能够学习网络参数就可以了。</p><h2 id="有隐状态的循环神经网络"><a href="#有隐状态的循环神经网络" class="headerlink" title="有隐状态的循环神经网络"></a>有隐状态的循环神经网络</h2><p>有了隐状态后，情况就完全不同了。假设我们在时间步$t$有小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$。换言之，对于$n$个序列样本的小批量，$\mathbf{X}<em>t$的每一行对应于来自该序列的时间步$t$处的一个样本。接下来，用$\mathbf{H}<em>t  \in \mathbb{R}^{n \times h}$表示时间步$t$的隐藏变量。与多层感知机不同的是，我们在这里保存了前一个时间步的隐藏变量$\mathbf{H}</em>{t-1}$，并引入了一个新的权重参数$\mathbf{W}</em>{hh} \in \mathbb{R}^{h \times h}$，来描述如何在当前时间步中使用前一个时间步的隐藏变量。具体地说，当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：</p><p>$$\mathbf{H}<em>t &#x3D; \phi(\mathbf{X}<em>t \mathbf{W}</em>{xh} + \mathbf{H}</em>{t-1} \mathbf{W}<em>{hh}  + \mathbf{b}<em>h).$$与上式多了$\mathbf{H}</em>{t-1} \mathbf{W}</em>{hh}$，</p><p>从相邻时间步的隐藏变量$\mathbf{H}<em>t$和$\mathbf{H}</em>{t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为<em>隐状态</em>（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，因此<code>rnn_h_with_state</code>的计算是<em>循环的</em>（recurrent）。<br>于是基于循环计算的隐状态神经网络被命名为<em>循环神经网络</em>（recurrent neural network）。<br>在循环神经网络中执行<code>rnn_h_with_state</code>计算的层称为<em>循环层</em>（recurrent layer）。</p><p>有许多不同的方法可以构建循环神经网络，由 <code>rnn_h_with_state</code>定义的隐状态的循环神经网络是非常常见的一种。对于时间步$t$，输出层的输出类似于多层感知机中的计算：</p><p>$$\mathbf{O}_t &#x3D; \mathbf{H}<em>t \mathbf{W}</em>{hq} + \mathbf{b}_q.$$</p><p>循环神经网络的参数包括隐藏层的权重$\mathbf{W}<em>{xh} \in \mathbb{R}^{d \times h}, \mathbf{W}</em>{hh} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}<em>h \in \mathbb{R}^{1 \times h}$，以及输出层的权重$\mathbf{W}</em>{hq} \in \mathbb{R}^{h \times q}$<br>和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$。值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。</p><p><code>fig_rnn</code>展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步$t$，隐状态的计算可以被视为：</p><ol><li>拼接当前时间步$t$的输入$\mathbf{X}<em>t$和前一时间步$t-1$的隐状态$\mathbf{H}</em>{t-1}$；</li><li>将拼接的结果送入带有激活函数$\phi$的全连接层。<br>全连接层的输出是当前时间步$t$的隐状态$\mathbf{H}_t$。</li></ol><p>在本例中，模型参数是$\mathbf{W}<em>{xh}$和$\mathbf{W}</em>{hh}$的拼接，以及$\mathbf{b}_h$的偏置，所有这些参数都来自<code>rnn_h_with_state</code>。<br>当前时间步$t$的隐状态$\mathbf{H}<em>t$将参与计算下一时间步$t+1$的隐状态$\mathbf{H}</em>{t+1}$。而且$\mathbf{H}_t$还将送入全连接输出层，用于计算当前时间步$t$的输出$\mathbf{O}_t$。</p><p>我们刚才提到，隐状态中$\mathbf{X}<em>t \mathbf{W}</em>{xh} + \mathbf{H}<em>{t-1} \mathbf{W}</em>{hh}$的计算，相当于$\mathbf{X}<em>t$和$\mathbf{H}</em>{t-1}$的拼接<br>与$\mathbf{W}<em>{xh}$和$\mathbf{W}</em>{hh}$的拼接的矩阵乘法。虽然这个性质可以通过数学证明，但在下面我们使用一个简单的代码来说明一下。<br>首先，我们定义矩阵<code>X</code>、<code>W_xh</code>、<code>H</code>和<code>W_hh</code>，它们的形状分别为$(3，1)$、$(1，4)$、$(3，4)$和$(4，4)$。分别将<code>X</code>乘以<code>W_xh</code>，将<code>H</code>乘以<code>W_hh</code>，然后将这两个乘法相加，我们得到一个形状为$(3，4)$的矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>X, W_xh = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">1</span>)), torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">4</span>))<br>H, W_hh = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br>torch.matmul(X, W_xh) + torch.matmul(H, W_hh)<br></code></pre></td></tr></table></figure><p>现在，我们沿列（轴1）拼接矩阵<code>X</code>和<code>H</code>，沿行（轴0）拼接矩阵<code>W_xh</code>和<code>W_hh</code>。这两个拼接分别产生形状$(3, 5)$和形状$(5, 4)$的矩阵。再将这两个拼接的矩阵相乘，我们得到与上面相同形状$(3, 4)$的输出矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.matmul(torch.cat((X, H), <span class="hljs-number">1</span>), torch.cat((W_xh, W_hh), <span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><h2 id="基于循环神经网络的字符级语言模型"><a href="#基于循环神经网络的字符级语言模型" class="headerlink" title="基于循环神经网络的字符级语言模型"></a>基于循环神经网络的字符级语言模型</h2><p>回想一下<code>sec_language_model</code>中的语言模型，我们的目标是根据过去的和当前的词元预测下一个词元，因此我们将原始序列移位一个词元作为标签。Bengio等人首先提出使用神经网络进行语言建模<br>接下来，我们看一下如何使用循环神经网络来构建语言模型。设小批量大小为1，批量中的那个文本序列为“machine”。为了简化后续部分的训练，我们考虑使用<em>字符级语言模型</em>（character-level language model），将文本词元化为字符而不是单词。<code>fig_rnn_train</code>演示了如何通过基于字符级语言建模的循环神经网络，使用当前的和先前的字符预测下一个字符。</p><p>在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。由于隐藏层中隐状态的循环计算，<code>fig_rnn_train</code>中的第$3$个时间步的输出$\mathbf{O}_3$由文本序列“m”、“a”和“c”确定。由于训练数据中这个文本序列的下一个字符是“h”，因此第$3$个时间步的损失将取决于下一个字符的概率分布，而下一个字符是基于特征序列“m”、“a”、“c”和这个时间步的标签“h”生成的。</p><p>在实践中，我们使用的批量大小为$n&gt;1$，每个词元都由一个$d$维向量表示。因此，在时间步$t$输入$\mathbf X_t$将是一个$n\times d$矩阵，这与我们在<code>subsec_rnn_w_hidden_states</code>中的讨论相同。</p><h2 id="困惑度（Perplexity）"><a href="#困惑度（Perplexity）" class="headerlink" title="困惑度（Perplexity）"></a>困惑度（Perplexity）</h2><p>最后，让我们讨论如何度量语言模型的质量，<br>这将在后续部分中用于评估基于循环神经网络的模型。<br>一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。<br>考虑一下由不同的语言模型给出的对“It is raining .”（“.下雨了”）的续写：</p><ol><li>“It is raining outside”（外面下雨了）</li><li>“It is raining banana tree”（香蕉树下雨了）</li><li>“It is raining piouw;kcj pwepoiut”（piouw;kcj pwepoiut下雨了）</li></ol><p>就质量而言，例$1$显然是最合乎情理、在逻辑上最连贯的。虽然这个模型可能没有很准确地反映出后续词的语义，<br>比如，“It is raining in San Francisco”（旧金山下雨了）<br>和“It is raining in winter”（冬天下雨了）</p><p>可能才是更完美的合理扩展，但该模型已经能够捕捉到跟在后面的是哪类单词。<br>例$2$则要糟糕得多，因为其产生了一个无意义的续写。尽管如此，至少该模型已经学会了如何拼写单词，以及单词之间的某种程度的相关性。最后，例$3$表明了训练不足的模型是无法正确地拟合数据的。</p><p>我们可以通过计算序列的似然概率来度量模型的质量。然而这是一个难以理解、难以比较的数字。毕竟，较短的序列比较长的序列更有可能出现，因此评估模型产生托尔斯泰的巨著《战争与和平》的可能性不可避免地会比产生圣埃克苏佩里的中篇小说《小王子》可能性要小得多。而缺少的可能性值相当于平均数。</p><p>在这里，信息论可以派上用场了。我们在引入softmax回归。如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能让我们更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：</p><p>$$\frac{1}{n} \sum_{t&#x3D;1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),$$</p><p>其中$P$由语言模型给出，$x_t$是在时间步$t$从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了可比性。<br>由于历史原因，自然语言处理的科学家更喜欢使用一个叫做<em>困惑度</em>（perplexity）的量。简而言之，它是 <code>eq_avg_ce_for_lm</code>的指数：</p><p>$$\exp\left(-\frac{1}{n} \sum_{t&#x3D;1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).$$</p><p>困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。<br>我们看看一些案例：</p><ul><li>在最好的情况下，模型总是完美地估计标签词元的概率为1。<br>在这种情况下，模型的困惑度为1。</li><li>在最坏的情况下，模型总是预测标签词元的概率为0。<br>在这种情况下，困惑度是正无穷大。</li><li>在基线上，该模型的预测是词表的所有可用词元上的均匀分布。<br>在这种情况下，困惑度等于词表中唯一词元的数量。<br>事实上，如果我们在没有任何压缩的情况下存储序列，<br>这将是我们能做的最好的编码方式。<br>因此，这种方式提供了一个重要的上限，<br>而任何实际模型都必须超越这个上限。</li></ul><p>在接下来的小节中，我们将基于循环神经网络实现字符级语言模型，并使用困惑度来评估这样的模型。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>对隐状态使用循环计算的神经网络称为循环神经网络（RNN）。</li><li>循环神经网络的隐状态可以捕获直到当前时间步序列的历史信息。</li><li>循环神经网络模型的参数数量不会随着时间步的增加而增加。</li><li>我们可以使用循环神经网络创建字符级语言模型。</li><li>我们可以使用困惑度来评价语言模型的质量。</li></ul><p>模型参数大小计算：</p><p>Y的大小 &#x3D; 【X.shape[0] * X.shape[1],   len(vocab)】</p><p>W的大小 &#x3D; 【X.shape[0],  num_hiddens】</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—torch中nn与F的区别</title>
    <link href="/2022/07/01/DeepLearning/module%E5%92%8Cfunctional%E7%9A%84%E8%BE%A8%E6%9E%90/"/>
    <url>/2022/07/01/DeepLearning/module%E5%92%8Cfunctional%E7%9A%84%E8%BE%A8%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="nn-与-nn-functional的区别"><a href="#nn-与-nn-functional的区别" class="headerlink" title="nn 与 nn.functional的区别"></a>nn 与 nn.functional的区别</h1><p>两者的相同之处：</p><ul><li><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout，。。。。。； </li><li>运行效率也是近乎相同。</li></ul><p><code>nn.functional.xxx</code>是函数接口，而<code>nn.Xxx</code>是<code>nn.functional.xxx</code>的类封装，并且**<code>nn.Xxx</code>都继承于一个共同祖先<code>nn.Module</code>。**这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict </code>等。</p><p>两者的差别之处：</p><ul><li><strong>两者的调用方式不同。</strong></li></ul><p><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs = torch.rand(<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">244</span>, <span class="hljs-number">244</span>)<br>conv = nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>out = conv(inputs)<br></code></pre></td></tr></table></figure><p><code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">weight = torch.rand(<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br>bias = torch.rand(<span class="hljs-number">64</span>) <br>out = nn.functional.conv2d(inputs, weight, bias, padding=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><ul><li><strong><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">fm_layer = nn.Sequential(<br>           nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>           nn.BatchNorm2d(num_features=<span class="hljs-number">64</span>),<br>           nn.ReLU(),<br>           nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>),<br>           nn.Dropout(<span class="hljs-number">0.2</span>)<br> )<br></code></pre></td></tr></table></figure><ul><li><strong><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</strong></li></ul><p>使用<code>nn.Xxx</code>定义一个CNN 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module):<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CNN, self).__init__()<br><br>        self.cnn1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,  out_channels=<span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">0</span>)<br>        self.relu1 = nn.ReLU()<br>        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>)<br><br>        self.cnn2 = nn.Conv2d(in_channels=<span class="hljs-number">16</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>,  padding=<span class="hljs-number">0</span>)<br>        self.relu2 = nn.ReLU()<br>        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>)<br><br>        self.linear1 = nn.Linear(<span class="hljs-number">4</span> * <span class="hljs-number">4</span> * <span class="hljs-number">32</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.maxpool1(self.relu1(self.cnn1(x)))<br>        out = self.maxpool2(self.relu2(self.cnn2(out)))<br>        out = self.linear1(out.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>使用<code>nn.function.xxx</code>定义一个与上面相同的CNN。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module):<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CNN, self).__init__()<br><br>        self.cnn1_weight = nn.Parameter(torch.rand(<span class="hljs-number">16</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>        self.bias1_weight = nn.Parameter(torch.rand(<span class="hljs-number">16</span>))<br><br>        self.cnn2_weight = nn.Parameter(torch.rand(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>        self.bias2_weight = nn.Parameter(torch.rand(<span class="hljs-number">32</span>))<br><br>        self.linear1_weight = nn.Parameter(torch.rand(<span class="hljs-number">4</span> * <span class="hljs-number">4</span> * <span class="hljs-number">32</span>, <span class="hljs-number">10</span>))<br>        self.bias3_weight = nn.Parameter(torch.rand(<span class="hljs-number">10</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = F.conv2d(x, self.cnn1_weight, self.bias1_weight)<br>        out = F.relu(out)<br>        out = F.max_pool2d(out)<br><br>        out = F.conv2d(x, self.cnn2_weight, self.bias2_weight)<br>        out = F.relu(out)<br>        out = F.max_pool2d(out)<br><br>        out = F.linear(x, self.linear1_weight, self.bias3_weight)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>PyTorch官方推荐：具有学习参数的（例如，conv2d, linear, batch_norm)采用<code>nn.Xxx</code>方式，没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用<code>nn.functional.xxx</code>或者<code>nn.Xxx</code>方式。但关于dropout，个人强烈推荐使用<code>nn.Xxx</code>方式，因为一般情况下只有训练阶段才进行dropout，在eval阶段都不会进行dropout。使用<code>nn.Xxx</code>方式定义dropout，在调用<code>model.eval()</code>之后，model中所有的dropout layer都关闭，但以<code>nn.function.dropout</code>方式定义dropout，在调用<code>model.eval()</code>之后并不能关闭dropout。</p><p>在<code>nn.Xxx</code>不能满足你的功能需求时，<code>nn.functional.xxx</code>是更佳的选择，因为<code>nn.functional.xxx</code>更加的灵活(更加接近底层），你可以在其基础上定义出自己想要的功能。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—tensor基本概念  torch版</title>
    <link href="/2022/07/01/DeepLearning/tensor%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"/>
    <url>/2022/07/01/DeepLearning/tensor%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="Tensor的概念"><a href="#Tensor的概念" class="headerlink" title="Tensor的概念"></a>Tensor的概念</h1><p>说起张量（tensor）就不得不说他和scalar、vertor、matrix之间的关系了，直接上图：</p><p><img src="/%5Csrc%5C2274220-20211212114510782-517507267.png" alt="img"></p><blockquote><p>标量（scalar）：只有大小概念，没有方向的概念。通过一个具体的数值就能表达完整。比如：重量、温度、长度、提及、时间、热量等都数据标量。</p><p>向量（vector）：物理学上也叫矢量，指由大小和方向共同决定的量（跟「标量」相区别）。向量主要有2个维度：大小、方向。比如：力、速度等<em>。</em></p><p>矩阵（matrix）：（学过线性代数的都知道，可参见之前的<a href="https://www.cnblogs.com/young978/p/15669123.html#_label1">线代笔记</a>）</p><p>张量（tensor）：一个<strong>多维数组</strong>，它是标量、向量、矩阵的高位扩展 </p></blockquote><p>当下主流的深度网络学习框架是PyTorch和Tensorflow。</p><p>对于这两个框架而言，我只想用一句代码来阐述我心中的杂乱： </p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs coq"><span class="hljs-keyword">Import</span> torch <span class="hljs-built_in">as</span> tf               <br></code></pre></td></tr></table></figure><p>但是，我还是选择了PyTorch（ ^<em>^ &#x2F;\ ~</em>~ ）</p><p>tensor,是PyTorch中最基础的数据类型,也是进行数据存储和运算的基本单元。</p><p>数组array这个概念，数组是类似于列表的高阶对象，是有序的元素序列。</p><p><img src="/%5Csrc%5C2274220-20211212121653659-1556888048.png" alt="img"></p><p>Tensor在PyTorch的地位相当于Array在Numpy中地位。</p><p>实质上Pytorch将Numpy的Array包装成Tensor，为其定义了各种各样的运算方法和函数</p><p>以至于处理tensor的时候，觉得自己在调用Numpy和Pandas这两个包处理。</p><h1 id="Tensor的属性"><a href="#Tensor的属性" class="headerlink" title="Tensor的属性"></a>Tensor的属性</h1><p>每一个tensor都有三个属性：<code>torch.dtype，``torch.device，</code>和<code>torch.layout</code>.</p><h2 id="torch-dtype"><a href="#torch-dtype" class="headerlink" title="torch.dtype"></a>torch.dtype</h2><p>Pytorch拥有12个不同的数据类型。</p><p><img src="/%5Csrc%5C2274220-20211212124655518-1573408127.png" alt="img"></p><p> tensor类型 相关代码</p><blockquote><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">float_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.<span class="hljs-built_in">float</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">double_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.double)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">complex_float_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.complex64)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">complex_double_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.complex128)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">int_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.<span class="hljs-built_in">int</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">long_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.long)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">uint_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.uint8)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">double_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.double)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">bool_tensor = torch.ones(<span class="hljs-number">1</span>, dtype=torch.<span class="hljs-built_in">bool</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">long_zerodim = torch.tensor(<span class="hljs-number">1</span>, dtype=torch.long)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">int_zerodim = torch.tensor(<span class="hljs-number">1</span>, dtype=torch.<span class="hljs-built_in">int</span>)</span><br><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">torch.add(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>).dtype</span><br>torch.int64<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">(int_tensor + <span class="hljs-number">5</span>).dtype</span><br>torch.int32<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">(int_tensor + long_zerodim).dtype</span><br>torch.int32<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">(long_tensor + int_tensor).dtype</span><br>torch.int64<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">(bool_tensor + long_tensor).dtype</span><br>torch.int64<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">(bool_tensor + uint_tensor).dtype</span><br>torch.uint8<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">(float_tensor + double_tensor).dtype</span><br>torch.float64<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">(complex_float_tensor + complex_double_tensor).dtype</span><br>torch.complex128<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">(bool_tensor + int_tensor).dtype</span><br>torch.int32<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">torch.add(long_tensor, float_tensor).dtype</span><br>torch.float32<br></code></pre></td></tr></table></figure></blockquote><h2 id="torch-device"><a href="#torch-device" class="headerlink" title="torch.device"></a>torch.device</h2><p> 由于pytorch可以在Gpu上运行tensor的相关操作。</p><p>torch.device是一个对象，表示正在或将要分配 torch.tensor的设备。</p><p>torch.device一半可选的内容有 cpu，cuda或者一些设备类型的可选设备序号</p><blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros">&gt;&gt;&gt; torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<br>device(<span class="hljs-attribute">type</span>=<span class="hljs-string">&#x27;cuda&#x27;</span>, <span class="hljs-attribute">index</span>=0)<br><br>&gt;&gt;&gt; torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br>device(<span class="hljs-attribute">type</span>=<span class="hljs-string">&#x27;cpu&#x27;</span>)<br><br>&gt;&gt;&gt; torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)  # current cuda device<br>device(<span class="hljs-attribute">type</span>=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>&gt;&gt;&gt; torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>, 0)<br>device(<span class="hljs-attribute">type</span>=<span class="hljs-string">&#x27;cuda&#x27;</span>, <span class="hljs-attribute">index</span>=0)<br><br>&gt;&gt;&gt; torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>, 0)<br>device(<span class="hljs-attribute">type</span>=<span class="hljs-string">&#x27;cpu&#x27;</span>, <span class="hljs-attribute">index</span>=0)<br></code></pre></td></tr></table></figure></blockquote><h2 id="torch-layout"><a href="#torch-layout" class="headerlink" title="torch.layout"></a><strong>torch.layout</strong></h2><p>torch.layout 是表示 torch.tensor 的内存布局的对象。目前，torch.tensor支持torch.strided（密集张量）和sparse_coo（稀疏的COO张量）。</p><p><img src="/%5Csrc%5C2274220-20211212211034421-51741505.png" alt="img"></p><p>stride()方法其实指的就是，tensor每个维度变化1在实际物理存储空间变化的大小。<img src="/%5Csrc%5Cloading.gif" alt="img"></p><blockquote><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">x = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]])</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">x.stride()</span><br>(5, 1)<br><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">x.t().stride()</span><br>(1, 5)<br></code></pre></td></tr></table></figure></blockquote><h1 id="Tensor的创建与访问"><a href="#Tensor的创建与访问" class="headerlink" title="Tensor的创建与访问"></a>Tensor的创建与访问</h1><h2 id="Tensor的创建方式"><a href="#Tensor的创建方式" class="headerlink" title="Tensor的创建方式"></a>Tensor的创建方式</h2><ul><li>可以根据数据、元组、NumPy的ndarray、标量和其他类型，来构建tensor。方法有：torch.tensor()、torch.ones()、torch.zeros()、torch.from_array().</li><li>也可以从一些分布中采样构建新的张量：torch.arange()、torch.range()、torch.rand()、torch.randn()、torch.normal().</li><li>当然也有一些比较特殊的搭建：torch.eye()用于构建一个对角矩阵</li></ul><h2 id="Tensor的访问方式"><a href="#Tensor的访问方式" class="headerlink" title="Tensor的访问方式"></a>Tensor的访问方式</h2><ul><li>当tensor是标量或者一维数据的时候，可以使用torch.item()访问</li><li>当tensor是多维数据的时候，可以使用索引的方式，用法类似于a[:,0,0]</li><li>当然我们也可以使用掩膜的方式，固定的访问一些我们想要的数据。</li></ul><blockquote><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">X</span> = torch.randn((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))<br><span class="hljs-attr">mask</span> = torch.randn((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))&gt;<span class="hljs-number">0</span><br><span class="hljs-attr">X1</span> = X[mask]<br><span class="hljs-attr">X2</span> = torch.masked_select(X,mask)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5C2274220-20211212220038950-808356639.png" alt="img"></p></blockquote><ul><li>还有条件选择这种不错的方式，也是一个值得推荐的方式。</li></ul><blockquote><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">x</span> = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><span class="hljs-attr">y</span> = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><span class="hljs-attr">z</span> = torch.where(x&gt;y,x,y)<br></code></pre></td></tr></table></figure><p><img src="/%5Csrc%5C2274220-20211212220334767-1100716490.png" alt="img"></p></blockquote><h1 id="Tensor的简单操作"><a href="#Tensor的简单操作" class="headerlink" title="Tensor的简单操作"></a>Tensor的简单操作</h1><h2 id="常用数学运算"><a href="#常用数学运算" class="headerlink" title="常用数学运算"></a>常用数学运算</h2><ul><li><p>torch.add()：加法</p></li><li><p>torch.mul()：乘法</p></li><li><p>torch.div()：除法</p></li><li><p>torch.abs()：tensor内每个元素取绝对值</p></li><li><p>torch.round()：tensor内每个元素取整数部分</p></li><li><p>torch.frac()：tensor内每个元素取小数部分</p></li><li><p>torch.log()：tensor内每个元素取对数</p></li><li><p>torch.pow()：tensor内每个元素取幂函数</p></li><li><p>torch.exp()：tensor内每个元素取指数</p></li><li><p>torch.sigmoid()：tensor内每个元素取sigmoid函数值</p></li><li><p>torch.mean()：tensor所有元素的均值</p></li><li><p>torch.norm()：tensor所有元素的范数值</p></li><li><p>torch.prod()：tensor所有元素积</p></li><li><p>torch.sum()：tensor所有元素和</p></li><li><p>torch.max()：tensor所有元素最大值</p></li><li><p>torch.min()：tensor所有元素最小值</p></li></ul><p>注意：例如add()这类函数一般都是会返回一直tensor值的，但是add_()这类加了下划线的方法是在tensor的原空间处理的，会覆盖之前的值</p><p>同样也可以是用以下方式直接处理</p><ol><li>加 tensor1 + tensor2；</li><li>减 tensor1 - tensor2；</li><li>乘 tensor1 * tensor2；</li><li>除 tensor1 &#x2F; tensor2；</li><li>内积 tensor1 @ tensor2；</li><li>幂运算 tensor1 ** n</li></ol><h2 id="线性代数运算"><a href="#线性代数运算" class="headerlink" title="线性代数运算"></a>线性代数运算</h2><ul><li>torch.dot()：向量内积运算</li><li>torch.mv()：矩阵与向量的乘法</li><li>torch.mm()：矩阵乘法</li><li>torch.eig()：方阵的特征值和特征向量</li><li>torch.inverse()：方阵的逆</li><li>torch.ger()：两个向量的张量积</li></ul><h2 id="Tensor连接、分片、变形"><a href="#Tensor连接、分片、变形" class="headerlink" title="Tensor连接、分片、变形"></a>Tensor连接、分片、变形</h2><ul><li>torch.cat()：多个tensor的拼接</li><li>torch.reshape()\torch.view()：返回一个张量，其数据和元素数量与输入相同，但具有指定的形状  对于标量来说x.numel()</li><li>torch.transpose()\torch.t()：指定tensor的两个维度进行转置，torch.t()方法只适用与二维tensor</li><li>torch.squeeze()&#x2F;unsqueeze()：tensor对于张量中大小为1的维度的压缩与扩张</li><li>torch.permute()：返回维度排列后的原始张量输入的视图。</li></ul><p>说到计算就不得不说，tensor 的一种计算机制，<strong>BroadCasting</strong>。在一定条件下，他会使维度不同、维度大小同的tensor完成数学计算</p><p>以A+B为例，Broadcasting的过程如下： 从A,B的最后一个维度开始匹配长度，如果该维度长度一致，则匹配前一个维度。如果某个维度的长度不一致，并且两者长度都&gt;1，那么维度不匹配，无法进行运算，如果有一个长度维1，那么就对该维度进行扩展（广播）,扩展后的两个维度长度一致。依次类推直到第一个维度。如果两个张量维度不同，则在第0维进行升维并做广播。</p><p>粗略的来说，就是利用squeeze()方法使低维的数据升到高维的数据，再利用cat()方法拼接成同样的大小。（前提是，该维度长度为1，或者没有维度才可以）</p><p>下面这张图就可以很清晰的表达BroadCasting的效果。</p><p> <img src="/%5Csrc%5C2274220-20211213161318232-1359331427.png" alt="img"></p><p>范数调用torch.norm(input, p&#x3D;’fro’, dim&#x3D;None, keepdim&#x3D;False, out&#x3D;None, dtype&#x3D;None)<br>$$<br>p&#x3D;1 \to L_1 \ p&#x3D;2 \to L_2 \ P&#x3D;inf \to L_\infty<br>$$<br>向量范数 默认二范数  矩阵范数 默认F范数范数 默认F范数</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—torch使用小记</title>
    <link href="/2022/07/01/DeepLearning/torch/"/>
    <url>/2022/07/01/DeepLearning/torch/</url>
    
    <content type="html"><![CDATA[<p>使用<code>x=x+y</code>会增加内存的开销，如果不是必要的话，可以使用<code>x+=y</code>或者<code>x[:]=x+y</code>来减少内存的消耗</p><p><code>B = A.clone()</code>  # 通过分配新内存，将A的一个副本分配给</p><p>通过调用<code>max</code> 函数中<code>keepdims=True</code>参数，可以让输出的tensor保持input的shape</p><p>每个计算图进行一次backward之后，各个节点的值会清除，这样直接进行第二次backward会报错，如果在第一次backward加上retain_graph&#x3D;&#x3D;True后,可以再来一次backward。</p><p>当<code>y</code>不是标量时，向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵。对于高阶和高维的<code>y</code>和<code>x</code>，求导的结果可以是一个高阶张量。然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中），但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 所以我们需要使用<code>y.sum().backward()</code></p><p>torch.utils.data.TensorDataset 用于对tensor进行打包， 类似于zip一样的操作。</p><p>torch.utils.data.DataLoader就是用来包装所使用的数据，每次抛出一批数据</p><blockquote><p><strong>num_workers通过影响数据加载速度，从而影响训练速度</strong></p><p>在调整num_workers的过程中，发现训练速度并没有变化</p><p>原因在于：</p><p>num_workers是加载数据（batch）的线程数目</p><p>当加载batch的时间 &lt; 数据训练的时间</p><p>　　GPU每次训练完都可以直接从CPU中取到next batch的数据</p><p>　　无需额外的等待，因此也不需要多余的worker，即使增加worker也不会影响训练速度</p><p>当加载batch的时间 &gt; 数据训练的时间</p><p>　　GPU每次训练完都需要等待CPU完成数据的载入</p><p>　　若增加worker，即使worker_1还未就绪，GPU也可以取worker_2的数据来训练</p><p>仅限单线程训练情况</p></blockquote><p>transforms.ToTensor() （1） transforms.ToTensor() 将numpy的ndarray或PIL.Image读的图片转换成形状为(C,H, W)的Tensor格式，且&#x2F;255归一化到[0,1.0]之间 （2）通道的具体顺序与cv2读的还是PIL.Image读的图片有关系 </p><p>torch.clamp()和np.clip的效果是一样的，都是把数值放缩到一定范围内</p><p>跨设备的运算会出现问题，在CPU上的数据只能与CPU上的数据进行运算， 同理，某GPU上的数据是能和同块GPU上的数据进行计算，也不能在另一块GPU上的数据进行运算</p><p>nn.AdaptiveAvgPool2d()相较于nn.AvgPool2d，强调在一个自适应的环节上， Avgpool2d需要自己去计算这个kernel的大小，来获取我们需要大小的output_featuremap 但是AdaptiveAvgpool2d则不需要，直接引入输出大小参数， 即可以获得想要的东西。</p><p>numpy、tensor中对于切片中None的用法：None 的作用就是在相应的位置上增加了一个维度，在这个维度上只有一个元素。假设 x.shape &#x3D;&#x3D; (a, b)，则</p><ul><li><code>(a, b)</code> &#x3D;&#x3D;&gt; <code>[None, :, :]</code> &#x3D;&#x3D;&gt; <code>(1, a, b)</code></li><li><code>(a, b)</code> &#x3D;&#x3D;&gt; <code>[:, None, :]</code> &#x3D;&#x3D;&gt; <code>(a, 1, b)</code></li><li><code>(a, b)</code> &#x3D;&#x3D;&gt; <code>[:, :, None]</code> &#x3D;&#x3D;&gt; <code>(a, b, 1)</code></li></ul><p>PyTorch中的<a href="https://so.csdn.net/so/search?q=repeat&spm=1001.2101.3001.7020">repeat</a>()函数可以对张量进行重复扩充。当参数只有两个时：（列的重复倍数，行的重复倍数）。1表示不重复。当参数有三个时：（通道数的重复倍数，列的重复倍数，行的重复倍数）。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—torch常用操作</title>
    <link href="/2022/07/01/DeepLearning/torch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <url>/2022/07/01/DeepLearning/torch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="1-基本配置"><a href="#1-基本配置" class="headerlink" title="1. 基本配置"></a>1. 基本配置</h1><h3 id="导入包和版本查询"><a href="#导入包和版本查询" class="headerlink" title="导入包和版本查询"></a>导入包和版本查询</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import torch<br>import torch<span class="hljs-selector-class">.nn</span> as nn<br>import torchvision<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.__version__)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.version.cuda)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.backends.cudnn.version()</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.cuda.get_device_name(<span class="hljs-number">0</span>)</span></span>)<br></code></pre></td></tr></table></figure><h3 id="可复现性"><a href="#可复现性" class="headerlink" title="可复现性"></a>可复现性</h3><p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">np<span class="hljs-selector-class">.random</span><span class="hljs-selector-class">.seed</span>(<span class="hljs-number">0</span>)<br>torch<span class="hljs-selector-class">.manual_seed</span>(<span class="hljs-number">0</span>)<br>torch<span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.manual_seed_all</span>(<span class="hljs-number">0</span>)<br><br>torch<span class="hljs-selector-class">.backends</span><span class="hljs-selector-class">.cudnn</span><span class="hljs-selector-class">.deterministic</span> = True<br>torch<span class="hljs-selector-class">.backends</span><span class="hljs-selector-class">.cudnn</span><span class="hljs-selector-class">.benchmark</span> = False<br></code></pre></td></tr></table></figure><h3 id="显卡设置"><a href="#显卡设置" class="headerlink" title="显卡设置"></a>显卡设置</h3><p>如果只需要一张显卡</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># Device configuration</span><br><span class="hljs-attr">device</span> = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> if torch.cuda.is_available() else <span class="hljs-string">&#x27;cpu&#x27;</span>)<br></code></pre></td></tr></table></figure><p>如果需要指定多张显卡，比如0，1号显卡。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import osos<span class="hljs-selector-class">.environ</span><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>]</span> = <span class="hljs-string">&#x27;0,1&#x27;</span><br></code></pre></td></tr></table></figure><p>也可以在命令行运行代码时设置显卡：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-number">0</span>,<span class="hljs-number">1</span> python train.py<br></code></pre></td></tr></table></figure><p>清除显存</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">torch<span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.empty_cache</span>()<br></code></pre></td></tr></table></figure><p>也可以使用在命令行重置GPU的指令</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">nvidia-smi  -gpu-reset -<span class="hljs-selector-tag">i</span> <span class="hljs-selector-attr">[gpu_id]</span><br></code></pre></td></tr></table></figure><h1 id="2-张量-Tensor-处理"><a href="#2-张量-Tensor-处理" class="headerlink" title="2. 张量(Tensor)处理"></a>2. 张量(Tensor)处理</h1><h3 id="张量的数据类型"><a href="#张量的数据类型" class="headerlink" title="张量的数据类型"></a>张量的数据类型</h3><p>PyTorch有9种CPU张量类型和9种GPU张量类型。</p><p><img src="F:\notes\深度学习\src\640.jpeg" alt="图片"></p><h3 id="张量基本信息"><a href="#张量基本信息" class="headerlink" title="张量基本信息"></a>张量基本信息</h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs fortran">tensor = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(tensor.<span class="hljs-keyword">type</span>())  # 数据类型<br><span class="hljs-built_in">print</span>(tensor.<span class="hljs-built_in">size</span>())  # 张量的<span class="hljs-built_in">shape</span>，是个元组<br><span class="hljs-built_in">print</span>(tensor.<span class="hljs-built_in">dim</span>())   # 维度的数量<br></code></pre></td></tr></table></figure><h3 id="命名张量"><a href="#命名张量" class="headerlink" title="命名张量"></a>命名张量</h3><p>张量命名是一个非常有用的方法，这样可以方便地使用维度的名字来做索引或其他操作，大大提高了可读性、易用性，防止出错。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 在PyTorch 1.3之前，需要使用注释</span><br><span class="hljs-comment"># Tensor[N, C, H, W]</span><br>images = torch.randn(32, 3, 56, 56)<br>images.sum(<span class="hljs-attribute">dim</span>=1)<br>images.select(<span class="hljs-attribute">dim</span>=1, <span class="hljs-attribute">index</span>=0)<br><span class="hljs-comment"># PyTorch 1.3之后</span><br>NCHW = [‘N’, ‘C’, ‘H’, ‘W’]<br>images = torch.randn(32, 3, 56, 56, <span class="hljs-attribute">names</span>=NCHW)<br>images.sum(<span class="hljs-string">&#x27;C&#x27;</span>)images.select(<span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-attribute">index</span>=0)<br><span class="hljs-comment"># 也可以这么设置</span><br>tensor = torch.rand(3,4,1,2,names=(<span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;N&#x27;</span>, <span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;W&#x27;</span>))<br><span class="hljs-comment"># 使用align_to可以对维度方便地排序</span><br>tensor = tensor.align_to(<span class="hljs-string">&#x27;N&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;W&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h3><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensortorch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="hljs-comment"># 类型转换</span><br><span class="hljs-attr">tensor</span> = tensor.cuda()<br><span class="hljs-attr">tensor</span> = tensor.cpu()<br><span class="hljs-attr">tensor</span> = tensor.float()<br><span class="hljs-attr">tensor</span> = tensor.long()<br></code></pre></td></tr></table></figure><h3 id="torch-Tensor与np-ndarray转换"><a href="#torch-Tensor与np-ndarray转换" class="headerlink" title="torch.Tensor与np.ndarray转换"></a><strong>torch.Tensor与np.ndarray转换</strong></h3><p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">ndarray</span> = tensor.cpu().numpy()<br><span class="hljs-attr">tensor</span> = torch.from_numpy(ndarray).float()<br><span class="hljs-attr">tensor</span> = torch.from_numpy(ndarray.copy()).float() <br><span class="hljs-comment"># If ndarray has negative stride.</span><br></code></pre></td></tr></table></figure><h3 id="Torch-tensor与PIL-Image转换"><a href="#Torch-tensor与PIL-Image转换" class="headerlink" title="Torch.tensor与PIL.Image转换"></a><strong>Torch.tensor与PIL.Image转换</strong></h3><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># pytorch中的张量默认采用<span class="hljs-literal">[N, C, H, W]</span>的顺序，并且数据范围在<span class="hljs-literal">[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]</span>，需要进行转置和规范化<br># torch.Tensor  &gt; PIL.Imageimage = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">PIL</span>.</span><span class="hljs-module"><span class="hljs-identifier">Image</span>.</span></span>fromarray(torch.clamp(tensor*<span class="hljs-number">255</span>, min=<span class="hljs-number">0</span>, max=<span class="hljs-number">255</span>).byte<span class="hljs-literal">()</span>.permute(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>).cpu<span class="hljs-literal">()</span>.numpy<span class="hljs-literal">()</span>)<br>image = torchvision.transforms.functional.<span class="hljs-keyword">to</span><span class="hljs-constructor">_pil_image(<span class="hljs-params">tensor</span>)</span>  <br># Equivalently way<br><br># PIL.Image  &gt; torch.Tensorpath = r&#x27;./figure.jpg&#x27;tensor = torch.from<span class="hljs-constructor">_numpy(<span class="hljs-params">np</span>.<span class="hljs-params">asarray</span>(PIL.Image.<span class="hljs-params">open</span>(<span class="hljs-params">path</span>)</span>)).permute(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>).<span class="hljs-built_in">float</span><span class="hljs-literal">()</span><span class="hljs-operator"> / </span><span class="hljs-number">255</span><br>tensor = torchvision.transforms.functional.<span class="hljs-keyword">to</span><span class="hljs-constructor">_tensor(PIL.Image.<span class="hljs-params">open</span>(<span class="hljs-params">path</span>)</span>) <br># Equivalently way<br></code></pre></td></tr></table></figure><h3 id="np-ndarray与PIL-Image的转换"><a href="#np-ndarray与PIL-Image的转换" class="headerlink" title="np.ndarray与PIL.Image的转换"></a><strong>np.ndarray与PIL.Image的转换</strong></h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">image = PIL<span class="hljs-selector-class">.Image</span><span class="hljs-selector-class">.fromarray</span>(ndarray<span class="hljs-selector-class">.astype</span>(np.uint8))<br>ndarray = np<span class="hljs-selector-class">.asarray</span>(PIL<span class="hljs-selector-class">.Image</span><span class="hljs-selector-class">.open</span>(path))<br></code></pre></td></tr></table></figure><h3 id="从只包含一个元素的张量中提取值"><a href="#从只包含一个元素的张量中提取值" class="headerlink" title="从只包含一个元素的张量中提取值"></a><strong>从只包含一个元素的张量中提取值</strong></h3><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">value</span> <span class="hljs-operator">=</span> torch.rand(<span class="hljs-number">1</span>).item()<br></code></pre></td></tr></table></figure><h3 id="张量形变"><a href="#张量形变" class="headerlink" title="张量形变"></a><strong>张量形变</strong></h3><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，</span><br><span class="hljs-comment"># 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。</span><br><span class="hljs-attr">tensor</span> = torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attr">shape</span> = (<span class="hljs-number">6</span>, <span class="hljs-number">4</span>)<br><span class="hljs-attr">tensor</span> = torch.reshape(tensor, shape)<br></code></pre></td></tr></table></figure><h3 id="打乱顺序"><a href="#打乱顺序" class="headerlink" title="打乱顺序"></a><strong>打乱顺序</strong></h3><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">tensor</span> = tensor[torch.randperm(tensor.size(<span class="hljs-number">0</span>))]  <span class="hljs-comment"># 打乱第一个维度</span><br></code></pre></td></tr></table></figure><h3 id="水平翻转"><a href="#水平翻转" class="headerlink" title="水平翻转"></a><strong>水平翻转</strong></h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以通过张量索引实现</span><br><span class="hljs-comment"># 假设张量的维度为[N, D, H, W].</span><br><span class="hljs-attribute">tensor</span> = tensor[:,:,:,torch.arange(tensor.size(<span class="hljs-number">3</span>)   <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>).long()]<br></code></pre></td></tr></table></figure><h3 id="复制张量"><a href="#复制张量" class="headerlink" title="复制张量"></a><strong>复制张量</strong></h3><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs coq"># Operation                 |  <span class="hljs-type">New</span>/Shared memory | <span class="hljs-type">Still</span> <span class="hljs-built_in">in</span> computation graph |<span class="hljs-type">tensor</span>.clone()            # |        <span class="hljs-type">New</span>         |          <span class="hljs-type">Yes</span>               |<span class="hljs-type">tensor</span>.detach()           # |      <span class="hljs-type">Shared</span>        |          <span class="hljs-type">No</span>               |<span class="hljs-type">tensor</span>.detach.clone()()   # |        <span class="hljs-type">New</span>         |          <span class="hljs-type">No</span>                <br></code></pre></td></tr></table></figure><h3 id="张量拼接"><a href="#张量拼接" class="headerlink" title="张量拼接"></a><strong>张量拼接</strong></h3><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">&#x27;&#x27;&#x27;<br>注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，<br>而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，<br>而torch.stack的结果是3x10x5的张量。<br>&#x27;&#x27;&#x27;<br>tensor = torch.cat(list<span class="hljs-emphasis">_of_tensors, dim=0)</span><br><span class="hljs-emphasis">tensor = torch.stack(list_of_</span>tensors, dim=0)<br></code></pre></td></tr></table></figure><h3 id="将整数标签转为one-hot编码"><a href="#将整数标签转为one-hot编码" class="headerlink" title="将整数标签转为one-hot编码"></a><strong>将整数标签转为one-hot编码</strong></h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># pytorch的标记默认从0开始</span><br><span class="hljs-attribute">tensor</span> = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])<br><span class="hljs-attribute">N</span> = tensor.size(<span class="hljs-number">0</span>)<br><span class="hljs-attribute">num_classes</span> = <span class="hljs-number">4</span><br><span class="hljs-attribute">one_hot</span> = torch.zeros(N, num_classes).long()<br><span class="hljs-attribute">one_hot</span>.scatter_(dim=<span class="hljs-number">1</span>, index=torch.unsqueeze(tensor, dim=<span class="hljs-number">1</span>), src=torch.ones(N, num_classes).long())<br></code></pre></td></tr></table></figure><h3 id="得到非零元素"><a href="#得到非零元素" class="headerlink" title="得到非零元素"></a><strong>得到非零元素</strong></h3><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">torch.nonzero<span class="hljs-params">(tensor)</span>               <span class="hljs-comment"># index of non-zero elements</span><br>torch.nonzero<span class="hljs-params">(<span class="hljs-attr">tensor</span>==0)</span>            <span class="hljs-comment"># index of zero elements</span><br>torch.nonzero<span class="hljs-params">(tensor)</span><span class="hljs-string">.size</span><span class="hljs-params">(0)</span>       <span class="hljs-comment"># number of non-zero elements</span><br>torch.nonzero<span class="hljs-params">(<span class="hljs-attr">tensor</span> == 0)</span><span class="hljs-string">.size</span><span class="hljs-params">(0)</span>  <span class="hljs-comment"># number of zero elements</span><br></code></pre></td></tr></table></figure><h3 id="判断两个张量相等"><a href="#判断两个张量相等" class="headerlink" title="判断两个张量相等"></a><strong>判断两个张量相等</strong></h3><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">torch.allclose<span class="hljs-comment">(tensor1, tensor2)</span>  <span class="hljs-attr"># float tensortorch.equal(tensor1</span>, te<span class="hljs-symbol">nsor2</span>)     <span class="hljs-attr"># int tensor</span><br></code></pre></td></tr></table></figure><h3 id="张量扩展"><a href="#张量扩展" class="headerlink" title="张量扩展"></a><strong>张量扩展</strong></h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Expand tensor of shape 64*512 to shape 64*512*7*7.</span><br><span class="hljs-attribute">tensor</span> = torch.rand(<span class="hljs-number">64</span>,<span class="hljs-number">512</span>)<br><span class="hljs-attribute">torch</span>.reshape(tensor, (<span class="hljs-number">64</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).expand(<span class="hljs-number">64</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>)<br></code></pre></td></tr></table></figure><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a><strong>矩阵乘法</strong></h3><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># Matrix multiplcation: (m*n) * (n*p) *  &gt; (m*p).</span><br><span class="hljs-attr">result</span> = torch.mm(tensor1, tensor2)<br><br><span class="hljs-comment"># Batch matrix multiplication: (b*m*n) * (b*n*p)  &gt; (b*m*p)</span><br><span class="hljs-attr">result</span> = torch.bmm(tensor1, tensor2)<br><br><span class="hljs-comment"># Element-wise multiplication.</span><br><span class="hljs-attr">result</span> = tensor1 * tensor2<br></code></pre></td></tr></table></figure><h3 id="计算两组数据之间的两两欧式距离"><a href="#计算两组数据之间的两两欧式距离" class="headerlink" title="计算两组数据之间的两两欧式距离"></a><strong>计算两组数据之间的两两欧式距离</strong></h3><p>利用broadcast机制</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran">dist = torch.<span class="hljs-built_in">sqrt</span>(torch.<span class="hljs-built_in">sum</span>((X1[:,<span class="hljs-keyword">None</span>,:]   X2) ** <span class="hljs-number">2</span>, <span class="hljs-built_in">dim</span>=<span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><h2 id="3-模型定义和操作"><a href="#3-模型定义和操作" class="headerlink" title="3. 模型定义和操作"></a>3. 模型定义和操作</h2><h3 id="一个简单两层卷积网络的示例"><a href="#一个简单两层卷积网络的示例" class="headerlink" title="一个简单两层卷积网络的示例"></a>一个简单两层卷积网络的示例</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># convolutional neural network (2 convolutional layers)</span><br>class ConvNet(nn.Module):    <br>def __init__(self, <span class="hljs-attribute">num_classes</span>=10):        <br>super(ConvNet, self).__init__()        <br>self.layer1 = nn.Sequential( <br>nn.Conv2d(1, 16, <span class="hljs-attribute">kernel_size</span>=5, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=2),<br>nn.BatchNorm2d(16),<br>nn.ReLU(),<br>nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=2, <span class="hljs-attribute">stride</span>=2))  <br>        self.layer2 = nn.Sequential( <br>        nn.Conv2d(16, 32, <span class="hljs-attribute">kernel_size</span>=5, <span class="hljs-attribute">stride</span>=1, <span class="hljs-attribute">padding</span>=2),<br>        nn.BatchNorm2d(32),<br>        nn.ReLU(), <br>        nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=2, <span class="hljs-attribute">stride</span>=2))<br>        self.fc = nn.Linear(7<span class="hljs-number">*7</span><span class="hljs-number">*32</span>, num_classes)<br>        <br>    def forward(self, x):<br>    out = self.layer1(x)<br>        out = self.layer2(out)<br>        out = out.reshape(out.size(0),  1)<br>        out = self.fc(out)<br>        return out<br><br>model = ConvNet(num_classes).<span class="hljs-keyword">to</span>(device)<br></code></pre></td></tr></table></figure><p>卷积层的计算和展示可以用这个网站辅助。</p><h3 id="双线性汇合（bilinear-pooling）"><a href="#双线性汇合（bilinear-pooling）" class="headerlink" title="双线性汇合（bilinear pooling）"></a><strong>双线性汇合（bilinear pooling）</strong></h3><figure class="highlight irpf90"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs irpf90">X = torch.<span class="hljs-built_in">reshape</span>(N, D, H * W)        # Assume X has <span class="hljs-built_in">shape</span> N*D*H*W<br>X = torch.bmm(X, torch.<span class="hljs-built_in">transpose</span>(X, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) / (H * W)  # Bilinear pooling<br><span class="hljs-keyword">assert</span>  X.<span class="hljs-built_in">size</span>() == (N, D, D)<br>X = torch.<span class="hljs-built_in">reshape</span>(X, (N, D * D))<br>X = torch.<span class="hljs-built_in">sign</span>(X) * torch.<span class="hljs-built_in">sqrt</span>(torch.<span class="hljs-built_in">abs</span>(X) + <span class="hljs-number">1e-5</span>)   # Signed-<span class="hljs-built_in">sqrt</span> normalization<br>X = torch.nn.functional.normalize(X)                  # L2 normalization<br></code></pre></td></tr></table></figure><h3 id="多卡同步-BN（Batch-normalization）"><a href="#多卡同步-BN（Batch-normalization）" class="headerlink" title="多卡同步 BN（Batch normalization）"></a><strong>多卡同步 BN（Batch normalization）</strong></h3><p>当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差，同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sync_bn = torch.nn.SyncBatchNorm(num_features, <span class="hljs-attribute">eps</span>=1e-05, <span class="hljs-attribute">momentum</span>=0.1, <span class="hljs-attribute">affine</span>=<span class="hljs-literal">True</span>,                                  <span class="hljs-attribute">track_running_stats</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h3 id="将已有网络的所有BN层改为同步BN层"><a href="#将已有网络的所有BN层改为同步BN层" class="headerlink" title="将已有网络的所有BN层改为同步BN层"></a>将已有网络的所有BN层改为同步BN层</h3><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">def convert<span class="hljs-constructor">BNtoSyncBN(<span class="hljs-params">module</span>, <span class="hljs-params">process_group</span>=None)</span>:    <br>&#x27;&#x27;&#x27;Recursively replace all BN layers <span class="hljs-keyword">to</span> SyncBN layer.<br>Args:        <br><span class="hljs-keyword">module</span><span class="hljs-literal">[<span class="hljs-identifier">torch</span>.<span class="hljs-identifier">nn</span>.M<span class="hljs-identifier">odule</span>]</span>. Network    <br>&#x27;&#x27;&#x27;    <br><span class="hljs-keyword">if</span> isinstance(<span class="hljs-keyword">module</span>, torch.nn.modules.batchnorm._BatchNorm):        <br>sync_bn = torch.nn.<span class="hljs-constructor">SyncBatchNorm(<span class="hljs-params">module</span>.<span class="hljs-params">num_features</span>, <span class="hljs-params">module</span>.<span class="hljs-params">eps</span>, <span class="hljs-params">module</span>.<span class="hljs-params">momentum</span>, <span class="hljs-params">module</span>.<span class="hljs-params">affine</span>, <span class="hljs-params">module</span>.<span class="hljs-params">track_running_stats</span>, <span class="hljs-params">process_group</span>)</span>        <br>sync_bn.running_mean = <span class="hljs-keyword">module</span>.running_mean       <br>        sync_bn.running_var = <span class="hljs-keyword">module</span>.running_var        <br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">module</span>.affine:            <br>        sync_bn.weight = <span class="hljs-keyword">module</span>.weight.clone<span class="hljs-literal">()</span>.detach<span class="hljs-literal">()</span>            <br>        sync_bn.bias = <span class="hljs-keyword">module</span>.bias.clone<span class="hljs-literal">()</span>.detach<span class="hljs-literal">()</span>        <br>        return sync_bn    <br>    <span class="hljs-keyword">else</span>:        <br>    <span class="hljs-keyword">for</span> name, child_module <span class="hljs-keyword">in</span> <span class="hljs-keyword">module</span>.named<span class="hljs-constructor">_children()</span>:            <br>    setattr(<span class="hljs-keyword">module</span>, name) = convert<span class="hljs-constructor">_syncbn_model(<span class="hljs-params">child_module</span>, <span class="hljs-params">process_group</span>=<span class="hljs-params">process_group</span>)</span>)        <br>    return <span class="hljs-keyword">module</span><br></code></pre></td></tr></table></figure><h3 id="类似-BN-滑动平均"><a href="#类似-BN-滑动平均" class="headerlink" title="类似 BN 滑动平均"></a><strong>类似 BN 滑动平均</strong></h3><p>如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-variable constant_">BN</span>(torch.nn.Module):    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):        <br>.        <br><span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&#x27;running_mean&#x27;</span>, torch.zeros(num_features))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, X</span>):        <br>    .        <br>    <span class="hljs-variable language_">self</span>.running_mean += momentum * (current   <span class="hljs-variable language_">self</span>.running_mean)<br></code></pre></td></tr></table></figure><h3 id="计算模型整体参数量"><a href="#计算模型整体参数量" class="headerlink" title="计算模型整体参数量"></a><strong>计算模型整体参数量</strong></h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran">num_parameters = <span class="hljs-built_in">sum</span>(torch.numel(<span class="hljs-keyword">parameter</span>) for <span class="hljs-keyword">parameter</span> <span class="hljs-keyword">in</span> model.parameters())<br></code></pre></td></tr></table></figure><h3 id="查看网络中的参数"><a href="#查看网络中的参数" class="headerlink" title="查看网络中的参数"></a><strong>查看网络中的参数</strong></h3><p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">params = <span class="hljs-built_in">list</span>(model<span class="hljs-selector-class">.named_parameters</span>())<br>(name, param) = params<span class="hljs-selector-attr">[28]</span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(name)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(param.grad)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;                                                 &#x27;</span>)</span></span><br>(name2, param2) = params<span class="hljs-selector-attr">[29]</span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(name2)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(param2.grad)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;                                                 &#x27;</span>)</span></span><br>(name1, param1) = params<span class="hljs-selector-attr">[30]</span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(name1)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(param1.grad)</span></span><br></code></pre></td></tr></table></figure><h3 id="模型可视化（使用pytorchviz）"><a href="#模型可视化（使用pytorchviz）" class="headerlink" title="模型可视化（使用pytorchviz）"></a>模型可视化（使用pytorchviz）</h3><p>szagoruyko&#x2F;pytorchvizgithub.com</p><h3 id="类似-Keras-的-model-summary-输出模型信息，使用pytorch-summary"><a href="#类似-Keras-的-model-summary-输出模型信息，使用pytorch-summary" class="headerlink" title="类似 Keras 的 model.summary() 输出模型信息，使用pytorch-summary"></a>类似 Keras 的 model.summary() 输出模型信息，使用pytorch-summary</h3><p>sksq96&#x2F;pytorch-summarygithub.com</p><p><strong>模型权重初始化</strong></p><p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># Common practise <span class="hljs-keyword">for</span> initialization.<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.modules<span class="hljs-literal">()</span>:    <br><span class="hljs-keyword">if</span> isinstance(layer, torch.nn.Conv2d): <br>torch.nn.init.kaiming<span class="hljs-constructor">_normal_(<span class="hljs-params">layer</span>.<span class="hljs-params">weight</span>, <span class="hljs-params">mode</span>=&#x27;<span class="hljs-params">fan_out</span>&#x27;,                                      <span class="hljs-params">nonlinearity</span>=&#x27;<span class="hljs-params">relu</span>&#x27;)</span>        <br><span class="hljs-keyword">if</span> layer.bias is not None:            <br>torch.nn.init.constant<span class="hljs-constructor">_(<span class="hljs-params">layer</span>.<span class="hljs-params">bias</span>, <span class="hljs-params">val</span>=0.0)</span>   <br>elif isinstance(layer, torch.nn.BatchNorm2d):      <br>torch.nn.init.constant<span class="hljs-constructor">_(<span class="hljs-params">layer</span>.<span class="hljs-params">weight</span>, <span class="hljs-params">val</span>=1.0)</span>  <br>torch.nn.init.constant<span class="hljs-constructor">_(<span class="hljs-params">layer</span>.<span class="hljs-params">bias</span>, <span class="hljs-params">val</span>=0.0)</span>  <br>    elif isinstance(layer, torch.nn.Linear): <br>    torch.nn.init.xavier<span class="hljs-constructor">_normal_(<span class="hljs-params">layer</span>.<span class="hljs-params">weight</span>)</span>        <br>    <span class="hljs-keyword">if</span> layer.bias is not None:            <br>    torch.nn.init.constant<span class="hljs-constructor">_(<span class="hljs-params">layer</span>.<span class="hljs-params">bias</span>, <span class="hljs-params">val</span>=0.0)</span><br># Initialization <span class="hljs-keyword">with</span> given tensor.layer.weight = torch.nn.<span class="hljs-constructor">Parameter(<span class="hljs-params">tensor</span>)</span><br></code></pre></td></tr></table></figure><h3 id="提取模型中的某一层"><a href="#提取模型中的某一层" class="headerlink" title="提取模型中的某一层"></a><strong>提取模型中的某一层</strong></h3><p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 取模型中的前两层<br>new_model = nn.<span class="hljs-constructor">Sequential(<span class="hljs-operator">*</span><span class="hljs-params">list</span>(<span class="hljs-params">model</span>.<span class="hljs-params">children</span>()</span>)<span class="hljs-literal">[:<span class="hljs-number">2</span>]</span> <br><br># 如果希望提取出模型中的所有卷积层，可以像下面这样操作：<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.named<span class="hljs-constructor">_modules()</span>:    <br><span class="hljs-keyword">if</span> isinstance(layer<span class="hljs-literal">[<span class="hljs-number">1</span>]</span>,nn.Conv2d):         <br>conv_model.add<span class="hljs-constructor">_module(<span class="hljs-params">layer</span>[0],<span class="hljs-params">layer</span>[1])</span><br></code></pre></td></tr></table></figure><h3 id="部分层使用预训练模型"><a href="#部分层使用预训练模型" class="headerlink" title="部分层使用预训练模型"></a><strong>部分层使用预训练模型</strong></h3><p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">model.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(&#x27;<span class="hljs-params">model</span>.<span class="hljs-params">pth</span>&#x27;)</span>, strict=False)<br></code></pre></td></tr></table></figure><h3 id="将在-GPU-保存的模型加载到-CPU"><a href="#将在-GPU-保存的模型加载到-CPU" class="headerlink" title="将在 GPU 保存的模型加载到 CPU"></a><strong>将在 GPU 保存的模型加载到 CPU</strong></h3><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">model.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(&#x27;<span class="hljs-params">model</span>.<span class="hljs-params">pth</span>&#x27;, <span class="hljs-params">map_location</span>=&#x27;<span class="hljs-params">cpu</span>&#x27;)</span>)<br></code></pre></td></tr></table></figure><h2 id="导入另一个模型的相同部分到新的模型"><a href="#导入另一个模型的相同部分到新的模型" class="headerlink" title="导入另一个模型的相同部分到新的模型"></a><strong>导入另一个模型的相同部分到新的模型</strong></h2><p>模型导入参数时，如果两个模型结构不一致，则直接导入参数会报错。用下面方法可以把另一个模型的相同的部分导入到新的模型中。</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs haxe"><span class="hljs-meta"># model_new代表新的模型</span><br><span class="hljs-meta"># model_saved代表其他模型，比如用torch.load导入的已保存的模型</span><br>model_new<span class="hljs-type">_dict</span> = model_new<span class="hljs-type"></span>.state_dict()<br>model_common_dict = &#123;k:<span class="hljs-type">v for k</span>, v <span class="hljs-keyword">in</span> model_saved.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">in</span> model_new<span class="hljs-type">_dict</span>.keys()&#125;<br>model_new<span class="hljs-type">_dict</span>.update(model_common_dict)<br>model_new<span class="hljs-type"></span>.load_state_dict(model_new<span class="hljs-type">_dict</span>)<br></code></pre></td></tr></table></figure><h2 id="4-数据处理"><a href="#4-数据处理" class="headerlink" title="4. 数据处理"></a><strong>4. 数据处理</strong></h2><h3 id="计算数据集的均值和标准差"><a href="#计算数据集的均值和标准差" class="headerlink" title="计算数据集的均值和标准差"></a><strong>计算数据集的均值和标准差</strong></h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> os<br><span class="hljs-attribute">import</span> cv2<br><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">from</span> torch.utils.data <br><span class="hljs-attribute">import</span> Dataset<br><span class="hljs-attribute">from</span> PIL import Image<br><br><span class="hljs-attribute">def</span> compute_mean_and_std(dataset):    <br><span class="hljs-comment"># 输入PyTorch的dataset，输出均值和标准差    </span><br><span class="hljs-attribute">mean_r</span> = <span class="hljs-number">0</span>    <br><span class="hljs-attribute">mean_g</span> = <span class="hljs-number">0</span>    <br><span class="hljs-attribute">mean_b</span> = <span class="hljs-number">0</span><br><br>    <span class="hljs-attribute">for</span> img, _ in dataset:        <br>    <span class="hljs-attribute">img</span> = np.asarray(img) # change PIL Image to numpy array        <br>    <span class="hljs-attribute">mean_b</span> += np.mean(img[:, :, <span class="hljs-number">0</span>])        <br>    <span class="hljs-attribute">mean_g</span> += np.mean(img[:, :, <span class="hljs-number">1</span>])        <br>    <span class="hljs-attribute">mean_r</span> += np.mean(img[:, :, <span class="hljs-number">2</span>])<br>    <br>    <span class="hljs-attribute">mean_b</span> /= len(dataset)    <br>    <span class="hljs-attribute">mean_g</span> /= len(dataset)    <br>    <span class="hljs-attribute">mean_r</span> /= len(dataset)<br>    <br>    <span class="hljs-attribute">diff_r</span> = <span class="hljs-number">0</span>    <br>    <span class="hljs-attribute">diff_g</span> = <span class="hljs-number">0</span>    <br>    <span class="hljs-attribute">diff_b</span> = <span class="hljs-number">0</span><br>    <br>    <span class="hljs-attribute">N</span> = <span class="hljs-number">0</span><br>    <span class="hljs-attribute">for</span> img, _ in dataset:        <br>    <span class="hljs-attribute">img</span> = np.asarray(img)<br>    <br>    <span class="hljs-attribute">diff_b</span> += np.sum(np.power(img[:, :, <span class="hljs-number">0</span>]   mean_b, <span class="hljs-number">2</span>))        <br>    <span class="hljs-attribute">diff_g</span> += np.sum(np.power(img[:, :, <span class="hljs-number">1</span>]   mean_g, <span class="hljs-number">2</span>))        <br>    <span class="hljs-attribute">diff_r</span> += np.sum(np.power(img[:, :, <span class="hljs-number">2</span>]   mean_r, <span class="hljs-number">2</span>))<br>        <span class="hljs-attribute">N</span> += np.prod(img[:, :, <span class="hljs-number">0</span>].shape)<br>        <br>    <span class="hljs-attribute">std_b</span> = np.sqrt(diff_b / N)    <br>    <span class="hljs-attribute">std_g</span> = np.sqrt(diff_g / N)    <br>    <span class="hljs-attribute">std_r</span> = np.sqrt(diff_r / N)<br>    <br>    <span class="hljs-attribute">mean</span> = (mean_b.item() / <span class="hljs-number">255</span>.<span class="hljs-number">0</span>, mean_g.item() / <span class="hljs-number">255</span>.<span class="hljs-number">0</span>, mean_r.item() / <span class="hljs-number">255</span>.<span class="hljs-number">0</span>)    <br>    <span class="hljs-attribute">std</span> = (std_b.item() / <span class="hljs-number">255</span>.<span class="hljs-number">0</span>, std_g.item() / <span class="hljs-number">255</span>.<span class="hljs-number">0</span>, std_r.item() / <span class="hljs-number">255</span>.<span class="hljs-number">0</span>)   <br>    <span class="hljs-attribute">return</span> mean, std<br></code></pre></td></tr></table></figure><h3 id="得到视频数据基本信息"><a href="#得到视频数据基本信息" class="headerlink" title="得到视频数据基本信息"></a><strong>得到视频数据基本信息</strong></h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> cv2<br>video = cv2.VideoCapture(mp4_path)<br>height = <span class="hljs-type">int</span>(video.<span class="hljs-keyword">get</span>(cv2.CAP_PROP_FRAME_HEIGHT))<br>width = <span class="hljs-type">int</span>(video.<span class="hljs-keyword">get</span>(cv2.CAP_PROP_FRAME_WIDTH))<br>num_frames = <span class="hljs-type">int</span>(video.<span class="hljs-keyword">get</span>(cv2.CAP_PROP_FRAME_COUNT))<br>fps = <span class="hljs-type">int</span>(video.<span class="hljs-keyword">get</span>(cv2.CAP_PROP_FPS))<br>video.<span class="hljs-keyword">release</span>()<br></code></pre></td></tr></table></figure><h3 id="TSN-每段（segment）采样一帧视频"><a href="#TSN-每段（segment）采样一帧视频" class="headerlink" title="TSN 每段（segment）采样一帧视频"></a><strong>TSN 每段（segment）采样一帧视频</strong></h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">K = self.<span class="hljs-title">_num</span>_segments<br><span class="hljs-keyword">if</span> is_train:    <br><span class="hljs-keyword">if</span> num_frames &gt; K:        <br><span class="hljs-comment"># Random index for each segment.        </span><br>frame_indices = torch.randint(  high=num_frames<span class="hljs-comment"> // K, size=(K,), dtype=torch.long)</span><br>frame_indices += num_frames<span class="hljs-comment"> // K * torch.arange(K)    </span><br><span class="hljs-keyword">else</span>:<br>    frame_indices = torch.randint(high=num_frames, size=(K - num_frames,), dtype=torch.<span class="hljs-keyword">long</span>) <br>    frame_indices = torch.<span class="hljs-built_in">sort</span>(torch.cat(( torch.arange(num_frames), frame_indices)))[<span class="hljs-number">0</span>]<br><span class="hljs-keyword">else</span>:    <br><span class="hljs-keyword">if</span> num_frames &gt; K:       <br>    <span class="hljs-comment"># Middle index for each segment.        </span><br>    frame_indices = num_frames / K<span class="hljs-comment"> // 2        </span><br>    frame_indices += num_frames<span class="hljs-comment"> // K * torch.arange(K)    </span><br>    <span class="hljs-keyword">else</span>:        <br>    frame_indices = torch.<span class="hljs-built_in">sort</span>(torch.cat((torch.arange(num_frames), torch.arange(K   num_frames))))[<span class="hljs-number">0</span>]<br>assert frame_indices.size() == (K,)<br><span class="hljs-literal">return</span> [frame_indices[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(K)]<br></code></pre></td></tr></table></figure><h3 id="常用训练和验证数据预处理"><a href="#常用训练和验证数据预处理" class="headerlink" title="常用训练和验证数据预处理"></a><strong>常用训练和验证数据预处理</strong></h3><p>其中 ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，数值范围为 [0.0, 1.0] 的 torch.Tensor。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">train_transform</span> = torchvision.transforms.Compose([<br><span class="hljs-attribute">torchvision</span>.transforms.RandomResizedCrop(size=<span class="hljs-number">224</span>,                                             scale=(<span class="hljs-number">0</span>.<span class="hljs-number">08</span>, <span class="hljs-number">1</span>.<span class="hljs-number">0</span>)),<br><span class="hljs-attribute">torchvision</span>.transforms.RandomHorizontalFlip(),<br><span class="hljs-attribute">torchvision</span>.transforms.ToTensor(),    <br><span class="hljs-attribute">torchvision</span>.transforms.Normalize(mean=(<span class="hljs-number">0</span>.<span class="hljs-number">485</span>, <span class="hljs-number">0</span>.<span class="hljs-number">456</span>, <span class="hljs-number">0</span>.<span class="hljs-number">406</span>),                                     std=(<span class="hljs-number">0</span>.<span class="hljs-number">229</span>, <span class="hljs-number">0</span>.<span class="hljs-number">224</span>, <span class="hljs-number">0</span>.<span class="hljs-number">225</span>)), ]) <br><span class="hljs-attribute">val_transform</span> = torchvision.transforms.Compose([ <br><span class="hljs-attribute">torchvision</span>.transforms.Resize(<span class="hljs-number">256</span>),    <br><span class="hljs-attribute">torchvision</span>.transforms.CenterCrop(<span class="hljs-number">224</span>),    <br><span class="hljs-attribute">torchvision</span>.transforms.ToTensor(),    <br><span class="hljs-attribute">torchvision</span>.transforms.Normalize(mean=(<span class="hljs-number">0</span>.<span class="hljs-number">485</span>, <span class="hljs-number">0</span>.<span class="hljs-number">456</span>, <span class="hljs-number">0</span>.<span class="hljs-number">406</span>),std=(<span class="hljs-number">0</span>.<span class="hljs-number">229</span>, <span class="hljs-number">0</span>.<span class="hljs-number">224</span>, <span class="hljs-number">0</span>.<span class="hljs-number">225</span>)),])<br></code></pre></td></tr></table></figure><h2 id="5-模型训练和测试"><a href="#5-模型训练和测试" class="headerlink" title="5. 模型训练和测试"></a>5. 模型训练和测试</h2><h3 id="分类模型训练代码"><a href="#分类模型训练代码" class="headerlink" title="分类模型训练代码"></a>分类模型训练代码</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment"># Loss and optimizer</span><br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<br><br><span class="hljs-comment"># Train the model</span><br>total_step = <span class="hljs-built_in">len</span>(train_loader)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):    <br><span class="hljs-keyword">for</span> i ,(images, labels) <span class="hljs-keyword">in</span> enumerate(train_loader):        <br>images = images.<span class="hljs-built_in">to</span>(device)        <br>labels = labels.<span class="hljs-built_in">to</span>(device)<br>        <span class="hljs-comment"># Forward pass        </span><br>        outputs = model(images)       <br>        loss = criterion(outputs, labels)<br>        <span class="hljs-comment"># Backward and optimizer        </span><br>        optimizer.zero_grad()        <br>        loss.backward()       <br>        optimizer.step()<br>        <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            <br>        print(<span class="hljs-string">&#x27;Epoch: [&#123;&#125;/&#123;&#125;], Step: [&#123;&#125;/&#123;&#125;], Loss: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(epoch+<span class="hljs-number">1</span>, num_epochs, i+<span class="hljs-number">1</span>, total_step, loss.<span class="hljs-keyword">item</span>()))<br></code></pre></td></tr></table></figure><h3 id="分类模型测试代码"><a href="#分类模型测试代码" class="headerlink" title="分类模型测试代码"></a>分类模型测试代码</h3><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># Test the modelmodel.<span class="hljs-built_in">eval</span>()  <br># <span class="hljs-built_in">eval</span> mode(<span class="hljs-built_in">batch</span> norm uses moving <span class="hljs-built_in">mean</span>/variance               <br>#instead of mini-<span class="hljs-built_in">batch</span> <span class="hljs-built_in">mean</span>/variance)<br>with torch.no_grad():    <br>correct = <span class="hljs-number">0</span>    <br>total = <span class="hljs-number">0</span>    <br><span class="hljs-keyword">for</span> images, <span class="hljs-built_in">labels</span> <span class="hljs-keyword">in</span> test_loader: <br>    images = images.to(device)        <br>    <span class="hljs-built_in">labels</span> = <span class="hljs-built_in">labels</span>.to(device)       <br>        outputs = model(images)        <br>        <span class="hljs-symbol">_</span>, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)       <br>        total += <span class="hljs-built_in">labels</span>.size(<span class="hljs-number">0</span>)        <br>        correct += (predicted == <span class="hljs-built_in">labels</span>).<span class="hljs-built_in">sum</span>().item()<br>    <span class="hljs-built_in">print</span>(&#x27;Test accuracy of the model on the <span class="hljs-number">10000</span> test images: &#123;&#125; <span class="hljs-symbol">%</span>&#x27;.format(<span class="hljs-number">100</span> * correct / total))<br></code></pre></td></tr></table></figure><h3 id="自定义loss"><a href="#自定义loss" class="headerlink" title="自定义loss"></a>自定义loss</h3><p>继承torch.nn.Module类写自己的loss。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyLoss</span>(torch.nn.Moudle):    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):        <br><span class="hljs-variable language_">super</span>(MyLoss, <span class="hljs-variable language_">self</span>).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, x, y</span>):        <br>    loss = torch.mean((x   y) ** <span class="hljs-number">2</span>)        <br>    <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><h3 id="标签平滑（label-smoothing）"><a href="#标签平滑（label-smoothing）" class="headerlink" title="标签平滑（label smoothing）"></a><strong>标签平滑（label smoothing）</strong></h3><p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LSR</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, e=<span class="hljs-number">0.1</span>, reduction=<span class="hljs-string">&#x27;mean&#x27;</span></span>):        <br>    <span class="hljs-built_in">super</span>().__init__()<br>        self.log_softmax = nn.LogSoftmax(dim=<span class="hljs-number">1</span>)        <br>        self.e = e       <br>        self.reduction = reduction<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_one_hot</span>(<span class="hljs-params">self, labels, classes, value=<span class="hljs-number">1</span></span>):        <br>    <span class="hljs-string">&quot;&quot;&quot;            </span><br><span class="hljs-string">    Convert labels to one hot vectors</span><br><span class="hljs-string">        Args:            </span><br><span class="hljs-string">        labels: torch tensor in format [label1, label2, label3, .]           </span><br><span class="hljs-string">        classes: int, number of classes            </span><br><span class="hljs-string">        value: label value in one hot vector, default to 1</span><br><span class="hljs-string">        Returns: return one hot format labels in shape [batchsize, classes]       </span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        one_hot = torch.zeros(labels.size(<span class="hljs-number">0</span>), classes)<br>        <span class="hljs-comment">#labels and value_added  size must match        </span><br>        labels = labels.view(labels.size(<span class="hljs-number">0</span>),  <span class="hljs-number">1</span>)        <br>        value_added = torch.Tensor(labels.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>).fill_(value)<br>        value_added = value_added.to(labels.device)       <br>        one_hot = one_hot.to(labels.device)<br>        one_hot.scatter_add_(<span class="hljs-number">1</span>, labels, value_added)<br>        <span class="hljs-keyword">return</span> one_hot<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_smooth_label</span>(<span class="hljs-params">self, target, length, smooth_factor</span>):        <br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    convert targets to one-hot format, and smooth them.        </span><br><span class="hljs-string">    Args: </span><br><span class="hljs-string">    target: target in form with [label1, label2, label_batchsize]            </span><br><span class="hljs-string">    length: length of one-hot format(number of classes)           </span><br><span class="hljs-string">            smooth_factor: smooth factor for label smooth</span><br><span class="hljs-string">        Returns:  </span><br><span class="hljs-string">        smoothed labels in one hot format        </span><br><span class="hljs-string">        &quot;&quot;&quot;</span>        <br>        one_hot = self._one_hot(target, length, value=<span class="hljs-number">1</span>   smooth_factor)       <br>        one_hot += smooth_factor / (length   <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> one_hot.to(target.device)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, target</span>):<br>        <span class="hljs-keyword">if</span> x.size(<span class="hljs-number">0</span>) != target.size(<span class="hljs-number">0</span>):           <br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)&#x27;</span>                    .<span class="hljs-built_in">format</span>(x.size(<span class="hljs-number">0</span>), target.size(<span class="hljs-number">0</span>)))<br>        <span class="hljs-keyword">if</span> x.dim() &lt; <span class="hljs-number">2</span>:            <br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Expected input tensor to have least 2 dimensions(got &#123;&#125;)&#x27;</span>                    .<span class="hljs-built_in">format</span>(x.size(<span class="hljs-number">0</span>)))<br>        <span class="hljs-keyword">if</span> x.dim() != <span class="hljs-number">2</span>:            <br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Only 2 dimension tensor are implemented, (got &#123;&#125;)&#x27;</span>                    .<span class="hljs-built_in">format</span>(x.size()))<br><br>        smoothed_target = self._smooth_label(target, x.size(<span class="hljs-number">1</span>), self.e)        <br>        x = self.log_softmax(x)       <br>        loss = torch.<span class="hljs-built_in">sum</span>(  x * smoothed_target, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> self.reduction == <span class="hljs-string">&#x27;none&#x27;</span>:            <span class="hljs-keyword">return</span> loss<br>        <span class="hljs-keyword">elif</span> self.reduction == <span class="hljs-string">&#x27;sum&#x27;</span>:            <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(loss)<br>        <span class="hljs-keyword">elif</span> self.reduction == <span class="hljs-string">&#x27;mean&#x27;</span>:            <span class="hljs-keyword">return</span> torch.mean(loss)<br>        <span class="hljs-keyword">else</span>:  <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;unrecognized option, expect reduction to be one of none, mean, sum&#x27;</span>)<br></code></pre></td></tr></table></figure><p>或者直接在训练文件里做label smoothing</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs css">for images, labels in train_loader:    <br>images, labels = images.<span class="hljs-built_in">cuda</span>(), labels.<span class="hljs-built_in">cuda</span>()    <br>N = labels.<span class="hljs-built_in">size</span>(<span class="hljs-number">0</span>)    <br># C is the number of classes.    <br>smoothed_labels = torch.<span class="hljs-built_in">full</span>(size=(N, C), fill_value=<span class="hljs-number">0.1</span> / (C   <span class="hljs-number">1</span>)).<span class="hljs-built_in">cuda</span>()<br>smoothed_labels.<span class="hljs-built_in">scatter_</span>(dim=<span class="hljs-number">1</span>, index=torch.<span class="hljs-built_in">unsqueeze</span>(labels, dim=<span class="hljs-number">1</span>), value=<span class="hljs-number">0.9</span>)<br><br>    score = <span class="hljs-built_in">model</span>(images)<br>    log_prob = torch.nn.functional.<span class="hljs-built_in">log_softmax</span>(score, dim=<span class="hljs-number">1</span>)<br>    loss = -torch.<span class="hljs-built_in">sum</span>(log_prob * smoothed_labels) / N<br>    optimizer.<span class="hljs-built_in">zero_grad</span>()<br>    loss.<span class="hljs-built_in">backward</span>()<br>    optimizer.<span class="hljs-built_in">step</span>()<br></code></pre></td></tr></table></figure><h3 id="Mixup训练"><a href="#Mixup训练" class="headerlink" title="Mixup训练"></a>Mixup训练</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">beta_distribution </span>= torch.<span class="hljs-keyword">distributions.beta.Beta(alpha, </span>alpha)<br><br>for images, labels in train_loader:    <br>images, labels = images.cuda(), labels.cuda()<br>    <span class="hljs-comment"># Mixup images and labels.    </span><br>    lambda_ = <span class="hljs-keyword">beta_distribution.sample([]).item() </span>   <br>    index = torch.randperm(images.size(<span class="hljs-number">0</span>)).cuda()    <br>    mixed_images = lambda_ * images + (<span class="hljs-number">1</span>   lambda_) * images[index, :]    <br>    label_a, label_b = labels, labels[index]<br>    <span class="hljs-comment"># Mixup loss.    </span><br>    <span class="hljs-keyword">scores </span>= model(mixed_images)    <br>    loss = (lambda_ * loss_function(<span class="hljs-keyword">scores, </span>label_a) + (<span class="hljs-number">1</span> - lambda_) * loss_function(<span class="hljs-keyword">scores, </span>label_b))    <br>    optimizer.zero_grad()    <br>    loss.<span class="hljs-keyword">backward() </span>   <br>    optimizer.step()<br></code></pre></td></tr></table></figure><h3 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a><strong>L1 正则化</strong></h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">l1_regularization = torch.nn.L1Loss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)<br>loss = .  <br><span class="hljs-comment"># Standard cross-entropy loss</span><br><span class="hljs-keyword">for</span> <span class="hljs-built_in">param</span> <span class="hljs-keyword">in</span> model.parameters():    <br>loss += torch.<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">abs</span>(<span class="hljs-built_in">param</span>))<br>loss.backward()<br></code></pre></td></tr></table></figure><h3 id="不对偏置项进行权重衰减（weight-decay）"><a href="#不对偏置项进行权重衰减（weight-decay）" class="headerlink" title="不对偏置项进行权重衰减（weight decay）"></a><strong>不对偏置项进行权重衰减（weight decay）</strong></h3><p>pytorch里的weight decay相当于l2正则</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">bias_list</span> = (param for name, param in model.named_parameters() if name[-<span class="hljs-number">4</span>:] == <span class="hljs-string">&#x27;bias&#x27;</span>)<br><span class="hljs-attr">others_list</span> = (param for name, param in model.named_parameters() if name[-<span class="hljs-number">4</span>:] != <span class="hljs-string">&#x27;bias&#x27;</span>)<br><span class="hljs-attr">parameters</span> = [&#123;<span class="hljs-string">&#x27;parameters&#x27;</span>: bias_list, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>&#125;,&#123;<span class="hljs-string">&#x27;parameters&#x27;</span>: others_list&#125;]<br><span class="hljs-attr">optimizer</span> = torch.optim.SGD(parameters, lr=<span class="hljs-number">1</span>e-<span class="hljs-number">2</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1</span>e-<span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure><h3 id="梯度裁剪（gradient-clipping）"><a href="#梯度裁剪（gradient-clipping）" class="headerlink" title="梯度裁剪（gradient clipping）"></a><strong>梯度裁剪（gradient clipping）</strong></h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">torch<span class="hljs-selector-class">.nn</span><span class="hljs-selector-class">.utils</span><span class="hljs-selector-class">.clip_grad_norm_</span>(model<span class="hljs-selector-class">.parameters</span>(), max_norm=<span class="hljs-number">20</span>)<br></code></pre></td></tr></table></figure><h3 id="得到当前学习率"><a href="#得到当前学习率" class="headerlink" title="得到当前学习率"></a><strong>得到当前学习率</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># If there is one global learning rate (which is the common case).</span><br>lr = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(optimizer.param_groups))[<span class="hljs-string">&#x27;lr&#x27;</span>]<br><span class="hljs-comment"># If there are multiple learning rates for different layers.</span><br>all_lr = []<br><span class="hljs-keyword">for</span> param_group <span class="hljs-keyword">in</span> optimizer.param_groups:    <br>all_lr.append(param_group[<span class="hljs-string">&#x27;lr&#x27;</span>])<br></code></pre></td></tr></table></figure><p>另一种方法，在一个batch训练代码里，当前的lr是optimizer.param_groups[0][‘lr’]</p><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a><strong>学习率衰减</strong></h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># Reduce learning rate when validation accuarcy plateau.</span><span class="hljs-built_in"></span><br><span class="hljs-built_in">scheduler </span>= torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, <span class="hljs-attribute">mode</span>=<span class="hljs-string">&#x27;max&#x27;</span>, <span class="hljs-attribute">patience</span>=5, <span class="hljs-attribute">verbose</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(0, 80):    <br>train(.)    <br>val(.)    <br>scheduler.<span class="hljs-keyword">step</span>(val_acc)<br><br><span class="hljs-comment"># Cosine annealing learning rate.</span><span class="hljs-built_in"></span><br><span class="hljs-built_in">scheduler </span>= torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, <span class="hljs-attribute">T_max</span>=80)<br><br><span class="hljs-comment"># Reduce learning rate by 10 at given epochs.</span><span class="hljs-built_in"></span><br><span class="hljs-built_in">scheduler </span>= torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], <span class="hljs-attribute">gamma</span>=0.1)<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(0, 80):    <br>scheduler.<span class="hljs-keyword">step</span>()        <br>train(.)    <br>val(.)<br><br><span class="hljs-comment"># Learning rate warmup by 10 epochs.</span><span class="hljs-built_in"></span><br><span class="hljs-built_in">scheduler </span>= torch.optim.lr_scheduler.LambdaLR(optimizer, <span class="hljs-attribute">lr_lambda</span>=lambda t: t / 10)<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(0, 10):    <br>scheduler.<span class="hljs-keyword">step</span>()    <br>train(.)    <br>val(.)<br></code></pre></td></tr></table></figure><h3 id="优化器链式更新"><a href="#优化器链式更新" class="headerlink" title="优化器链式更新"></a>优化器链式更新</h3><p>从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br><span class="hljs-keyword">from</span> torch.optim import SGD<br><span class="hljs-keyword">from</span> torch.optim.lr_scheduler <br>import Exponential<br>LR, StepLRmodel = [torch.nn.Parameter(torch.randn(2, 2, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>))]<br>optimizer = SGD(model, 0.1)<br>scheduler1 = ExponentialLR(optimizer, <span class="hljs-attribute">gamma</span>=0.9)<br>scheduler2 = StepLR(optimizer, <span class="hljs-attribute">step_size</span>=3, <span class="hljs-attribute">gamma</span>=0.1)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(4):    <br><span class="hljs-built_in">print</span>(epoch, scheduler2.get_last_lr()[0])    <br>optimizer.<span class="hljs-keyword">step</span>()    <br>scheduler1.<span class="hljs-keyword">step</span>()    <br>scheduler2.<span class="hljs-keyword">step</span>()<br></code></pre></td></tr></table></figure><h3 id="模型训练可视化"><a href="#模型训练可视化" class="headerlink" title="模型训练可视化"></a>模型训练可视化</h3><p>PyTorch可以使用tensorboard来可视化训练过程。</p><p>安装和运行TensorBoard。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">pip install tensorboard<br>tensorboard  <span class="hljs-attribute">-logdir</span>=runs<br></code></pre></td></tr></table></figure><p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如’Loss&#x2F;train’和’Loss&#x2F;test’。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">from torch.utils.tensorboard <br>import SummaryWriter<br>import numpy <span class="hljs-keyword">as</span> np<br><br>writer = <span class="hljs-constructor">SummaryWriter()</span><br><span class="hljs-keyword">for</span> n_iter <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):   <br>writer.add<span class="hljs-constructor">_scalar(&#x27;Loss<span class="hljs-operator">/</span><span class="hljs-params">train</span>&#x27;, <span class="hljs-params">np</span>.<span class="hljs-params">random</span>.<span class="hljs-params">random</span>()</span>, n_iter)<br>writer.add<span class="hljs-constructor">_scalar(&#x27;Loss<span class="hljs-operator">/</span><span class="hljs-params">test</span>&#x27;, <span class="hljs-params">np</span>.<span class="hljs-params">random</span>.<span class="hljs-params">random</span>()</span>, n_iter)<br>writer.add<span class="hljs-constructor">_scalar(&#x27;Accuracy<span class="hljs-operator">/</span><span class="hljs-params">train</span>&#x27;, <span class="hljs-params">np</span>.<span class="hljs-params">random</span>.<span class="hljs-params">random</span>()</span>, n_iter)<br>writer.add<span class="hljs-constructor">_scalar(&#x27;Accuracy<span class="hljs-operator">/</span><span class="hljs-params">test</span>&#x27;, <span class="hljs-params">np</span>.<span class="hljs-params">random</span>.<span class="hljs-params">random</span>()</span>, n_iter)<br></code></pre></td></tr></table></figure><h3 id="保存与加载断点"><a href="#保存与加载断点" class="headerlink" title="保存与加载断点"></a><strong>保存与加载断点</strong></h3><p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs lua">start_epoch = <span class="hljs-number">0</span><br># Load checkpoint.<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">resume</span>: <br># <span class="hljs-built_in">resume</span>为参数，第一次训练时设为<span class="hljs-number">0</span>，中断再训练时设为<span class="hljs-number">1</span>    <br>model_path = <span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-string">&#x27;model&#x27;</span>, <span class="hljs-string">&#x27;best_checkpoint.pth.tar&#x27;</span>)    <br><span class="hljs-built_in">assert</span> <span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.isfile(model_path)    <br>checkpoint = torch.<span class="hljs-built_in">load</span>(model_path)    <br>best_acc = checkpoint[<span class="hljs-string">&#x27;best_acc&#x27;</span>]    <br>start_epoch = checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]    <br>model.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model&#x27;</span>])<br>optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer&#x27;</span>])   <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Load checkpoint at epoch &#123;&#125;.&#x27;</span>.<span class="hljs-built_in">format</span>(start_epoch))   <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Best accuracy so far &#123;&#125;.&#x27;</span>.<span class="hljs-built_in">format</span>(best_acc))<br>    <br># Train the model<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(start_epoch, num_epochs):     <br>. <br>    # Test the model    <br>    .<br>    # save checkpoint    <br>    is_best = current_acc &gt; best_acc    <br>    best_acc = <span class="hljs-built_in">max</span>(current_acc, best_acc)    <br>    checkpoint = &#123;<br>    <span class="hljs-string">&#x27;best_acc&#x27;</span>: best_acc,<br>    <span class="hljs-string">&#x27;epoch&#x27;</span>: epoch + <span class="hljs-number">1</span>,<br>    <span class="hljs-string">&#x27;model&#x27;</span>: model.state_dict(),<br>    <span class="hljs-string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(), &#125;    <br>    model_path = <span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-string">&#x27;model&#x27;</span>, <span class="hljs-string">&#x27;checkpoint.pth.tar&#x27;</span>)    <br>    best_model_path = <span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-string">&#x27;model&#x27;</span>, <span class="hljs-string">&#x27;best_checkpoint.pth.tar&#x27;</span>)<br>    torch.save(checkpoint, model_path)    <br>    <span class="hljs-keyword">if</span> is_best:<br>    shutil.copy(model_path, best_model_path)<br></code></pre></td></tr></table></figure><h3 id="提取-ImageNet-预训练模型某层的卷积特征"><a href="#提取-ImageNet-预训练模型某层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型某层的卷积特征"></a><strong>提取 ImageNet 预训练模型某层的卷积特征</strong></h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># VGG-16 relu5-3 feature.</span><br>model = torchvision.models.vgg16(<span class="hljs-attribute">pretrained</span>=<span class="hljs-literal">True</span>).features[:-1]<br><span class="hljs-comment"># VGG-16 pool5 feature.</span><br>model = torchvision.models.vgg16(<span class="hljs-attribute">pretrained</span>=<span class="hljs-literal">True</span>).features<br><span class="hljs-comment"># VGG-16 fc7 feature.</span><br>model = torchvision.models.vgg16(<span class="hljs-attribute">pretrained</span>=<span class="hljs-literal">True</span>)<br><br>model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3])<br><span class="hljs-comment"># ResNet GAP feature.</span><br>model = torchvision.models.resnet18(<span class="hljs-attribute">pretrained</span>=<span class="hljs-literal">True</span>)<br>model = torch.nn.Sequential(collections.OrderedDict(    list(model.named_children())[:-1]))<br><br>with torch.no_grad():    <br>model.eval()    <br>conv_representation = model(image)<br></code></pre></td></tr></table></figure><h3 id="提取-ImageNet-预训练模型多层的卷积特征"><a href="#提取-ImageNet-预训练模型多层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型多层的卷积特征"></a><strong>提取 ImageNet 预训练模型多层的卷积特征</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeatureExtractor</span>(torch.nn.Module):   <br><span class="hljs-string">&quot;&quot;&quot;Helper class to extract several convolution features from the given pre-trained model.</span><br><span class="hljs-string">    Attributes:        </span><br><span class="hljs-string">    _model, torch.nn.Module.        </span><br><span class="hljs-string">    _layers_to_extract, list&lt;str&gt; or set&lt;str&gt;</span><br><span class="hljs-string">    Example:        </span><br><span class="hljs-string">    &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)       </span><br><span class="hljs-string">    &gt;&gt;&gt; model = torch.nn.Sequential(collections.OrderedDict(                list(model.named_children())[:-1]))        </span><br><span class="hljs-string">    &gt;&gt;&gt; conv_representation = FeatureExtractor( pretrained_model=model,                layers_to_extract=&#123;&#x27;layer1&#x27;, &#x27;layer2&#x27;, &#x27;layer3&#x27;, &#x27;layer4&#x27;&#125;)(image)    </span><br><span class="hljs-string">    &quot;&quot;&quot;</span>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pretrained_model, layers_to_extract</span>):  <br>    torch.nn.Module.__init__(self)        <br>    self._model = pretrained_model       <br>        self._model.<span class="hljs-built_in">eval</span>()        <br>        self._layers_to_extract = <span class="hljs-built_in">set</span>(layers_to_extract)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>): <br>    <span class="hljs-keyword">with</span> torch.no_grad():           <br>        conv_representation = []            <br>        <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> self._model.named_children():  <br>            x = layer(x)               <br>            <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> self._layers_to_extract: <br>            conv_representation.append(x)           <br>        <span class="hljs-keyword">return</span> conv_representation<br></code></pre></td></tr></table></figure><h3 id="微调全连接层"><a href="#微调全连接层" class="headerlink" title="微调全连接层"></a><strong>微调全连接层</strong></h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros">model = torchvision.models.resnet18(<span class="hljs-attribute">pretrained</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():    <br>param.requires_grad = <span class="hljs-literal">False</span><br>model.fc = nn.Linear(512, 100)  # Replace the last fc layer<br>optimizer = torch.optim.SGD(model.fc.parameters(), <span class="hljs-attribute">lr</span>=1e-2, <span class="hljs-attribute">momentum</span>=0.9, <span class="hljs-attribute">weight_decay</span>=1e-4)<br></code></pre></td></tr></table></figure><h3 id="以较大学习率微调全连接层，较小学习率微调卷积层"><a href="#以较大学习率微调全连接层，较小学习率微调卷积层" class="headerlink" title="以较大学习率微调全连接层，较小学习率微调卷积层"></a><strong>以较大学习率微调全连接层，较小学习率微调卷积层</strong></h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">model = torchvision<span class="hljs-selector-class">.models</span><span class="hljs-selector-class">.resnet18</span>(pretrained=True)<br>finetuned_parameters = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(id, model<span class="hljs-selector-class">.fc</span><span class="hljs-selector-class">.parameters</span>()))<br>conv_parameters = (<span class="hljs-selector-tag">p</span> <span class="hljs-keyword">for</span> <span class="hljs-selector-tag">p</span> <span class="hljs-keyword">in</span> model<span class="hljs-selector-class">.parameters</span>() <span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span>(p) not <span class="hljs-keyword">in</span> finetuned_parameters)<br>parameters = <span class="hljs-selector-attr">[&#123;<span class="hljs-string">&#x27;params&#x27;</span>: conv_parameters, <span class="hljs-string">&#x27;lr&#x27;</span>: 1e-3&#125;,  &#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.fc.parameters()&#125;]</span><br>optimizer = torch<span class="hljs-selector-class">.optim</span><span class="hljs-selector-class">.SGD</span>(parameters, lr=<span class="hljs-number">1</span>e-<span class="hljs-number">2</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1</span>e-<span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure><h2 id="6-其他注意事项"><a href="#6-其他注意事项" class="headerlink" title="6. 其他注意事项"></a>6. 其他注意事项</h2><p>不要使用太大的线性层。因为nn.Linear(m,n)使用的是的内存，线性层太大很容易超出现有显存。</p><p>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</p><p>model(x) 前用 model.train() 和 model.eval() 切换网络状态。</p><p>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</p><p>model.eval() 和 torch.no_grad() 的区别在于，model.eval() 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()。</p><p>model.zero_grad()会把整个模型的参数的梯度都归零, 而optimizer.zero_grad()只会把传入其中的参数的梯度归零.</p><p>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</p><p>loss.backward() 前用 optimizer.zero_grad() 清除累积梯度。</p><p>torch.utils.data.DataLoader 中尽量设置 pin_memory&#x3D;True，对特别小的数据集如 MNIST 设置 pin_memory&#x3D;False 反而更快一些。num_workers 的设置需要在实验中找到最快的取值。</p><p>用 del 及时删除不用的中间变量，节约 GPU 存储。</p><p>使用 inplace 操作可节约 GPU 存储，如</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">x</span> = torch.nn.functional.relu(x, inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</p><p>使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</p><p>时常使用 assert tensor.size() &#x3D;&#x3D; (N, D, H, W) 作为调试手段，确保张量维度和你设想中一致。</p><p>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</p><p>统计代码各部分耗时</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">with torch.autograd.profiler.profile(<span class="hljs-attribute">enabled</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">use_cuda</span>=<span class="hljs-literal">False</span>) as profile:    .<span class="hljs-built_in">print</span>(profile)# 或者在命令行运行python -m torch.utils.bottleneck main.py<br></code></pre></td></tr></table></figure><p>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean"># pip install torchsnooperimport torchsnooper# 对于函数，使用修饰器@torchsnooper.snoop()# 如果不是函数，使用 <span class="hljs-keyword">with</span> 语句来激活 TorchSnooper，把训练的那个循环装进 <span class="hljs-keyword">with</span> 语句中去。<span class="hljs-keyword">with</span> torchsnooper.snoop():    原本的代码<br></code></pre></td></tr></table></figure><p><a href="https://github.com/zasdfgbnm/TorchSnoopergithub.com">https://github.com/zasdfgbnm/TorchSnoopergithub.com</a></p><p>模型可解释性，使用captum库：<a href="https://captum.ai/captum.aittps://captum.ai/captum.ai">https://captum.ai/captum.aittps://captum.ai/captum.ai</a></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—Optimizer</title>
    <link href="/2022/07/01/DeepLearning/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    <url>/2022/07/01/DeepLearning/%E4%BC%98%E5%8C%96%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="优化的目标"><a href="#优化的目标" class="headerlink" title="优化的目标"></a>优化的目标</h1><p>尽管优化提供了一种最大限度地减少深度学习损失函数的方法，但实质上，优化和深度学习的目标是根本不同的。<br>前者主要关注的是最小化目标，后者则关注在给定有限数据量的情况下寻找合适的模型。</p><h1 id="深度学习优化的挑战"><a href="#深度学习优化的挑战" class="headerlink" title="深度学习优化的挑战"></a>深度学习优化的挑战</h1><p>其中一些最令人烦恼的是局部最小值、鞍点和梯度消失。让我们来看看它们。</p><h3 id="局部最小值"><a href="#局部最小值" class="headerlink" title="局部最小值"></a>局部最小值</h3><p>对于任何目标函数$f(x)$，如果在$x$处对应的$𝑓(𝑥)$值小于在$x$附近任何其他点的$f(x)$值，那么$f(x)$可能是局部最小值。如果$f(x)$在$x$处的值是整个域上目标函数的最小值，那么$f(x)$是全局最小值。</p><p><img src="/%5Csrc%5Cimage-20220309141205687.png" alt="image-20220309141205687"></p><p>深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数<em>局部</em>最优，而不是<em>全局</em>最优。只有一定程度的噪声可能会使参数超出局部最小值。事实上，这是小批量随机梯度下降的有利特性之一，在这种情况下，小批量上梯度的自然变化能够将参数从局部极小值中移出。</p><h3 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h3><p>除了局部最小值之外，鞍点也是梯度消失的另一个原因。<em>鞍点</em>（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数$𝑓(𝑥)&#x3D;𝑥^3$。它的一阶和二阶导数在$𝑥&#x3D;0$时消失。这时优化可能会停止，尽管它不是最小值。</p><p><img src="/%5Csrc%5Cimage-20220309141416324.png" alt="image-20220309141416324"></p><p>较高维度的鞍点甚至更加隐蔽。考虑这个函数$𝑓(𝑥,𝑦)&#x3D;𝑥^2−𝑦^2$。它的鞍点为(0,0)。这是关于$y$的最大值，也是关于$x$的最小值。此外，它<em>看起来</em>像马鞍，这就是这个数学属性的名字由来。</p><p><img src="/%5Csrc%5Cimage-20220309141507927.png" alt="image-20220309141507927"></p><p>我们假设函数的输入是k维向量，其输出是标量，因此其Hessian矩阵（也称黑塞矩阵）将有k特征值函数的解决方案可以是局部最小值、局部最大值或函数梯度为零的位置处的鞍点：</p><ul><li>当函数在零梯度位置处的Hessian矩阵的特征值全部为正值时，我们有该函数的局部最小值。</li><li>当函数在零梯度位置处的Hessian矩阵的特征值全部为负值时，我们有该函数的局部最大值。</li><li>当函数在零梯度位置处的Hessian矩阵的特征值为负值和正值时，我们对函数有一个鞍点。</li></ul><p>对于高维度问题，至少<em>部分</em>特征值为负的可能性相当高。这使得鞍点比局部最小值更有可能。我们将在下一节介绍凸性时讨论这种情况的一些例外情况。简而言之，凸函数是Hessian函数的特征值永远不是负值的函数。不幸的是，大多数深度学习问题并不属于这个类别。尽管如此，它还是研究优化算法的一个很好的工具。</p><h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>可能遇到的最隐蔽的问题是梯度消失。例如，假设我们想最小化函数$f(x)&#x3D;tanh⁡(x)$，然后我们恰好从$𝑥&#x3D;4$开始。正如我们所看到的那样，$𝑓$的梯度接近零。更具体地说，$𝑓′(𝑥)&#x3D;1−tanh2(𝑥)$，因此是$𝑓′(4)&#x3D;0.0013$。因此，在我们取得进展之前，优化将会停滞很长一段时间。事实证明，这是在引入ReLU激活函数之前训练深度学习模型相当棘手的原因之一。</p><p><img src="/src%5Cimage-20220309141749050.png" alt="image-20220309141749050"></p><p>所以激活函数用Relu是好的。</p><h1 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h1><p><em>凸性</em>（convexity）在优化算法的设计中起到至关重要的作用， 这主要是由于在这种情况下对算法进行分析和测试要容易得多。 换言之，如果该算法甚至在凸性条件设定下的效果很差， 通常我们很难在其他条件下看到好的结果。 此外，即使深度学习中的优化问题通常是非凸的， 它们也经常在局部极小值附近表现出一些凸性。</p><p><strong>凸函数的优化算法或者损失函数是一定存在解析解的。</strong></p><h2 id="什么是凸集、凸函数"><a href="#什么是凸集、凸函数" class="headerlink" title="什么是凸集、凸函数"></a>什么是凸集、凸函数</h2><p><strong><em>凸集</em>（convex set）</strong>是凸性的基础。 简单地说，如果对于任何$𝑎，𝑏∈X$，连接$a$和$b$的线段也位于$X$中，则向量空间中的一个集合$X$是<em>凸</em>（convex）的。 在数学术语上，这意味着对于所有$𝜆∈[0,1]$​，我们得到<br>$$<br>\lambda a + (1- \lambda)b \in X, \qquad &lt;当a,b\in X&gt;<br>$$<br>假设$X$和$Y$是凸集，那么$X∩Y$也是凸集的。</p><p>通常，深度学习中的问题是在凸集上定义的。 例如，$\mathbb{R}^d$，即实数的$d$维向量的集合是凸集（毕竟$\mathbb{R}^d$中任意两点之间的线存在$\mathbb{R}^d$）中。 在某些情况下，我们使用有界长度的变量，例如球的半径定义为${𝐱|𝐱\in \mathbb{R}^d and ‖𝐱‖≤𝑟}$。</p><p>现在我们有了<strong>凸集</strong>，我们可以引入<strong><em>凸函数</em>（convex function）</strong>$f$。 给定一个凸集$X$，如果对于所有$x,x′∈X$和所有$λ∈[0,1]$，一个函数$f:X\to R$​是凸的，我们可以得到<br>$$<br>\lambda f(x)+(1-\lambda)f(x’)\ge f(\lambda x+(1-\lambda x’))<br>$$</p><h2 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h2><p>这是一种对于凸性定义的一个推广<br>$$<br>\sum _{i}\alpha_if(x_i) \ge f(\sum_i\alpha_ix_i)  \quad and \quad E_X[f(X)]\ge f(E_X[X])<br>$$<br>其中$\alpha_i$是非负实数，因此$∑_i\alpha_i&#x3D;1$且$X$是随机变量。凸函数的期望不小于期望的凸函数。</p><h3 id="常见应用：用一个较简单的表达式约束一个较复杂的表达式"><a href="#常见应用：用一个较简单的表达式约束一个较复杂的表达式" class="headerlink" title="常见应用：用一个较简单的表达式约束一个较复杂的表达式"></a><strong>常见应用：用一个较简单的表达式约束一个较复杂的表达式</strong></h3><p>例如，它可以应用于部分观察到的随机变量的对数似然。 具体地说，由于$\int 𝑃(𝑌)𝑃(𝑋∣𝑌)𝑑𝑌&#x3D;𝑃(𝑋)$，所以<br>$$<br>𝐸_{𝑌∼𝑃(𝑌)}[−log𝑃(𝑋∣𝑌)]\ge−log𝑃(𝑋),<br>$$</p><p>这里，$Y$是典型的未观察到的随机变量，$P(Y)$是它可能如何分布的最佳猜测，$P(X)$是将$Y$积分后的分布。 例如，在聚类中$Y$可能是簇标签，而在应用簇标签时，$P(X∣Y)$是生成模型。</p><h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ul><li><p><strong>局部极小值是全局极小值</strong>这一性质是很方便的。 这意味着如果我们最小化函数，我们就不会“卡住”。但是，请注意，这并不意味着不能有多个全局最小值，或者可能不存在一个全局最小值。</p></li><li><p>$f$为凸函数，当且仅当任意二次可微一维函数$f:\mathbb{R}^n\to \mathbb{R}$是凸的。 对于任意二次可微多维函数$f:\mathbb{R}^n\to \mathbb{R}$， 它是凸的当且仅当它的Hessian$∇^2𝑓⪰0$。</p></li><li><p>凸优化的一个很好的特性是能够让我们有效地处理<em><strong>约束</strong></em>（constraints）。 即它使我们能够解决以下形式的<em><strong>约束优化</strong></em>（constrained optimization）问题</p><ul><li><p>拉格朗日函数<br>$$<br>L(\mathbb{x}, \alpha_1,\dots, \alpha_n)&#x3D;f(\mathbb{x})+\sum_{i&#x3D;1}^n\alpha_ic_i(\mathbb{x}) \qquad where \quad \alpha_i\ge 0.<br>$$<br>这里的变量$\alpha_i(i&#x3D;1,\dots,n)$是所谓的<em>拉格朗日乘数</em>（Lagrange multipliers），它确保约束被正确地执行。 选择它们的大小足以确保所有$i$的$c_i(\mathbb{x})$。</p></li><li><p>惩罚: 一种至少近似地满足约束优化问题的方法是采用拉格朗日函数$L$。除了满足$c_i(\mathbb{x})$之外，我们只需将$\alpha_ic_i(\mathbb{x})$添加到目标函数$f(x)$。 这样可以确保不会严重违反约束。</p><p>事实上，我们一直在使用这个技巧。在目标函数中加入$\frac{\lambda}{2}|\mathcal{<br>w}|^2$​​，以确保$\mathcal{<br>w}$​不会增长太大。 使用约束优化的观点，我们可以看到，对于若干半径$r$​，这将确保$|\mathcal{<br>w}|^2 - r^2 \le 0$​。 通过调整$\lambda$​的值，我们可以改变$\mathcal{<br>w}$的大小。</p><p>通常，添加惩罚是确保近似满足约束的一种好方法。 在实践中，这被证明比精确的满意度更可靠。 此外，对于非凸问题，许多使精确方法在凸情况下的性质（例如，可求最优解）不再成立。</p></li><li><p>投影:例如我们在进行梯度剪裁的时候，就运用的到投影的技巧。$g*min(1, \frac{\theta}{||g||}) \to g$</p></li></ul></li></ul><h1 id="梯度下降Gradient-Descent"><a href="#梯度下降Gradient-Descent" class="headerlink" title="梯度下降Gradient Descent"></a>梯度下降Gradient Descent</h1><p>梯度下降实际上是利用泰勒公式对损失函数进行展开，考究原函数与一阶导数之间的关系。</p><p>其中最关键的参数就是步长$\eta$，在机器学习中又称为学习率。</p><p>学习率决定目标函数能否收敛到局部最小值，以及何时能够收敛到最小值</p><p>学习率太大容易发生震荡， 太小难以收敛。那么如何选择一个合适的学习率就是一个值得考量的事情。</p><ul><li>牛顿法：$\nabla f(x) + H\xi \to \xi&#x3D;-H^{-1}\nabla f(x)$ 但是若H为负数， 那么这个损失函数就不会收敛。我们可以利用绝对值或是重新定义学习率来解决</li><li>预处理：整个Hessian矩阵用于计算与存贮是一件非常昂贵的事情。为了规避这一点， 我们可以只计算对角线上的数值。即$x \gets x- \eta diag(H^{-1})\nabla f(x)$</li></ul><p>综上，我们可以发现一个问题， 梯度下降可能会陷入局部最小值，而得不到全局最小值</p><p>常用的三种梯度下降方法，梯度下降法， 随机梯度下降， 小批量随机梯度下降。</p><p>小批量随机梯度下降往往能够提升运算速度， 平均梯度的方式减少了方差， 但是在产生下降过程中易出现左右振荡的现象。而动量梯度下降法通过减小振荡对算法进行优化。动量梯度下降法的核心便是对一系列梯度进行指数加权平均。</p><p><img src="/%5Csrc%5Cimage-20220309160114281.png" alt="image-20220309160114281"><img src="/%5Csrc%5Cimage-20220309160122493.png" alt="image-20220309160122493"></p><p>动量法计算计算公式。他使用指数加权法 让梯度拥有所有信息<br>$$<br>V_t \gets \beta V_{t-1} + g_{t,t-1}\<br>X_t \gets X_{t-1} + \eta_{t}V_t<br>$$</p><ul><li><p>在 <code>sgd</code>中，我们学习了：随机梯度下降在解决优化问题时比梯度下降更有效。<br>$$<br>X \gets X - \eta\nabla f_i(x)<br>$$</p></li><li><p>在 <code>minibatch_sgd</code>中，我们学习了：在一个小批量中使用更大的观测值集，可以通过向量化提供额外效率。这是高效的多机、多GPU和整体并行处理的关键。<br>$$<br>X \gets X - \eta\nabla_w \frac{1}{|B_t|}\sum_{i\in B_t}f(x_i,w)<br>$$</p></li><li><p>在 <code>momentum</code>中我们添加了一种机制，用于汇总过去梯度的历史以加速收敛。<br>利用泄漏平均值替代梯度计算<br>$$<br>g_t &#x3D; \nabla_w \frac{1}{|B_t|}\sum_{i\in B_t}f(x_i,w)\<br>V_t &#x3D; \beta V_{t-1} + g_t  ,\quad \beta\in(0, 1)<br>$$<br>$\beta^{-1}$作为控制泄漏平均值能够记住多少过去梯度的平均值，又被称为半衰期。$V_t$被称为动量</p></li><li><p>在 <code>adagrad</code>中，我们使用每个坐标缩放来实现计算效率的预处理。<br>$$<br>g_t &#x3D; \nabla_w l(y_t, f(x_w, w))\<br>S_t &#x3D; \rho S_{t-1} + (1-\rho)g_t^2\<br>w_t &#x3D; w_{t-1}-\frac{\eta}{\sqrt{S_t+\xi}}g_t<br>$$</p></li><li><p>在 <code>rmsprop</code>中，我们通过学习率的调整来分离每个坐标的缩放。<br>$$<br>S_t &#x3D; \rho S_{t-1} + (1-\rho)g_t^2\<br>X_t &#x3D; X_{t-1}-\frac{\eta}{\sqrt{S_t+\xi}}\odot g_t<br>$$</p></li><li><p>在<code>adadelta</code>中，我们取消了学习率， 利用两个状态变量，$S_t$用于存储梯度二阶导数的漏平均值，$\Delta x_t$​用于存储模型本身中参数变化二阶导数的泄露平均值。<br>$$<br>S_t &#x3D; \rho S_{t-1} + (1-\rho)g_t^2 \<br>g_t’ &#x3D; \frac{\sqrt{\Delta x_{t-1}+\xi}}{\sqrt{S_t+\xi}}\odot g_t \<br> X_t &#x3D;  X_{t-1} - g_t’\<br>\Delta x_t &#x3D; \rho \Delta x_{t-1} + (1-\rho){g_t’}^2<br>$$</p></li><li><p>而<code>adam</code>,其算法的关键组成部分之一是：它使用指数加权移动平均值来估算梯度的动量和第二力矩，即它使用状态变量<br>$$<br>V_t\gets \beta_1V_{t−1}+(1−\beta_1)g_t,\<br>S_t\gets \beta_2S_{t−1}+(1−\beta_2)g_t^2.<br>$$<br>这里$ 𝛽1 $和 $𝛽2$ 是非负加权参数。 他们的常见设置是 $𝛽1&#x3D;0.9$ 和$ 𝛽2&#x3D;0.999 $​。 也就是说，方差的估计比动量的估计移动得远远更慢。为了避免初始化的干扰，<br>$$<br>\hat{V_t} &#x3D; \frac{V_t}{1-\beta_1^t} \quad and \quad \hat{S_t} &#x3D; \frac{S_t}{1-\beta_2^t} \<br>g_t’ &#x3D; \frac{\eta\hat{V_t}}{\sqrt{\hat{S_t}}+\xi} \<br>X_t \gets X_{t-1}-g_t’<br>$$</p></li></ul><h1 id="学习率调度器"><a href="#学习率调度器" class="headerlink" title="学习率调度器"></a>学习率调度器</h1><ul><li>首先，学习率的大小很重要。如果它太大，优化就会发散；如果它太小，训练就会需要过长时间，或者我们最终只能得到次优的结果。我们之前看到问题的条件数很重要。直观地说，这是最不敏感与最敏感方向的变化量的比率。</li><li>其次，衰减速率同样很重要。如果学习率持续过高，我们可能最终会在最小值附近弹跳，从而无法达到最优解。 简而言之，我们希望速率衰减，但要比$\Omega(t^{-\frac{1}{2}})$慢，这样能成为解决凸问题的不错选择。</li><li>另一个同样重要的方面是初始化。这既涉及参数最初的设置方式，又关系到它们最初的演变方式。这被戏称为<em>预热</em>（warmup），即我们最初开始向着解决方案迈进的速度有多快。一开始的大步可能没有好处，特别是因为最初的参数集是随机的。最初的更新方向可能也是毫无意义的。</li><li>最后，还有许多优化变体可以执行周期性学习率调整。</li></ul><p>多项式衰减和分段常数表。 此外，余弦学习率调度在实践中的一些问题上运行效果很好。 在某些问题上，最好在使用较高的学习率之前预热优化器。<a href="https://pytorch.org/docs/stable/optim.html">详情参考</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">model = [Parameter(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, requires_grad=<span class="hljs-literal">True</span>))]<br>optimizer = SGD(model, <span class="hljs-number">0.1</span>)<br>scheduler = ExponentialLR(optimizer, gamma=<span class="hljs-number">0.9</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>        optimizer.zero_grad()<br>        output = model(<span class="hljs-built_in">input</span>)<br>        loss = loss_fn(output, target)<br>        loss.backward()<br>        optimizer.step()<br>    scheduler.step()<br></code></pre></td></tr></table></figure><ul><li>在训练期间逐步降低学习率可以提高准确性，并且减少模型的过拟合。</li><li>在实验中，每当进展趋于稳定时就降低学习率，这是很有效的。从本质上说，这可以确保我们有效地收敛到一个适当的解，也只有这样才能通过降低学习率来减小参数的固有方差。</li><li>余弦调度器在某些计算机视觉问题中很受欢迎。</li><li>优化之前的预热期可以防止发散。</li><li>优化在深度学习中有多种用途。对于同样的训练误差而言，选择不同的优化算法和学习率调度，除了最大限度地减少训练时间，可以导致测试集上不同的泛化和过拟合量。</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>最小化训练误差并<strong>不能</strong>保证我们找到最佳的参数集来最小化泛化误差。</li><li>优化问题可能有许多局部最小值。</li><li>问题可能有更多的鞍点，因为通常问题不是凸的。</li><li>梯度消失可能会导致优化停滞，重参数化通常会有所帮助。对参数进行良好的初始化也可能是有益的。</li><li>根据Jensen不等式，“一个多变量凸函数的总期望值”大于或等于“用每个变量的期望值计算这个函数的总值“。</li><li>一个二次可微函数是凸函数，当且仅当其Hessian（二阶导数矩阵）是半正定的。</li><li>动量法用过去梯度的平均值来替换梯度，这大大加快了收敛速度。</li><li>对于无噪声梯度下降和嘈杂随机梯度下降，动量法都是可取的。</li><li>动量法可以防止在随机梯度下降的优化过程停滞的问题。</li><li>由于对过去的数据进行了指数降权，有效梯度数为11−𝛽11−β。</li><li>在凸二次问题中，可以对动量法进行明确而详细的分析。</li><li>动量法的实现非常简单，但它需要我们存储额外的状态向量（动量𝐯）。</li><li>AdaGrad算法会在单个坐标层面动态降低学习率。</li><li>AdaGrad算法利用梯度的大小作为调整进度速率的手段：用较小的学习率来补偿带有较大梯度的坐标。</li><li>在深度学习问题中，由于内存和计算限制，计算准确的二阶导数通常是不可行的。梯度可以作为一个有效的代理。</li><li>如果优化问题的结构相当不均匀，AdaGrad算法可以帮助缓解扭曲。</li><li>AdaGrad算法对于稀疏特征特别有效，在此情况下由于不常出现的问题，学习率需要更慢地降低。</li><li>在深度学习问题上，AdaGrad算法有时在降低学习率方面可能过于剧烈。</li><li>RMSProp算法与Adagrad算法非常相似，因为两者都使用梯度的平方来缩放系数。</li><li>RMSProp算法与动量法都使用泄漏平均值。但是，RMSProp算法使用该技术来调整按系数顺序的预处理器。</li><li>Adam算法将许多优化算法的功能结合到了相当强大的更新规则中。规则中。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—Layer说明</title>
    <link href="/2022/07/01/DeepLearning/%E5%90%84%E4%B8%AA%E5%B1%82%E7%9A%84%E8%A7%A3%E9%87%8A/"/>
    <url>/2022/07/01/DeepLearning/%E5%90%84%E4%B8%AA%E5%B1%82%E7%9A%84%E8%A7%A3%E9%87%8A/</url>
    
    <content type="html"><![CDATA[<p><img src="/src/640-165691974012416.png" alt="图片"></p><h1 id="1-卷积运算与卷积层"><a href="#1-卷积运算与卷积层" class="headerlink" title="1.卷积运算与卷积层"></a>1.卷积运算与卷积层</h1><p>说卷积层，我们得先从卷积运算开始，卷积运算就是卷积核在输入信号（图像）上滑动， 相应位置上进行<strong>「乘加」</strong>。卷积核又称为滤过器，过滤器，可认为是某种模式，某种特征。</p><p>卷积过程类似于用一个模板去图像上寻找与它相似的区域， 与卷积核模式越相似， 激活值越高， 从而实现特征提取。好吧，估计依然懵逼，下面我们就看看 1d、2d、3d的卷积示意图，通过动图的方式看看卷积操作到底在干啥？</p><h2 id="1-1-1d-2d-3d-卷积示意"><a href="#1-1-1d-2d-3d-卷积示意" class="headerlink" title="1.1 1d 2d 3d 卷积示意"></a>1.1 1d 2d 3d 卷积示意</h2><p>一般情况下，卷积核在几个维度上滑动，就是几维卷积。下面再看几张动图感受一下不同维度的卷积操作，注意下面都是一个卷积核：</p><p>一维卷积示意</p><p><img src="/src/640-16464011129871-165691974235919.gif" alt="图片"></p><p>二维卷积示意</p><p><img src="/src/640-16464011129872-165691974500122.gif" alt="图片"></p><p>三维卷积示意</p><p><img src="/src/640-16464011129893-165691975338925.gif" alt="图片"></p><h2 id="1-2-nn-Conv2d"><a href="#1-2-nn-Conv2d" class="headerlink" title="1.2 nn.Conv2d"></a>1.2 nn.Conv2d</h2><p><code>nn.Conv2d</code>: 对多个二维信号进行二维卷积</p><p><img src="/src/640-16464011129894-165691975764428.png" alt="图片"></p><p>主要参数：</p><ul><li>in_channels: 输入通道数</li><li>out_channels: 输出通道数， 等价于卷积核个数</li><li>kernel_size: 卷积核尺寸， 这个代表着卷积核的大小</li><li>stride: 步长， 这个指的卷积核滑动的时候，每一次滑动几个像素。下面看个动图来理解步长的概念：左边那个的步长是 1， 每一次滑动 1 个像素，而右边的步长是 2，会发现每一次滑动 2个像素。</li></ul><p><img src="/src/640-16464011129895-165691976075031.gif" alt="图片"></p><ul><li>padding: 填充个数，通常用来保持输入和输出图像的一个尺寸的匹配，依然是一个动图展示，看左边那个图，这个是没有 padding 的卷积，输入图像是 4 * 4，经过卷积之后，输出图像就变成了 2 * 2 的了，这样分辨率会遍变低，并且我们会发现这种情况卷积的时候边缘部分的像素参与计算的机会比较少。所以加入考虑 padding 的填充方式，这个也比较简单，就是在原输入周围加入像素，这样就可以保证输出的图像尺寸分辨率和输入的一样，并且边缘部分的像素也受到同等的关注了。</li></ul><p><img src="/src/640-16464011129896-165691976230334.gif" alt="图片"></p><ul><li><p>dilation: 孔洞卷积大小，下面依然是一个动图：</p><p><img src="/src/640-16464011129897-165691976455037.gif" alt="图片"></p><p>孔洞卷积就可以理解成一个带孔的卷积核，常用于图像分割任务，主要功能就是提高感受野。也就是输出图像的一个参数，能看到前面图像更大的一个区域。</p></li><li><p>groups: 分组卷积设置，分组卷积常用于模型的轻量化。我们之前的 AlexNet 其实就可以看到分组的身影， 两组卷积分别进行提取，最后合并。<img src="/src/640-16464011129898-165691976638840.png" alt="图片"></p></li><li><p>bias: 偏置</p></li></ul><p>下面是尺寸计算的方式：</p><ol><li>没有 padding：</li><li>如果有 padding 的话：</li><li>如果再加上孔洞卷积的话：</li></ol><p>下面我们用代码看看卷积核是怎么提取特征的，毕竟有图才有真相：</p><p><img src="/src/640-16464011129909-165691977038443.png" alt="图片"></p><p>接下来，我们改变seed， 也就是相当于换一组卷积核， 看看提取到什么样的特征：</p><p><img src="/src/640-164640111299010-165691977227646.png" alt="图片"></p><p>再换一个随机种子：</p><p><img src="/src/640-164640111299011-165691977391849.png" alt="图片"></p><p>通过上面，我们会发现不同权重的卷积核代表不同的模式，会关注不同的特征，这样我们只要设置多个卷积核同时对图片的特征进行提取，就可以提取不同的特征。</p><p>下面我们看一下图像尺寸的变化：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">卷积前尺寸:torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 3, 512, 512]</span>)<br>卷积后尺寸:torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 1, 510, 510]</span>)<br></code></pre></td></tr></table></figure><p>卷积前，图像尺寸是 , 卷积后，图像尺寸是 。我们这里的卷积核设置，输入通道 3，卷积核个数 1，卷积核大小 3，无 padding，步长是 1，那么我们根据上面的公式，输出尺寸:</p><p>下面再来看一下卷积层有哪些参数：我们知道卷积层也是继承于 nn.Module 的，所以肯定又是那 8 个字典属性， 我们主要看看它的<code>_modules</code>参数和<code>_parameters</code>参数字典。</p><p><img src="F:\notes\深度学习\src\640-164640111299012.png" alt="图片"></p><p>我们可以看到 Conv2d 下面的<code>_parameters</code>存放着权重参数，这里的 weight 的形状是 [1, 3, 3, 3]， 这个应该怎么理解呢？首先 1 代表着卷积核的个数，第 1 个 3 表示的输入通道数，后面两个 3 是二维卷积核的尺寸。那么这里有人可能会有疑问，我们这里是3维的卷积核啊，怎么实现的二维卷积呢？下面再通过一个示意图看看：</p><p><img src="F:\notes\深度学习\src\640-164640111299013.gif" alt="图片"></p><p>我们的图像是 RGB 3 个通道的图像，我们创建 3 个二维的卷积核，这 3 个二维的卷积核分别对应一个通道进行卷积，比如红色通道上，只有一个卷积核在上面滑动，每一次滑动，对应元素相乘然后相加得到一个数， 这三个卷积核滑动一次就会得到三个数，这三个数之和加上偏置才是我们的一个输出结果。这里我们看到了，一个卷积核只在 2 个维度上滑动，所以最后得到的就是 2 维卷积。这也能理解开始的卷积维度的概念了（一般情况下，卷积核在几个维度上滑动，就是几维卷积），为什么最后会得到的 3 维的张量呢？这是因为我们不止这一个卷积核啊，我们有多个卷积核的时候，每个卷积核都产生一个二维的结果，那么最后的输出不就成 3 维的了，第三个维度就是卷积核的个数了。下面用一个网站上的神图来看一下多个卷积核的提取特征，下面每一块扫描都是对应元素相乘再相加得到最后的的结果：</p><p><img src="/src/640-164640111299014-165691977665652.gif" alt="图片"></p><p>上面这一个是一个三维的卷积示意，并且使用了 2 个卷积核。最后会得到 2 个二维的张量。</p><p>二维卷积差不多说到这里吧，不明白我也没招了，我这已经浑身解数了，看这些动图也能看到吧，哈哈。毕竟这里主要讲 Pytorch，关于这些深度学习的基础这里不做过多的描述。下面再介绍一个转置卷积，看看这又是个啥？</p><h2 id="1-3-转置卷积"><a href="#1-3-转置卷积" class="headerlink" title="1.3 转置卷积"></a>1.3 转置卷积</h2><p>转置卷积又称为反卷积和部分跨越卷积（当然转置卷积这个名字比逆卷积要好，原因在下面），用于对图像进行上采样。在图像分割任务中经常被使用。首先为什么它叫转置卷积呢？</p><p>在解释这个之前，我们得先来看看正常的卷积在代码实现过程中的一个具体操作：对于正常的卷积，我们需要实现大量的相乘相加操作，而这种乘加的方式恰好是矩阵乘法所擅长的。所以在代码实现的时候，通常会借助矩阵乘法快速的实现卷积操作， 那么这是怎么做的呢？</p><p>我们假设图像尺寸为 , 卷积核为 , padding&#x3D;0, stride&#x3D;1，也就是下面这个图：</p><p><img src="/src/640-164640111299015-165691978158855.gif" alt="图片"></p><p>首先将图像尺寸的 拉长成 ，16 代表所有的像素，1 代表只有 1 张图片。然后 的卷积核会变成一个 的一个矩阵，一脸懵逼了，这是怎么变的，首先这个 16，是先把 9 个权值拉成一列，然后下面补 7 个 0 变成16， 这个 4 是根据我们输出的尺寸计算的，根据输入尺寸，卷积核大小，padding， stride 信息可以得到输出尺寸是 ， 所以输出是 ，那么拉成一列就是 4。这样我们的输出：</p><p>这样就得到了最后一列输出 4 个元素，然后 reshape 就得到了 的一个输出特征图了。这就是用矩阵乘法输出一个二维卷积的这样一个示例。这就是那个过程：</p><p><img src="/src/640-164640111299016-165691978313158.png" alt="图片"></p><p>下面我们看看转置卷积是怎么样的：</p><p>转置卷积是一个上采样，输入的图像尺寸是比较小的，经过转置卷积之后，会输出一个更大的图像，看下面示意图：</p><p><img src="/src/640-164640111299117-165691981478961.gif" alt="图片"></p><p>我们这里的输入图像尺寸是 ， 卷积核为 ， padding&#x3D;0， stride&#x3D;1, 我们的输入图像尺寸是 ，我们看看这个在代码中是怎么通过矩阵乘法进行实现的。首先，依然是把输入的尺寸进行拉长，成一个 的一个向量，然后我们的卷积核会变成 的，注意这里的卷积核尺寸，这个 4 依然是根据卷积核得来的，记得上面的那个 16 吗？我们是把卷积核拉长然后补 0， 在这里我们不是补 0 了，而是采用剔除的方法，因为我们根据上面的图像可以发现，虽然这里的卷积核有 9 个权值，可是能与图像相乘的最多只有四个（也就是卷积核在中间的时候），所以这里采用剔除的方法，从 9 个权值里面剔除 5 个，得到 4 个权重， 而这个 16，依然是根据输出图像的尺寸计算得来的。因为我们这里的输出是 ， 这个可以用上面尺寸运算的逆公式。所以这里的输出：</p><p>这次注意这个卷积核的尺寸是 ，而我们正常卷积运算的卷积核尺寸 ，所以在形状上这两个卷积操作卷积核恰恰是转置的关系，这也就是转置卷积的由来了。这是因为卷积核在转变成矩阵的时候，与正常卷积的卷积核形状上是互为转置，注意是形状，具体数值肯定是不一样的。所以正常卷积核转置卷积的关系并不是可逆的，故逆卷积这个名字不好。下面就具体学习 Pytorch 提供的转置卷积的方法：</p><p><code>nn.ConvTranspose2d</code>: 转置卷积实现上采样</p><p><img src="/src/640-164640111299118-165691981878464.png" alt="图片"></p><p>这个参数和卷积运算的参数差不多，就不再解释一遍了。</p><p>下面看看转置卷积的尺寸计算（卷积运算的尺寸逆）：</p><ol><li>无 padding：</li><li>有 padding：</li><li>有 padding 和孔洞卷积：</li></ol><p>下面从代码中看看转置卷积怎么用：</p><p><img src="/src/640-164640111299119-165691982268167.png" alt="图片"></p><p>转置卷积有个通病叫做“棋盘效应”，看上面图，这是由于不均匀重叠导致的。至于如何解决，这里就不多说了。</p><p>关于尺寸变化：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">卷积前尺寸:torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 3, 512, 512]</span>)<br>卷积后尺寸:torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 1, 1025, 1025]</span>)<br></code></pre></td></tr></table></figure><p>我们发现，输入图像是 512 的，卷积核大小是 3，stride&#x3D;2， 所以输出尺寸：</p><p>简单梳理，卷积部分主要是卷积运算，卷积尺寸的计算，然后又学习了转置卷积。下面我们看看 nn 中其他常用的层。</p><h1 id="2-池化层"><a href="#2-池化层" class="headerlink" title="2.池化层"></a>2.池化层</h1><p>池化运算：对信号进行“<strong>「收集」</strong>”并“<strong>「总结」</strong>”， 类似水池收集水资源， 因而美其名曰池化层。</p><ul><li>收集：多变少，图像的尺寸由大变小</li><li>总结：最大值&#x2F;平均值</li></ul><p>下面是一个最大池化的动态图看一下（平均池化就是这些元素去平均值作为最终值）：</p><p><img src="/src/640-164640111299120-165691982723470.gif" alt="图片"></p><p>最大池化就是这些元素里面去最大的值作为最终的结果。</p><p>下面看看 Pytorch 提供的最大池化和平均池化的函数：</p><p>nn.MaxPool2d: 对二维信号(图像）进行最大值池化。</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299121.png" alt="图片"></p><ul><li>kernel_size: 池化核尺寸</li><li>stride: 步长</li><li>padding: 填充个数</li><li>dilation: 池化核间隔大小</li><li>ceil_mode: 尺寸向上取整</li><li>return_indices: 记录池化像素索引</li></ul><p>前四个参数和卷积的其实类似，最后一个参数常在最大值反池化的时候使用，那什么叫最大值反池化呢？看下图：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299122.png" alt="图片"></p><p>反池化就是将尺寸较小的图片通过上采样得到尺寸较大的图片，看右边那个，那是这些元素放到什么位置呢？ 这时候就需要当时最大值池化记录的索引了。用来记录最大值池化时候元素的位置，然后在最大值反池化的时候把元素放回去。</p><p>下面看一下最大池化的效果：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299123.png" alt="图片"></p><p>可以发现，图像基本上看不出什么差别，但是图像的尺寸减少了一半， 所以池化层是可以帮助我们剔除一些冗余像素的。</p><p>除了最大池化，还有一个叫做平均池化：</p><p>nn.AvgPool2d: 对二维信号(图像)进行平均值池化</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299124.png" alt="图片"></p><ul><li>count_include_pad: 填充值用于计算</li><li>divisor_override: 除法因子， 这个是求平均的时候那个分母，默认是有几个数相加就除以几，当然也可以自己通过这个参数设定</li></ul><p>下面也是通过代码看一下结果：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299125.png" alt="图片">代码</p><p>这个平均池化和最大池化在这上面好像看不出区别来，其实最大池化的亮度会稍微亮一些，毕竟它都是取的最大值，而平均池化是取平均值。</p><p>好了，这就是池化操作了，下面再整理一个反池化操作，就是上面提到的 nn.MaxUnpool2d: 这个的功能是对二维信号(图像)进行最大池化上采样</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299126.png" alt="图片"></p><p>这里的参数与池化层是类似的。唯一的不同就是前向传播的时候我们需要传进一个 indices， 我们的索引值，要不然不知道把输入的元素放在输出的哪个位置上呀，就像上面的那张图片：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299127.png" alt="图片"></p><p>下面通过代码来看一下反池化操作：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299228.png" alt="图片"></p><h1 id="3-线性层"><a href="#3-线性层" class="headerlink" title="3.线性层"></a>3.线性层</h1><p>线性层又称为全连接层，其每个神经元与上一层所有神经元相连实现对前一层的<strong>「线性组合，线性变换」</strong></p><p>线性层的具体计算过程在这里不再赘述，直接学习 Pytorch 的线性模块。</p><p><code>nn.Linear(in_features, out_features, bias=True)</code> : 对一维信号（向量）进行线性组合</p><ul><li>in_features: 输入节点数</li><li>out_features: 输出节点数</li><li>bias: 是否需要偏置</li></ul><p>计算公式：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299229.png" alt="图片"></p><p>下面可以看代码实现：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">inputs = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1., 2, 3]</span>])<br>linear_layer = nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>linear_layer<span class="hljs-selector-class">.weight</span><span class="hljs-selector-class">.data</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1., 1., 1.]</span>,<br>                                             <span class="hljs-selector-attr">[2., 2., 2.]</span>,<br>                                             <span class="hljs-selector-attr">[3., 3., 3.]</span>,<br>                                             <span class="hljs-selector-attr">[4., 4., 4.]</span>])<br><br>linear_layer<span class="hljs-selector-class">.bias</span><span class="hljs-selector-class">.data</span><span class="hljs-selector-class">.fill_</span>(<span class="hljs-number">0.5</span>)<br>output = <span class="hljs-built_in">linear_layer</span>(inputs)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(inputs, inputs.shape)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(linear_layer.weight.data, linear_layer.weight.data.shape)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(output, output.shape)</span></span><br></code></pre></td></tr></table></figure><p>这个就比较简单了，不多说。</p><h1 id="4-激活函数层"><a href="#4-激活函数层" class="headerlink" title="4.激活函数层"></a>4.激活函数层</h1><p>激活函数 Udine 特征进行非线性变换， 赋予多层神经网络具有<strong>「深度」</strong>的意义。</p><p>如果没有激活函数，我们可以看一下下面的计算：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299230.png" alt="图片"></p><p>我们如果没有激活函数， 那么:</p><p>这里就可以看到，一个三层的全连接层，其实和一个线性层一样。这是因为我们线性运算的矩阵乘法的结合性，无论多少个线性层的叠加，其实就是矩阵的一个连乘，最后还是一个矩阵。所以如果没有激活函数，再深的网络也没有啥意义。</p><p>下面介绍几个常用的非线性激活函数：</p><ol><li>sigmoid 函数<img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299231.png" alt="图片"></li><li>nn.tanh<img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299232.png" alt="图片"></li><li>nn.ReLU<img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299233.png" alt="图片">ReLU 相对于前面的两个，效果要好一些， 因为不容易造成梯度消失，但是依然存在问题，所以下面就是对ReLU 进行的改进。<img src="https://gitee.com/xiang976young/note/raw/master/img/640-164640111299234.png" alt="图片"></li></ol><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h1><p>这篇文章的内容到这里就差不多了，这次以基础的内容为主，简单的梳理一下，首先我们上次知道了构建神经网络的两个步骤：<strong>搭建子模块和拼接子模块。</strong>而这次就是学习各个子模块的使用。从比较重要的卷积层开始，学习了1d 2d 3d 卷积到底在干什么事情，采用了动图的方式进行演示，卷积运算其实就是通过不同的卷积核去提取不同的特征。然后学习了 Pytorch 的二维卷积运算及转置卷积运算，并进行了对比和分析了代码上如何实现卷积操作。</p><p>第二块是池化运算和池化层的学习，关于池化，一般和卷积一块使用，目的是收集和合并卷积提取的特征，去除一些冗余，分为最大池化和平均池化。然后学习了全连接层，这个比较简单，不用多说，最后是非线性激活函数，比较常用的 sigmoid，tanh, relu等。</p><p>今天的内容就到这里，模型模块基本上到这里也差不多了，根据我们的那个步骤：数据模块 -&gt; 模型模块 -&gt; 损失函数 -&gt; 优化器 -&gt; 迭代训练。所以下一次开始学习损失函数模块，但是在学习损失函数之前，还得先看一下常用的权重初始化方法，这个对于模型来说也是非常重要的。��模型来说也是非常重要的。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning——安装深度学习环境 torch、pyg</title>
    <link href="/2022/07/01/DeepLearning/%E5%AE%89%E8%A3%85pytorch/"/>
    <url>/2022/07/01/DeepLearning/%E5%AE%89%E8%A3%85pytorch/</url>
    
    <content type="html"><![CDATA[<h1 id="PyTorch——配置深度学习环境"><a href="#PyTorch——配置深度学习环境" class="headerlink" title="PyTorch——配置深度学习环境"></a><a href="https://www.cnblogs.com/young978/p/15686819.html">PyTorch——配置深度学习环境</a></h1><p>还在为各种包的版本匹配发愁，还在冥想这个错误到底从何而来。<br><strong>放弃吧，重装才是王道。</strong></p><h2 id="安装Cuda"><a href="#安装Cuda" class="headerlink" title="安装Cuda"></a>安装Cuda</h2><p>tensor在GPU上运行是可以提高一定的处理效果的<br>首先需要检查一下你的GPU，查看一下版本<br>很简单进入任务管理器，打开性能，就能看见你GPU的参数啦（一般游戏笔记本是有两张显卡的，一张独显，一张集显）</p><p><img src="/src/1339557184.png" alt="img"></p><p>前往<a href="https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exe_local">Cuda官网</a> 下载驱动，安装即可</p><p><img src="/src/1206934061.png" alt="img"></p><p>下载后，默认安装就好。然后打开cmd，输入 nvidia-smi 即可查看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs visual">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ <br>conda config --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/free/<br>conda config --set show_channel_urls yes<br></code></pre></td></tr></table></figure><p><img src="/src/1805709415.png" alt="img"></p><h2 id="安装Anaconda"><a href="#安装Anaconda" class="headerlink" title="安装Anaconda"></a>安装Anaconda</h2><p>硬件好了，我们需要一个集成环境帮我们管理包和他们之间的依赖 anaconda当然是很好的选择</p><p>这里推荐两个路径：<a href="https://www.anaconda.com/products/individual">Anaconda官网</a>和<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/">清华软件镜像园</a></p><p>同样也是下载安装即可，注意勾选默认到Path环境变量就行。</p><p>这里我们需要注意，修改一下anaconda的源：清华大学开源软件镜像站和中国科技大学镜像站 方便我们下载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/free/<br>conda config --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/main/<br>conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/menpo/<br>conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/<br>conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/msys2/<br>conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/<br><br>conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ <br>conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/<br>conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ <br>conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/<br>conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/<br>conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/peterjc123/<br>conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/<br>conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/    #安装pytorch的小伙伴这个源一定不要漏<br><br>conda config --set show_channel_urls yes<br></code></pre></td></tr></table></figure><blockquote><p>衷心提醒网址一定要是<strong>http</strong> 不能是<strong>https</strong>，要不然会访问报错。 网址也不需要加引号<strong>切记</strong></p></blockquote><p>当然也可以自己手动去 C：用户下修改.condarc（这里贴出我的文件内容）</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs sh">ssl_verify: <span class="hljs-literal">true</span><br>channels:<br>  - defaults<br>show_channel_urls: <span class="hljs-literal">true</span><br>channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda<br>default_channels:<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/peterjc123/<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/<br>  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/<br>custom_channels:<br>  conda-forge: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud<br>  msys2: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud<br>  bioconda: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud<br>  menpo: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud<br>  pytorch: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud<br>  simpleitk: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud<br></code></pre></td></tr></table></figure><p>查看conda版本 <code>conda -version</code></p><p>升级conda工具包 <code>conda upgrade --all</code> </p><p>查看当前conda环境 <code>conda env list</code> </p><p>生成所需环境的txt文件 <code>pip freeze &gt; requirements.txt</code></p><p>conda安装txt  <code>conda install --yes --file requirements.txt</code></p><p>conda环境，导出格式为.yml文件或.txt文件： <code>conda env export &gt; requirements.yml</code></p><p><code>conda list -e &gt; requirements.txt</code></p><p>安装环境 : <code>conda env create -f requirements.yml</code></p><p>创建虚拟环境 <code>conda create -n &lt;name&gt; python=3.8</code> 这样anaconda就会去找python对应的包 耐心等待就好</p><p>删除环境<code>conda remove -n &lt;name&gt; --all</code></p><p>激活环境 <code>conda activate py38</code> 我这里的py38就是我新建的虚拟环境名称</p><p>conda还有一个好处 就是可以版本回滚 方便回到之间的状态</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">conda list <span class="hljs-comment">--revisions</span><br></code></pre></td></tr></table></figure><p><img src="/src/1358582774.png" alt="img"></p><p> 若想回滚到某个时刻，直接输入那个版本号就行了，例如我想回滚到第10个版本，即</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">conda install <span class="hljs-comment">--revision 10</span><br></code></pre></td></tr></table></figure><h2 id="安装torch"><a href="#安装torch" class="headerlink" title="安装torch"></a>安装torch</h2><p>直接在<a href="https://pytorch.org/">官网</a>寻找就好了，对应好自己的操作平台，和cuda版本下载即可</p><p><img src="/src/1001831579.png" alt="img"></p><p> 之前设置的源，会为我们找到国内的镜像路径，加快下载速度</p><h2 id="测试torch"><a href="#测试torch" class="headerlink" title="测试torch"></a>测试torch</h2><p>如果在Pycharm内，直接进入project选择新建虚拟环境的python.exe即可<br>如果在Jupyter Notebook中，则需要多安装一个包  nb_conda<br>安装完以后你就会发现你的jupyter会多一个conda插卡，可以查看conda的虚拟环境</p><p><img src="/src/842195767.png" alt="img"></p><p>运行的程序的时候，注意选择kernel是哪个环境的 以免发生版本错误<br>这些都是显示torch信息的一些命令可以用于测试的哦</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import torch<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.__version__)</span></span><br>torch<span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.is_available</span>()<br>torch<span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.device_count</span>()<br>torch<span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.get_device_name</span>(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p><img src="/src/1271380634.png" alt="img"></p><h2 id="安装PyG"><a href="#安装PyG" class="headerlink" title="安装PyG"></a>安装PyG</h2><p>首先安装torch-geometric之前需要准备的是，要查看好你当前的环境需要配置的文件</p><p>硬性要求python版本在3.7-3.10之间、Pytorch版本最低为1.8。</p><p>同样也是要在<a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html">官网</a>寻找，对应好自己的操作平台，和torch版本、cuda版本。详细的版本对应在<a href="https://data.pyg.org/whl/">这</a></p><p>对于目前比较稳定的torch1.12版本，在conda的镜像源中没直接的版本对应pyg。即使换成1.11版本的 运行<code>conda install pyg -c pyg</code>代码后测试的仍然存在一点小问题</p><p>这里我之前安装的环境是python3.8和torch1.11.0，cuda11.5</p><p>所以我这里安装的代码就是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html<br>pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html<br>pip install torch-geometric<br>pip install torch-cluster -f https://data.pyg.org/whl/torch-1.11.0+cu113.html<br>pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.11.0+cu113.html<br></code></pre></td></tr></table></figure><p>当然如果安装出现错误，不妨看看pyg官网最下面有一些<a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html#id1">Frequently Asked Questions</a>还是相当不错的</p><blockquote><p>在Neural Network 2022 关于GTN的文章中，涉及到一个关于torch-sparse-old的包</p><p>从作者的md中可以看到，为了解决torch.geometric删除了稀疏矩阵的backward()这个bug，他们把自己的torch-sparse上传到pip上，名字是torch-sparse-old。</p><p>但是<code>pip install torch-sparse-old</code> 会出现错误</p><p><code>cuda/spspmm_kernel.cu(76): error: identifier &quot;cusparseXcsrgemmNnz&quot; is undefined</code><br><code>cuda/spspmm_kernel.cu(84): error: identifier &quot;cusparseScsrgemm&quot; is undefined</code><br><code>cuda/spspmm_kernel.cu(84): error: identifier &quot;cusparseDcsrgemm&quot; is undefined</code></p><p>这个问题我尝试了很多方法，包括但不仅限于修改gcc，修改cuda版本，修改python、torch版本，以及删除环境重新来过。在这个关于pytorch_sparse的<a href="https://github.com/rusty1s/pytorch_sparse/issues/167">issue</a>内我也同样发现一个朋友有同样的错误。但是并没有看到好的解决方法</p><p>我后来直接在代码中把 import torch_sparse_old 修正为 import torch_sparse as  torch_sparse_old.</p><p>竟然惊奇的发现可以运行代码， 按理说backward()存在问题，model的train是无法正常进行的。那么一个合理的解释就是，新的torch-sparse、torch-geometric已经修正了这里无法传播的问题。</p></blockquote><p>�问题。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—损失函数小记</title>
    <link href="/2022/07/01/DeepLearning/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <url>/2022/07/01/DeepLearning/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="损失函数小记"><a href="#损失函数小记" class="headerlink" title="损失函数小记"></a>损失函数小记</h1><h2 id="1-L-1-范数损失"><a href="#1-L-1-范数损失" class="headerlink" title="1.$L_1$范数损失"></a>1.<strong>$L_1$范数损失</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.L1Loss（size_average = <span class="hljs-literal">None</span>，reduce = <span class="hljs-literal">None</span>，reduction = <span class="hljs-string">&#x27;mean&#x27;</span>）<br></code></pre></td></tr></table></figure><p>参数：<br>        size_average、reduce已经弃用了<br>        reduction-三个值，none: 不使用减少；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p><h2 id="2-均方误差损失-MSELoss"><a href="#2-均方误差损失-MSELoss" class="headerlink" title="2. 均方误差损失 MSELoss"></a>2. <strong>均方误差损失 MSELoss</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.MSELoss(reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>参数：reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p><h2 id="3-交叉熵损失-CrossEntropyLoss"><a href="#3-交叉熵损失-CrossEntropyLoss" class="headerlink" title="3. 交叉熵损失 CrossEntropyLoss"></a>3. <strong>交叉熵损失 CrossEntropyLoss</strong></h2><p>当训练有 C 个类别的分类问题时很有效. 可选参数 weight 必须是一个1维 Tensor, 权重将被分配给各个类别. 对于不平衡的训练集非常有效。</p><p>在多分类任务中，经常采用 softmax 激活函数+交叉熵损失函数，因为交叉熵描述了两个概率分布的差异，然而神经网络输出的是向量，并不是概率分布的形式。所以需要 softmax激活函数将一个向量进行“归一化”成概率分布的形式，再采用交叉熵损失函数计算 loss。<br>$$<br>\operatorname{loss}(x, \text { class })&#x3D;w e i g h t[\text { class }]\left(-x[\text { class }]+\log \left(\sum_{\jmath} \exp (x[j])\right)\right)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.CrossEntropyLoss(weight=<span class="hljs-literal">None</span>, ignore_index=-<span class="hljs-number">100</span>, reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>weight (Tensor, optional) – 自定义的每个类别的权重. 必须是一个长度为 C 的 Tensor</li><li>ignore_index (int, optional) – 设置一个目标值, 该目标值会被忽略, 从而不会影响到 输入的梯度。</li><li>reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</li></ul><h2 id="4-KL-散度损失-KLDivLoss"><a href="#4-KL-散度损失-KLDivLoss" class="headerlink" title="4. KL 散度损失 KLDivLoss"></a>4. <strong>KL 散度损失 KLDivLoss</strong></h2><p>计算 input 和 target 之间的 KL 散度。KL 散度可用于衡量不同的连续分布之间的距离, 在连续的输出分布的空间上(离散采样)上进行直接回归时很有效.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.KLDivLoss(reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>参数：reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p><h2 id="5-二进制交叉熵损失-BCELoss"><a href="#5-二进制交叉熵损失-BCELoss" class="headerlink" title="5.二进制交叉熵损失 BCELoss"></a>5.<strong>二进制交叉熵损失 BCELoss</strong></h2><p>二分类任务时的交叉熵计算函数。用于测量重构的误差, 例如自动编码机. 注意目标的值 t[i] 的范围为0到1之间.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.BCELoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>weight (Tensor, optional) – 自定义的每个 batch 元素的 loss 的权重. 必须是一个长度为 “nbatch” 的 的 Tensor</li><li>pos_weight(Tensor, optional) – 自定义的每个正样本的 loss 的权重. 必须是一个长度 为 “classes” 的 Tensor</li></ul><h2 id="6-BCEWithLogitsLoss"><a href="#6-BCEWithLogitsLoss" class="headerlink" title="6.** BCEWithLogitsLoss**"></a>6.** BCEWithLogitsLoss**</h2><p>BCEWithLogitsLoss损失函数把 Sigmoid 层集成到了 BCELoss 类中. 该版比用一个简单的 Sigmoid 层和 BCELoss 在数值上更稳定, 因为把这两个操作合并为一个层之后, 可以利用 log-sum-exp 的 技巧来实现数值稳定.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.BCEWithLogitsLoss(weight=<span class="hljs-literal">None</span>, reduction=<span class="hljs-string">&#x27;mean&#x27;</span>, pos_weight=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><p>weight (Tensor, optional) – 自定义的每个 batch 元素的 loss 的权重. 必须是一个长度 为 “nbatch” 的 Tensor</p><p>pos_weight(Tensor, optional) – 自定义的每个正样本的 loss 的权重. 必须是一个长度 为 “classes” 的 Tensor</p><h2 id="7-MarginRankingLoss"><a href="#7-MarginRankingLoss" class="headerlink" title="7. MarginRankingLoss"></a>7. <strong>MarginRankingLoss</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.MarginRankingLoss(margin=<span class="hljs-number">0.0</span>, reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>对于 mini-batch(小批量) 中每个实例的损失函数如下:<br>$$<br>\operatorname{loss}(x, y)&#x3D;\max (0,-y *(x 1-x 2)+\text { margin })<br>$$<br>参数：margin:默认值0</p><h2 id="8-HingeEmbeddingLoss"><a href="#8-HingeEmbeddingLoss" class="headerlink" title="8. HingeEmbeddingLoss"></a>8. <strong>HingeEmbeddingLoss</strong></h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.HingeEmbeddingLoss(<span class="hljs-attribute">margin</span>=1.0,  <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>对于 mini-batch(小批量) 中每个实例的损失函数如下:<br>$$<br>l_{n}&#x3D;\left{\begin{array}{ll}<br>x_{n}, &amp; \text { if } y_{n}&#x3D;1 \<br>\max \left{0, \Delta-x_{n}\right}, &amp; \text { if } y_{n}&#x3D;-1<br>\end{array}\right.<br>$$<br>参数：margin:默认值1</p><h2 id="9-多标签分类损失-MultiLabelMarginLoss"><a href="#9-多标签分类损失-MultiLabelMarginLoss" class="headerlink" title="9. 多标签分类损失 MultiLabelMarginLoss"></a>9. <strong>多标签分类损失 MultiLabelMarginLoss</strong></h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">torch.nn.<span class="hljs-constructor">MultiLabelMarginLoss(<span class="hljs-params">reduction</span>=&#x27;<span class="hljs-params">mean</span>&#x27;)</span><br></code></pre></td></tr></table></figure><p>对于mini-batch(小批量) 中的每个样本按如下公式计算损失:<br>$$<br>\operatorname{loss}(x, y)&#x3D;\sum_{i \jmath} \frac{\max (0,1-(x\lfloor y \mid j\rfloor|-x| i\rfloor))}{\mathrm{x} \cdot \operatorname{size}(0)}<br>$$</p><h2 id="10-平滑版L1损失-SmoothL1Loss"><a href="#10-平滑版L1损失-SmoothL1Loss" class="headerlink" title="10.平滑版L1损失 SmoothL1Loss"></a>10.<strong>平滑版L1损失 SmoothL1Loss</strong></h2><p>也被称为 Huber 损失函数。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">torch.nn.<span class="hljs-constructor">SmoothL1Loss(<span class="hljs-params">reduction</span>=&#x27;<span class="hljs-params">mean</span>&#x27;)</span><br></code></pre></td></tr></table></figure><p>$$<br>loss(x, y)&#x3D;\frac{1}{n}\sum_iz_i \<br>z_i&#x3D;\left{\begin{array}{ll}<br>0.5(x_i-y_i)^2, &amp; \text { if } |x_i-y_i| \lt 1 \<br>|x_i-y_i|-0.5, &amp;  otherwise<br>\end{array}\right.<br>$$</p><h2 id="11-2分类的logistic损失-SoftMarginLoss"><a href="#11-2分类的logistic损失-SoftMarginLoss" class="headerlink" title="11.2分类的logistic损失 SoftMarginLoss"></a>11.<strong>2分类的logistic损失 SoftMarginLoss</strong></h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">torch.nn.<span class="hljs-constructor">SoftMarginLoss(<span class="hljs-params">reduction</span>=&#x27;<span class="hljs-params">mean</span>&#x27;)</span><br></code></pre></td></tr></table></figure><p>$$<br>\operatorname{loss}(x, y)&#x3D;-\frac{1}{C} * \sum_{i} y[i] * \log \left((1+\exp (-x[i]))^{-1}\right)+(1-y[i]) * \log \left(\frac{\exp (-x[i])}{(1+\exp (-x[i]))}\right)<br>$$</p><h2 id="12-多标签-one-versus-all-损失-MultiLabelSoftMarginLoss"><a href="#12-多标签-one-versus-all-损失-MultiLabelSoftMarginLoss" class="headerlink" title="12.多标签 one-versus-all 损失 MultiLabelSoftMarginLoss"></a>12.<strong>多标签 one-versus-all 损失 MultiLabelSoftMarginLoss</strong></h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.MultiLabelSoftMarginLoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>$$<br>\operatorname{loss}(x, y)&#x3D;-\frac{1}{C} * \sum_{i} y[i] * \log \left((1+\exp (-x[i]))^{-1}\right)+(1-y[i]) * \log \left(\frac{\exp (-x[i])}{(1+\exp (-x[i]))}\right)<br>$$</p><h2 id="13-cosine-损失-CosineEmbeddingLoss"><a href="#13-cosine-损失-CosineEmbeddingLoss" class="headerlink" title="13. cosine 损失 CosineEmbeddingLoss"></a><strong>13. cosine 损失 CosineEmbeddingLoss</strong></h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.CosineEmbeddingLoss(<span class="hljs-attribute">margin</span>=0.0, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>$$<br>\operatorname{loss}(x, y)&#x3D;\left{\begin{array}{ll}<br>1-\cos \left(x_{1}, x_{2}\right), &amp; \text { if } y&#x3D;&#x3D;1 \<br>\max \left(0, \cos \left(x_{1}, x_{2}\right)-\operatorname{margin}\right), &amp; \text { if } y&#x3D;&#x3D;-1<br>\end{array}\right.<br>$$</p><p>参数：margin:默认值0</p><h2 id="14-多类别分类的hinge损失-MultiMarginLoss"><a href="#14-多类别分类的hinge损失-MultiMarginLoss" class="headerlink" title="14.  多类别分类的hinge损失 MultiMarginLoss"></a><strong>14.  多类别分类的hinge损失 MultiMarginLoss</strong></h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.MultiMarginLoss(<span class="hljs-attribute">p</span>=1, <span class="hljs-attribute">margin</span>=1.0, <span class="hljs-attribute">weight</span>=None,  <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>$$<br>\operatorname{loss}(x, y)&#x3D;\frac{\left.\sum_{l} \max (0, w[y] *(\operatorname{margin}-x[y]+x[i]))^{p}\right)}{\mathrm{x} \cdot \operatorname{size}(0)}<br>$$</p><p>参数：p&#x3D;1或者2 默认值：1 margin:默认值1</p><h2 id="15-三元组损失-TripletMarginLoss"><a href="#15-三元组损失-TripletMarginLoss" class="headerlink" title="15. 三元组损失 TripletMarginLoss"></a><strong>15. 三元组损失 TripletMarginLoss</strong></h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.TripletMarginLoss(<span class="hljs-attribute">margin</span>=1.0, <span class="hljs-attribute">p</span>=2.0, <span class="hljs-attribute">eps</span>=1e-06, <span class="hljs-attribute">swap</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>$$<br>L(a, p, n)&#x3D;\max \left{d\left(a_{\imath}, p_{\imath}\right)-d\left(a_{\imath}, n_{2}\right)+\operatorname{margin}, 0\right} \<br>d\left(a_{\imath}, p_{\imath}\right) &#x3D; ||x_i-y_i||_p<br>$$</p><h2 id="16-连接时序分类损失-CTCLoss"><a href="#16-连接时序分类损失-CTCLoss" class="headerlink" title="16 连接时序分类损失 CTCLoss"></a><strong>16 连接时序分类损失 CTCLoss</strong></h2><p>CTC连接时序分类损失，可以对没有对齐的数据进行自动对齐，主要用在没有事先对齐的序列化数据训练上。比如语音识别、OC识别等等。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.CTCLoss(<span class="hljs-attribute">blank</span>=0, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>参数：reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p><h2 id="2-17-负对数似然损失-NLLLoss"><a href="#2-17-负对数似然损失-NLLLoss" class="headerlink" title="2-17 负对数似然损失 NLLLoss"></a><strong>2-17 负对数似然损失 NLLLoss</strong></h2><p>负对数似然损失. 用于训练 C 个类别的分类问题.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.NLLLoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">ignore_index</span>=-100,  <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>weight (Tensor, optional) – 自定义的每个类别的权重. 必须是一个长度为 C 的 Tensor</li><li>ignore_index (int, optional) – 设置一个目标值, 该目标值会被忽略, 从而不会影响到 输入的梯度.</li></ul><h2 id="2-18-NLLLoss2d"><a href="#2-18-NLLLoss2d" class="headerlink" title="2-18 NLLLoss2d"></a><strong>2-18 NLLLoss2d</strong></h2><p>对于图片输入的负对数似然损失. 它计算每个像素的负对数似然损失.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.NLLLoss2d(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">ignore_index</span>=-100, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>weight (Tensor, optional) – 自定义的每个类别的权重. 必须是一个长度为 C 的 Tensor</li><li>reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</li></ul><h2 id="2-19-PoissonNLLLoss"><a href="#2-19-PoissonNLLLoss" class="headerlink" title="2-19 PoissonNLLLoss"></a><strong>2-19 PoissonNLLLoss</strong></h2><p>目标值为泊松分布的负对数似然损失</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.PoissonNLLLoss(<span class="hljs-attribute">log_input</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">full</span>=<span class="hljs-literal">False</span>,  <span class="hljs-attribute">eps</span>=1e-08,  <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>log_input (bool, optional) – 如果设置为 True , loss 将会按照公 式 exp(input) - target * input 来计算, 如果设置为 False , loss 将会按照 input - target * log(input+eps) 计算.</li><li>full (bool, optional) – 是否计算全部的 loss, i. e. 加上 Stirling 近似项 target * log(target) - target + 0.5 * log(2 * pi * target).</li><li>eps (float, optional) – 默认值: 1e-8</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Foundation—机器学习与深度学习的不同</title>
    <link href="/2022/07/01/DeepLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8D%E5%90%8C/"/>
    <url>/2022/07/01/DeepLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8D%E5%90%8C/</url>
    
    <content type="html"><![CDATA[<h2 id="1、机器学习的算法流程"><a href="#1、机器学习的算法流程" class="headerlink" title="1、机器学习的算法流程"></a>1、机器学习的算法流程</h2><p>实际上机器学习研究的就是数据科学（听上去有点无聊），下面是机器学习算法的主要流程：</p><ol><li>数据集准备</li><li>探索性的对数据进行分析</li><li>数据预处理</li><li>数据分割</li><li>机器学习算法建模</li><li>选择机器学习任务</li><li>最后就是评价机器学习算法对实际数据的应用情况如何</li></ol><p><img src="/%5Csrc%5C640.jpeg" alt="图片"></p><h3 id="1-1-数据集"><a href="#1-1-数据集" class="headerlink" title="1.1 数据集"></a>1.1 数据集</h3><p>首先我们要研究的是数据的问题，数据集是构建机器学习模型流程的起点。简单来说，数据集本质上是一个M×N矩阵，其中M代表列（特征），N代表行（样本）。</p><p>列可以分解为X和Y，X是可以指特征、独立变量或者是输入变量。Y也是可以指类别标签、因变量和输出变量。</p><p><img src="/%5Csrc%5C640-16462864273428.jpeg" alt="图片"></p><h3 id="1-2-数据分析"><a href="#1-2-数据分析" class="headerlink" title="1.2 数据分析"></a>1.2 数据分析</h3><p>进行探索性数据分析（Exploratory data analysis, EDA）是为了获得对数据的初步了解。EDA主要的工作是：对数据进行清洗，对数据进行描述（描述统计量，图表），查看数据的分布，比较数据之间的关系，培养对数据的直觉，对数据进行总结等。</p><blockquote><p>探索性数据分析方法简单来说就是去了解数据，分析数据，搞清楚数据的分布。主要注重数据的真实分布，强调数据的可视化，使分析者能一目了然看出数据中隐含的规律，从而得到启发，以此帮助分析者找到适合数据的模型。</p></blockquote><p>在一个典型的机器学习算法流程和数据科学项目里面，我做的第一件事就是通过 “盯住数据”，以便更好地了解数据。个人通常使用的三大EDA方法包括：</p><h4 id="描述性统计"><a href="#描述性统计" class="headerlink" title="描述性统计"></a><strong>描述性统计</strong></h4><p>平均数、中位数、模式、标准差。</p><p><img src="/%5Csrc%5C640-164628645466925.jpeg" alt="图片"></p><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a><strong>数据可视化</strong></h4><p>热力图（辨别特征内部相关性）、箱形图（可视化群体差异）、散点图（可视化特征之间的相关性）、主成分分析（可视化数据集中呈现的聚类分布）等。</p><p><img src="/%5Csrc%5C640-164628646837727.jpeg" alt="图片"></p><h4 id="数据整形"><a href="#数据整形" class="headerlink" title="数据整形"></a><strong>数据整形</strong></h4><p>对数据进行透视、分组、过滤等。</p><p><img src="/%5Csrc%5C640-16462864273429.jpeg" alt="图片"></p><h3 id="1-3-数据预处理"><a href="#1-3-数据预处理" class="headerlink" title="1.3 数据预处理"></a>1.3 数据预处理</h3><p>数据预处理，其实就是对数据进行清理、数据整理或普通数据处理。指对数据进行各种检查和校正过程，以纠正缺失值、拼写错误、使数值正常化&#x2F;标准化以使其具有可比性、转换数据(如对数转换)等问题。</p><blockquote><p>例如对图像进行resize成统一的大小或者分辨率。</p></blockquote><p>数据的质量将对机器学习算法模型的质量好坏产生很大的影响。因此，为了达到最好的机器学习模型质量，传统的机器学习算法流程中，其实很大一部分工作就是在对数据进行分析和处理。</p><p>一般来说，数据预处理可以轻松地占到机器学习项目流程中80%的时间，而实际的模型建立阶段和后续的模型分析大概仅占到剩余的20%。</p><h3 id="1-4-数据分割"><a href="#1-4-数据分割" class="headerlink" title="1.4 数据分割"></a>1.4 数据分割</h3><h4 id="训练集-amp-测试集"><a href="#训练集-amp-测试集" class="headerlink" title="训练集 &amp; 测试集"></a><strong>训练集 &amp; 测试集</strong></h4><p>在机器学习模型的开发流程中，希望训练好的模型能在新的、未见过的数据上表现良好。为了模拟新的、未见过的数据，对可用数据进行数据分割，从而将已经处理好的数据集分割成2部分：训练集合测试集。</p><p>第一部分是较大的数据子集，用作训练集（如占原始数据的80%）；第二部分通常是较小的子集，用作测试集（其余20%的数据）。</p><p>接下来，利用训练集建立预测模型，然后将这种训练好的模型应用于测试集（即作为新的、未见过的数据）上进行预测。根据模型在测试集上的表现来选择最佳模型，为了获得最佳模型，还可以进行超参数优化。</p><h4 id=""><a href="#" class="headerlink" title=""></a><img src="/%5Csrc%5C640-164628648188729.jpeg" alt="图片"></h4><p>另一种常见的数据分割方法是将数据分割成3部分：</p><ol><li>训练集</li><li>验证集</li><li>测试集</li></ol><p>训练集用于建立预测模型，同时对验证集进行评估，据此进行预测，可以进行模型调优（如超参数优化），并根据验证集的结果选择性能最好的模型。</p><p>验证集的操作方式跟训练集类似。不过值得注意的是，测试集不参与机器学习模型的建立和准备，是机器学习模型训练过程中单独留出的样本集，用于调整模型的超参数和对模型的能力进行初步评估。通常边训练边验证，这里的验证就是用验证集来检验模型的初步效果。</p><p><img src="/%5Csrc%5C640-164628642734210.jpeg" alt="图片"></p><h4 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a><strong>交叉验证</strong></h4><p>实际上数据是机器学习流程中最宝贵的，为了更加经济地利用现有数据，通常使用N倍交叉验证，将数据集分割成N个。在这样的N倍数据集中，其中一个被留作测试数据，而其余的则被用作建立模型的训练数据。通过反复交叉迭代的方式来对机器学习流程进行验证。</p><blockquote><p>这种交叉验证的方法在机器学习流程中被广泛的使用，但是深度学习中使用得比较少哈。</p></blockquote><p><img src="/%5Csrc%5C640-164628642734211.jpeg" alt="图片"></p><h3 id="1-5-机器学习算法建模"><a href="#1-5-机器学习算法建模" class="headerlink" title="1.5 机器学习算法建模"></a>1.5 机器学习算法建模</h3><p>下面是最有趣的部分啦，数据筛选和处理过程其实都是很枯燥乏味的，现在可以使用精心准备的数据来建模。根据taget变量（通常称为Y变量）的数据类型，可以建立一个分类或回归模型。</p><h3 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h3><p>机器学习算法可以大致分为以下三种类型之一：</p><h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a><strong>监督学习</strong></h4><p>是一种机器学习任务，建立输入X和输出Y变量之间的数学（映射）关系。这样的(X、Y)对构成了用于建立模型的标签数据，以便学习如何从输入中预测输出。</p><h4 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a><strong>无监督学习</strong></h4><p>是一种只利用输入X变量的机器学习任务。X变量是未标记的数据，学习算法在建模时使用的是数据的固有结构。</p><h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a><strong>强化学习</strong></h4><p>是一种决定下一步行动方案的机器学习任务，它通过试错学习（trial and error learning）来实现这一目标，努力使reward回报最大化。</p><h3 id="参数调优"><a href="#参数调优" class="headerlink" title="参数调优"></a>参数调优</h3><p>传说中的调参侠主要干的就是这个工作啦。超参数本质上是机器学习算法的参数，直接影响学习过程和预测性能。由于没有万能的超参数设置，可以普遍适用于所有数据集，因此需要进行超参数优化。</p><blockquote><p>以随机森林为例。在使用randomForest时，通常会对两个常见的超参数进行优化，其中包括mtry和ntree参数。mtry（maxfeatures）代表在每次分裂时作为候选变量随机采样的变量数量，而ntree（nestimators）代表要生长的树的数量。</p></blockquote><p>另一种在10年前仍然非常主流的机器学习算法是支持向量机SVM。需要优化的超参数是径向基函数(RBF)内核的C参数和gamma参数。C参数是一个限制过拟合的惩罚项，而gamma参数则控制RBF核的宽度。</p><p>调优通常是为了得出超参数的较佳值集，很多时候不要去追求找到超参一个最优值，其实调参侠只是调侃调侃，真正需要理解掌握算法原理，找到适合数据和模型的参数就可以啦。</p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>特征选择从字面上看就是从最初的大量特征中选择一个特征子集的过程。除了实现高精度的模型外，机器学习模型构建最重要的一个方面是获得可操作的见解，为了实现这一目标，能够从大量的特征中选择出重要的特征子集非常重要。</p><p>特征选择的任务本身就可以构成一个全新的研究领域，在这个领域中，大量的努力都是为了设计新颖的算法和方法。从众多可用的特征选择算法中，一些经典的方法是基于模拟退火和遗传算法。除此之外，还有大量基于进化算法（如粒子群优化、蚁群优化等）和随机方法（如蒙特卡洛）的方法。</p><p><img src="/%5Csrc%5C640-164628642734312.jpeg" alt="图片"></p><h3 id="1-6-机器学习任务"><a href="#1-6-机器学习任务" class="headerlink" title="1.6 机器学习任务"></a>1.6 机器学习任务</h3><p>在监督学习中，两个常见的机器学习任务包括分类和回归。</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>一个训练好的分类模型将一组变量作为输入，并预测输出的类标签。下图是由不同颜色和标签表示的三个类。每一个小的彩色球体代表一个数据样本。三类数据样本在二维中的显示，这种可视化图可以通过执行PCA分析并显示前两个主成分（PC）来创建；或者也可以选择两个变量的简单散点图可视化。</p><p><img src="/%5Csrc%5C640-164628642734313.jpeg" alt="图片"></p><h4 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a><strong>性能指标</strong></h4><p>如何知道训练出来的机器学习模型表现好或坏？就是使用性能评价指标（metrics），一些常见的评估分类性能的指标包括<strong>准确率（AC）、灵敏度（SN）、特异性（SP）和马太相关系数（MCC）</strong>。</p><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>最简单的回归模式，可以通过以下简单等式很好地总结：**Y &#x3D; f(X)**。其中，Y对应量化输出变量，X指输入变量，f指计算输出值作为输入特征的映射函数（从机器学习模型中得到）。</p><p>上面的回归例子公式的实质是，如果X已知，就可以推导出Y。一旦Y被计算（预测）出来，一个流行的可视化方式是将实际值与预测值做一个简单的散点图，如下图所示。</p><p><img src="/%5Csrc%5C640-164628642734314.jpeg" alt="图片"></p><p>对回归模型的性能进行评估，以评估拟合模型可以准确预测输入数据值的程度。评估回归模型性能的常用指标是确定系数（R²）。此外，均方误差（MSE）以及均方根误差（RMSE）也是衡量残差或预测误差的常用指标。</p><h2 id="2、深度学习算法流程"><a href="#2、深度学习算法流程" class="headerlink" title="2、深度学习算法流程"></a>2、深度学习算法流程</h2><p>深度学习实际上是机器学习中的一种范式，所以他们的主要流程是差不多的。深度学习则是优化了数据分析，建模过程的流程也是缩短了，由神经网络统一了原来机器学习中百花齐放的算法。</p><p>在深度学习正式大规模使用之前呢，机器学习算法流程中药花费很多时间去收集数据，然后对数据进行筛选，尝试各种不同的特征提取机器学习算法，或者结合多种不同的特征对数据进行分类和回归。</p><p><img src="/%5Csrc%5C640-164628642734315.jpeg" alt="图片"></p><p>下面是机器学习算法的主要流程：主要从</p><ol><li>数据集准备</li><li>数据预处理</li><li>数据分割</li><li>定义神经网络模型</li><li>训练网络</li></ol><p>深度学习不需要我们自己去提取特征，而是通过神经网络自动对数据进行高维抽象学习，减少了特征工程的构成，在这方面节约了很多时间。</p><p>但是同时因为引入了更加深、更复杂的网络模型结构，所以调参工作变得更加繁重啦。例如：定义神经网络模型结构、确认损失函数、确定优化器，最后就是反复调整模型参数的过程。</p><p>接下来具体解释一下：</p><h3 id="2-1背景"><a href="#2-1背景" class="headerlink" title="2.1背景"></a>2.1背景</h3><p>传统的机器学习模型在处理结构化数据方面一直非常强大，并且已经被企业广泛用于信用评分、客户流失预测、消费者定位等。</p><p>这些模型的成功很大程度上取决于特征工程阶段的性能：<strong>我们越接近业务，从结构化数据中提取相关知识越多，模型就越强大。</strong></p><p>当涉及到非结构化数据（图像、文本、语音、视频）时，手动特征工程是耗时的、脆弱的，而且在实践中是不可扩展的，这就是神经网络因其可以从原始数据中自动发现特征或者分类所需要的表示而越来越受欢迎的原因。它取代手动特征工程，允许一台机器既能学习特征又能利用这些特征执行特定的任务。</p><p>硬件（GPU）和软件（与AI相关的高级模型&#x2F;研究）的进步也提升了神经网络的学习效果。</p><h3 id="2-2基本架构"><a href="#2-2基本架构" class="headerlink" title="2.2基本架构"></a>2.2<strong>基本架构</strong></h3><p>深度学习的基本组成部分是感知器，它是神经网络中的单个神经元。</p><p>给定一组有限的m个输入（例如，m个单词或m个像素），我们将每个输入乘以一个权重$\theta_0 - \theta_m$，然后对输入的加权组合进行求和，并添加一个偏差，最后将它们输入非线性激活函数，输出$\hat{y}$。</p><p><img src="/src/640-16577802216005.jpeg" alt="图片"></p><ul><li>偏差会向输入空间添加另一个维度。因此，在输入全零向量的情况下，激活函数依然提供输出。它在某种程度上是输出的一部分，与输入无关。</li><li>激活函数的目的是将非线性引入网络。实际上，无论输入分布如何，线性激活函数都会产生线性决策。非线性使我们能够更好地逼近任意复杂的函数，这里有一些常见的激活函数示例：</li></ul><p><img src="/src/640-16577802216016.jpeg" alt="图片"></p><p>深度神经网络只是组合多个感知器（隐藏层），以产生输出。</p><p><img src="/src/640-16577802216017.jpeg" alt="图片"></p><p>现在，我们已经了解了深度神经网络的基本架构，让我们看看它如何用于给定的任务。</p><h3 id="2-3训练神经网络"><a href="#2-3训练神经网络" class="headerlink" title="2.3训练神经网络"></a>2.3<strong>训练神经网络</strong></h3><p>比方说，对于一组X射线图像，我们需要模型来自动区分病人与正常人。</p><p>为此，机器学习模型需要像人一样通过<strong>观察</strong>病人和正常人的图像来学会区分这两类图像。因此，它们自动理解可以更好地描述每类图像的模式，这就是我们所说的训练阶段。</p><p>具体地说，模式是一些输入（图像、部分图像或其他模式）的加权组合。因此，<strong>训练阶段只不过是我们估计模型权重（也称为参数）的阶段。</strong></p><p>当谈论估计时，谈论的是我们必须优化的目标函数。构建这个函数，应该最好地反映训练阶段的性能。当涉及到预测任务时，这个目标函数通常称为损失函数，度量不正确的预测所产生的成本。当模型预测非常接近真实输出的东西时，损失函数非常低，反之亦然。</p><p>在存在输入数据的情况下，我们计算经验损失（分类的二元交叉熵损失和回归的均方差损失）来度量整个数据集上的总损失。<br>$$<br>\boldsymbol{J}(\boldsymbol{\theta})&#x3D;\frac{1}{n} \sum_{i&#x3D;1}^{n} \mathcal{L} (\underbrace{f(x^{(i)} ; \theta)}<em>{Predicted}, \underbrace{\left.y^{(i)}\right.}</em>{Actual})<br>$$</p><p>由于损失是网络权重的函数，我们的任务是找到实现最低损失的权重集：<br>$$<br>\boldsymbol{\theta}^{*}&#x3D;\underset{\boldsymbol{\theta}}{\operatorname{argmin}} J(\boldsymbol{\theta})<br>$$<br>如果只有两个权重$\theta_0 \ and \ \theta_1$，我们可以绘制下面的损失函数图。我们想要做的是找出这种损失的最小值，从而找出损失达到最小值时的权重值。</p><p><img src="/src/640-165778022160111.jpeg" alt="图片"></p><p>为了最小化损失函数，我们可应用梯度下降算法：</p><ul><li><p>首先，我们随机选择权重的初始p向量（例如，遵循正态分布）。</p></li><li><p>然后，我们计算初始p向量中损失函数的梯度。</p></li><li><p>梯度方向指示的是使损失函数最大化的方向。 因此，我们在梯度的反方向上迈出了一小步，使用此更新规则相应地更新权重值：<br>$$<br>\theta \leftarrow \theta-\eta \frac{\partial J(\theta)}{\partial \theta}<br>$$</p></li><li><p>我们不断移动，直至收敛达到这张图的最低点（局部最小值）。</p></li></ul><p><strong>注意：</strong></p><ul><li>在更新规则中，$\eta$是<strong>学习率</strong>，决定着我们在梯度反方向上移动步伐的大小。它的选择非常重要，因为现代神经网络的架构是非常非凸的。如果学习率太低，模型可能陷入局部最小值；如果学习率太大，模型则可能发散。可以采用<strong>自适应学习率</strong>来调整梯度每次迭代时的学习率。有关更详细的说明，请阅读Sebastian Ruder对梯度下降优化算法的概述。</li><li>为了计算给定向量的损失函数的梯度，我们使用反向传播。考虑上面的简单神经网路络，它包含一个隐含层和一个输出层。想要计算与每个参数有关的损失函数的梯度$\theta_1$，首先，应用链规则，因为$J(\theta)仅仅依赖于\hat{y}$。然后，再次应用链规则来将梯度反向传播给前一层。出于同样的原因，我们可以这样做，因为$z_1$(隐藏状态）仅取决于输入$x和\theta_1$。因此，反向传播包括<strong>使用来自后面层的梯度对网络中每个权重重复这样的过程。</strong></li></ul><p><img src="/src/640-165778022160318.png" alt="图片"></p><p><strong>神经网络实践</strong></p><ul><li>在存在大数据集的情况下，关于每个权重的梯度计算可能非常昂贵（考虑反向传播中的链规则）。为此，我们可以计算数据子集（小批量）的梯度，将其作为真实梯度的估计值。它能给出比仅随机进行一次观察的随机梯度下降（SGD）更准确的梯度估计值，而且比使用全部数据进行梯度计算的方式要快。每次迭代使用小批量梯度计算，可以快速训练模型，特别是当我们使用不同的线程（GPU）时。我们可以并行计算每个迭代：每个权重和梯度的批处理在单独的线程中计算。然后，收集计算以完成迭代。</li><li>和任何其他“经典”机器学习算法一样，神经网络可能面临过度拟合的问题。理想情况下，在机器学习中，我们希望构建这样的模型：能够从训练数据中学习问题描述，而且也能够很好地概括看不见的测试数据。正则化是一种限制优化问题、避免产生复杂模型（即避免记忆数据）的方法。当谈论正则化时，我们通常会讨论Dropout。它在训练阶段随机丢弃（丢弃，即将相关激活设置为0）某些隐藏神经元或者在有可能过度拟合的情况下提前结束训练过程。为此，我们计算相对于训练迭代次数的训练和测试阶段的损失。当测试阶段的损失函数开始增加时，我们停止学习。�增加时，我们停止学习。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—炼丹经验小记</title>
    <link href="/2022/07/01/DeepLearning/%E7%82%BC%E4%B8%B9%E5%A4%A7%E6%B3%95/"/>
    <url>/2022/07/01/DeepLearning/%E7%82%BC%E4%B8%B9%E5%A4%A7%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="·1"><a href="#·1" class="headerlink" title="·1"></a>·1</h1><p><strong>可复现性和一致性』</strong></p><p>有的同学在打比赛的时候，从头到尾只维护若干份代码，每次载入前一次的训练参数，改一下代码再炼，俗称<strong>老丹</strong>。这样会有几个问题：某次引入一个 bug，过了很久才发现，然后不知道影响范围；得到一个好模型，但是不知道它是怎么来的；忘了自己的 baseline，不知道改动是正面还是负面。</p><p>要尽可能确保每一个模型有可复现性，实践上建议代码不应该在训练后再改动，训练新的模型时，把旧的代码复制一遍。得到的实验结果要开个文档记下来以便日后总结，避免遗忘。我经常通过阅读自己和别人的记录来得到灵感。</p><p>实验一致性上也要多做努力，理想状态是有合理的基准来测模型的性能，同一个代码不应该由于超参的微小改动而有显著结果差异。出现这种情况可能是数据太少或基准设置不当。</p><p><strong>『资源利用』</strong><br>对于新入行的同学，不要试图在玩具级别的数据集或任务上做靠谱的研究，比如 MNIST。</p><p>不是每一个实验都要出一个好模型，<strong>实验是为了验证结论的</strong>。如果每个实验都要 8 张卡跑两个星期，人力物力都耗不起。尽力把实验控制在单卡一天以内，理想状态是半天得一次结论。理论上来说，水多加面面多加水（加数据加计算量）的做法无限涨点。建议先设一个目标，比如说就是在一天的训练时间下做对比实验。</p><p>我的实践经验是，首先用小图小模型，比如 128 x 128 输入的 ResNet18；用 cProfile 来找性能瓶颈，比如我发现某个模型，训练的时候有一大半时间耗费在等数据，数据处理中一大半时间在调用 numpy 的 round 函数，前期把精力集中在提高做实验的效率上。</p><p><strong>『模型不 work』</strong><br>先把锦上添花的东西去掉，比如数据增广，玄学学习率和超参，魔幻损失函数，异形模型。如果世界上有一个非要加旋转增广和 1.96e-4 学习率 42 batchsize，配上四种混合损失函数才能训练好的模型，它应该存在于<strong>灵能文明</strong>。可以先造一些尽量玩具的模型，验证代码正确性。</p><p><strong>『需要进一步改进』</strong><br>先确认影响模型性能的组件。感性认识就是，数据是否需要增加或增广。模型是大了还是小了，再根据速度和精度期望开始寻找合适的模型。能用全卷积的任务，少用全连接层，参数量小。基本模型上 ResNet, Unet 结构还是主流。</p><p>当你的模型有 Batch Normalization，初始化通常不需要操心，激活函数默认 Relu 即可（某引用数万的大佬说的）。一般顺序是 Conv - BN - Relu。如果没有 BN（很多任务上，BN降低训练难度，但是可能影响最终性能 ），试着要做一些数据归一化。</p><p>虽然有至少十种激活函数，但在 Relu 外只推荐试一下 Swish。优化器只推荐 Momentum 和 Adam。在这些方面做尝试意义不大，如果性能提升反倒可能说明模型不成熟。不推荐做人肉模型设计，比如把某层卷积改大一点，或者微调一下通道数。除非有特别 insight，不要自己乱设计新组件。</p><p>超参上，learning rate 最重要，推荐了解 cosine learning rate 和 cyclic learning rate，其次是 batchsize 和 weight decay。当你的模型还不错的时候，可以试着做数据增广和改损失函数锦上添花了。</p><h1 id="·2"><a href="#·2" class="headerlink" title="·2"></a>·2</h1><p>1、一上来就自己动手写模型。建议首先用成熟的开源项目及其默认配置（例如 Gluon 对经典模型的各种复现、各个著名模型作者自己放出来的代码仓库）在自己的数据集上跑一遍，在等程序运行结束的时间里仔细研究一下代码里的各种细节，最后再自己写或者改代码。</p><p>2、训 RNN 不加 gradient clipping，导致训练一段时间以后 loss 突然变成 Nan。</p><p>3、tying input &amp; output embedding（就是词向量层和输出 softmax 前的矩阵共享参数，在语言模型或机器翻译中常用）时学习率需要设置得非常小，不然容易 Nan。</p><p>4、在数据集很大的情况下，一上来就跑全量数据。建议先用 1&#x2F;100、1&#x2F;10 的数据跑一跑，对模型性能和训练时间有个底，外推一下全量数据到底需要跑多久。在没有足够的信心前不做大规模实验。</p><p>5、只喜欢漂亮的模型结构，瞧不起调参数的论文&#x2F;实验报告，看论文时经常不看超参数设置等细节。举个例子，现在还有相当多的人不知道 BERT 的激活函数是 GELU 而不是 transformer 原论文中的 ReLU。在自己没有太多资源实验的情况下，实验报告类文章简直是业界良心好不好！</p><p>NLP 领域主要推荐以下几篇：</p><p>Regularizing and Optimizing LSTM Language Models（LSTM 的训练技巧）</p><p>Massive Exploration of Neural Machine Translation Architectures（NMT 里各个超参的影响）</p><p>Training Tips for the Transformer Model（训练 Transformer 时会发生的各种现象）</p><p>RoBERTa: A Robustly Optimized BERT Pretraining Approach（BERT 预训练技巧，虽然跟大部分人没啥关系）</p><p>CV 我不算太熟，不过也可以勉强推荐几篇：</p><p>Training ImageNet in 1 Hour（大批量训练技巧）</p><p>Bag of Tricks for Image Classification with Convolutional Neural Networks（各种训练技巧集大成）</p><p>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks（当前对参数利用最有效的 CNN）</p><p>6、初始学习率：</p><p>有时受 batch size、sequence length 各种因素的影响，loss 很大（比如说好几万），对于这种数字人是没有数感的，建议首先计算一下 per token loss（如果是多任务，可以每个任务单独算；类似地，某些 CV 任务可以计算 per pixel loss），心里有点感觉。脱离损失函数的形式谈学习率没有意义（例如单是对 batch size 求和或者取平均这个差异就会使梯度差成百上千倍）。</p><p>在确定初始学习率的时候，从一个很小的值（例如 1e-7）开始，然后每一步指数增大学习率（例如扩大1.05 倍）进行训练。训练几百步应该能观察到损失函数随训练步数呈对勾形，选择损失下降最快那一段的学习率即可。</p><p>7、Adam 可以解决一堆奇奇怪怪的问题（有时 loss 降不下去，换 Adam 瞬间就好了），也可以带来一堆奇奇怪怪的问题（比如单词词频差异很大，当前 batch 没有的单词的词向量也被更新；再比如Adam和L2正则结合产生的复杂效果）。用的时候要胆大心细，万一遇到问题找各种魔改 Adam（比如 MaskedAdam, AdamW 啥的）抢救。</p><p>8、subword 总是会很稳定地涨点，只管用就对了。</p><p><strong>9、要有耐心</strong>！</p><p>这一条放在最后，是因为很多人不把它当一回事儿。可能是觉得这一条不需要写代码所以不重要？我见过太多人因为这条浪费时间了，所以专门强调一下。</p><p>有些指标是有滞后性的，需要等训练一段时间才开始动。很多人训练几步看没什么效果就把程序停掉开始 debug 了，但其实代码毫无问题。如此反复好几天甚至一两周都在原地踏步，其实需要做的仅仅是让程序自个儿安安静静地跑上几个小时或者一天……</p><h1 id="·3"><a href="#·3" class="headerlink" title="·3"></a>·3</h1><p><strong>参数初始化。</strong></p><p>下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。</p><p>下面的n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5</p><p>Xavier初始法论文：</p><p><a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a></p><p>He初始化论文：<a href="https://arxiv.org/abs/1502.01852">https://arxiv.org/abs/1502.01852</a></p><ul><li><p>uniform均匀分布初始化：<br>w &#x3D; np.random.uniform(low&#x3D;-scale, high&#x3D;scale, size&#x3D;[n_in,n_out])</p></li><li><ul><li>Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale &#x3D; np.sqrt(3&#x2F;n)</li><li>He初始化，适用于ReLU：scale &#x3D; np.sqrt(6&#x2F;n)</li></ul></li><li><p>normal高斯分布初始化：<br>w &#x3D; np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0</p></li><li><ul><li>Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev &#x3D; np.sqrt(n)</li><li>He初始化，适用于ReLU：stdev &#x3D; np.sqrt(2&#x2F;n)</li></ul></li><li><p>svd初始化：对RNN有比较好的效果。参考论文：<a href="https://arxiv.org/abs/1312.6120">https://arxiv.org/abs/1312.6120</a></p></li></ul><p><strong>数据预处理方式</strong></p><ul><li>zero-center ,这个挺常用的.<br>X -&#x3D; np.mean(X, axis &#x3D; 0) # zero-center<br>X &#x2F;&#x3D; np.std(X, axis &#x3D; 0) # normalize</li><li>PCA whitening,这个用的比较少.</li></ul><p><strong>训练技巧</strong></p><ul><li>要做梯度归一化,即算出来的梯度除以minibatch size</li><li>clip c(梯度裁剪): 限制最大梯度,其实是value &#x3D; sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15</li><li>dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:<a href="http://arxiv.org/abs/1409.2329">http://arxiv.org/abs/1409.2329</a></li><li>adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。</li><li>除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。</li><li>rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.</li><li>word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.</li><li>尽量对数据做shuffle</li><li>LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf</a>, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.</li><li>Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift</li><li>如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: <a href="http://arxiv.org/abs/1505.00387">http://arxiv.org/abs/1505.00387</a></li><li>来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。</li></ul><p><strong>Ensemble</strong></p><p>Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式</p><ul><li>同样的参数,不同的初始化方式</li><li>不同的参数,通过cross-validation,选取最好的几组</li><li>同样的参数,模型训练的不同阶段，即不同迭代次数的模型。</li><li>不同的模型,进行线性融合. 例如RNN和传统模型.</li></ul><h1 id="·4"><a href="#·4" class="headerlink" title="·4"></a>·4</h1><p><strong>1.</strong> <strong>激活函数选择：</strong></p><p>常用的激活函数有relu、leaky-relu、sigmoid、tanh等。对于输出层，多分类任务选用softmax输出，二分类任务选用sigmoid输出，回归任务选用线性输出。而对于中间隐层，则优先选择relu激活函数（relu激活函数可以有效的解决sigmoid和tanh出现的梯度弥散问题，多次实验表明它会比其他激活函数以更快的速度收敛）。另外，构建序列神经网络（RNN）时要优先选用tanh激活函数。</p><p><strong>2、学习率设定：</strong></p><p>一般学习率从0.1或0.01开始尝试。学习率设置太大会导致训练十分不稳定，甚至出现Nan，设置太小会导致损失下降太慢。学习率一般要随着训练进行衰减。衰减系数设0.1，0.3，0.5均可，衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后自动进行衰减。</p><p><strong>3、防止过拟合：</strong></p><p>一般常用的防止过拟合方法有使用L1正则项、L2正则项、dropout、提前终止、数据集扩充等。如果模型在训练集上表现比较好但在测试集上表现欠佳可以选择增大L1或L2正则的惩罚力度（L2正则经验上首选1.0，超过10很少见），或增大dropout的随机失活概率（经验首选0.5）；或者当随着训练的持续在测试集上不增反降时，使用提前终止训练的方法。当然最有效的还是增大训练集的规模，实在难以获得新数据也可以使用数据集增强的方法，比如CV任务可以对数据集进行裁剪、翻转、平移等方法进行数据集增强，这种方法往往都会提高最后模型的测试精度。</p><p><strong>4、优化器选择：</strong></p><p>如果数据是稀疏的，就用自适应方法，即 Adagrad, Adadelta, RMSprop, Adam。整体来讲，Adam 是最好的选择。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。</p><p><strong>5、残差块与BN层：</strong></p><p>如果你希望训练一个更深更复杂的网络，那么残差块绝对是一个重要的组件，它可以让你的网络训练的更深。</p><p>BN层具有加速训练速度，有效防止梯度消失与梯度爆炸，具有防止过拟合的效果，所以构建网络时最好要加上这个组件。</p><p><strong>6.自动调参方法：</strong></p><p>（1）Grid Search：网格搜索，在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果。其原理就像是在数组里找最大值。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。</p><p>（2）Random Search：经验上，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。另外Random Search往往会和由粗到细的调参策略结合使用，即在效果比较好的参数附近进行更加精细的搜索。</p><p>（3）Bayesian Optimization：贝叶斯优化，考虑到了不同参数对应的   实验结果值，因此更节省时间，贝叶斯调参比Grid Search迭代次数少，  速度快；而且其针对非凸问题依然稳健。</p><p><strong>7.参数随机初始化与数据预处理：</strong></p><p>参数初始化很重要，它决定了模型的训练速度与是否可以躲开局部极小。relu激活函数初始化推荐使用He normal，tanh初始化推荐使用Glorot normal，其中Glorot normal也称作Xavier normal初始化；数据预处理方法一般也就采用数据归一化即可。</p><h1 id="5"><a href="#5" class="headerlink" title="5"></a>5</h1><p>关于参数:</p><ul><li>通常情况下, 更新参数的方法默认用 <strong>Adam</strong> 效果就很好</li><li>如果可以载入全部数据 (full batch updates), 可以使用 <strong>L-BFGS</strong></li></ul><p>Model Ensembles:</p><ul><li>训练多个模型, 在测试时将结果平均起来, 大约可以得到 <strong>2%</strong> 提升.</li><li>训练单个模型时, 平均不同时期的 checkpoints 的结果, 也可以有提升.</li><li>测试时可以将测试的参数和训练的参数组合起来:</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—目标检测方法小记</title>
    <link href="/2022/07/01/DeepLearning/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <url>/2022/07/01/DeepLearning/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<p>one-stage(YOLO,YOLO9000,YOLOV3,YOLOV4, YOLOV5,SSD等)</p><p>two-stage(OverFeat，R-CNN，Fast R-CNN，Faster R-CNN 等)</p><p><img src="/src%5Cimage-20220303134112628.png" alt="image-20220303134112628">g)</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning—访问深度模型网络参数</title>
    <link href="/2022/07/01/DeepLearning/%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0/"/>
    <url>/2022/07/01/DeepLearning/%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="如何访问网络模型的参数"><a href="#如何访问网络模型的参数" class="headerlink" title="如何访问网络模型的参数"></a>如何访问网络模型的参数</h1><ul><li><p>Way1: net[2]指的是net模型中第2层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">net[<span class="hljs-number">2</span>].stat_dict()<br>net[<span class="hljs-number">2</span>].bias<br>net[<span class="hljs-number">2</span>].bias.data<br>net[<span class="hljs-number">2</span>].weight.grad<br></code></pre></td></tr></table></figure></li><li><p>Way2： 利用named_paramters()函数指示出所有网络参数， 进行迭代</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">*[(name, param.shape) <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> net.named_paramters()]<br></code></pre></td></tr></table></figure></li><li><p>Way3：是Way1的变种</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">net.stat_dict()[<span class="hljs-string">&#x27;2.bias&#x27;</span>].data<br></code></pre></td></tr></table></figure></li></ul><h1 id="如何对网络模型参数初始化"><a href="#如何对网络模型参数初始化" class="headerlink" title="如何对网络模型参数初始化"></a>如何对网络模型参数初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_param</span>(<span class="hljs-params">m</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.normal_(m.weight, mean=<span class="hljs-number">0</span>, std=<span class="hljs-number">0.01</span>)<br>        nn.init.zero_(m.bias)<br>        <br>net.apply(init_param)<br></code></pre></td></tr></table></figure><p>当然不止使用正态分布初始化参数，置0法。还可以使用以下这几种方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.init.constant_(m.weight, <span class="hljs-number">1</span>)<br>nn.init.uniform_(m.weight, -<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>nn.init.xvaier_normal_(m.weight)<br>nn.init.xvaier_uniform_(m.weight)<br></code></pre></td></tr></table></figure><p>xvaier方法是一种参数初始化的方式。已知只有输入和输出维度相同时，才能保证在该层不会出现梯度消失和梯度爆炸的情况，这对于一般的MLP网络来说是不可能的。所以这xvaier方法使$\gamma_t$参数长度为$n_{t-1},n_t$的平均数互为倒数，即$\gamma_t&#x3D;\frac{2}{n_{t-1}+n_t}$</p><p>则<br>$$<br>Normal \sim N(0, \sqrt{\frac{2}{n_{t-1}+n_t}} )\<br>Uniform \sim U(-\sqrt{\frac{6}{n_{t-1}+n_t}},\sqrt{\frac{6}{n_{t-1}+n_t}})<br>$$</p><h1 id="网络模型参数修改"><a href="#网络模型参数修改" class="headerlink" title="网络模型参数修改"></a>网络模型参数修改</h1><p>很简单，其实就是对参数访问后进行赋值操作。例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">net[<span class="hljs-number">0</span>].weight.data += <span class="hljs-number">1</span><br>net[<span class="hljs-number">2</span>].weight.data[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>] = <span class="hljs-number">42</span><br></code></pre></td></tr></table></figure><p>还有一种比较奇特的结构，参数共享。先定义一个层，然后在module内多次调用这个层，这些层的参数都是一个实体。所以达到一个参数共享的效果</p><h1 id="网络模型参数的读写"><a href="#网络模型参数的读写" class="headerlink" title="网络模型参数的读写"></a>网络模型参数的读写</h1><p>网络模型参数的保存很简单</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(x, <span class="hljs-string">&#x27;x.file&#x27;</span>)<br>torch.save([x, y], <span class="hljs-string">&#x27;x.file&#x27;</span>)<br>torch.save(&#123;<span class="hljs-string">&#x27;x&#x27;</span>:x, <span class="hljs-string">&#x27;y&#x27;</span>:y&#125;, <span class="hljs-string">&#x27;x.file&#x27;</span>)<br>torch.save(net.stat_dict(), <span class="hljs-string">&#x27;mlp.params&#x27;</span>)<br></code></pre></td></tr></table></figure><p>网络模型的读取则需要先存在这个模型，才能往模型内放置参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">clone = MLP()<br>Clone.load_stat_dict(torch.load(<span class="hljs-string">&#x27;mlp.params&#x27;</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——注意力小记</title>
    <link href="/2022/07/01/Essay/Attention/"/>
    <url>/2022/07/01/Essay/Attention/</url>
    
    <content type="html"><![CDATA[<h1 id="Attention-注意力机制"><a href="#Attention-注意力机制" class="headerlink" title="Attention 注意力机制"></a>Attention 注意力机制</h1><blockquote><p>引用：</p><p>​1.<a href="https://cloud.tencent.com/developer/article/1480510">https://cloud.tencent.com/developer/article/1480510</a><br>​2.<a href="https://blog.csdn.net/Leon_winter/article/details/101231034">https://blog.csdn.net/Leon_winter/article/details/101231034</a><br>​3.<a href="https://zhuanlan.zhihu.com/p/350953067">https://zhuanlan.zhihu.com/p/350953067</a></p></blockquote><h2 id="一、机制原理"><a href="#一、机制原理" class="headerlink" title="一、机制原理"></a>一、机制原理</h2><p>通俗的来说，实际上就是对于某个时刻的输出$y$，他在输入$x$上各个部分的注意力（这里的注意力也可以理解为权重，即输入$x$的各个部分对输出$y$贡献的权重）。</p><h2 id="二、计算方式"><a href="#二、计算方式" class="headerlink" title="二、计算方式"></a>二、计算方式</h2><p><img src="/src/image-20220704211532515.png" alt="image-20220704211532515"></p><p>对上图进行解释：</p><ol><li><p><strong>Additive attention</strong>：论文：<a href="https://arxiv.org/pdf/1410.5401.pdf">Neural Turing Machines</a>  对于attention的相似度度量使用的是余弦相似度<br>$$<br>cosine[s_t,h_i]&#x3D;\frac{s_t * h_i}{||s_t||*||h_i||}<br>$$</p></li><li><p><strong>Additive Attention</strong>: 论文：<a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation By Jointly Learning to Align and Translate</a></p></li><li><p><strong>Location-Base Attention、General attention、Dot-Product Attention</strong>： 论文：<a href="https://arxiv.org/pdf/1508.04025.pdf">Effective Approaches to Attention-based Neural Machine Translation</a></p></li><li><p><strong>ScaledDot-Product attention</strong>: 论文：<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need</a></p></li></ol><h2 id="三、Attention发展历程及演变"><a href="#三、Attention发展历程及演变" class="headerlink" title="三、Attention发展历程及演变"></a>三、Attention发展历程及演变</h2><h3 id="3-1-seq2seq中引入attention"><a href="#3-1-seq2seq中引入attention" class="headerlink" title="3.1 seq2seq中引入attention"></a>3.1 seq2seq中引入attention</h3><p>首先attention机制常用在seq2seq模型中，下图一是传统的seq2seq，输出$y$对输入序列$x1,x2,x3.$没有区分，没有辨识度，下图二中我们引入了attention机制，每个输出的词$y$受输入$X1,X2,X3.$影响的权重不同，这个权重便是由Attention计算，因此可以把Attention机制看成注意力分配系数，计算输入每一项对输出权重影响大小</p><p><img src="/src/1620-16523234483093.jpeg" alt="img"></p><p><img src="/src/1620-16523234761996.jpeg" alt="img"></p><p>第一张图是整体的一个架构，第二张图是输入对某个输出的计算过程演示</p><p>介绍一下上图中attention实现过程</p><ol><li><p>首先利用双向RNN结构得到隐层状态$(h_1, h_2,\dots, h_n)$</p></li><li><p>如当前已经decoder到隐层$S_{t-1}$，接下来计算每一个输入位置$h_j$对当前位置$i$​的影响<br>$$<br>e_{ij} &#x3D; a(s_{i-1}, h_j)<br>$$<br>这里对attention的计算方式有很多种，例如点乘、加权点乘、求和等</p></li><li><p>对$e_{ij}$进行softmax将其normalization得到attention权重<br>$$<br>\alpha_{ij}&#x3D;\frac{exp(e_{ij})}{\sum <em>{k&#x3D;1}^{T_x} exp(e</em>{ik})}<br>$$</p></li><li><p>利用$\alpha_{ij}$​进行加权求和得到相应的context vector<br>$$<br>c_i &#x3D; \sum_{j&#x3D;1}^{T_x}\alpha_{ij}h_j<br>$$</p></li><li><p>计算最终输出<br>$$<br>s_i &#x3D; f(s_{i-1},y_{i-1}, c_i)\<br>p(y_i|y_1,\dots,y_{i-1},x)&#x3D;g(y_{i-1},s_i,c_i)<br>$$</p></li></ol><h3 id="3-2-Soft-Attention和Hard-Attention"><a href="#3-2-Soft-Attention和Hard-Attention" class="headerlink" title="3.2 Soft Attention和Hard Attention"></a>3.2 Soft Attention和Hard Attention</h3><p>Soft Attention是参数化的，是可导的可嵌入到模型中直接训练，上述seq2seq中的即是soft attention；Hard attention是一个随机的过程，为了实现梯度的反向传播，需要采用蒙特卡洛采样的方法来估计模块的梯度，目前更多的研究和应用还是更倾向于使用Soft Attention，因为其可以直接求导，进行梯度反向传播，大致了解一下即可</p><h3 id="3-3-Global-Attention和Local-Attention"><a href="#3-3-Global-Attention和Local-Attention" class="headerlink" title="3.3 Global Attention和Local Attention"></a>3.3 Global Attention和Local Attention</h3><p><img src="/src/1620-16523245340069.jpeg" alt="img"></p><p>以上左图是Global Attention，右图是Local Attention。<br>（图片来源于论文：Effective Approaches to Attention-based Neural Machine Translation）</p><p>全局Attention和局部Attention存在共同之处，例如在翻译模型的解码阶段都是将LSTM的顶层隐藏状态$h_t$作为输入，目标是活得一个上下文向量$c_t$来捕获源信息帮助预测当前单词，两种方式不同便在于对上下文向量$c_t$的获取方式不同，但其后步骤是共享的，即首先由上下文信息和隐藏层信息获得注意力隐层状态,再通过注意力隐层状态通过Softmax获得预测分布<br>$$<br>\hat{h_t}&#x3D;tanh(W_c[c_t;h_t])\<br>p(y_t|y_{&lt;t},x)&#x3D;softmax(W_s\hat{h_t})<br>$$</p><h4 id="3-3-1-Global-Attention"><a href="#3-3-1-Global-Attention" class="headerlink" title="3.3.1 Global Attention"></a>3.3.1 Global Attention</h4><p>全局Attention在计算上下文向量$c_t$时考虑编码器所有的隐层状态，attention向量是通过比较当前目标隐藏状态$h_t$和每个源隐藏状态$h_s$​得到：<br>$$<br>\begin{align}<br>a_t(s)&amp;&#x3D;align(h_t,\bar{h_s}) \<br>&amp;&#x3D;\frac{exp(score(h_t,\bar{h_s}))}{\sum_{s’}exp(score(h_t,\bar{h_{s’}}))}<br>\end{align}<br>$$<br>$h_t$​和$h_s$​之间的计算方式可以采用如下几种：<br>$$<br>score(h_t, \bar{h_s})&#x3D;\left{\begin{array}{ll}<br>        {h_t}^T\bar{h_s} &amp; {dot} \<br>        {h_t}^TW_a\bar{h_s} &amp; {general} \<br>        {v_a}^Ttanh(W_a[h_t;\bar{h_s}]) &amp; {concat}<br>        \end{array}\right.<br>$$<br>当然也是可以利用基于location的方式计算attention得分：<br>$$<br>a_t &#x3D; softmax(W_ah_t)\qquad location<br>$$</p><h4 id="3-3-2-Local-Attention"><a href="#3-3-2-Local-Attention" class="headerlink" title="3.3.2 Local Attention"></a>3.3.2 Local Attention</h4><p>全局Attention的缺点在于对每个目标单词必须关注源序列中的所有单词，消耗资源因此局部Attention的思想是对每个目标单词只选择一部分源单词进行预测上下文向量$c_t$，具体来说，在时刻t 模型首先为每个目标单词生成一个对齐位置$p_t$。上下文向量$c_t$ 通过窗口$[p_t -D,p_t +D] $内的源隐藏状态集合加权平均计算得到；$D $根据经验选择。与全局方法不同，局部对齐向量$a_t$是固定维度的，即$a_t\in \mathbb{R}^{2D+1}$。<br>$$<br>p_t &#x3D; S\cdot sigmoid(v_p^Ttanh(W_ph_t)) \<br>a_t(s)&#x3D;align(h_t,\bar{h_s})exp(-\frac{(s-p)^2}{2\sigma^2})<br>$$</p><h3 id="3-4-Scaled-Dot-Product-Attention和Multi-Head-Attention"><a href="#3-4-Scaled-Dot-Product-Attention和Multi-Head-Attention" class="headerlink" title="3.4 Scaled Dot-Product Attention和Multi-Head Attention"></a>3.4 Scaled Dot-Product Attention和Multi-Head Attention</h3><p><img src="/src/1324.jpeg" alt="img"><br>（图片来源于论文：Attention is all you need)</p><p>ScaledDot-Product Attention：通过Q,K矩阵计算矩阵V的权重系数</p><p>Multi-HeadAttention：多头注意力是将Q,K,V通过一个线性映射成h个Q,K,V，然后每个都计算Scaled Dot-Product Attention，最后再合起来，Multi-HeadAttention目的还是对特征进行更全面的抽取</p><h2 id="四、Attention实例分析"><a href="#四、Attention实例分析" class="headerlink" title="四、Attention实例分析"></a>四、Attention实例分析</h2><p><img src="/src/1620-16523277764253.jpeg" alt="img"></p><p>每个输出的词$Y$受输入$X_1,X_2,X_3,X_4$影响的权重不同，这个权重便是由Attention计算</p><p>因此可以把Attention机制看成注意力分配系数，计算输入每一项对输出权重影响大小</p><p>下面一张图给出了Attention机制在机器翻译中实际权重计算过程</p><p><img src="/src/1620-165232815064318.jpeg" alt="img"></p><p>首先由原始数据经过矩阵变化得到Q、K、V向量，如下图</p><p><img src="/src/1620-165232815064419.jpeg" alt="img"></p><p>以单词Thinking为例，先用Thinking的q向量和所有词的k向量相乘，使用下面公式：<br>$$<br>softmax(\frac{QK^T}{\sqrt{d_k}})<br>$$<br>这种得到每个单词对单词Thinking的贡献权重，然后再用得到的权重和每个单词的向量v相乘，得到最终Thinking向量的输出<br>$$<br>Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><h2 id="五、Attention机制实现分析"><a href="#五、Attention机制实现分析" class="headerlink" title="五、Attention机制实现分析"></a>五、Attention机制实现分析</h2><p><img src="/src/1620-165232827795630.jpeg" alt="img"></p><p>HAN结构包含了Word encoder、Word attention、Sentence encoder、Sentence attention，其中有word attention和sentence attention</p><p>h是隐层GRU的输出，设置$w,b,u_s$三个随机变量，先一个全连接变换经过激活函数tanh得到$u_i$，然后在计算$u_s$和$u_i$相乘，计算softmax输出$a$，最后得到权重$a$之后再和$h$相乘，得到最终经过注意力机制的输出$v$，下左图是word attention计算公式，下右图是sentece attention计算公式<br>$$<br>u_{it}&#x3D;tanh(W_wh_{it}+b_w) \qquad u_i &#x3D; tanh(W_sh_i+b_s)\<br>\alpha_{it}&#x3D;\frac{exp(u_{it}^Tu_w)}{\sum_texp(u_{it}^Tu_w)}\qquad<br>\alpha_{i}&#x3D;\frac{exp(u_{i}^Tu_s)}{\sum_texp(u_{i}^Tu_s)}\<br>s_i&#x3D;\sum_t\alpha_{it}h_{it} \qquad v&#x3D;\sum_i\alpha_ih_i\<br>$$<br>m_i\alpha_ih_i\<br>$$</p>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——聚类小记</title>
    <link href="/2022/07/01/Essay/Clustering/"/>
    <url>/2022/07/01/Essay/Clustering/</url>
    
    <content type="html"><![CDATA[<h4 id="广泛使用的聚类算法可以大致分为几组，包括分区、分层、基于网格、基于模型和基于密度的算法。"><a href="#广泛使用的聚类算法可以大致分为几组，包括分区、分层、基于网格、基于模型和基于密度的算法。" class="headerlink" title="广泛使用的聚类算法可以大致分为几组，包括分区、分层、基于网格、基于模型和基于密度的算法。"></a>广泛使用的聚类算法可以大致分为几组，包括分区、分层、基于网格、基于模型和基于密度的算法。</h4><ul><li>基于层次的算法要求时间序列具有相同的长度，当数据长度过长时很难得到满意的结果</li><li>K-Means 和 K-Medoids 等分区算法需要一些超参数，例如簇的数量，这些超参数很难确定</li><li>基于网格的方法很少用于时间序列聚类，因为它们要么运行速度非常慢，要么在大型数据集上不准确。</li><li>基于模型的方法假设它们包括每个集群的统计方法（例如，COBWEB）或神经网络方法（例如，ART ），并找到最适合模型的数据。然而，这些方法通常遵循很强的假设（例如，高斯混合可以对时间序列进行建模 ），这很难应用于复杂的数据集。</li><li>根据样本的密度分布进行基于密度的聚类算法（例如，DBSCAN ）。这些算法不需要指定集群的数量.</li></ul><blockquote><p>quote from Bibliograph: Robust KPI Anomaly Detection for Large-Scale Software Services with Partial Labels</p><ul><li>K-Means: “Efﬁcient registration of multi-view point sets by k-means clustering”</li><li>K-Medoids: “Comparison of k-means clustering method and k-medoids on twitter data”</li><li>COBWEB:“Knowledge acquisition via incremental conceptual clustering”</li><li>ART：“A massively parallel architecture for a self-organizing neural pattern recognition machine”</li><li>Gaussian Mixture Model：“Variable selection for model-based clustering using the integrated complete-data likelihood”</li><li>DBSCAN： “Real-time superpixel segmentation by dbscan clustering algorithm”</li></ul></blockquote><p>基于聚类的异常检测方法通常依赖下列假设，1）正常数据实例属于数据中的一个簇，而异常数据实例不属于任何簇；2）正常数据实例靠近它们最近的簇质心，而异常数据离它们最近的簇质心很远；3）正常数据实例属于大而密集的簇，而异常数据实例要么属于小簇，要么属于稀疏簇；通过将数据归分到不同的簇中，异常数据则是那些属于小簇或者不属于任何一簇或者远离簇中心的数据。</p><ul><li>将距离簇中心较远的数据作为异常点：这类方法有 SOM、K-means、最大期望( expectation maximization，EM)及基于语义异常因子( semantic anomaly factor)算法等；</li><li>将聚类所得小簇数据作为异常点：代表方法有K-means聚类；</li><li>将不属于任何一簇作为异常点：代表方法有 DBSCAN、ROCK、SNN 聚类。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——模型类别</title>
    <link href="/2022/07/01/Essay/Discrimination&amp;Generative/"/>
    <url>/2022/07/01/Essay/Discrimination&amp;Generative/</url>
    
    <content type="html"><![CDATA[<h1 id="Discrimination-amp-Generative"><a href="#Discrimination-amp-Generative" class="headerlink" title="Discrimination&amp;Generative"></a><strong>Discrimination</strong>&amp;<strong>Generative</strong></h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>在机器学习中，对于有监督学习可以将其分为两类模型：判别式模型和生成式模型。简单地说，判别式模型是针对条件分布建模，而生成式模型则针对联合分布进行建模。</p><p>First，无论是生成式还是判别式模型都是为了求得最大的条件概率$p(y|x)$，来作为一个新样本$x$的类别。只不过判别式模型和生成式的做法不同</p><p><strong>判别式</strong>：直接根据训练数据得到分类函数和分界面，比如说根据SVM模型得到一个分界面，然后直接计算条件概率$p(y|x)$，我们将最大的$p(y|x)$作为新样本的分类。判别式模型是对<strong>条件概率</strong>建模，学习不同类别之间的<strong>最优边界</strong>，无法反映训练数据本身的特性，能力有限，其只能告诉我们分类的类别。</p><p><strong>生成式</strong>： 有多少类就构建多少个类别，计算新样本$x$和每个类别的联合概率分布$p(x,y)$，再依据Bayes Laws：$P(y|x)&#x3D;\frac{p(x,y)}{p(x)}$，分别计算出$p(y|x)$，在选择其中最大的$p(y|x)$作为样本的类别</p><p><strong>Summary</strong>：两个方法都是为了求解条件概率$p(y|x)$。但是生成式模型可以体现更多数据本身的分布信息，其普适性更广。</p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p><strong>判别式</strong>：直接学习$p(y|x)$。不能反映训练数据本身的特性，但它寻找不同类别之间的最优分裂面，反映的是异类数据之间的差异，直接面对预测往往学习准确度更高。具体来说有以下特点：</p><ul><li>对条件概率建模，学习不同类别之间的最优边界。</li><li>捕捉不同类别特征的差异信息，不学习本身分布信息，无法反应数据本身特性。</li><li>学习成本较低，需要的计算资源较少。</li><li>需要的样本数可以较少，少样本也能很好学习。</li><li>预测时拥有较好性能。</li><li>无法转换成生成式。</li></ul><p><strong>生成式：</strong>学习的是联合概率分布$p(x,y)$​，可以从统计的角度表示分布的情况，能够反映同类数据本身的相似度，它不关心到底划分不同类的边界在哪里。生成式模型的学习收敛速度更快，当样本容量增加时，学习到的模型可以更快的收敛到真实模型，当存在隐变量时，依旧可以用生成式模型，此时判别式方法就不行了。具体来说，有以下特点：</p><ul><li>对联合概率建模，学习所有分类数据的分布。</li><li>学习到的数据本身信息更多，能反应数据本身特性。</li><li>学习成本较高，需要更多的计算资源。</li><li>需要的样本数更多，样本较少时学习效果较差。</li><li>推断时性能较差。</li><li>一定条件下能转换成判别式。</li></ul><h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p><img src="https://gitee.com/xiang976young/note/raw/master/img/v2-c052779ddaa1b9d854658e161d9d2509_720w_magic.jpg" alt="v2-c052779ddaa1b9d854658e161d9d2509_720w_magic"></p><p>上图左边为判别式模型而右边为生成式模型，可以很清晰地看到差别，判别式模型是在寻找一个决策边界，通过该边界来将样本划分到对应类别。而生成式模型则不同，它学习了每个类别的边界，它包含了更多信息，可以用来生成样本。</p><h2 id="两者可用的算法"><a href="#两者可用的算法" class="headerlink" title="两者可用的算法"></a>两者可用的算法</h2><pre><code class=" mermaid">graph LR;    MechineLearning--&gt;Discrimination;    MechineLearning--&gt;Generative;    Discrimination--&gt; LinearRegression;    Discrimination--&gt; LogisticRegression;    Discrimination--&gt; LinearDiscriminationAnalysis;    Discrimination--&gt; SupportVertorMechine;    Discrimination--&gt; Classification&amp;RegressionTree;    Discrimination--&gt; NerualNetwork;    Discrimination--&gt; GaussianProcess;    Discrimination--&gt; ConditionalRandomField;    Generative--&gt;NaiveBayes    Generative--&gt;KNN    Generative--&gt;MixtureGaussianModel    Generative--&gt;HMM    Generative--&gt;BayesNetwork    Generative--&gt;SigmoidBeliefNetworks    Generative--&gt;MarkovRandomField    Generative--&gt;DeepBeliefNetwork    Generative--&gt;LatentDirichletAllocation        </code></pre>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——联邦学习小记</title>
    <link href="/2022/07/01/Essay/FederatedLearning/"/>
    <url>/2022/07/01/Essay/FederatedLearning/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是Federated-Learning（联邦学习）"><a href="#什么是Federated-Learning（联邦学习）" class="headerlink" title="什么是Federated Learning（联邦学习）"></a><strong>什么是Federated Learning（联邦学习）</strong></h1><p>Federated Learning是一种训练数据去中心化的机器学习解决方案，最早于2016年由谷歌公司提出，目的在于通过对保存在大量终端的分布式数据开展训练学习一个高质量中心化的机器学习模型，解决数据孤岛的问题。</p><p>（什么是数据孤岛：良好的AI是需要大量优质的数据来支撑运算的。但是机构之间往往不会提供数据交流，从而形成一个个数据孤岛）</p><p>为了解决现在大量存在的数据孤岛，以及隐私保护问题，<strong>联邦学习</strong>应运而生。</p><p>本质：联邦学习本质上是一种<strong>分布式</strong>机器学习技术，或机器学习<strong>框架</strong>。</p><p>目标：联邦学习的目标是在保证数据隐私安全及合法合规的基础上，实现共同建模，提升AI模型的效果。</p><p>前身：联邦学习最早在 2016 年由谷歌提出，原本用于解决安卓手机终端用户在本地更新模型的问题；</p><p>Federated Learning示意图：<img src="/src/4gatnzkcb8.png" alt="img"></p><p><strong>Federated Learning不断循环以下步骤，直至训练出最终模型：</strong></p><ol><li>在符合条件的用户集合中挑选出部分用户，分别从<a href="https://cloud.tencent.com/product/cvm?from=10680">服务器</a>端下载当前的模型；</li><li>被选择的用户用各自的数据训练模型；</li><li>各个用户将训练好的模型传输给服务器；</li><li>服务器将接收到的各个用户的模型聚合成一个最终的模型。</li></ol><p><strong>2个参与角色：用户（client）- 服务器（server）</strong></p><p><strong>用户的特点：</strong></p><ul><li><strong>数据存在用户端，不同用户之间以及用户与服务器之间的数据不共享（最大的特点）</strong></li><li>数量大</li><li>用户网络状态允许不稳定，可以随时被选择加入或退出训练</li><li>用户数据的不平衡性，有些用户训练数据量大，有些用户训练数据少</li><li>典型的用户：比如手机终端</li></ul><p><strong>服务器的特点：</strong></p><ul><li>通过迭代方式不断聚合来自不同用户训练好的模型，训练出一个最终的模型。</li></ul><h2 id="联邦学习的分类"><a href="#联邦学习的分类" class="headerlink" title="联邦学习的分类"></a>联邦学习的分类</h2><p>我们把每个参与共同建模的企业称为参与方，根据多参与方之间数据分布的不同，把联邦学习分为三类：横向联邦学习、纵向联邦学习和联邦迁移学习。</p><p><img src="/src/v2-2f90ef909f950e2e83e5bd9aad4f5432_720w.jpg" alt="v2-2f90ef909f950e2e83e5bd9aad4f5432_720w"></p><h3 id="横向联邦学习"><a href="#横向联邦学习" class="headerlink" title="横向联邦学习"></a>横向联邦学习</h3><p><strong>适用场景</strong>横向联邦学习的本质是<strong>样本的联合</strong>，适用于参与者间业态相同但触达客户不同，即特征重叠多，用户重叠少时的场景，比如不同地区的银行间，他们的业务相似（特征相似），但用户不同（样本不同）</p><p><strong>学习过程</strong><img src="/src/v2-23616816b92a6d62be206b0aa28ba393_720w.jpg" alt="v2-23616816b92a6d62be206b0aa28ba393_720w"></p><p>Step 1：参与方各自从服务器A下载最新模型；</p><p>Step 2：每个参与方利用本地数据训练模型，加密梯度上传给服务器A，服务器A聚合各用户的梯度更新模型参数；</p><p>Step 3：服务器A返回更新后的模型给各参与方；</p><p>Step 4：各参与方更新各自模型。</p><p><strong>步骤解读：</strong>在传统的机器学习建模中，通常是把模型训练需要的数据集合到一个数据中心然后再训练模型，之后预测。在横向联邦学习中，可以看作是<strong>基于样本的分布式模型训练</strong>，分发全部数据到不同的机器，每台机器从服务器下载模型，然后利用本地数据训练模型，之后返回给服务器需要更新的参数；服务器聚合各机器上的返回的参数，更新模型，再把最新的模型反馈到每台机器。</p><p>在这个过程中，每台机器下都是<strong>相同且完整的模型</strong>，且机器之间不交流不依赖，在预测时每台机器也可以<strong>独立预测</strong>，可以把这个过程看作成基于样本的分布式模型训练。谷歌最初就是采用横向联邦的方式解决安卓手机终端用户在本地更新模型的问题的。</p><h3 id="纵向联邦学习"><a href="#纵向联邦学习" class="headerlink" title="纵向联邦学习"></a>纵向联邦学习</h3><p><strong>适用场景：</strong>纵向联邦学习的本质是<strong>特征的联合</strong>，适用于用户重叠多，特征重叠少的场景，比如同一地区的商超和银行，他们触达的用户都为该地区的居民（样本相同），但业务不同（特征不同）。</p><p><strong>学习过程：</strong><img src="/src/v2-3d3a6fbef04273a7729364789f0902fb_720w.jpg" alt="v2-3d3a6fbef04273a7729364789f0902fb_720w"></p><p>纵向联邦学习的本质是交叉用户在不同业态下的特征联合，比如商超A和银行B，在传统的机器学习建模过程中，需要将两部分数据集中到一个数据中心，然后再将每个用户的特征join成一条数据用来训练模型，所以就需要双方有用户交集（基于join结果建模），并有一方存在label。其学习步骤如上图所示，分为两大步：</p><p>第一步：加密样本对齐。是在系统级做这件事，因此在企业感知层面不会暴露非交叉用户。</p><p>第二步：对齐样本进行模型加密训练：</p><p>step 1：由第三方C向A和B发送公钥，用来加密需要传输的数据；</p><p>step 2： A和B分别计算和自己相关的特征中间结果，并加密交互，用来求得各自梯度和损失；</p><p>step 3： A和B分别计算各自加密后的梯度并添加掩码发送给C，同时B计算加密后的损失发送给C；</p><p>step 4： C解密梯度和损失后回传给A和B，A、B去除掩码并更新模型。</p><p><strong>在整个过程中参与方都不知道另一方的数据和特征，且训练结束后参与方只得到自己侧的模型参数，即半模型。</strong></p><p><strong>预测过程：</strong>由于各参与方只能得到与自己相关的模型参数，预测时需要双方协作完成，如下图所示：<img src="/src/v2-741e2effe86325903be656a4c2c39fb2_720w.jpg" alt="v2-741e2effe86325903be656a4c2c39fb2_720w"></p><p><strong>共同建模的结果：</strong></p><ul><li>双方均获得数据保护</li><li>共同提升模型效果</li><li>模型无损失</li></ul><h3 id="联邦迁移学习"><a href="#联邦迁移学习" class="headerlink" title="联邦迁移学习"></a><strong>联邦迁移学习</strong></h3><p><strong>适用场景：</strong></p><p>当参与者间特征和样本重叠都很少时可以考虑使用联邦迁移学习，如不同地区的银行和商超间的联合。主要适用于以深度神经网络为基模型的场景。</p><p><strong>迁移学习介绍：</strong>迁移学习，是指利用数据、任务、或模型之间的相似性，将在源领域学习过的模型，应用于 目标领域的一种学习过程。</p><p>联邦迁移学习的步骤与纵向联邦学习相似，只是中间传递结果不同（实际上每个模型的中间传递结果都不同)。这里重点讲一下联邦迁移的思想：<img src="/src/v2-ee8292d033fc88148a030765dd18918d_720w.jpg" alt="v2-ee8292d033fc88148a030765dd18918d_720w"></p><hr><h2 id="如何保证安全性"><a href="#如何保证安全性" class="headerlink" title="如何保证安全性"></a>如何保证安全性</h2><p>各个用户将训练好的模型加密后传输给服务器，服务器仅仅可以将接收到的来自多个用户聚合后才能成功解密，而无法对单个模型解密。用户端的加密算法通过添加零和掩码来加扰训练好的模型结果，服务器端聚合各个模型后噪声互相抵消，从而无法反推出各个用户模型的训练数据达到安全性保护的效果。</p><h2 id="federated-learning的优点"><a href="#federated-learning的优点" class="headerlink" title="federated learning的优点"></a>federated learning的优点</h2><ul><li>保护用户的隐私，服务器不获取用户的数据</li><li>分布式的数据架构，减轻数据集中存储的压力</li></ul><h2 id="Federated-Learning的局限性"><a href="#Federated-Learning的局限性" class="headerlink" title="Federated Learning的局限性"></a>Federated Learning的局限性</h2><ul><li>需要用户对齐或特征对齐才能共同训练模型</li><li>模型传输（信息传递）存在一定的局限</li><li>用户获取的间歇性</li><li>数据获取的间歇性</li><li>大规模的分布式网络结构带来压力</li></ul><h2 id="当前开源框架"><a href="#当前开源框架" class="headerlink" title="当前开源框架"></a>当前开源框架</h2><p>目前的Federated Learning 开源框架主要有两个：</p><ul><li>谷歌的TensorFlow Federated (TFF)框架</li><li>微众银行的Federated Learning开源框架FATE</li></ul><blockquote><p>Reference</p><ol><li><a href="http://ai.googleblog.com/2017/04/federated-learning-collaborative.html">Federated Learning: Collaborative Machine Learning without Centralized Training Data, 2017.        </a><a href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html">https://ai.googleblog.com/2017/04/federated-learning-collaborative.html.</a></li><li><a href="https://federated.withgoogle.com/">Federated Learning. </a><a href="https://federated.withgoogle.com/">https://federated.withgoogle.com.</a></li><li>Federated Learning: Strategies for Improving Communication Efficiency, 2016. <a href="https://ai.google/research/pubs/pub45648">https://ai.google/research/pubs/pub45648.</a></li><li>《<a href="https://cloud.tencent.com/product/fele?from=10680">联邦学习</a>白皮书 V1.0》<a href="https://img.fedai.org.cn/fedweb/1552917119598.pdf">https://img.fedai.org.cn/fedweb/1552917119598.pdf.</a></li><li>Federated Learning: Machine Learning on Decentralized Data (Google I&#x2F;O’19).</li><li><a href="https://cloud.tencent.com/developer/article/1489037">https://cloud.tencent.com/developer/article/1489037</a></li><li><a href="https://zhuanlan.zhihu.com/p/79284686m/p/79284686">https://zhuanlan.zhihu.com/p/79284686m/p/79284686</a></li></ol></blockquote>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——GAN小记</title>
    <link href="/2022/07/01/Essay/GAN/"/>
    <url>/2022/07/01/Essay/GAN/</url>
    
    <content type="html"><![CDATA[<h1 id="Generative-Adversarial-Networks-生成对抗网络"><a href="#Generative-Adversarial-Networks-生成对抗网络" class="headerlink" title="Generative Adversarial Networks(生成对抗网络)"></a>Generative Adversarial Networks(生成对抗网络)</h1><p>生成对抗网络（Generative Adversarial Nets，GAN）是一种基于对抗学习的深度生成模型，最早由Ian Goodfellow于2014年在《<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1406.2661.pdf">Generative Adversarial Nets</a>》中提出，一经提出便成为了学术界研究的热点，也将生成模型的热度推向了另一个新的高峰。</p><h1 id="1、前言"><a href="#1、前言" class="headerlink" title="1、前言"></a>1、前言</h1><p>机器学习模型可以分为“判别式”和“生成式”（二者的具体区别，可参考Discrimination&amp;Generative，前者直接建模条件分布，后者建模联合分布并借助贝叶斯定理来计算条件分布。</p><p>生成模型由于能够学习数据的隐含特征表示，近些年得到了长足的发展，在机器学习领域也愈来愈重要。</p><p>传统的生成模型如朴素贝叶斯、HMM等，采用浅层结构，由于其建模和表示能力有限，在遇到一些复杂的涉及自然信号（比如人类语言、自然图像和视觉场景）的问题时往往表现不佳。当前，基于深度学习的深度生成模型，应用更为广泛。</p><p>主流的深度生成模型有以下几种：</p><ul><li>自回归网络（Auto-Regressive Networks）</li><li>变分自编码器（Variational AutoEncoder，VAE）</li><li>生成对抗网络（Generative Adversarial Nets，GAN）</li><li>流模型（Flow-based Model）</li></ul><h1 id="2、结构"><a href="#2、结构" class="headerlink" title="2、结构"></a>2、结构</h1><p><img src="/src/v2-9fa7e1088a9661a1dadfc391f12bbbc4_720w.jpg" alt="img"></p><p>如上图所示GAN由一个判别器（Discriminator）和一个生成器（Generator）两个网络组成。</p><p>训练时先训练判别器：将训练集数据（Training Set）打上真标签（1）和生成器（Generator）生成的假图片（Fake image）打上假标签（0）一同组成batch送入判别器（Discriminator），对判别器进行训练。计算loss时使判别器对真数据（Training Set）输入的判别趋近于真（1），对生成器（Generator）生成的假图片（Fake image）的判别趋近于假（0）。此过程中只更新判别器（Discriminator）的参数，不更新生成器（Generator）的参数。</p><p>然后再训练生成器：将高斯分布的噪声z（Random noise）送入生成器（Generator），然后将生成器（Generator）生成的假图片（Fake image）打上真标签（1）送入判别器（Discriminator）。计算loss时使判别器对生成器（Generator）生成的假图片（Fake image）的判别趋近于真（1）。此过程中只更新生成器（Generator）的参数，不更新判别器（Discriminator）的参数。</p><p>GAN的一些缺点：</p><ul><li>训练不稳定，不容易收敛</li><li>容易出现模型坍缩（Mode Collapse），生成器总是生成相同的样本</li><li>生成过程不可控</li><li>不具备可解释性， $p_g$没有显式的表达</li></ul><h1 id="3、发展"><a href="#3、发展" class="headerlink" title="3、发展"></a>3、发展</h1><p>关于GAN的研究，截至目前已多不胜数，后人也提出了一系列的变种模型，这些模型基本上在两方面进行改进：模型结构、数学原理。</p><p>对于GAN系列模型的发展，这里我们做一些简要的罗列及介绍：</p><ul><li>DCGAN：在生成器和判别器中引入了卷积结构（原始GAN采用的是简单的MLP）</li><li>CGAN：在生成器和判别器中引入了条件变量，使用额外信息对模型增加条件，使得生成器能够朝着规定方向进行生成</li><li>WGAN：修改了原始GAN的目标函数，用Wasserstein距离替代JS散度来作为两个分布之间的距离度量（具体可参考<a href="https://zhuanlan.zhihu.com/p/25071913">这篇文章</a>）��</li></ul>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——GCN小记</title>
    <link href="/2022/07/01/Essay/GCN/"/>
    <url>/2022/07/01/Essay/GCN/</url>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_41683065/article/details/104547135">https://blog.csdn.net/qq_41683065/article/details/104547135</a></p><p>GCN的优点<br>权值共享，参数共享，从 A X W AXWAXW 可以看出每一个节点的参数矩阵都是 W WW，权值共享；<br>具有局部性Local Connectivity，也就是局部连接的，因为每次聚合的只是一阶邻居； 上述两个特征也是CNN中进行参数减少的核心思想<br>感受野正比于卷积层层数，第一层的节点只包含与直接相邻节点有关的信息，第二层以后，每个节点还包含相邻节点的相邻节点的信息，这样的话，参与运算的信息就会变多。层数越多，感受野越大，参与运算的信息量越充分。也就是说随着卷积层的增加，从远处邻居的信息也会逐渐聚集过来。<br>复杂度大大降低，不用再计算拉普拉斯矩阵及其特征分解<br>GCN的不足<br>扩展性差：由于训练时需要需要知道关于训练节点、测试节点在内的所有节点的邻接矩阵 A AA，因此是transductive的，不能处理大图，然而工程实践中几乎面临的都是大图问题，因此在扩展性问题上局限很大，为了解决transductive的的问题，GraphSAGE：Inductive Representation Learning on Large Graphs 被提出；<br>局限于浅层：GCN论文中表明，目前GCN只局限于浅层，实验中使用2层GCN效果最好，为了加深，需要使用残差连接等trick，但是即使使用了这些trick，也只能勉强保存性能不下降，并没有提高，Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning一文也针对When GCNs Fail ？这个问题进行了分析。虽然有一篇论文：DeepGCNs-Can GCNs Go as Deep as CNNs?就是解决GCN局限于浅层的这个问题的，但个人觉得并没有解决实质性的问题，这方面还有值得研究的空间。<br>不能处理有向图：理由很简单，推导过程中用到拉普拉斯矩阵的特征分解需要满足拉普拉斯矩阵是对称矩阵的条件。</p><p>$$<br>H^{(l+1)}&#x3D;\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)<br>$$</p>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——GNN小记</title>
    <link href="/2022/07/01/Essay/Graph%20Nerual%20Network/"/>
    <url>/2022/07/01/Essay/Graph%20Nerual%20Network/</url>
    
    <content type="html"><![CDATA[<h1 id="Graph-Neural-Network"><a href="#Graph-Neural-Network" class="headerlink" title="Graph Neural Network"></a>Graph Neural Network</h1><p>GNN是一种数据结构，它对一组对象（节点）及其关系（边）进行建模。</p><p>GNN是一个非结构化数据学习，拥有强大的表达能力。侧重于节点分类，链接预测，图分类和图生成问题</p><p><a href="https://blog.csdn.net/qq_41683065/article/details/104547135">https://blog.csdn.net/qq_41683065/article/details/104547135</a></p><h2 id="发展动机"><a href="#发展动机" class="headerlink" title="发展动机"></a>发展动机</h2><p>有两点：</p><ol><li>欧几里德数据向非欧几里德数据转变过程中，需要用到深度神经网络和图神经网络</li><li>图表示学习：图的信息传播机制导致节点参数无法共享、效率低下；同时也导致模型泛化能力低，无法处理新来的图。</li></ol><pre><code class=" mermaid">graph LR;    GNN--&gt;RecurrentGraphNerualNetwork;    GNN--&gt;ConvolutionGraphNerualNetwork;    GNN--&gt;GraphAutoEncoders;    GNN--&gt;Spatial-TemporalGraphNerualNetwork;</code></pre><h2 id="图的任务"><a href="#图的任务" class="headerlink" title="图的任务"></a>图的任务</h2><p>可以分布在节点上、链接上、图上。任务可以是监督、半监督或者无监督的形式。对于不同的任务的图来说，可以是</p><ul><li>Propagation Module：节点之间传播信息，正常的<ol><li>谱方法：学习滤波器依赖于图结构，所以一般仅处理同构图。</li><li>空间方法——基本方法：利用不同大小的kernel进行卷积运算，并保持CNN的局部不变的性质</li><li>空间方法——注意力方法：可以对不同程度的节点应用不同的权重，可以减轻噪音，获得更好的结果。</li><li>空间方法——框架</li></ol></li><li>Sampling Module：图太大，需要抽取局部信息；如果我们追溯到多个GNN层，支持邻居的大小将随着深度呈指数增长。为了缓解这种“邻居爆炸”的问题，一种有效的方法是采样。</li><li>Pooling Module：高级图，从不同节点中提取信息；从卷积出的特征中获得更加一般的特征。</li></ul><p><img src="/src/image-20220608151908637.png" alt="image-20220608151908637"></p><p>许多应用程序展开或堆叠图形神经网络层，目的是获得更好的结果，因为更多层（即k层）使每个节点从邻居k个跳跃中聚集更多信息。然而，在许多实验中观察到，更深层次的模型不能提高性能，更深层次的模型甚至性能更差。这主要是因为更多的层也可以传播来自数量呈指数增长的扩展邻域成员的噪声信息。这也会导致过度平滑问题，因为当模型深入时，节点在聚合操作后往往具有类似的表示。因此，许多方法尝试添加“跳过连接”，以使GNN模型更深入。</p><p><img src="/src/image-20220611222753237-165694008370946.png" alt="image-20220611222753237"></p><p>针对上述GNN中不同模块的改进，该综述对前人的工作做了一个详尽的描述和分类。</p><h2 id="图分类"><a href="#图分类" class="headerlink" title="图分类"></a>图分类</h2><p>图可以分成以下几类：</p><ol><li><p>是否有向：有向图是GNN模型中最基础的模型。</p></li><li><p>是否同构：Homogeneous&#x2F;Heterogeneous，考虑到节点之间、链接之间不同的属性。即多模态、多类型</p><ol><li>元路径方法：元路径是一种路径方案，它确定路径每个位置的节点类型。学习到的新连接可以连接彼此之间有多个跳跃但关系密切的节点，这些节点起到元路径的作用。</li><li>基于边的方法：通常针对不同类型的邻居和边在采样、聚合等方面使用不同的函数。它为不同的元关系分配不同的注意权重矩阵，使模型能够考虑类型信息。</li><li>基于关系图的方法：一些图的边可能包含比类型更多的信息，或者类型的数量可能太大，这给应用基于元路径或元关系的方法带来了困难。</li><li>基于复杂图的方法：在更复杂的场景中，图中的一对节点可以与不同类型的多条边相关联。通过在不同类型的边下查看，图形可以形成多个层，其中每个层表示一种类型的关系。</li></ol></li><li><p>静态图或动态图：考虑图中的属性（边、节点）是否会随着时间的变化而变化。</p></li><li><p>其他图类型：例如：超图、有符号图</p><p><img src="/src%5Cimage-20220612102140294.png" alt="image-20220612102140294"></p></li></ol><p>设计图：</p><ol><li>发现图结构</li><li>指定图形类型与比例</li><li>设计损失函数</li><li>使用计算模块，构建模型</li></ol><h2 id="图工作的应用"><a href="#图工作的应用" class="headerlink" title="图工作的应用"></a>图工作的应用</h2><p>图工作可以应用的场景和领域非常广泛。监督、半监督、无监督以及强化学习可以运用到图神经网络上。</p><ol><li>显性结构化：图匹配，图聚类，知识图、生成图、交通图、推荐系统</li><li>隐形非结构化：图像（少量图分类）、视觉问答、语义分割、文本分类、文本标签、关系提取等等。</li></ol><h2 id="图神经-目前存在的问题"><a href="#图神经-目前存在的问题" class="headerlink" title="图神经 目前存在的问题"></a>图神经 目前存在的问题</h2><ol><li>鲁棒性</li><li>可解释性</li><li>图的预训练</li><li>负责的图结构</li></ol><h2 id="GNN-SOTA-Methods"><a href="#GNN-SOTA-Methods" class="headerlink" title="GNN SOTA Methods"></a>GNN SOTA Methods</h2><p>GCN：是一种简化的基于频谱的GNN，也可以被视为基于空间的GNN</p><p>GAT：将注意力机制纳入GNN中，GNN使用注意力分数差异聚合图上邻居的表示。</p><p>GraphSAGE：通过变量聚合操作扩展了GNN的操作范围，用于归纳设置，为看不见的节点生成表示。</p><p>JKNets（Jumping Knowledge Networks）:通过采用跳跃连接和混合跳,利用归一化邻接矩阵的幂的组合来聚合不同距离的特征，从而利用了灵活的邻域范围。</p><h2 id="HGNN-SOTA-Methods"><a href="#HGNN-SOTA-Methods" class="headerlink" title="HGNN SOTA Methods"></a>HGNN SOTA Methods</h2><p>具有关系特定参数的GNN：</p><p>关系图卷积网络（R-GCN）:使用具有关系特定卷积（或权重矩阵）的GCN来处理异构图。</p><p>异构图变换器（HGT）可参数化每种边类型的元关系三元组，并使用了一种利用变换器架构的自注意力的结构来学习不同关系的特定模式。</p><p>具有基于关系的图变换的GNN：</p><p>异构图注意力网络（HAN）首先使用手动选择的元路径将异构图转换为同质图，并在图上应用基于注意力的GNN。HAN具有局限性，即它是一种多阶段方法，需要在每个数据集中手动选择元路径。此外，元路径的选择会显著影响性能。<br>�。</p>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——多模态小记</title>
    <link href="/2022/07/01/Essay/MultiModel/"/>
    <url>/2022/07/01/Essay/MultiModel/</url>
    
    <content type="html"><![CDATA[<p>多模态一般有三种常见的结构</p><pre><code class=" mermaid">classDiagram      MultiModel &lt;|-- Early Fusion      MultiModel &lt;|-- Late Fusion      MultiModel &lt;|-- Intermediate Fusion      class Early Fusion&#123;          +Date Level          +多源数据整理成单个数据      &#125;      class Late Fusion&#123;          +Decision Level          +模型集成      &#125;      class Intermediate Fusion&#123;        +Most popular          +Input映射到Network上,          同样可以捕捉到多源数据间隐藏关联      &#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
      <tag>Algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——前端总结</title>
    <link href="/2022/07/01/Essay/%E5%89%8D%E7%AB%AF/"/>
    <url>/2022/07/01/Essay/%E5%89%8D%E7%AB%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="看书、看视频涨技术没有做项目来的快-，但是驱动学习的一定是你的目标，你的知识需要"><a href="#看书、看视频涨技术没有做项目来的快-，但是驱动学习的一定是你的目标，你的知识需要" class="headerlink" title="看书、看视频涨技术没有做项目来的快 ，但是驱动学习的一定是你的目标，你的知识需要"></a>看书、看视频涨技术没有做项目来的快 ，但是驱动学习的一定是你的目标，你的知识需要</h1><p><img src="/src/image-20220614093643553.png" alt="image-20220614093643553"></p><p>翻到学习文件，应该还可以<a href="https://github.com/qianguyihao/Web">Github资源</a>，主页地址在<a href="https://web.qianguyihao.com/">文字资源都在这</a></p><p>面试刷题：高级前端面试（vx公众号）、前端面试星球（vx公众号）、<a href="https://muyiy.cn/question/">https://muyiy.cn/question/</a></p><p>基础：</p><ol><li>环境搭建 webstorm，hbuilder，vscode（最好），sublime。 </li><li>html<a href="https://link.zhihu.com/?target=https://developer.mozilla.org/en-US/docs/Web/HTML">MDN HTML Doc</a></li><li>css<a href="https://link.zhihu.com/?target=https://developer.mozilla.org/en-US/docs/Web/CSS">MDN CSS Doc</a></li><li>JavaScript<a href="https://link.zhihu.com/?target=https://developer.mozilla.org/en-US/docs/Web/JavaScript">MDN JavaScript DOC</a></li></ol><p>要先看基础，b站上好多视频，选择一个喜欢的老师教学视频跟着看，html ，css，js，先看基础的，看完之后，搜索高阶js，js一定要多看，html，css可以换个老师再看一遍，会有不同的收获（我是这样学的，看完一个老师的视频，有不懂的地方，我重新找一个老师，选择性看） （师姐说的）</p><p>我也认为基础是重要的，框架并没有那么重要，你需要成为一个做框架的人，而不是一个只会用框架的人。会用框架说明不了什么（但是能找到工作），我觉得前端的核心会了，框架应该也是挺顺的。<br> 学完基础后，可以仿照电商网站（例如京东、小米）做首页的布局。我记得当时学校培训的时候，我们老师就是这么干的。</p><p>进阶：</p><ol><li>纯js操作dom、jQuery、Ajax等。jQuery没有过时（师姐说的）。</li><li>深入学习js：知道作用域和闭包，解读原型链，原型替换，this指向和对象原型等，es6，es7的新语法。es6，这部分属于JS新增的语法，面试必问。其中，关于 promise、async 等内容要尤其关注，学习地址在下方。</li><li>HTML5和CSS3。要熟悉其中的新特性。 <a href="https://link.zhihu.com/?target=http://lesscss.cn/">Less 中文网</a></li><li>ui框架之旅推荐从bootstrap开始，移动Web开发、Bootstrap等。要注意移动开发中的适配和兼容性问题。</li><li>UI框架：<a href="https://element.eleme.cn/#/">Element UI</a></li><li>前端框架：vue，react，angular2选一种，我建议从vue开始，react的jsx语法不适合你入门去学习，angular2的typescript语法难度也比较高，vue学起来平滑些，结合vue，学一下vue-router，vuex，element,ui，axios，webpack。做个项目玩一下这个全家桶。</li><li>前端工程化</li><li>前端综合</li></ol><p>学习框架，react或者vue或者angular，这三个框架看自己兴趣选择一个先学，再从b站上找一些项目，尚硅谷的之类的跟着学，学习的这些都要记笔记，因为内容很多。如果以后找工作的话，还要刷题，多看看题目就好了。如果还有时间的话，这些都学了，可以多学几个框架，vue和react都掌握了是最好。（TS的内容比ES还要多。。。）</p><p>其他：</p><ol><li>git与nodejs <a href="https://link.zhihu.com/?target=https://nodejs.org/en/docs/guides/">Guides | Node.js</a>   git是基本操作，仓库这个东西还是要会的。node.js我记得vue里面就用到了一点</li><li>canvas。面试时，有的公司不一定会问canvas，靠运气。如果时间不够，这部分的内容可以先不学。但如果你会，绝对属于加分项。(我记得是用来做网页小游戏的吧，我好想写过)  <a href="https://github.com/bxm0927/canvas-special">代码</a></li></ol><h2 id="基础项目："><a href="#基础项目：" class="headerlink" title="基础项目："></a>基础项目：</h2><ol><li>animate <a href="https://github.com/animate-css/animate.css">代码</a> <a href="https://animate.style/">效果</a></li><li>CSS-Inspiration<a href="https://github.com/chokcoco/CSS-Inspiration">代码</a> <a href="https://chokcoco.github.io/CSS-Inspiration/#/">效果</a></li><li>vanillawebprojects 表单验证、音视频播放器、打字等小应用 <a href="https://github.com/bradtraversy/vanillawebprojects">代码</a></li><li>Ant Design 基于 Ant Design 设计体系的 React UI 组件库  <a href="https://github.com/ant-design/ant-design/">React 代码</a> <a href="https://github.com/vueComponent/ant-design-vue/">Vue代码</a></li><li>Echarts 基于 JavaScript 的开源可视化图表库 <a href="https://github.com/apache/echarts">代码</a></li><li>ECMAScript是JavaScript的语言标准，描述了JS的能力，API，基本实现步骤。 es6<a href="https://es6.ruanyifeng.com/">地址</a></li></ol><h2 id="中级项目："><a href="#中级项目：" class="headerlink" title="中级项目："></a>中级项目：</h2><ol><li>particles.js   当我们浏览网页的时候，会看见一些网站的背景有粒子效果，看完是不是也很好奇它是怎么实现的呢？在particles.js这个库里面，我们可以制造许多粒子效果，用在网页的背景可是相当好看的。<a href="https://codepen.io/VincentGarreau/pen/bGxvQd">演示</a> <a href="https://github.com/VincentGarreau/particles.js">代码</a></li><li>reveal.js是一款网页ppt，界面优美，又支持许多ppt功能，同时也有许多插件满足前端的需求，例如markdown、code等代码高亮插件。<a href="https://revealjs.com/">演示</a> <a href="https://github.com/hakimel/reveal.js">代码</a></li><li>TS  现在的前端开发中，对于TypeScript的使用越来越多，尤其是大型迭代项目中，JavaScript不能满足需要，学习TypeScript会是一个必备的技能 <a href="https://github.com/jkchao/typescript-book-chinese">书</a></li><li>音乐器  还在因为云音乐因为版权下架了一批你自己喜欢的歌曲，但是又不想单独为了几首歌换平台而烦恼吗，那你可以试试这个库，可以使用 QQ &#x2F; 虾米 &#x2F; 百度 &#x2F; 酷狗 &#x2F; 酷我 &#x2F; 咪咕 &#x2F; JOOX 等音源把云音乐灰色歌曲解锁播放。<a href="https://github.com/nondanee/UnblockNeteaseMusic">代码</a></li></ol><h2 id="Vue-js项目："><a href="#Vue-js项目：" class="headerlink" title="Vue.js项目："></a>Vue.js项目：</h2><ol><li>vue-element-admin 是一个后台前端解决方案，它基于和 element-ui 实现<a href="https://panjiachen.github.io/vue-element-admin">效果</a>、<a href="https://github.com/PanJiaChen/vue-element-admin">代码</a> 、<a href="https://panjiachen.github.io/vue-element-admin-site/zh/guide/">介绍</a>、<a href="https://github.com/PanJiaChen/vue-admin-template">模版</a></li><li>基于Vue.js + Element UI 的后台管理系统解决方案  <a href="https://lin-xin.gitee.io/example/work/#/login">效果</a>、<a href="https://github.com/lin-xin/vue-manage-system">代码</a></li><li><a href="https://github.com/biaochenxuying/blog-vue-typescript">Vue + TypeScript + Element-Ui 支持 markdown 渲染的博客前台展示</a><br> 这个博客主挺6的，vue、react的项目 啥都做了 他给的其他项目连接都在这个网页的下面<img src="/src/image-20220614104848594.png" alt="image-20220614104848594"></li></ol><h2 id="React-js项目："><a href="#React-js项目：" class="headerlink" title="React.js项目："></a>React.js项目：</h2><ol><li>h5-dooring是一款可视化编辑器，底层是用react写的，使用此工具可以让我们快速生成h5页面。同时通过这个平台也能了解到低代码的相关知识。 <a href="https://github.com/MrXujiang/h5-Dooring/blob/master/zh.md">代码</a> <a href="http://h5.dooring.cn/doc">文档</a></li><li>一套优秀的中后台前端解决方案<a href="https://github.com/zuiidea/antd-admin/blob/master/README-zh_CN.md">代码</a> <a href="https://antd-admin.zuiidea.com/">效果</a></li><li>网易云音乐第三方<a href="https://github.com/trazyn/ieaseMusic">代码</a></li><li>饿了么，还原度相当高，实现了各类动效<a href="https://github.com/stoneWeb/elm-react-native">代码</a></li></ol><h2 id="Angular项目："><a href="#Angular项目：" class="headerlink" title="Angular项目："></a>Angular项目：</h2><ol><li><a href="https://github.com/windiest/Angular-news">新闻客户端</a></li></ol><h2 id="Node-js项目："><a href="#Node-js项目：" class="headerlink" title="Node.js项目："></a>Node.js项目：</h2><ol><li>基于 node.js + Mongodb 构建的后台系统<a href="https://github.com/bailicangdu/node-elm">代码</a><br>根据饿了么官网接口写的，所以后台系统也保持了和官网一致的API接口。</li><li><a href="https://github.com/tumobi/nideshop-mini-program">基于Node.js+MySQL开发的开源微信小程序商城（微信小程序）</a>ram)</li></ol>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tool</tag>
      
      <tag>summary</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——时间序列预测技术</title>
    <link href="/2022/07/01/Essay/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%8A%80%E6%9C%AF/"/>
    <url>/2022/07/01/Essay/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%8A%80%E6%9C%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="Holt"><a href="#Holt" class="headerlink" title="Holt"></a>Holt</h1><p>趋向于被作为趋势序列的通用模型。</p><p>霍尔特指数平滑法是一种高级的线性指数平滑方法，该方法的优点是可以用不同的平滑参数对原序列的两种因素进行平滑，具有很大的灵活性，因此在实践中被广泛地应用。</p><p>Holt指数平滑模型由Holt于1957年提出。它与一般的指数平滑模型不同的是它对趋势数据直接进行平滑并对原时间序列进行预测，需要考虑的是两个平滑参数以及初值的选取问题，也被成为Holt双参数线性指数平滑模型。</p><p>它的计算公式如下：<br>$$<br>L_{t+1}&#x3D;\alpha D_t+(1-\alpha)(L_t+T_t)\<br>T_{t+1}&#x3D;\beta(L_{t+1}-L_t)+(1-\beta)T_t<br>$$<br>以及预测公式：<br>$$<br>F_{t+1}&#x3D;L_{t+1}+T_{t+1}<br>$$<br>$\alpha,\beta$​是分别代表影响预测值的两个平滑参数；$D_t$代表实际值；$F_{t+1}$代表预测值；<br>$L_t$代表平均需求；$T_t$代表增长的趋势，前者是对时间序列趋势的平滑式；后者是对趋势增量的平滑式。</p><h1 id="STL"><a href="#STL" class="headerlink" title="STL"></a>STL</h1><p>Seasonal and Trend decomposition using Loess</p><p>这是以鲁棒局部加权回归作为平滑方法的时间序列分解方法。</p><p>其中Loess(locally weighted scatterplot smoothing,LOWESS or LOESS)为局部多项式回归拟合，是对两维散点图进行平滑的常用方法，它结合了传统线性回归的简洁性和非线性回归的灵活性。当要估计某个响应变量值时，先从其预测变量附近取一个数据子集，然后对该子集进行线性回归或二次回归，回归时采用加权最小二乘法，即越靠近估计点的值其权重越大，最后利用得到的局部回归模型来估计响应变量的值。用这种方法进行逐点运算得到整条拟合曲线。</p><p>将时序分解为趋势项、季节项（周、月等）、余项。利用Lowess局部加权回归技术进行平滑；通过外循环设计体现鲁棒性。</p><p>分别用$Y_v， T_v，S_v，R_v$分别代表数据，趋势项、季节项和余项，v的范围为$[0,N]$，那么$Y_v&#x3D;T_v+S_v+R_v$ ，其中$v&#x3D;1,\dots,N$ （加法模型中，各项具有相同量纲、STL只能处理加法模型，可以先将数据取对数，进行STL分解后的各分量结果取指数即可）</p><p><img src="/src/765608-20160305160907487-76859704.png" alt="img"><br>$$<br>log(Y_t)&#x3D;log(Trend_t<em>Seasonal_t</em>Irregular_t)<br>$$<br><img src="/src/765608-20160305172536268-1238138049.png" alt="img"></p><h1 id="Holt-Winters"><a href="#Holt-Winters" class="headerlink" title="Holt-Winters"></a><a href="https://blog.csdn.net/u010665216/article/details/78051192?locationNum=11&fps=1">Holt-Winters</a></h1><p>在做时序预测时，一个显然的思路是：认为离着预测点越近的点，作用越大。即距离预测点远的数据点，其权重按照指数级进行衰减。这就是指数平滑法的基本思想。</p><p>指数平滑法有几种不同形式：一次指数平滑法针对<strong>没有趋势和季节性</strong>的序列，二次指数平滑法<strong>针对有趋势但没有季节性</strong>的序列，三次指数平滑法针对<strong>有趋势也有季节性</strong>的序列。<strong>“Holt-Winters”有时特指三次指数平滑法。</strong></p><h2 id="一次平滑"><a href="#一次平滑" class="headerlink" title="一次平滑"></a>一次平滑</h2><p>计算公式：<br>$$<br>s_i&#x3D;\alpha x_i+(1-\alpha)s_{i-1} \quad 0\le\alpha\le1<br>$$<br>$s_i$是平滑后的值，$x_i$是当前的实际数据，$\alpha$是记忆衰减因子，越接近1，只保留当前数据点，越接近0，保留前面的平滑值（导致整个曲线都是平的）</p><p>预测公式：<br>$$<br>x_{i+h}&#x3D;s_i \ \qquad( h&#x3D;1)\quad s_i \ is \ the \ last\  element<br>$$</p><h2 id="二次平滑"><a href="#二次平滑" class="headerlink" title="二次平滑"></a>二次平滑</h2><p>计算公式：<br>$$<br>\begin{align}<br>s_i&amp;&#x3D;\alpha x_i+(1-\alpha)(s_{i-1}+t_{i-1}) \quad 0\le\alpha\le1 \<br>t_i&amp;&#x3D;\beta(s_{i}-s_{i-1})+(1-\beta)t_{i-1}<br>\end{align}<br>$$<br>$s_i$是平滑后的信号, $t_i$是平滑后的趋势</p><p>预测公式：<br>$$<br>x_{i+h}&#x3D;s_i +ht_i<br>$$</p><h2 id="三次指数平滑法（holt-winters）"><a href="#三次指数平滑法（holt-winters）" class="headerlink" title="三次指数平滑法（holt-winters）"></a>三次指数平滑法（holt-winters）</h2><p>计算公式：<br>$$<br>\begin{align}<br>s_i&amp;&#x3D;\alpha (x_i-p_{i-k})+(1-\alpha)(s_{i-1}+t_{i-1}) \quad 0\le\alpha\le1 \<br>t_i&amp;&#x3D;\beta(s_{i}-s_{i-1})+(1-\beta)t_{i-1} \<br>p_i&amp;&#x3D;\gamma(x_{i}-s_{i})+(1-\gamma)p_{i-k} \quad  k \ is\ the\ length\ of\ period<br>\end{align}<br>$$<br>$s_i$是平滑后的信号, $t_i$是平滑后的趋势</p><p>预测公式：<br>$$<br>x_{i+h}&#x3D;s_i +ht_i+p_{i-k+h}<br>$$<br>参数选择：$\alpha,\beta,\gamma   \in [0,1]$，$s,t,p$的初始值对算法整体影响并不大，一般$s_0&#x3D;x_0,t_0&#x3D;x_1-x_0$,累乘时$p&#x3D;0$,累加时$p&#x3D;1$</p>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Essay——融合解读多标签方法小记</title>
    <link href="/2022/07/01/Essay/%E8%9E%8D%E5%90%88%E8%A7%A3%E5%86%B3%E5%A4%9A%E6%A0%87%E7%AD%BE%E9%97%AE%E9%A2%98/"/>
    <url>/2022/07/01/Essay/%E8%9E%8D%E5%90%88%E8%A7%A3%E5%86%B3%E5%A4%9A%E6%A0%87%E7%AD%BE%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>这里给了两个的方案:</p><h1 id="SST"><a href="#SST" class="headerlink" title="SST"></a>SST</h1><p>与大多数机器学习模型一样,这里的目标是构造一个将输入映射到输出的函数,在这种情况下，输出将是一组向量.单一目标(ST)考虑m个单一模型来预测多标签。此外，还引入了stack的方式(SST)提高效果。SST有两个预测阶段。在第一阶段，用m个模型预测m个目标。在后一阶段，通过变换训练集D，为每个目标学习一组m’元模型。在变换后的训练集中，它使用输出空间的估计值。</p><p><img src="/src%5C640-16462838142622.png" alt="图片"></p><h1 id="ERC"><a href="#ERC" class="headerlink" title="ERC"></a>ERC</h1><p>这里需要注意的是,训练的时候我们依赖的是真实标签Y1,.,Ym-1，因为我们已经有了真实值，但对于预测，ERC必须依赖于估计值。但作为ML中的一个假设，输入和输出变量应该是独立的。为了解决这个问题，在训练中拆除了一部分样本用于训练,并对估计值进行了k-折叠交叉验证，并将其用于训练部分,代码见参考资料。</p><p><img src="/src%5C640-16462838142623.png" alt="图片">3.png)</p>]]></content>
    
    
    <categories>
      
      <category>Essay</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Essay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning——分析思维模型</title>
    <link href="/2022/07/01/MachineLearning/%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/07/01/MachineLearning/%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="1-非线性思维"><a href="#1-非线性思维" class="headerlink" title="1.非线性思维"></a>1.<strong>非线性思维</strong></h1><p>由于人类长期的进化结果，我们更倾向于用“线性”思维方式理解世界。</p><p>然而，世界的本质是非线性的。这个世界之所以有些人能取得巨大成功，而大部分人平平凡凡，关键于他们的思维方式是线性的还是非线性的。 </p><p>非线性思维的核心：</p><p>\1. 理解世界是非线性、跳跃发展、不可精确预测的；</p><p>\2. 理解引起质变的核心原因，加速或者等待临界点的来临，获取质变期的巨大收益；</p><p>\3. 在积累期勇于放弃短期利益，做能够对促进上一条有益的事情（如在积累期投入更多本金、积累更多的工作经验等）。 </p><p><strong>非线性思维方式具有系统辩证思维、发散思维、逆向思维、直觉思维和灵感思维五种基本形式。</strong></p><p><img src="/src/640-165778284238272.jpeg" alt="图片"></p><h1 id="2-黄金圈思维"><a href="#2-黄金圈思维" class="headerlink" title="2.黄金圈思维"></a>2.<strong>黄金圈思维</strong></h1><p>黄金圈是认知世界的方式，有三个层面。从why出发，探究问题的本质，是最快速透析问题根源的一把利器。</p><p>思考模式是先从Why出发，为什么要这么做，这么做的理念是什么，从内心激发出感性的情感，产生驱动力；</p><p>近而再思考How，设定目标一步步解决；最后做出来的结果就是What，更加贴合最初的理念。</p><p>Why：思考为什么要这么做，我们的目标、理念；</p><p>How：采用什么方法、措施；</p><p>What：结果的表现形式。 </p><p><img src="/src/640-165778284238273.jpeg" alt="图片"></p><p>对一家企业、一个人思考why的过程则是对原则、边界、价值观的一个确定，提供了一个解决问题的通用模型。</p><h1 id="3-10-10-10旁观思维模型"><a href="#3-10-10-10旁观思维模型" class="headerlink" title="3.10+10+10旁观思维模型"></a>3.<strong>10+10+10旁观思维模型</strong></h1><p>面对一个决策或选择，当你犹豫不决时，可以想一下：</p><p>10分钟后，自己是怎么看待自己现在的决策，依然保持一致亦或会后悔；</p><p>10个月后，你又会如何思考自己10个月以前的这个决策；</p><p>10年后，自己如何看待自己这个10年前的判断与决策。</p><p>这个思维模型，可以应用在：<strong>临时的判断，大的决策，预测自己的未来等。</strong></p><p><img src="/src/640-165778284238274.jpeg" alt="图片"></p><h1 id="4-反熵增思维模型"><a href="#4-反熵增思维模型" class="headerlink" title="4.反熵增思维模型"></a>4.<strong>反熵增思维模型</strong></h1><p>从一个系统、一个组织，再到一个星球，甚至我们每个人，都符合熵增定律。</p><p>我们会变得混乱、无序、僵化、没有活力，直至在这种混乱中走向消亡。</p><p><strong>值得庆幸的是，宇宙是平衡的，在熵增的大前提下，给了我们一条不同的路，反熵增，也即自组织、生命化。</strong></p><p>亚马逊的CEO贝佐斯对于反熵增的理解非常深刻。他把亚马逊的服务做得足够开放，所以才有了亚马逊云。</p><p>对于新生事物，哪怕是取得一点点的成绩，他都会大加赞赏，要求主管以上级别员工每周都要读一遍《创新者的窘境》。</p><p>反熵增就是重现生命。行星从星云中诞生，行星上产生了岩石圈、大气、河流、季风、泉水、矿藏，这些都是组织化的过程，是对无序的反抗。</p><p>这种有序化、组织化进程的顶峰，便是生命的产生：DNA团块、单细胞生物、多细胞生物、植物和动物，一直到最精巧的组织化结构——人类。</p><p>人老后，最好的状态就是复归婴儿和自然。</p><p>一个企业，要让自己反熵增，就是让自己更有活力，更开放，更多地适应外部变化，让其在环境中自我进化。</p><p>同时，可以不断生出独立的新的业务，新的业务也可以不断进化。</p><p><strong>从本质上来讲，延续的发展就是一个熵增的过程，而要想突破非连续性，必须要“生”出新的曲线，这才是对抗熵增最好的办法。</strong></p><h1 id="5-三层解释思维模型"><a href="#5-三层解释思维模型" class="headerlink" title="5.三层解释思维模型"></a>5.<strong>三层解释思维模型</strong></h1><p>对于一个事物，可以有三层解释：<strong>现实层、技术层、底层。</strong></p><p>现实层会解释浅显的，大多数人能看到的因果；</p><p>技术层会解释现实背后的规律，是现象之下，是一种背景带来的规律，是一种看得见技能的规律；</p><p>而底层，则是一种可以广泛适用的规律，是深层次思维模型，通达人性，洞悉法则。</p><p>三层解释思维模型，可以应用于深度剖析一个事件的原因，了解一个复杂事物的规律。</p><h1 id="6-反脆弱思维模型"><a href="#6-反脆弱思维模型" class="headerlink" title="6.反脆弱思维模型"></a>6.<strong>反脆弱思维模型</strong></h1><p>**</p><p>事物可分为三类：<strong>脆弱、强悍、反脆弱。</strong></p><p>脆弱的事物，在受到外界压力时会破碎、受损，就像玻璃杯掉到地上会碎；强悍的事物，在遭遇外界压力时，不会受其影响。</p><p>而反脆弱的事物，可以在这个波动的世界中，伴随压力而进化，让自己变得更强大。</p><p>不要被压力打垮，它是你进化最好的能量。外界的环境变化会筛选出真正的反脆弱者。反脆弱思维模型可以应用于：</p><p>1.从困境中理性分析，提升认知，获得能量；</p><p>2.从压力中回归内心，提升胸怀和气度；</p><p>3.从局限中重新定位，提升眼光和格局。</p><p>而自然，也具备反脆弱性。在这个世界上，自然的事物很多都有弹性，具有反脆弱性。</p><p>自然的特点有很多，在自然中有春夏秋冬这样普遍的规律，适应性强的生物从这样的波动中进化，而体型大笨拙的生物如恐龙因此而灭绝；</p><p>人的身体也符合自然的规律，也具有反脆弱性，人骨折了，生出来的骨头会更加强壮，中过毒的人也会具有抗毒的特性。</p><p><strong>反脆弱本身是可以面对风险，可以从风险中受益成长。</strong></p><p>面对未来的不确定性，我们可以这样做：</p><p>\1. 利用不对称性，小投入，高产出。</p><p>\2. 成为多元化，让自己不只一种技能或身份。</p><p>\3. 相信未来的非线性，会给我们更多的机会与乐趣。这个世界，正因为有了非线性才会有新的生物不断出现，新的事物代替旧的事物，我们才会不断成长不断精进。</p><h1 id="7-复利原理"><a href="#7-复利原理" class="headerlink" title="7.复利原理"></a>7.<strong>复利原理</strong></h1><p>爱因斯坦说：复利是世界的第八大奇迹。</p><p><img src="/src/640-165778284238275.jpeg" alt="图片"></p><p>巴菲特说：人生就像滚雪球，关键是要找到足够湿的雪和足够长的坡。</p><p>查理·芒格说：同时理解复利的力量和获得它的困难，是理解许多事情的核心和灵魂。</p><p><strong>在有限的时间，把有限的精力和财富，持续而反复地投入到某一领域，长期坚持下去，最终产生的积极影响，会如雪球越滚越大，它带来的回报一定超过你的想象。</strong></p><p>这就是经济学中典型的复利思维。</p><p>大多数人终其一生都不会去运用复利，也体会不到复利的威力。</p><p>复利思维需要我们用发展和长远的眼光去看待事物。</p><h1 id="8-完型融合思维模型"><a href="#8-完型融合思维模型" class="headerlink" title="8.完型融合思维模型"></a>8.<strong>完型融合思维模型</strong></h1><p>完成一件事情，除了要有完整、清晰的逻辑思路，还需要多种元素进行融合， 比如：<strong>人的资源、物钱的资源、时机大环境等。</strong></p><p>这就像完成一个陶瓷作品的创作：</p><p>除了有创造的图纸、陶瓷还需要泥，相当于是物钱相关的资源；</p><p>而水就像是人，需要把泥组合起来；陶瓷的烧制，还需要合适的温度，速度，这就像看不见的时机与背景环境；</p><p>而陶瓷上的雕琢，就像是对事情细节的处理。先做什么后做什么的顺序，也至关重要。</p><p>所以，一个项目要取得成功，除了要有可能实现的规划，还需要有充足的人力准备、事情分解分层、方法论支持、流程制度激励支持、 稳定明确的责权分解说明，还要有应急方案与思路。</p><p>这样做起事来，才可行云流水，事半功倍。</p><p><strong>完型融合思维模型可以应用在：事情的执行、思考事物产生原因、预测人的判断等。</strong></p><p>只有考虑足够多的因素，以及整体需要的相互支持，因素之间的相互关系，才能更准确地判断与分析。</p><h1 id="9-笛卡尔思维模型"><a href="#9-笛卡尔思维模型" class="headerlink" title="9.笛卡尔思维模型"></a>9.<strong>笛卡尔思维模型</strong></h1><p>笛卡尔是一个伟大的哲学家，他提出一个大胆的观点，你怎么能证明这个世界是真实的？这个世界有什么是不能被怀疑的？</p><p>他思考再思考，一直没有抓到那个基石。直到某一瞬间，就像闪电击中了他。</p><p>他意识到，只有一件事不能再被怀疑了，那就是：<strong>我在怀疑这件事。</strong></p><p><img src="/src/640-165778284238376.jpeg" alt="图片"></p><p>也就是：I think,therefore I am。</p><p>这件事情是确定的，所以我是存在的，所以不管当下世界是不是真，一定存在一个世界。</p><p>而当下世界，我不知道它是不是真的存在。</p><p><strong>这就是批判性思维，大胆质疑一切。我们以为科学的演化是继承，其实是颠覆。</strong></p><p>下一代科学家几乎都颠覆上一代。</p><p>就如乔布斯说：stay hungry,stay foolish；股神会说：我唯一知道的事情，就是我什么都不知道。</p><p><strong>在人的认知经验里，过往的成功范式越多，给自己的束缚也越大。</strong></p><h1 id="10-获得性偏差思维模型"><a href="#10-获得性偏差思维模型" class="headerlink" title="10.获得性偏差思维模型"></a>10.<strong>获得性偏差思维模型</strong></h1><p>人们对一个事物判断失误，往往不是因为他什么都不知道；而是因为把注意力太多都放在了已知部分。</p><p>从一个大的完型角度，你了解的信息，可能不足5%，所以，自己已经获得的信息也许不仅不重要，很有可能还是局限的，有偏见的。</p><p>获得性偏差会出现在哪些生活中的场景呢？比如：</p><p>\1. 人们在找新工作时，对自己曾经做过的部分，往往难以放下。</p><p>\2. 对固有观念的坚持，甚至是固执。</p><p>\3. 对没用旧物的留恋。</p><p>我们很多人都遇到过类似情况，很多东西明知没用却舍不得扔。</p><p>殊不知，房间在熵增，你的生活环境因为不够简洁，你也会变得混乱、迟钝、得不偿失。</p><p>此外，新事物也很难进来，你的生活逐渐陷入了低效与僵化。</p><p><strong>如何克服获得性偏差呢？</strong></p><p>\1. 放低自己，谦虚心态，虚怀若谷；</p><p>\2. 独立思考，从本质入手，更多用演绎法而不是归纳法，用事物的本质去推理，而不是持续的用之前的经验；</p><p>\3. 分类整理。把信息归类整理，把一切变得秩序化；</p><p>\4. 重新分析，评估环境和信息，哪些是合理的，哪些是落后的，哪些是可以升级的；</p><p>\5. 懂得舍弃和放下；</p><p>\6. 动态心态，拥有只是暂时，变化是永恒的。</p><h1 id="11-排列组合思维模型"><a href="#11-排列组合思维模型" class="headerlink" title="11.排列组合思维模型"></a>11.<strong>排列组合思维模型</strong></h1><p>和复利模型一样，排列组合模型不仅是一种数学工具，也是一种可以提升我们决策质量的思维方式。</p><p><strong>很多时候，影响决策的因素很多，通过分类、分步，就可以形成不同的排列组合方式。</strong> </p><p>万事万物都是由其构成的元素排列组合成的，思维是知识的运动，是知识的排列组合、取舍。</p><p>前所未有的知识排列组合就是创新，合乎客观的有价值的思维创新的外化和物化，就是理论创新、创造发明。<br>查理·芒格的多元思维模型，本质就是一种多维组合思想，把不同的跨学科的知识汇集在一起，解决一个问题。</p><p>不同问题和学科所占权重不一样，综合起来考虑问题就更全面，正确概率就更高。 </p><p>一个大问题，可以分解成很多相关元素，这样方便我们看清楚整个问题，然后找到关键点，从关键点入手可以起到事半功倍的效果。</p><p><strong>很多元素组合在一起，可以对一个问题看得更全面，避免陷入查理·芒格口中的“铁锤人”思维。</strong></p><h1 id="12-费马帕斯卡系统"><a href="#12-费马帕斯卡系统" class="headerlink" title="12.费马帕斯卡系统"></a>12.<strong>费马帕斯卡系统</strong></h1><p>在知识、能力、努力、耐心这些所有的品质中，查理·芒格最看重的是理性。</p><p>查理·芒格说：<strong>“你必须看到这个世界真实的样子，而不是你以为的样子、或者你希望的样子，只有这样你才能做出正确的选择。”费马帕斯卡系统就是认识真实世界的基本工具。</strong> </p><p>费马帕斯卡系统是概率论的基本原则。</p><p>在生活中，充满了各种诱惑，人们根据经验和各种心理倾向做决定，从而掉入了很多的陷阱。<br>通过费马帕斯卡系统的学习理解，我们要从认知上明白，事情的实际概率是多少，在有了清晰的认识之后，再做决定将更加理性。</p><p>把这些基本的基础数学概率方法，变成我们生活中的一部分，才不会将自己的优势拱手送给别人。</p><h1 id="13-前景理论"><a href="#13-前景理论" class="headerlink" title="13.前景理论"></a>13.<strong>前景理论</strong></h1><p>前景理论，是风险决策的一个重要思维模型，由诺贝尔经济学奖心理学家丹尼尔·卡尼曼提出，描述和预测人们在面临风险决策过程中的行为理论。</p><p>前景理论分析框架有三个特征：</p><p><strong>一是大多数人在面临获得时，是风险规避的；</strong></p><p><strong>二是大多数人在面临损失时，是风险偏爱的；</strong></p><p><strong>三是人们对损失比获得更敏感。</strong></p><p>具体看：</p><p>\1. 确定效应：“二鸟在林，不如一鸟在手”，在确定的收益和“赌一把”之间，多数人会选择确定的好处，落袋为安心理。</p><p>\2. 反射效应：在确定的损失和“赌一把”之间，做一个抉择，多数人会选择“赌一把”。</p><p>\3. 损失规避：白捡的100元所带来的快乐，难以抵消丢失100元所带来的痛苦。</p><h1 id="14-心流模型"><a href="#14-心流模型" class="headerlink" title="14.心流模型"></a>14.<strong>心流模型</strong></h1><p>“心流”是指我们在做某些事情时，那种全神贯注、投入忘我的状态——这种状态下，你甚至感觉不到时间的存在。在事情完成后，我们会有一种充满能量且非常满足的感受。 </p><p>这是关于人幸福快乐的理论，让人沉浸在心流中的条件需要：</p><p>\1. 明确的目标，合适难度的任务——既不会焦虑，也不会无趣。</p><p>\2. 及时的反馈——对进步的满足感。</p><p>​为何游戏如此受欢迎，就是游戏的设计满足心流理论，如果游戏的定义扩大，把学习、工作、运动、教育包含进来，去评估自己在做的事是否满足心流三点，这样自然会过得幸福快乐。</p><p>\3. 发挥自己的某种能力。</p><p>\4. 精神完全集中，对手上的事情有控制力 。</p><p><img src="/src/640-165778284238377.jpeg" alt="图片"></p><p>心流状态，是我们能够在工作中达到的最美好、最平和的状态。通过一种心流状态又会产生新的心流状态。</p><p>成功的人，能够成功地将他们的一生，变成一种单纯的心流状态。他们在生命中各个部分紧紧地连接到了一起，所有活动都有了意义。</p><p>15.<strong>奥卡姆剃刀定律</strong></p><p>“奥卡姆剃刀定律”，又称“简约法则”， 这个原理最早至少能追溯到亚里士多德的“自然界选择最短的道路”。</p><p>核心内容为 “如无必要，勿增实体”，即简单有效原理。 </p><p>依据最小能量原则，我们的生活应尽可能简单，而简单的有效途径就是养成好的习惯。</p><p>自动挡汽车在逐步替代手动挡汽车，也是因为自动挡对大多数用户来说满足“简单有效原理”。</p><p><img src="/src/640-165778284238378.jpeg" alt="图片"></p><p>面对复杂的投资市场，应拿起奥卡姆剃刀，把复杂事情简单化，简化自己的投资策略，对那些消耗了大量金钱、时间、精力的事情加以区分，然后釆取步骤去摆脱它们。</p><p>最后我们以弗里德里克·迈特兰德的名言作结：<strong>“简单是长期努力工作的结果，而不是起点。”</strong>起点。”**</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning Foundation——概率论和数理统计</title>
    <link href="/2022/07/01/MachineLearning/%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
    <url>/2022/07/01/MachineLearning/%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="概率论和数理统计"><a href="#概率论和数理统计" class="headerlink" title="概率论和数理统计"></a>概率论和数理统计</h1><h3 id="1-概率的基本要素"><a href="#1-概率的基本要素" class="headerlink" title="1. 概率的基本要素"></a>1. 概率的基本要素</h3><p>为了定义集合上的概率，我们需要一些基本元素，</p><ul><li><p>样本空间$\Omega $：随机实验的所有结果的集合。在这里，每个结果 $w \in \Omega $ 可以被认为是实验结束时现实世界状态的完整描述。</p></li><li><p>事件集（事件空间）$\mathcal{F}$：元素 $A \in \mathcal{F}$ 的集合（称为事件）是 $\Omega $ 的子集（即每个 $A \subseteq \Omega$ 是一个实验可能结果的集合）。</p><p>备注：$\mathcal{F}$需要满足以下三个条件：</p><p>(1) $\emptyset  \in \mathcal{F}$</p><p>(2) $A \in \mathcal{F} \Longrightarrow \Omega \backslash A \in \mathcal{F}$</p><p>(3) $A_1,A_2,\cdots A_{i} \in \mathcal{F}\Longrightarrow\cup_{i} A_{i} \in \mathcal{F}$</p></li><li><p>概率度量$P$：函数$P$是一个$  \mathcal{F} \rightarrow \mathbb{R}$的映射，满足以下性质：</p><ul><li><p>对于每个 $A \in \mathcal{F}$，$P(A) \geq 0$, </p></li><li><p>$P(\Omega) &#x3D; 1$</p></li><li><p>如果$A_1 ,A_2 ,\cdots$ 是互不相交的事件 (即 当$ i \neq j$时，$A_{i} \cap A_{j}&#x3D;\emptyset$ ), 那么：</p><p>$$<br>P\left(\cup_{i} A_{i}\right)&#x3D;\sum_{i} P\left(A_{i}\right)<br>$$</p></li></ul></li></ul><p>以上三条性质被称为<strong>概率公理</strong>。</p><p><strong>举例</strong>：</p><p>考虑投掷六面骰子的事件。样本空间为$\Omega&#x3D; {1，2，3，4，5，6}$。最简单的事件空间是平凡事件空间$\mathcal{F} &#x3D; {\emptyset,\Omega}$.另一个事件空间是$\Omega$的所有子集的集合。对于第一个事件空间，满足上述要求的唯一概率度量由$P(\emptyset) &#x3D; 0$，$p(\Omega)&#x3D; 1$给出。对于第二个事件空间，一个有效的概率度量是将事件空间中每个事件的概率分配为$i&#x2F;6$，这里$i$ 是这个事件集合中元素的数量；例如$P({1,2,3,4}) &#x3D;4&#x2F;6$，$P({1,2,3}) &#x3D;3&#x2F;6$。</p><p><strong>性质：</strong></p><ul><li>如果$A \subseteq B$，则：$ P(A) \leq P(B)$</li><li>$P(A \cap B) \leq min(P(A),P(B) )$</li><li>(布尔不等式)：$P(A \cup B) \leq P(A)+P(B)$</li><li>$P(\Omega |A ) &#x3D;1-P(A)$</li><li>(全概率定律)：如果$A_1，\cdots，A_k$是一些互不相交的事件并且它们的并集是$\Omega$，那么它们的概率之和是1</li></ul><h4 id="1-1-事件的关系与运算"><a href="#1-1-事件的关系与运算" class="headerlink" title="1.1 事件的关系与运算"></a><strong>1.1 事件的关系与运算</strong></h4><p>(1) 子事件：$A \subset B$，若$A$发生，则$B$发生。</p><p>(2) 相等事件：$A &#x3D; B$，即$A \subset B$，且$B \subset A$ 。</p><p>(3) 和事件：$A\bigcup B$（或$A + B$），$A$与$B$中至少有一个发生。</p><p>(4) 差事件：$A - B$，$A$发生但$B$不发生。</p><p>(5) 积事件：$A\bigcap B$（或${AB}$），$A$与$B$同时发生。</p><p>(6) 互斥事件（互不相容）：$A\bigcap B$&#x3D;$\varnothing$。</p><p>(7) 互逆事件（对立事件）：<br>$A\bigcap B&#x3D;\varnothing ,A\bigcup B&#x3D;\Omega ,A&#x3D;\bar{B},B&#x3D;\bar{A}$</p><h4 id="1-2-运算律"><a href="#1-2-运算律" class="headerlink" title="1.2 运算律"></a><strong>1.2 运算律</strong></h4><p>(1) 交换律：$A\bigcup B&#x3D;B\bigcup A,A\bigcap B&#x3D;B\bigcap A$<br>(2) 结合律：$(A\bigcup B)\bigcup C&#x3D;A\bigcup (B\bigcup C)$<br>(3) 分配律：$(A\bigcap B)\bigcap C&#x3D;A\bigcap (B\bigcap C)$</p><h4 id="1-3-德-centerdot-摩根律"><a href="#1-3-德-centerdot-摩根律" class="headerlink" title="1.3 德$\centerdot $摩根律"></a><strong>1.3 德$\centerdot $摩根律</strong></h4><p>$\overline{A\bigcup B}&#x3D;\bar{A}\bigcap \bar{B}$</p><p> $\overline{A\bigcap B}&#x3D;\bar{A}\bigcup \bar{B}$</p><h4 id="1-4-完全事件组"><a href="#1-4-完全事件组" class="headerlink" title="1.4 完全事件组"></a><strong>1.4 完全事件组</strong></h4><p>${ {A}<em>{1} }{ {A}</em>{2} }\cdots { {A}<em>{n} }$两两互斥，且和事件为必然事件，即${ {A}</em>{i} }\bigcap { {A}<em>{j} }&#x3D;\varnothing, i\ne j ,U</em>{i&#x3D;1}^n &#x3D; \Omega$</p><h4 id="1-5-概率的基本公式"><a href="#1-5-概率的基本公式" class="headerlink" title="1.5 概率的基本公式"></a><strong>1.5 概率的基本公式</strong></h4><p><strong>(1)条件概率:</strong><br>$P(A | B) \triangleq \frac{P(A \cap B)}{P(B)}$,表示$A$发生的条件下，$B$发生的概率。</p><p><strong>(2)全概率公式：</strong><br>$P(A)&#x3D;\sum\limits_{i&#x3D;1}^{n}{P(A|{ {B}<em>{i} })P({ {B}</em>{i} }),{ {B}<em>{i} }{ {B}</em>{j} } }&#x3D;\varnothing ,i\ne j,\underset{i&#x3D;1}{\overset{n}{\mathop{\bigcup } } },{ {B}_{i} }&#x3D;\Omega$</p><p><strong>(3) Bayes 公式：</strong><br>$$<br>P({ {B}<em>{j} }|A)&#x3D;\frac{P(A|{ {B}</em>{j} })P({ {B}<em>{j} })}{\sum\limits</em>{i&#x3D;1}^{n}{P(A|{ {B}<em>{i} })P({ {B}</em>{i} })} },j&#x3D;1,2,\cdots ,n<br>$$</p><p>注：上述公式中事件${ {B}_{i} }$的个数可为可列个。</p><p><strong>(4)乘法公式：</strong><br>$P({ {A}<em>{1} }{ {A}</em>{2} })&#x3D;P({ {A}<em>{1} })P({ {A}</em>{2} }|{ {A}<em>{1} })&#x3D;P({ {A}</em>{2} })P({ {A}<em>{1} }|{ {A}</em>{2} })$<br>$P({ {A}<em>{1} }{ {A}</em>{2} }\cdots { {A}<em>{n} })&#x3D;P({ {A}</em>{1} })P({ {A}<em>{2} }|{ {A}</em>{1} })P({ {A}<em>{3} }|{ {A}</em>{1} }{ {A}<em>{2} })\cdots P({ {A}</em>{n} }|{ {A}<em>{1} }{ {A}</em>{2} }\cdots { {A}_{n-1} })$</p><h4 id="1-6-事件的独立性"><a href="#1-6-事件的独立性" class="headerlink" title="1.6.事件的独立性"></a><strong>1.6.事件的独立性</strong></h4><p>(1)$A$与$B$相互独立</p><p>$\Leftrightarrow P(AB)&#x3D;P(A)P(B)$</p><p>(2)$A$，$B$，$C$两两独立<br>$\Leftrightarrow P(AB)&#x3D;P(A)P(B)$;$P(BC)&#x3D;P(B)P(C)$ ;$P(AC)&#x3D;P(A)P(C)$;</p><p>(3)$A$，$B$，$C$相互独立<br>$\Leftrightarrow P(AB)&#x3D;P(A)P(B)$; $P(BC)&#x3D;P(B)P(C)$ ;<br>$P(AC)&#x3D;P(A)P(C)$ ; $P(ABC)&#x3D;P(A)P(B)P(C)$</p><h4 id="1-7-独立重复试验"><a href="#1-7-独立重复试验" class="headerlink" title="1.7 独立重复试验"></a><strong>1.7 独立重复试验</strong></h4><p>将某试验独立重复$n$次，若每次实验中事件 A 发生的概率为$p$，则$n$次试验中$A$发生$k$次的概率为：<br>$P(X&#x3D;k)&#x3D;C_{n}^{k}{ {p}^{k} }{ {(1-p)}^{n-k} }$</p><h4 id="1-8-重要公式与结论"><a href="#1-8-重要公式与结论" class="headerlink" title="1.8 重要公式与结论"></a><strong>1.8 重要公式与结论</strong></h4><p>$(1)P(\bar{A})&#x3D;1-P(A)$</p><p>$(2)P(A\bigcup B)&#x3D;P(A)+P(B)-P(AB)$<br>$P(A\bigcup B\bigcup C)&#x3D;P(A)+P(B)+P(C)-P(AB)-P(BC)-P(AC)+P(ABC)$</p><p>$(3)P(A-B)&#x3D;P(A)-P(AB)$</p><p>$(4)P(A\bar{B})&#x3D;P(A)-P(AB),P(A)&#x3D;P(AB)+P(A\bar{B}),$<br>$P(A\bigcup B)&#x3D;P(A)+P(\bar{A}B)&#x3D;P(AB)+P(A\bar{B})+P(\bar{A}B)$</p><p>(5)条件概率$P(\centerdot |B)$满足概率的所有性质，<br>例如：. $P({ {\bar{A} }<em>{1} }|B)&#x3D;1-P({ {A}</em>{1} }|B)$<br>$P({ {A}<em>{1} }\bigcup { {A}</em>{2} }|B)&#x3D;P({ {A}<em>{1} }|B)+P({ {A}</em>{2} }|B)-P({ {A}<em>{1} }{ {A}</em>{2} }|B)$<br>$P({ {A}<em>{1} }{ {A}</em>{2} }|B)&#x3D;P({ {A}<em>{1} }|B)P({ {A}</em>{2} }|{ {A}_{1} }B)$</p><p>(6)若${ {A}<em>{1} },{ {A}</em>{2} },\cdots ,{ {A}<em>{n} }$相互独立，则$P(\bigcap\limits</em>{i&#x3D;1}^{n}{ { {A}<em>{i} }})&#x3D;\prod\limits</em>{i&#x3D;1}^{n}{P({ {A}<em>{i} })},$<br>$P(\bigcup\limits</em>{i&#x3D;1}^{n}{ { {A}<em>{i} }})&#x3D;\prod\limits</em>{i&#x3D;1}^{n}{(1-P({ {A}_{i} }))}$</p><p>(7)互斥、互逆与独立性之间的关系：<br>$A$与$B$互逆$\Rightarrow$ $A$与$B$互斥，但反之不成立，$A$与$B$互斥（或互逆）且均非零概率事件$\Rightarrow $$A$与$B$不独立.</p><p>(8)若${ {A}<em>{1} },{ {A}</em>{2} },\cdots ,{ {A}<em>{m} },{ {B}</em>{1} },{ {B}<em>{2} },\cdots ,{ {B}</em>{n} }$相互独立，则$f({ {A}<em>{1} },{ {A}</em>{2} },\cdots ,{ {A}<em>{m} })$与$g({ {B}</em>{1} },{ {B}<em>{2} },\cdots ,{ {B}</em>{n} })$也相互独立，其中$f(\centerdot ),g(\centerdot )$分别表示对相应事件做任意事件运算后所得的事件，另外，概率为 1（或 0）的事件与任何事件相互独立.</p><h3 id="2-随机变量"><a href="#2-随机变量" class="headerlink" title="2. 随机变量"></a>2. 随机变量</h3><p>考虑一个实验，我们翻转10枚硬币，我们想知道正面硬币的数量。这里，样本空间$\Omega$的元素是长度为10的序列。例如，我们可能有$w_0 &#x3D; {H，H，T，H，T，H，H，T，T，T}\in\Omega$。然而，在实践中，我们通常不关心获得任何特定正反序列的概率。相反，我们通常关心结果的实值函数，比如我们10次投掷中出现的正面数，或者最长的背面长度。在某些技术条件下，这些函数被称为<strong>随机变量</strong>。</p><p>更正式地说，随机变量$X$是一个的$\Omega \longrightarrow \mathbb{R}$函数。通常，我们将使用大写字母$X(\omega)$或更简单的$X$(其中隐含对随机结果$\omega$的依赖)来表示随机变量。我们将使用小写字母$x$来表示随机变量的值。</p><p><strong>举例：</strong><br>在我们上面的实验中，假设$X(\omega)$是在投掷序列$\omega$中出现的正面的数量。假设投掷的硬币只有10枚，那么$X(\omega)$只能取有限数量的值，因此它被称为<strong>离散随机变量</strong>。这里，与随机变量$X$相关联的集合取某个特定值$k$的概率为：<br>$$<br>P(X&#x3D;k) :&#x3D;P({\omega : X(\omega) &#x3D;k})<br>$$<br><strong>举例：</strong><br>假设$X(\omega)$是一个随机变量，表示放射性粒子衰变所需的时间。在这种情况下，$X(\omega)$具有无限多的可能值，因此它被称为<strong>连续随机变量</strong>。我们将$X$在两个实常数$a$和$b$之间取值的概率(其中$a &lt; b$)表示为：<br>$$<br>P(a \leq X \leq b) :&#x3D;P({\omega : a \leq X(\omega) \leq b})<br>$$</p><h4 id="2-1-累积分布函数"><a href="#2-1-累积分布函数" class="headerlink" title="2.1 累积分布函数"></a>2.1 累积分布函数</h4><p>为了指定处理随机变量时使用的概率度量，通常可以方便地指定替代函数(<strong>CDF</strong>、<strong>PDF</strong>和<strong>PMF</strong>)，在本节和接下来的两节中，我们将依次描述这些类型的函数。</p><p><strong>累积分布函数(CDF)<strong>（简称分布函数）是函数$F_{X} : \mathbb{R} \rightarrow[0,1]$，它将概率度量指定为：<br>$$<br>F_{X}(x) \triangleq P(X \leq x)<br>$$<br>通过使用这个函数，我们可以计算任意事件发生的概率。图1显示了一个样本</strong>CDF</strong>函数。</p><p><img src="/src/c89fd93c5d6dccce89762d57fcd66dac.png"></p><center>图1：一个累计分布函数(CDF)</center><p><strong>性质：</strong></p><ul><li>$0 \leq F_{X}(x)\leq 1$</li><li>$\lim <em>{x \rightarrow-\infty} F</em>{X}(x)&#x3D;0$</li><li>$\lim <em>{x \rightarrow\infty} F</em>{X}(x)&#x3D;1$</li><li>$x \leq y \Longrightarrow  F_{X}(x)\leq F_{X}(y)$</li></ul><h4 id="2-2-概率质量函数"><a href="#2-2-概率质量函数" class="headerlink" title="2.2 概率质量函数"></a>2.2 概率质量函数</h4><p>当随机变量$X$取有限种可能值(即，$X$是离散随机变量)时，表示与随机变量相关联的概率度量的更简单的方法是直接指定随机变量可以假设的每个值的概率。特别地，概率质量函数(<strong>PMF</strong>)是函数 $p_{X} : \Omega \rightarrow \mathbb{R}$，这样：<br>$$<br>p_{X}(x) \triangleq P(X&#x3D;x)<br>$$</p><p>在离散随机变量的情况下，我们使用符号$Val(X)$表示随机变量$X$可能假设的一组可能值。例如，如果$X(\omega)$是一个随机变量，表示十次投掷硬币中的正面数，那么$Val(X) &#x3D;{0，1，2，.，10}$。</p><p><strong>性质：</strong></p><ul><li>$0 \leq p_{X}(x)\leq 1$</li><li>$\sum_{x \in V \text { al }(X)} p_{X}(x)&#x3D;1$</li><li>$\sum_{x \in A} p_{X}(x)&#x3D;P(X \in A)$</li></ul><h4 id="2-3-概率密度函数"><a href="#2-3-概率密度函数" class="headerlink" title="2.3 概率密度函数"></a>2.3 概率密度函数</h4><p>对于一些连续随机变量，累积分布函数$F_X (x)$处可微。在这些情况下，我们将**概率密度函数(PDF)**定义为累积分布函数的导数，即：<br>$$<br>f_{X}(x) \triangleq \frac{d F_{X}(x)}{d x}<br>$$</p><p>请注意，连续随机变量的概率密度函数可能并不总是存在的(即，如果它不是处处可微)。 </p><p>根据微分的性质，对于很小的$\Delta x$，<br>$$<br>P(x \leq X \leq x+\Delta x) \approx f_{X}(x) \Delta x<br>$$<br><strong>CDF</strong>和**PDF(<strong>当它们存在时！)都可用于计算不同事件的概率。但是应该强调的是，任意给定点的</strong>概率密度函数(PDF)**的值不是该事件的概率，即$f _X (x) \not &#x3D; P(X &#x3D; x)$。例如，$f _X (x)$可以取大于1的值(但是$f _X (x)$在$\mathbb{R}$的任何子集上的积分最多为1)。</p><p><strong>性质：</strong></p><ul><li>$f_X(x)\geq 0$</li><li>$\int_{-\infty}^{\infty} f_{X}(x)&#x3D;1$</li><li>$\int_{x \in A} f_{X}(x) d x&#x3D;P(X \in A)$</li></ul><h4 id="2-4-期望"><a href="#2-4-期望" class="headerlink" title="2.4 期望"></a>2.4 期望</h4><p>假设$X$是一个离散随机变量，其<strong>PMF</strong>为 $p_X (x)$，$g : \mathbb{R} \longrightarrow \mathbb{R}$是一个任意函数。在这种情况下，$g(X)$可以被视为随机变量，我们将$g(X)$的期望值定义为：<br>$$<br>E[g(X)] \triangleq \sum_{x \in V a l(X)} g(x) p_{X}(x)<br>$$<br>如果$X$是一个连续的随机变量，其<strong>PDF</strong> 为$f <em>X (x)$，那么$g(X)$的期望值被定义为：<br>$$<br>E[g(X)] \triangleq \int</em>{-\infty}^{\infty} g(x) f_{X}(x) d x<br>$$</p><p>直觉上，$g(X)$的期望值可以被认为是$g(x)$对于不同的$x$值可以取的值的“加权平均值”，其中权重由$p_X(x)$或$f_X(x)$给出。作为上述情况的特例，请注意，随机变量本身的期望值，是通过令$g(x) &#x3D; x$得到的，这也被称为随机变量的平均值。</p><p><strong>性质：</strong></p><ul><li>对于任意常数 $a \in \mathbb{R}$，$E[a]&#x3D;a$</li><li>对于任意常数 $a \in \mathbb{R}$，$E[af(X)]&#x3D;aE[f(X)]$</li><li>(线性期望)：$E[f(X)+g(X)]&#x3D;E[f(X)]+E[g(X)]$</li><li>对于一个离散随机变量$X$，$E[1{X&#x3D;k}]&#x3D;P(X&#x3D;k)$</li></ul><h4 id="2-5-方差"><a href="#2-5-方差" class="headerlink" title="2.5 方差"></a>2.5 方差</h4><p>随机变量$X$的<strong>方差</strong>是随机变量$X$的分布围绕其平均值集中程度的度量。形式上，随机变量$X$的方差定义为：<br>$\operatorname{Var}[X] \triangleq E\left[(X-E(X))^{2}\right]$<br>使用上一节中的性质，我们可以导出方差的替代表达式:<br>$\begin{aligned} E\left[(X-E[X])^{2}\right] &amp;&#x3D;E\left[X^{2}-2 E[X] X+E[X]^{2}\right] \ &amp;&#x3D;E\left[X^{2}\right]-2 E[X] E[X]+E[X]^{2} \ &amp;&#x3D;E\left[X^{2}\right]-E[X]^{2} \end{aligned}$</p><p>其中第二个等式来自期望的线性，以及$E[X]$相对于外层期望实际上是常数的事实。</p><p><strong>性质：</strong></p><ul><li>对于任意常数 $a \in \mathbb{R}$，$Val[a]&#x3D;0$</li><li>对于任意常数 $a \in \mathbb{R}$，$Var[af(X)]&#x3D;a^2Var[f(X)]$</li></ul><p><strong>举例：</strong></p><p>计算均匀随机变量$X$的平均值和方差，任意$x \in [0，1]$，其<strong>PDF</strong>为 $p_X(x)&#x3D; 1$，其他地方为0。</p><p>$$<br>E[X]&#x3D;\int_{-\infty}^{\infty} x f_{X}(x) d x&#x3D;\int_{0}^{1} x d x&#x3D;\frac{1}{2}<br>$$</p><p>$$<br>E\left[X^{2}\right]&#x3D;\int_{-\infty}^{\infty} x^{2} f_{X}(x) d x&#x3D;\int_{0}^{1} x^{2} d x&#x3D;\frac{1}{3}<br>$$</p><p>$$<br>Var[X]&#x3D;E[X^2]-E[X]^2&#x3D;\frac{1}{3}-\frac{1}{4}&#x3D;\frac{1}{12}<br>$$</p><p><strong>举例：</strong></p><p>假设对于一些子集$A \subseteq \Omega$，有$g(x) &#x3D; 1{x \in A}$，计算$E[g(X)]$?</p><p><strong>离散情况：</strong><br>$<br>E[g(X)]&#x3D;\sum_{x \in V a l(X)} 1{x \in A} P_{X}(x) d x&#x3D;\sum_{x \in A} P_{X}(x) d x&#x3D;P(x \in A)<br>$</p><p><strong>连续情况：</strong><br>$<br>E[g(X)]&#x3D;\int_{-\infty}^{\infty} 1{x \in A} f_{X}(x) d x&#x3D;\int_{x \in A} f_{X}(x) d x&#x3D;P(x \in A)<br>$</p><h4 id="2-6-一些常见的随机变量"><a href="#2-6-一些常见的随机变量" class="headerlink" title="2.6 一些常见的随机变量"></a>2.6 一些常见的随机变量</h4><p><strong>离散随机变量</strong></p><ul><li><p>伯努利分布：硬币掷出正面的概率为$p$（其中：$0 \leq p \leq 1$），如果正面发生，则为1，否则为0。</p><p>$$<br>p(x)&#x3D;\left{\begin{array}{ll}{p} &amp; {\text { if } p&#x3D;1} \ {1-p} &amp; {\text { if } p&#x3D;0}\end{array}\right.<br>$$</p></li><li><p>二项式分布：掷出正面概率为$p$（其中：$0 \leq p \leq 1$）的硬币$n$次独立投掷中正面的数量。</p><p>$$<br>p(x)&#x3D;\left(\begin{array}{l}{n} \ {x}\end{array}\right) p^{x}(1-p)^{n-x}<br>$$</p></li><li><p>泊松分布：用于模拟罕见事件频率的非负整数的概率分布（其中：$\lambda &gt;0$）。</p><p>$$<br>p(x)&#x3D;e^{-\lambda} \frac{\lambda^{x} }{x !}<br>$$</p></li><li><p>几何分布：掷出正面概率为$p$（其中：$p &gt;0$）的硬币第一次掷出正面所需要的次数。</p><p>$$<br>G(p):P(X &#x3D; k) &#x3D; {(1 - p)}^{k - 1}p,0 &lt; p &lt; 1,k &#x3D; 1,2,\cdots.<br>$$</p></li><li><p>超几何分布：$H(N,M,n):P(X &#x3D; k) &#x3D; \frac{C_{M}^{k}C_{N - M}^{n -k} }{C_{N}^{n} },k &#x3D;0,1,\cdots,min(n,M)$</p></li></ul><p><strong>连续随机变量</strong></p><ul><li><p>均匀分布：在$a$和$b$之间每个点概率密度相等的分布（其中：$a&lt;b$）。</p><p>$$<br>f(x)&#x3D;\left{\begin{array}{ll}{\frac{1}{b-a} } &amp; {\text { if } a \leq x \leq b} \ {0} &amp; {\text { otherwise } }\end{array}\right.<br>$$</p></li><li><p>指数分布：在非负实数上有衰减的概率密度（其中：$\lambda &gt;0$）。</p><p>$$<br>f(x)&#x3D;\left{\begin{array}{ll}{\lambda e^{-\lambda x} } &amp; {\text { if } x \geq 0} \ {0} &amp; {\text { otherwise } }\end{array}\right.<br>$$</p></li><li><p>正态分布：又被称为高斯分布。</p><p>$$<br>f(x)&#x3D;\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2 \sigma^{2} }(x-\mu)^{2} }<br>$$</p></li></ul><p>一些随机变量的概率密度函数和累积分布函数的形状如图2所示。</p><p><img src="/src/b958c16cfdce9e6bd2b810b10d71416e.png"></p><center>图2：一些随机变量的概率密度函数(PDF)和累积分布函数(CDF)</center><p>下表总结了这些分布的一些特性：</p><table><thead><tr><th align="center">分布</th><th align="center">概率密度函数(PDF)或者概率质量函数(<strong>PMF</strong>)</th><th align="center">均值</th><th align="center">方差</th></tr></thead><tbody><tr><td align="center">$Bernoulli(p)$(伯努利分布)</td><td align="center">$\left{\begin{array}{ll}{p} &amp; {\text { if } x&#x3D;1} \ {1-p} &amp; {\text { if } x&#x3D;0}\end{array}\right.$</td><td align="center">$p$</td><td align="center">$p(1-p)$</td></tr><tr><td align="center">$Binomial(n,p)$(二项式分布)</td><td align="center">$\left(\begin{array}{l}{n} \ {k}\end{array}\right) p^{k}(1-p)^{n-k}$  其中：$0 \leq k \leq n$</td><td align="center">$np$</td><td align="center">$npq$</td></tr><tr><td align="center">$Geometric(p)$(几何分布)</td><td align="center">$p(1-p)^{k-1}$  其中：$k&#x3D;1,2,\cdots$</td><td align="center">$\frac{1}{p}$</td><td align="center">$\frac {1-p}{p^2}$</td></tr><tr><td align="center">$Poisson(\lambda)$(泊松分布)</td><td align="center">$e^{-\lambda} \lambda^{x} &#x2F; x !$  其中：$k&#x3D;1,2,\cdots$</td><td align="center">$\lambda$</td><td align="center">$\lambda$</td></tr><tr><td align="center">$Uniform(a,b)$(均匀分布)</td><td align="center">$\frac{1}{b-a}$ 存在$x \in (a,b)$</td><td align="center">$\frac{a+b}{2}$</td><td align="center">$\frac{(b-a)^2}{12}$</td></tr><tr><td align="center">$Gaussian(\mu,\sigma^2)$(高斯分布)</td><td align="center">$\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2 \sigma^{2} }(x-\mu)^{2} }$</td><td align="center">$\mu$</td><td align="center">$\sigma^2$</td></tr><tr><td align="center">$Exponential(\lambda)$(指数分布)</td><td align="center">$\lambda e^{-\lambda x}$   $x\geq0,\lambda&gt;0$</td><td align="center">$\frac{1}{\lambda}$</td><td align="center">$\frac{1}{\lambda^2}$</td></tr></tbody></table><h4 id="2-7-重要公式与结论"><a href="#2-7-重要公式与结论" class="headerlink" title="2.7 重要公式与结论"></a>2.7 重要公式与结论</h4><p>(1) $X\sim N(0,1) \Rightarrow \varphi(0) &#x3D; \frac{1}{\sqrt{2\pi} },\Phi(0) &#x3D;\frac{1}{2},$ $\Phi( - a) &#x3D; P(X \leq - a) &#x3D; 1 - \Phi(a)$</p><p>(2) $X\sim N\left( \mu,\sigma^{2} \right) \Rightarrow \frac{X -\mu}{\sigma}\sim N\left( 0,1 \right),P(X \leq a) &#x3D; \Phi(\frac{a -\mu}{\sigma})$</p><p>(3) $X\sim E(\lambda) \Rightarrow P(X &gt; s + t|X &gt; s) &#x3D; P(X &gt; t)$</p><p>(4) $X\sim G(p) \Rightarrow P(X &#x3D; m + k|X &gt; m) &#x3D; P(X &#x3D; k)$</p><p>(5) 离散型随机变量的分布函数为阶梯间断函数；连续型随机变量的分布函数为连续函数，但不一定为处处可导函数。</p><p>(6) 存在既非离散也非连续型随机变量。</p><h3 id="3-两个随机变量"><a href="#3-两个随机变量" class="headerlink" title="3. 两个随机变量"></a>3. 两个随机变量</h3><p>到目前为止，我们已经考虑了单个随机变量。然而，在许多情况下，在随机实验中，我们可能有不止一个感兴趣的量。例如，在一个我们掷硬币十次的实验中，我们可能既关心$X(\omega) &#x3D;$出现的正面数量，也关心$Y (\omega) &#x3D;$连续最长出现正面的长度。在本节中，我们考虑两个随机变量的设置。</p><h4 id="3-1-联合分布和边缘分布"><a href="#3-1-联合分布和边缘分布" class="headerlink" title="3.1 联合分布和边缘分布"></a>3.1 联合分布和边缘分布</h4><p>假设我们有两个随机变量，一个方法是分别考虑它们。如果我们这样做，我们只需要$F_X (x)$和$F_Y (y)$。但是如果我们想知道在随机实验的结果中，$X$和$Y$同时假设的值，我们需要一个更复杂的结构，称为$X$和$Y$的<strong>联合累积分布函数</strong>，定义如下:<br>$$<br>F_{XY}(x,y)&#x3D;P(X \leq x,Y \leq y)<br>$$</p><p>可以证明，通过了解联合累积分布函数，可以计算出任何涉及到$X$和$Y$的事件的概率。</p><p>联合<strong>CDF</strong>: $F_{XY }(x,y)$和每个变量的联合分布函数$F_X(x)$和$F_Y (y)$分别由下式关联:<br>$$<br>F_{X}(x)&#x3D;\lim <em>{y \rightarrow \infty} F</em>{X Y}(x, y) d y<br>$$</p><p>$$<br>F_{Y}(y)&#x3D;\lim <em>{y \rightarrow \infty} F</em>{X Y}(x, y) dx<br>$$</p><p>这里我们称$F_X(x)$和$F_Y (y)$为 $F_{XY }(x,y)$的<strong>边缘累积概率分布函数</strong>。</p><p><strong>性质：</strong></p><ul><li>$0 \leq F_{XY }(x,y) \leq 1$</li><li>$\lim <em>{x, y \rightarrow \infty} F</em>{X Y}(x, y)&#x3D;1$</li><li>$\lim <em>{x, y \rightarrow -\infty} F</em>{X Y}(x, y)&#x3D;0$</li><li>$F_{X}(x)&#x3D;\lim <em>{y \rightarrow \infty} F</em>{X Y}(x, y)$</li></ul><h4 id="3-2-联合概率和边缘概率质量函数"><a href="#3-2-联合概率和边缘概率质量函数" class="headerlink" title="3.2 联合概率和边缘概率质量函数"></a>3.2 联合概率和边缘概率质量函数</h4><p>如果$X$和$Y$是离散随机变量，那么<strong>联合概率质量函数</strong>  $p_{X Y} : \mathbb{R} \times \mathbb{R} \rightarrow [0,1]$由下式定义：</p><p>$$<br>p_{X Y}(x,y)&#x3D;P(X&#x3D;x,Y&#x3D;y)<br>$$</p><p>这里, 对于任意$x$，$y$，$0 \leq P_{XY} (x,y) \leq 1$, 并且 $\sum_{x \in V a l(X)} \sum_{y \in V a l(Y)} P_{X Y}(x, y)&#x3D;1$</p><p>两个变量上的<strong>联合PMF</strong>分别与每个变量的概率质量函数有什么关系？事实上：<br>$$<br>p_{X}(x)&#x3D;\sum_{y} p_{X Y}(x, y)<br>$$</p><p>对于$p_Y (y)$类似。在这种情况下，我们称$p_X(x)$为$X$的边际概率质量函数。在统计学中，将一个变量相加形成另一个变量的边缘分布的过程通常称为“边缘化”。</p><h4 id="3-3-联合概率和边缘概率密度函数"><a href="#3-3-联合概率和边缘概率密度函数" class="headerlink" title="3.3 联合概率和边缘概率密度函数"></a>3.3 联合概率和边缘概率密度函数</h4><p>假设$X$和$Y$是两个连续的随机变量，具有联合分布函数$F_{XY}$。在$F_{XY}(x,y)$在$x$和$y$中处处可微的情况下，我们可以定义<strong>联合概率密度函数</strong>：<br>$$<br>f_{X Y}(x, y)&#x3D;\frac{\partial^{2} F_{X Y}(x, y)}{\partial x \partial y}<br>$$<br>如同在一维情况下，$f_{XY}(x,y)\not&#x3D; P(X &#x3D; x,Y &#x3D; y)$，而是：<br>$$<br>\iint_{x \in A} f_{X Y}(x, y) d x d y&#x3D;P((X, Y) \in A)<br>$$</p><p>请注意，概率密度函数$f_{XY}(x,y)$的值总是非负的，但它们可能大于1。尽管如此，可以肯定的是 $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X Y}(x, y)&#x3D;1$</p><p>与离散情况相似，我们定义:<br>$$<br>f_{X}(x)&#x3D;\int_{-\infty}^{\infty} f_{X Y}(x, y) d y<br>$$<br>作为$X$的<strong>边际概率密度函数</strong>(或<strong>边际密度</strong>)，对于$f_Y (y)$也类似。</p><h4 id="3-4-条件概率分布"><a href="#3-4-条件概率分布" class="headerlink" title="3.4 条件概率分布"></a>3.4 条件概率分布</h4><p>条件分布试图回答这样一个问题，当我们知道$X$必须取某个值$x$时，$Y$上的概率分布是什么？在离散情况下，给定$Y$的条件概率质量函数是简单的：<br>$$<br>p_{Y | X}(y | x)&#x3D;\frac{p_{X Y}(x, y)}{p_{X}(x)}<br>$$<br>假设分母不等于0。</p><p>在连续的情况下，在技术上要复杂一点，因为连续随机变量的概率等于零。忽略这一技术点，我们通过类比离散情况，简单地定义给定$X &#x3D; x$的条件概率密度为：<br>$$<br>f_{Y | X}(y | x)&#x3D;\frac{f_{X Y}(x, y)}{f_{X}(x)}<br>$$<br>假设分母不等于0。</p><h4 id="3-5-贝叶斯定理"><a href="#3-5-贝叶斯定理" class="headerlink" title="3.5 贝叶斯定理"></a>3.5 贝叶斯定理</h4><p>当试图推导一个变量给定另一个变量的条件概率表达式时，经常出现的一个有用公式是<strong>贝叶斯定理</strong>。</p><p>对于离散随机变量$X$和$Y$：<br>$$<br>P_{Y | X}(y | x)&#x3D;\frac{ {P_{XY} }(x, y)}{P_{X}(x)}&#x3D;\frac{P_{X | Y}(x | y) P_{Y}(y)}{\sum_{y^{\prime} \in V a l(Y)} P_{X | Y}\left(x | y^{\prime}\right) P_{Y}\left(y^{\prime}\right)}<br>$$</p><p>对于连续随机变量$X$和$Y$：</p><p>$$<br>f_{Y | X}(y | x)&#x3D;\frac{f_{X Y}(x, y)}{f_{X}(x)}&#x3D;\frac{f_{X | Y}(x | y) f_{Y}(y)}{\int_{-\infty}^{\infty} f_{X | Y}\left(x | y^{\prime}\right) f_{Y}\left(y^{\prime}\right) d y^{\prime} }<br>$$</p><h4 id="3-6-独立性"><a href="#3-6-独立性" class="headerlink" title="3.6 独立性"></a>3.6 独立性</h4><p>如果对于$X$和$Y$的所有值，$F_{XY}(x,y) &#x3D; F_X(x)F_Y(y)$，则两个随机变量$X$和$Y$是独立的。等价地，</p><ul><li>对于离散随机变量, 对于任意$x \in Val(X)$, $y \in Val(Y)$ ，$p_{XY}(x,y) &#x3D; p_X (x)p_Y (y)$。</li><li>对于离散随机变量, $p_Y |X (y|x) &#x3D; p_Y (y)$当对于任意$y \in Val(Y)$且$p_X (x) \not&#x3D; 0$。</li><li>对于连续随机变量, $f_{XY}(x,y) &#x3D; f_X (x)f_Y(y)$ 对于任意 $x,y \in \mathbb{R}$。</li><li>对于连续随机变量, $f_{Y |X} (y|x) &#x3D; f_Y (y)$ ，当$f_X (x)\not &#x3D; 0$对于任意$y \in \mathbb{R}$。</li></ul><p>非正式地说，如果“知道”一个变量的值永远不会对另一个变量的条件概率分布有任何影响，那么两个随机变量$X$和$Y$是独立的，也就是说，你只要知道$f(x)$和$f(y)$就知道关于这对变量$(X，Y)$的所有信息。以下引理将这一观察形式化:</p><p><strong>引理3.1</strong> </p><p>如果$X$和$Y$是独立的，那么对于任何$A，B⊆ \mathbb{R}$，我们有：<br>$$<br>P(X \in A, Y \in B)&#x3D;P(X \in A) P(Y \in B)<br>$$<br>利用上述引理，我们可以证明如果$X$与$Y$无关，那么$X$的任何函数都与$Y$的任何函数无关。</p><h4 id="3-7-期望和协方差"><a href="#3-7-期望和协方差" class="headerlink" title="3.7 期望和协方差"></a>3.7 期望和协方差</h4><p>假设我们有两个离散的随机变量$X$，$Y$并且$g : \mathbf{R}^{2} \longrightarrow \mathbf{R}$是这两个随机变量的函数。那么$g$的期望值以如下方式定义：<br>$$<br>E[g(X, Y)] \triangleq \sum_{x \in V a l(X)} \sum_{y \in V a l(Y)} g(x, y) p_{X Y}(x, y)<br>$$<br>对于连续随机变量$X$，$Y$，类似的表达式是：<br>$$<br>E[g(X, Y)]&#x3D;\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f_{X Y}(x, y) d x d y<br>$$<br>我们可以用期望的概念来研究两个随机变量之间的关系。特别地，两个随机变量的<strong>协方差</strong>定义为：<br>$$<br>{Cov}[X, Y] \triangleq E[(X-E[X])(Y-E[Y])]<br>$$<br>使用类似于方差的推导，我们可以将它重写为：<br>$$<br>\begin{aligned} {Cov}[X, Y] &amp;&#x3D;E[(X-E[X])(Y-E[Y])] \ &amp;&#x3D;E[X Y-X E[Y]-Y E[X]+E[X] E[Y]] \ &amp;&#x3D;E[X Y]-E[X] E[Y]-E[Y] E[X]+E[X] E[Y]] \ &amp;&#x3D;E[X Y]-E[X] E[Y] \end{aligned}<br>$$</p><p>在这里，说明两种协方差形式相等的关键步骤是第三个等号，在这里我们使用了这样一个事实，即$E[X]$和$E[Y]$实际上是常数，可以被提出来。当$cov[X，Y] &#x3D; 0$时，我们说$X$和$Y$不相关。</p><p><strong>性质：</strong></p><ul><li>(期望线性) $E[f(X,Y ) + g(X,Y)] &#x3D; E[f(X,Y )] + E[g(X,Y)]$</li><li>$V ar[X + Y ] &#x3D; V ar[X] + V ar[Y ] + 2Cov[X,Y]$</li><li>如果$X$和$Y$相互独立, 那么 $Cov[X,Y ] &#x3D; 0$</li><li>如果$X$和$Y$相互独立, 那么 $E[f(X)g(Y )] &#x3D; E[f(X)]E[g(Y)]$.</li></ul><h4 id="3-8-常见的二维随机变量"><a href="#3-8-常见的二维随机变量" class="headerlink" title="3.8 常见的二维随机变量"></a>3.8 常见的二维随机变量</h4><p>(1) 二维均匀分布：$(x,y) \sim U(D)$ ,$f(x,y) &#x3D; \begin{cases} \frac{1}{S(D)},(x,y) \in D \   0,其他  \end{cases}$</p><p>(2) 二维正态分布：$(X,Y)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$,$(X,Y)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$</p><p>$f(x,y) &#x3D; \frac{1}{2\pi\sigma_{1}\sigma_{2}\sqrt{1 - \rho^{2} }}.\exp\left{ \frac{- 1}{2(1 - \rho^{2})}\lbrack\frac{ {(x - \mu_{1})}^{2} }{\sigma_{1}^{2} } - 2\rho\frac{(x - \mu_{1})(y - \mu_{2})}{\sigma_{1}\sigma_{2} } + \frac{ {(y - \mu_{2})}^{2} }{\sigma_{2}^{2} }\rbrack \right}$</p><h4 id="3-9-重要公式与结论"><a href="#3-9-重要公式与结论" class="headerlink" title="3.9 重要公式与结论"></a>3.9 重要公式与结论</h4><p>(1) 边缘密度公式： $f_{X}(x) &#x3D; \int_{- \infty}^{+ \infty}{f(x,y)dy,}$<br>$f_{Y}(y) &#x3D; \int_{- \infty}^{+ \infty}{f(x,y)dx}$</p><p>(2) $P\left{ \left( X,Y \right) \in D \right} &#x3D; \iint_{D}^{}{f\left( x,y \right){dxdy} }$</p><p>(3) 若$(X,Y)$服从二维正态分布$N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$<br>则有：</p><ol><li><p>$X\sim N\left( \mu_{1},\sigma_{1}^{2} \right),Y\sim N(\mu_{2},\sigma_{2}^{2}).$</p></li><li><p>$X$与$Y$相互独立$\Leftrightarrow \rho &#x3D; 0$，即$X$与$Y$不相关。</p></li><li><p>$C_{1}X + C_{2}Y\sim N(C_{1}\mu_{1} + C_{2}\mu_{2},C_{1}^{2}\sigma_{1}^{2} + C_{2}^{2}\sigma_{2}^{2} + 2C_{1}C_{2}\sigma_{1}\sigma_{2}\rho)$</p></li><li><p>${\ X}$关于$Y&#x3D;y$的条件分布为： $N(\mu_{1} + \rho\frac{\sigma_{1} }{\sigma_{2} }(y - \mu_{2}),\sigma_{1}^{2}(1 - \rho^{2}))$</p></li><li><p>$Y$关于$X &#x3D; x$的条件分布为： $N(\mu_{2} + \rho\frac{\sigma_{2} }{\sigma_{1} }(x - \mu_{1}),\sigma_{2}^{2}(1 - \rho^{2}))$</p></li></ol><p>(4) 若$X$与$Y$独立，且分别服从$N(\mu_{1},\sigma_{1}^{2}),N(\mu_{1},\sigma_{2}^{2}),$<br>则：$\left( X,Y \right)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},0),$</p><p>$C_{1}X + C_{2}Y\tilde{\ }N(C_{1}\mu_{1} + C_{2}\mu_{2},C_{1}^{2}\sigma_{1}^{2} C_{2}^{2}\sigma_{2}^{2}).$</p><p>(5) 若$X$与$Y$相互独立，$f\left( x \right)$和$g\left( x \right)$为连续函数， 则$f\left( X \right)$和$g(Y)$也相互独立。</p><h3 id="4-多个随机变量"><a href="#4-多个随机变量" class="headerlink" title="4. 多个随机变量"></a>4. 多个随机变量</h3><p>上一节介绍的概念和想法可以推广到两个以上的随机变量。特别是，假设我们有$n$个连续随机变量，$X _1 (\omega),X_2 (\omega),\cdots X_n (\omega)$。在本节中，为了表示简单，我们只关注连续的情况，对离散随机变量的推广工作类似。</p><h4 id="4-1-基本性质"><a href="#4-1-基本性质" class="headerlink" title="4.1 基本性质"></a>4.1 基本性质</h4><p>我们可以定义$X_1,X_2,\cdots,X_n$的<strong>联合累积分布函数</strong>、<strong>联合概率密度函数</strong>，以及给定$X_2,\cdots,X_n$时$X_1$的<strong>边缘概率密度函数</strong>为：<br>$$<br>F_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots x_{n}\right)&#x3D;P\left(X_{1} \leq x_{1}, X_{2} \leq x_{2}, \ldots, X_{n} \leq x_{n}\right)<br>$$</p><p>$$<br>f_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots x_{n}\right)&#x3D;\frac{\partial^{n} F_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots x_{n}\right)}{\partial x_{1} \ldots \partial x_{n} }<br>$$</p><p>$$<br>f_{X_{1} }\left(X_{1}\right)&#x3D;\int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} f_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots x_{n}\right) d x_{2} \ldots d x_{n}<br>$$</p><p>$$<br>f_{X_{1} | X_{2}, \ldots, X_{n} }\left(x_{1} | x_{2}, \dots x_{n}\right)&#x3D;\frac{f_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \dots x_{n}\right)}{f_{X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots x_{n}\right)}<br>$$</p><p>为了计算事件$A \subseteq \mathbb{R}^{n}$的概率，我们有：<br>$$<br>P\left(\left(x_{1}, x_{2}, \ldots x_{n}\right) \in A\right)&#x3D;\int_{\left(x_{1}, x_{2}, \ldots x_{n}\right) \in A} f_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots x_{n}\right) d x_{1} d x_{2} \ldots d x_{n}<br>$$<br><strong>链式法则：</strong></p><p>从多个随机变量的条件概率的定义中，可以看出：<br>$$<br>\begin{aligned} f\left(x_{1}, x_{2}, \ldots, x_{n}\right) &amp;&#x3D;f\left(x_{n} | x_{1}, x_{2} \ldots, x_{n-1}\right) f\left(x_{1}, x_{2} \ldots, x_{n-1}\right) \ &amp;&#x3D;f\left(x_{n} | x_{1}, x_{2} \ldots, x_{n-1}\right) f\left(x_{n-1} | x_{1}, x_{2} \ldots, x_{n-2}\right) f\left(x_{1}, x_{2} \ldots, x_{n-2}\right) \ &amp;&#x3D;\cdots&#x3D;f\left(x_{1}\right) \prod_{i&#x3D;2}^{n} f\left(x_{i} | x_{1}, \ldots, x_{i-1}\right) \end{aligned}<br>$$</p><p>独立性:对于多个事件，$A_1,\cdots ,A_k$,我们说$A_1,\cdots ,A_k$ 是相互独立的,当对于任何子集$S \subseteq {1，2,\cdots,k}$，我们有：<br>$$<br>P\left(\cap_{i \in S} A_{i}\right)&#x3D;\prod_{i \in S} P\left(A_{i}\right)<br>$$</p><p>同样，我们说随机变量$X_1,X_2,\cdots,X_n$是独立的，如果：</p><p>$$<br>f(x_1,\cdots,x_n)&#x3D;f(x_1)f(x_2)\cdots f(x_n)<br>$$</p><p>这里，相互独立性的定义只是两个随机变量独立性到多个随机变量的自然推广。 </p><p>独立随机变量经常出现在机器学习算法中，其中我们假设属于训练集的训练样本代表来自某个未知概率分布的独立样本。为了明确独立性的重要性，考虑一个“坏的”训练集，我们首先从某个未知分布中抽取一个训练样本$(x^{ (1)},y^{(1)})$，然后将完全相同的训练样本的$m-1$个副本添加到训练集中。在这种情况下，我们有：<br>$$<br>P\left(\left(x^{(1)}, y^{(1)}\right), \ldots .\left(x^{(m)}, y^{(m)}\right)\right) \neq \prod_{i&#x3D;1}^{m} P\left(x^{(i)}, y^{(i)}\right)<br>$$</p><p>尽管训练集的大小为$m$，但这些例子并不独立！虽然这里描述的过程显然不是为机器学习算法建立训练集的明智方法，但是事实证明，在实践中，样本的不独立性确实经常出现，并且它具有减小训练集的“有效大小”的效果。</p><h4 id="4-2-随机向量"><a href="#4-2-随机向量" class="headerlink" title="4.2 随机向量"></a>4.2 随机向量</h4><p>假设我们有n个随机变量。当把所有这些随机变量放在一起工作时，我们经常会发现把它们放在一个向量中是很方便的.我们称结果向量为随机向量(更正式地说，随机向量是从$\Omega$到$\mathbb{R}^n$的映射)。应该清楚的是，随机向量只是处理$n$个随机变量的一种替代符号，因此联合概率密度函数和综合密度函数的概念也将适用于随机向量。</p><p><strong>期望:</strong></p><p>考虑$g : \mathbb{R}^n \rightarrow \mathbb{R}$中的任意函数。这个函数的期望值 被定义为<br>$$<br>E[g(X)]&#x3D;\int_{\mathbb{R}^{n} } g\left(x_{1}, x_{2}, \ldots, x_{n}\right) f_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots x_{n}\right) d x_{1} d x_{2} \ldots d x_{n}E[g(X)]\&#x3D;\int_{\mathbb{R}^{n} } g\left(x_{1}, x_{2}, \ldots, x_{n}\right) f_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots x_{n}\right) d x_{1} d x_{2} \ldots d x_{n}<br>$$<br>其中，$\int_{\mathbb{R}^{n} }$是从$-\infty$到$\infty$的$n$个连续积分。如果$g$是从$\mathbb{R}^n$到$\mathbb{R}^m$的函数，那么$g$的期望值是输出向量的元素期望值，即，如果$g$是：</p><p>$$<br>g(x)&#x3D;\left[\begin{array}{c}{g_{1}(x)} \ {g_{2}(x)} \ {\vdots} \ {g_{m}(x)}\end{array}\right]<br>$$</p><p>那么，<br>$$<br>E[g(X)]&#x3D;\left[\begin{array}{c}{E\left[g_{1}(X)\right]} \ {E\left[g_{2}(X)\right]} \ {\vdots} \ {E\left[g_{m}(X)\right]}\end{array}\right]<br>$$</p><p>协方差矩阵：对于给定的随机向量$X:\Omega\rightarrow \mathbb{R}^n$，其协方差矩阵$\Sigma$是$n \times n$平方矩阵，其输入由$\Sigma_{i j}&#x3D;{Cov}\left[X_{i}, X_{j}\right]$给出。从协方差的定义来看，我们有：<br>$$<br>\begin{aligned}<br>\begin{equation}<br>\Sigma&#x3D;\left[\begin{array}{ccc}{ {Cov}\left[X_{1}, X_{1}\right]} &amp; {\cdots} &amp; { {Cov}\left[X_{1}, X_{n}\right]} \ {\vdots} &amp; {\ddots} &amp; {\vdots} \ { {Cov}\left[X_{n}, X_{1}\right]} &amp; {\cdots} &amp; { {Cov}\left[X_{n}, X_{n}\right]}\end{array}\right]\</p><p>&#x3D;\left[\begin{array}{ccc}{E\left[X_{1}^{2}\right]-E\left[X_{1}\right] E\left[X_{1}\right]} &amp; {\cdots} &amp; {E\left[X_{1} X_{n}\right]-E\left[X_{1}\right] E\left[X_{n}\right]} \ {\vdots} &amp; {\ddots} &amp; {\vdots} \ {E\left[X_{n} X_{1}\right]-E\left[X_{n}\right] E\left[X_{1}\right]} &amp; {\cdots} &amp; {E\left[X_{n}^{2}\right]-E\left[X_{n}\right] E\left[X_{n}\right]}\end{array}\right]\</p><p>&#x3D;\left[\begin{array}{ccc}{E\left[X_{1}^{2}\right]} &amp; {\cdots} &amp; {E\left[X_{1} X_{n}\right]} \ {\vdots} &amp; {\ddots} &amp; {\vdots} \ {E\left[X_{n} X_{1}\right]} &amp; {\cdots} &amp; {E\left[X_{n}^{2}\right]}\end{array}\right]-\left[\begin{array}{ccc}{E\left[X_{1}\right] E\left[X_{1}\right]} &amp; {\cdots} &amp; {E\left[X_{1}\right] E\left[X_{n}\right]} \ {\vdots} &amp; {\ddots} &amp; {\vdots} \ {E\left[X_{n}\right] E\left[X_{1}\right]} &amp; {\cdots} &amp; {E\left[X_{n}\right] E\left[X_{n}\right]}\end{array}\right]\<br>&#x3D;E\left[X X^{T}\right]-E[X] E[X]^{T}&#x3D;\ldots&#x3D;E\left[(X-E[X])(X-E[X])^{T}\right]</p><p>\end{equation}<br>\end{aligned}<br>$$<br>其中矩阵期望以明显的方式定义。<br>协方差矩阵有许多有用的属性:</p><ul><li>$\Sigma \succeq 0$；也就是说，$\Sigma$是正半定的。 </li><li>$\Sigma&#x3D;\Sigma^T$；也就是说，$\Sigma$是对称的。</li></ul><h4 id="4-3-多元高斯分布"><a href="#4-3-多元高斯分布" class="headerlink" title="4.3 多元高斯分布"></a>4.3 多元高斯分布</h4><p>随机向量上概率分布的一个特别重要的例子叫做多元高斯或多元正态分布。随机向量$X\in \mathbb{R}^n$被认为具有多元正态(或高斯)分布，当其具有均值$\mu \in \mathbb{R}^n$和协方差矩阵$\Sigma \in \mathbb{S}<em>{++}^{n}$(其中$ \mathbb{S}</em>{++}^{n}$指对称正定$n \times n$矩阵的空间)</p><p>$f_{X_{1}, X_{2}, \ldots, X_{n} }\left(x_{1}, x_{2}, \ldots, x_{n} ; \mu, \Sigma\right)&#x3D;\frac{1}{(2 \pi)^{n &#x2F; 2}|\Sigma|^{1 &#x2F; 2} } \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)$</p><p>我们把它写成$X \sim \mathcal{N}(\mu, \Sigma)$。请注意，在$n &#x3D; 1$的情况下，它降维成普通正态分布，其中均值参数为$\mu_1$，方差为$\Sigma_{11}$。 </p><p>一般来说，高斯随机变量在机器学习和统计中非常有用，主要有两个原因：</p><p>首先，在统计算法中对“噪声”建模时，它们非常常见。通常，噪声可以被认为是影响测量过程的大量小的独立随机扰动的累积；根据中心极限定理，独立随机变量的总和将趋向于“看起来像高斯”。</p><p>其次，高斯随机变量便于许多分析操作，因为实际中出现的许多涉及高斯分布的积分都有简单的封闭形式解。我们将在本课程稍后遇到这种情况。</p><h3 id="5-随机变量的数字特征"><a href="#5-随机变量的数字特征" class="headerlink" title="5. 随机变量的数字特征"></a>5. 随机变量的数字特征</h3><p>一些具体的推导前文有过详细的阐述，这里不加缀述，此处仅作总结用</p><h4 id="5-1-数学期望"><a href="#5-1-数学期望" class="headerlink" title="5.1 数学期望"></a><strong>5.1 数学期望</strong></h4><p>离散型：$P\left{ X &#x3D; x_{i} \right} &#x3D; p_{i},E(X) &#x3D; \sum_{i}^{}{x_{i}p_{i} }$；</p><p>连续型： $X\sim f(x),E(X) &#x3D; \int_{- \infty}^{+ \infty}{xf(x)dx}$</p><p>性质：</p><p>(1) $E(C) &#x3D; C,E\lbrack E(X)\rbrack &#x3D; E(X)$</p><p>(2) $E(C_{1}X + C_{2}Y) &#x3D; C_{1}E(X) + C_{2}E(Y)$</p><p>(3) 若$X$和$Y$独立，则$E(XY) &#x3D; E(X)E(Y)$</p><p>(4)$\left\lbrack E(XY) \right\rbrack^{2} \leq E(X^{2})E(Y^{2})$</p><h4 id="5-2-方差："><a href="#5-2-方差：" class="headerlink" title="5.2 方差："></a><strong>5.2 方差</strong>：</h4><p>$D(X) &#x3D; E\left\lbrack X - E(X) \right\rbrack^{2} &#x3D; E(X^{2}) - \left\lbrack E(X) \right\rbrack^{2}$</p><h4 id="5-3-标准差："><a href="#5-3-标准差：" class="headerlink" title="5.3 标准差："></a><strong>5.3 标准差</strong>：</h4><ul><li><strong>离散型：</strong>$D(X) &#x3D; \sum_{i}^{}{\left\lbrack x_{i} - E(X) \right\rbrack^{2}p_{i} }$</li><li><strong>连续型：</strong>$D(X) &#x3D; {\int_{- \infty}^{+ \infty}\left\lbrack x - E(X) \right\rbrack}^{2}f(x)dx$</li></ul><p>性质：</p><p>(1)$\ D(C) &#x3D; 0,D\lbrack E(X)\rbrack &#x3D; 0,D\lbrack D(X)\rbrack &#x3D; 0$</p><p>(2) $X$与$Y$相互独立，则$D(X \pm Y) &#x3D; D(X) + D(Y)$</p><p>(3)$\ D\left( C_{1}X + C_{2} \right) &#x3D; C_{1}^{2}D\left( X \right)$</p><p>(4) 一般有 $D(X \pm Y) &#x3D; D(X) + D(Y) \pm 2Cov(X,Y) &#x3D; D(X) + D(Y) \pm<br>2\rho\sqrt{D(X)}\sqrt{D(Y)}$</p><p>(5)$\ D\left( X \right) &lt; E\left( X - C \right)^{2},C \neq E\left( X \right)$</p><p>(6)$\ D(X) &#x3D; 0 \Leftrightarrow P\left{ X &#x3D; C \right} &#x3D; 1$</p><h4 id="5-4-随机变量函数的数学期望"><a href="#5-4-随机变量函数的数学期望" class="headerlink" title="5.4 随机变量函数的数学期望"></a><strong>5.4 随机变量函数的数学期望</strong></h4><p>(1) 对于函数$Y &#x3D; g(x)$</p><p>$X$为离散型：$P{ X &#x3D; x_{i}} &#x3D; p_{i},E(Y) &#x3D; \sum_{i}^{}{g(x_{i})p_{i} }$；</p><p>$X$为连续型：$X\sim f(x),E(Y) &#x3D; \int_{- \infty}^{+ \infty}{g(x)f(x)dx}$</p><p>(2) $Z &#x3D; g(X,Y)$;$\left( X,Y \right)\sim P{ X &#x3D; x_{i},Y &#x3D; y_{j}} &#x3D; p_{ {ij} }$; $E(Z) &#x3D; \sum_{i}^{}{\sum_{j}^{}{g(x_{i},y_{j})p_{ {ij} }} }$ $\left( X,Y \right)\sim f(x,y)$;$E(Z) &#x3D; \int_{- \infty}^{+ \infty}{\int_{- \infty}^{+ \infty}{g(x,y)f(x,y)dxdy} }$</p><h4 id="5-5-协方差"><a href="#5-5-协方差" class="headerlink" title="5.5 协方差"></a><strong>5.5 协方差</strong></h4><p>$Cov(X,Y) &#x3D; E\left\lbrack (X - E(X)(Y - E(Y)) \right\rbrack$</p><h4 id="5-6-相关系数"><a href="#5-6-相关系数" class="headerlink" title="5.6 相关系数"></a><strong>5.6 相关系数</strong></h4><p>$\rho_{ {XY} } &#x3D; \frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)} }$,$k$阶原点矩 $E(X^{k})$;<br>$k$阶中心矩 $E\left{ {\lbrack X - E(X)\rbrack}^{k} \right}$</p><p>性质：</p><p>(1)$\ Cov(X,Y) &#x3D; Cov(Y,X)$</p><p>(2)$\ Cov(aX,bY) &#x3D; abCov(Y,X)$</p><p>(3)$\ Cov(X_{1} + X_{2},Y) &#x3D; Cov(X_{1},Y) + Cov(X_{2},Y)$</p><p>(4)$\ \left| \rho\left( X,Y \right) \right| \leq 1$</p><p>(5) $\ \rho\left( X,Y \right) &#x3D; 1 \Leftrightarrow P\left( Y &#x3D; aX + b \right) &#x3D; 1$ ，其中$a &gt; 0$</p><p>$\rho\left( X,Y \right) &#x3D; - 1 \Leftrightarrow P\left( Y &#x3D; aX + b \right) &#x3D; 1$<br>，其中$a &lt; 0$</p><h4 id="5-7-重要公式与结论"><a href="#5-7-重要公式与结论" class="headerlink" title="5.7 重要公式与结论"></a><strong>5.7 重要公式与结论</strong></h4><p>(1)$\ D(X) &#x3D; E(X^{2}) - E^{2}(X)$</p><p>(2)$\ Cov(X,Y) &#x3D; E(XY) - E(X)E(Y)$</p><p>(3) $\left| \rho\left( X,Y \right) \right| \leq 1,$且 $\rho\left( X,Y \right) &#x3D; 1 \Leftrightarrow P\left( Y &#x3D; aX + b \right) &#x3D; 1$，其中$a &gt; 0$</p><p>$\rho\left( X,Y \right) &#x3D; - 1 \Leftrightarrow P\left( Y &#x3D; aX + b \right) &#x3D; 1$，其中$a &lt; 0$</p><p>(4) 下面 5 个条件互为充要条件：</p><p>$\rho(X,Y) &#x3D; 0$ $\Leftrightarrow Cov(X,Y) &#x3D; 0$ $\Leftrightarrow E(X,Y) &#x3D; E(X)E(Y)$ $\Leftrightarrow D(X + Y) &#x3D; D(X) + D(Y)$ $\Leftrightarrow  D(X - Y) &#x3D; D(X) + D(Y)$</p><p>注：$X$与$Y$独立为上述 5 个条件中任何一个成立的充分条件，但非必要条件。</p><h3 id="6-数理统计的基本概念"><a href="#6-数理统计的基本概念" class="headerlink" title="6. 数理统计的基本概念"></a>6. 数理统计的基本概念</h3><h4 id="6-1-基本概念"><a href="#6-1-基本概念" class="headerlink" title="6.1 基本概念"></a><strong>6.1 基本概念</strong></h4><p>总体：研究对象的全体，它是一个随机变量，用$X$表示。</p><p>个体：组成总体的每个基本元素。</p><p>简单随机样本：来自总体$X$的$n$个相互独立且与总体同分布的随机变量$X_{1},X_{2}\cdots,X_{n}$，称为容量为$n$的简单随机样本，简称样本。</p><p>统计量：设$X_{1},X_{2}\cdots,X_{n},$是来自总体$X$的一个样本，$g(X_{1},X_{2}\cdots,X_{n})$）是样本的连续函数，且$g()$中不含任何未知参数，则称$g(X_{1},X_{2}\cdots,X_{n})$为统计量。</p><p>样本均值：$\overline{X} &#x3D; \frac{1}{n}\sum_{i &#x3D; 1}^{n}X_{i}$</p><p>样本方差：$S^{2} &#x3D; \frac{1}{n - 1}\sum_{i &#x3D; 1}^{n}{(X_{i} - \overline{X})}^{2}$</p><p>样本矩：样本$k$阶原点矩：$A_{k} &#x3D; \frac{1}{n}\sum_{i &#x3D; 1}^{n}X_{i}^{k},k &#x3D; 1,2,\cdots$</p><p>样本$k$阶中心矩：$B_{k} &#x3D; \frac{1}{n}\sum_{i &#x3D; 1}^{n}{(X_{i} - \overline{X})}^{k},k &#x3D; 1,2,\cdots$</p><h4 id="6-2-常用分布"><a href="#6-2-常用分布" class="headerlink" title="6.2 常用分布"></a><strong>6.2 常用分布</strong></h4><p>$\chi^{2}$分布：$\chi^{2} &#x3D; X_{1}^{2} + X_{2}^{2} + \cdots + X_{n}^{2}\sim\chi^{2}(n)$，其中$X_{1},X_{2}\cdots,X_{n},$相互独立，且同服从$N(0,1)$</p><p>$t$分布：$T &#x3D; \frac{X}{\sqrt{Y&#x2F;n} }\sim t(n)$ ，其中$X\sim N\left( 0,1 \right),Y\sim\chi^{2}(n),$且$X$，$Y$ 相互独立。</p><p>$F$分布：$F &#x3D; \frac{X&#x2F;n_{1} }{Y&#x2F;n_{2} }\sim F(n_{1},n_{2})$，其中$X\sim\chi^{2}\left( n_{1} \right),Y\sim\chi^{2}(n_{2}),$且$X$，$Y$相互独立。</p><p>分位数：若$P(X \leq x_{\alpha}) &#x3D; \alpha,$则称$x_{\alpha}$为$X$的$\alpha$分位数</p><h4 id="6-3-正态总体的常用样本分布"><a href="#6-3-正态总体的常用样本分布" class="headerlink" title="6.3 正态总体的常用样本分布"></a><strong>6.3 正态总体的常用样本分布</strong></h4><p>(1) 设$X_{1},X_{2}\cdots,X_{n}$为来自正态总体$N(\mu,\sigma^{2})$的样本，</p><p>$\overline{X} &#x3D; \frac{1}{n}\sum_{i &#x3D; 1}^{n}X_{i},S^{2} &#x3D; \frac{1}{n - 1}\sum_{i &#x3D; 1}^{n}{ {(X_{i} - \overline{X})}^{2},}$则：</p><ol><li><p>$\overline{X}\sim N\left( \mu,\frac{\sigma^{2} }{n} \right){\ \ }$或者$\frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n} }}\sim N(0,1)$</p></li><li><p>$\frac{(n - 1)S^{2} }{\sigma^{2} } &#x3D; \frac{1}{\sigma^{2} }\sum_{i &#x3D; 1}^{n}{ {(X_{i} - \overline{X})}^{2}\sim\chi^{2}(n - 1)}$</p></li><li><p>$\frac{1}{\sigma^{2} }\sum_{i &#x3D; 1}^{n}{ {(X_{i} - \mu)}^{2}\sim\chi^{2}(n)}$</p></li></ol><p>4)${\ \ }\frac{\overline{X} - \mu}{S&#x2F;\sqrt{n} }\sim t(n - 1)$</p><h4 id="6-4-重要公式与结论"><a href="#6-4-重要公式与结论" class="headerlink" title="6.4 重要公式与结论"></a><strong>6.4 重要公式与结论</strong></h4><p>(1) 对于$\chi^{2}\sim\chi^{2}(n)$，有$E(\chi^{2}(n)) &#x3D; n,D(\chi^{2}(n)) &#x3D; 2n;$</p><p>(2) 对于$T\sim t(n)$，有$E(T) &#x3D; 0,D(T) &#x3D; \frac{n}{n - 2}(n &gt; 2)$；</p><p>(3) 对于$F\tilde{\ }F(m,n)$，有 $\frac{1}{F}\sim F(n,m),F_{a&#x2F;2}(m,n) &#x3D; \frac{1}{F_{1 - a&#x2F;2}(n,m)};$</p><p>(4) 对于任意总体$X$，有 $E(\overline{X}) &#x3D; E(X),E(S^{2}) &#x3D; D(X),D(\overline{X}) &#x3D; \frac{D(X)}{n}$(X)}{n}$</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Mathematics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——15.PySpark案例</title>
    <link href="/2022/02/01/BigData&amp;Linux/PySpark%E6%A1%88%E4%BE%8B/"/>
    <url>/2022/02/01/BigData&amp;Linux/PySpark%E6%A1%88%E4%BE%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="一．选题"><a href="#一．选题" class="headerlink" title="一．选题"></a>一．选题</h1><h2 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h2><p>从 1934 年到 1963 年，旧金山因将一些世界上最臭名昭著的罪犯关押在无法逃脱的岛而臭名昭著恶魔 。</p><p>今天，这座城市以其科技场景而闻名，而不是它的犯罪历史。但是，随着财富不平等的加剧、住房短缺以及乘坐 BART 上班的昂贵数字玩具的激增，海湾边的城市并不缺乏犯罪。</p><p>从 Sunset 到 SOMA，从 Marina 到 Excelsior，本次比赛的数据集提供了旧金山所有社区近 12 年的犯罪报告。给定时间、地点和一定的描述，您必须预测发生的犯罪类别。</p><h2 id="1-2-主要工作"><a href="#1-2-主要工作" class="headerlink" title="1.2 主要工作"></a>1.2 主要工作</h2><p>为了解决上述提出的问题，我们从kaggle网站上San Francisco Crime Classification比赛中下载了相关数据集。从数据集介绍、导入，进行数据探索性分析和数据可视化，数据预处理，到构建多种分类预测模型，在测试集上全方位评估模型分类性能。对机器学习模型进行可解释性分析，从各个角度打破模型黑箱子，解释特征重要度，解释样本哪些特征导致对模型预测结果造成影响，从海量数据中挖掘相似性，让我们对模型充分了解、信任。</p><h1 id="二-准备数据"><a href="#二-准备数据" class="headerlink" title="二. 准备数据"></a>二. 准备数据</h1><h2 id="2-1数据集整体介绍"><a href="#2-1数据集整体介绍" class="headerlink" title="2.1数据集整体介绍"></a>2.1数据集整体介绍</h2><p>该数据集包含源自 SFPD 犯罪事件报告系统的事件。 数据范围从 1&#x2F;1&#x2F;2003 到 5&#x2F;13&#x2F;2015。 训练集和测试集每周轮换一次，即第 1、3、5、7.周属于测试集，第 2、4、6、8 周属于训练集。数据集整体介绍如下表所示：</p><table><thead><tr><th><strong>数据集</strong></th><th><strong>数据类型</strong></th><th><strong>属性数</strong></th><th><strong>实例数</strong></th><th><strong>值缺失</strong></th><th><strong>相关任务</strong></th></tr></thead><tbody><tr><td><strong>SFPD</strong></td><td>字符类型</td><td>9</td><td>878049</td><td>否</td><td>分类预测</td></tr></tbody></table><p>数据集来源：<a href="https://www.kaggle.com/c/sf-crime/data">https://www.kaggle.com/c/sf-crime/data</a></p><h2 id="2-2数据集属性介绍"><a href="#2-2数据集属性介绍" class="headerlink" title="2.2数据集属性介绍"></a>2.2数据集属性介绍</h2><p>如上表所示，该数据集总共有878049条记录数据，9个属性列，每个属性列均无缺失值，数据集中的数据类型均为字符类型，本实验主要用这个数据集进行数据分析和可视化，主要用到的机器学习中的分类算法。</p><p>本数据集中共有9个属性列，其中有8个特征列和1个标签列，关于各属性列的详细介绍如下表所示：</p><table><thead><tr><th><strong>Attribute</strong></th><th><strong>Explain</strong></th><th><strong>Type</strong></th></tr></thead><tbody><tr><td><strong>Dates</strong></td><td><strong>日期</strong></td><td><strong>字符型</strong></td></tr><tr><td><strong>Category</strong></td><td><strong>类别</strong></td><td><strong>字符型</strong></td></tr><tr><td><strong>Descript</strong></td><td><strong>描述</strong></td><td><strong>字符型</strong></td></tr><tr><td><strong>DayOfWeek</strong></td><td><strong>星期几</strong></td><td><strong>字符型</strong></td></tr><tr><td><strong>PdDistrict</strong></td><td><strong>辖区警察局</strong></td><td><strong>字符型</strong></td></tr><tr><td><strong>Resolution</strong></td><td><strong>解决方案</strong></td><td><strong>字符型</strong></td></tr><tr><td><strong>Address</strong></td><td><strong>发生地点</strong></td><td><strong>字符型</strong></td></tr><tr><td><strong>X</strong></td><td><strong>经度</strong></td><td><strong>字符型</strong></td></tr><tr><td><strong>Y</strong></td><td><strong>维度</strong></td><td><strong>字符型</strong></td></tr></tbody></table><h1 id="三．上传数据"><a href="#三．上传数据" class="headerlink" title="三．上传数据"></a>三．上传数据</h1><p>   Step1：在网站上把数据下载到本地。</p><p>   Step2：利用Xftp把本地数据集放到集群的master节点上，目录为&#x2F;bigdata&#x2F;pyspark&#x2F;data</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image008-1640092387227.png" alt="img"></p><p>   Step3：在hdfs文件中创建文件夹&#x2F;pyspark</p><table><thead><tr><th><strong># hadoop fs  -mkdir &#x2F;pyspark</strong></th></tr></thead><tbody><tr><td><strong># hadoop fs -ls  &#x2F;</strong></td></tr></tbody></table><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image010-16400923872271.png" alt="img"></p><p>Step4：上传数据到指定hdfs文件夹</p><table><thead><tr><th><strong># hadoop fs -put &#x2F;bigdata&#x2F;pyspark&#x2F;data&#x2F;train.csv  &#x2F;pyspark</strong></th></tr></thead><tbody><tr><td><strong># hadoop fs -ls &#x2F;pyspark</strong></td></tr></tbody></table><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image012-16400923872271.png" alt="img"></p><h1 id="四．数据预处理和环境准备"><a href="#四．数据预处理和环境准备" class="headerlink" title="四．数据预处理和环境准备"></a>四．数据预处理和环境准备</h1><p>集群配置简介：</p><p>三台虚拟机CentOS6.5 </p><p>Master 192.168.174.101</p><p>Node1 192.168.174.102</p><p>Node2 192.168.174.103</p><p>JDK 版本 1.8.0 安装路径 &#x2F;bigdata&#x2F;jdk1.8.0</p><p>Hadoop 版本2.6.0 安装路径 &#x2F;bigdata&#x2F;hadoop-2.6.0</p><p>Zookeeper 版本 3.4.6 安装路径 &#x2F;bigdata&#x2F;zookeeper-3.4.6</p><p>Hbase 版本 1.0.1.1 安装路径 &#x2F;bigdata&#x2F;hbase-1.0.1.1</p><p>MySQL 版本 5.1.73 安装路径 &#x2F;usr&#x2F;bin&#x2F;mysql</p><p>Hive 版本 1.2.1 安装路径 &#x2F;bigdata&#x2F;hive-1.2.1</p><p>Spark 版本 2.4.8 安装路径 &#x2F;bigdata&#x2F;spark-2.4.8</p><p>Scala 版本 2.11.8 安装路径 &#x2F;bigdata&#x2F;scala-2.11.8</p><p>Anaconda3 版本5.0.0 安装路径 &#x2F;root&#x2F;anaconda3</p><p>Python 版本3.6.2 Anaconda自带</p><p>Sqoop 版本 1.4.7 安装路径 &#x2F;bigdata&#x2F;sqoop-1.4.7</p><p>我们需要查看数据信息，并对数据经行清洗。为我们接下来的可视化分析和数据建模提供良好的数据源，方便处理。</p><p>Step1：加载数据并查看数据属性 <img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image014-16400923872271.png" alt="img"></p><p>Step2: 查看部分数据</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image016-16400923872271.png" alt="img"></p><p>Step3：查看是否存在空置或异常值</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image018-16400923872281.png" alt="img"></p><p>发现没有明显异常数据和空值。</p><p> <a href="https://gitee.com/xiang976young/note/raw/master/img/">https://gitee.com/xiang976young/note/raw/master/img/</a></p><h1 id="五．数据导入"><a href="#五．数据导入" class="headerlink" title="五．数据导入"></a>五．数据导入</h1><h2 id="5-1-创建Hive数据库和数据表"><a href="#5-1-创建Hive数据库和数据表" class="headerlink" title="5.1 创建Hive数据库和数据表"></a>5.1 创建Hive数据库和数据表</h2><p>进入hive shell环境，创建crime数据库和log外部表（外部表可以在修改表数据的同时不伤害本身原始数据，为数据做保护），并载入hdfs中的数据。</p><table><thead><tr><th><strong>hive&gt; create database crime;</strong></th></tr></thead><tbody><tr><td><strong>hive&gt; use crime;</strong></td></tr><tr><td><strong>hive&gt; create external table log( Dates string,  Category string, Descript String, PdDistrict string, Resolution string,  Address String, X string, Y string</strong>  <strong>)row format delimited fields  terminate by ‘,’</strong>  <strong>stored as  textfile location ‘&#x2F;spark’;</strong></td></tr><tr><td><strong>hive&gt; select * from log limit 10;</strong></td></tr></tbody></table><p>(数据一开始放在&#x2F;spark文件夹下，后来有创建了新的&#x2F;pyspark  数据一致)</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image020.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image022.png" alt="img"></p><p>为外部表log创建内部表inner_log</p><table><thead><tr><th><strong>hive&gt; create table inner_log( Dates string,  Category string, Descript String, DayOfWeek string ,PdDistrict string,  Resolution string, Address String, X string, Y string)</strong>   <strong>comment  ‘Welcome to XMU dblab! Now create inner table inner_log ‘</strong>   <strong>row format delimited fields  terminate by ‘,’</strong>  <strong>stored as  textfile;</strong></th></tr></thead><tbody><tr><td><strong>hive&gt; select * from log limit 10;</strong></td></tr></tbody></table><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image024.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image026.png" alt="img"></p><h2 id="5-2-创建MySQL数据库和数据表"><a href="#5-2-创建MySQL数据库和数据表" class="headerlink" title="5.2 创建MySQL数据库和数据表"></a>5.2 创建MySQL数据库和数据表</h2><p>因为sqoop在抓取数据的时候，会把所有的数据都抓成string类型，所以在创建表的时候，要注意属性的类型。同时也要查看当前数据库下是否支持utf-8编码。以免出现中文乱码的情况。</p><table><thead><tr><th><strong>mysql&gt; create database crime;</strong></th></tr></thead><tbody><tr><td><strong>mysql &gt; use crime;</strong></td></tr><tr><td><strong>mysql &gt; show variables like “char%”;</strong></td></tr><tr><td><strong>mysql  &gt;  CREATE TABLE <code>crime</code>.<code>log</code> (<code>Dates </code> varchar(255),<code> Category</code> varchar(255),<code>Descript</code> varchar(255),<code>DayOfWeek</code> varchar(20),<code> PdDistrict</code> varchar(255), <code>Resolution</code> varchar(255),<code>Address</code> varchar(255),<code>X</code>  varchar(255),<code>Y</code> varchar(255)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;</strong></td></tr><tr><td><strong>mysql &gt; select * from log limit 10;<img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image028.png" alt="img"></strong></td></tr></tbody></table><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image030.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image032.png" alt="img"></p><h2 id="5-3-利用Sqoop从Hive中导出数据到MySQL"><a href="#5-3-利用Sqoop从Hive中导出数据到MySQL" class="headerlink" title="5.3 利用Sqoop从Hive中导出数据到MySQL"></a>5.3 利用Sqoop从Hive中导出数据到MySQL</h2><p>在利用sqoop处理数据的时候要注意，如果hive表中存在属性为数值行的none值，要注意切换为字符NULL，不然会报错。</p><table><thead><tr><th><strong># sqoop export –connect  jdbc:mysql:&#x2F;&#x2F;192.168.174.101:3306&#x2F;crime –username root –password 123456  –table log –export-dir ‘&#x2F;user&#x2F;hive&#x2F;wavehouse&#x2F;crime.db&#x2F;log’ –fields-terminated-by  ‘,’ -m 1;</strong></th></tr></thead><tbody><tr><td><strong>mysql &gt; select * from crime.log limit 10;</strong></td></tr></tbody></table><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image034.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image036.png" alt="img"></p><h2 id="5-4-利用Sqoop从MySQL中导出数据到Hbase"><a href="#5-4-利用Sqoop从MySQL中导出数据到Hbase" class="headerlink" title="5.4 利用Sqoop从MySQL中导出数据到Hbase"></a>5.4 利用Sqoop从MySQL中导出数据到Hbase</h2><p>由于该数据集并没有主键的存在，并不能很好的标记各个记录。所以这里需要给mysql表添加一个id的主键，以标记各个记录。</p><table><thead><tr><th><code>**mysql &gt; alter table log add id int**</code></th></tr></thead><tbody><tr><td><strong>mysql &gt; alter  table <code>node_table</code> change id id int not null auto_increment primary key;</strong>  <strong>mysql &gt;  select * from log limit 10;</strong></td></tr></tbody></table><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image038.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image040.png" alt="img"></p><p>创建hbase表：</p><table><thead><tr><th><code>**hbase(main):003:0&gt; create ”log”,”info”**</code></th></tr></thead><tbody><tr><td><code>**hbase(main):004:0&gt; list**</code></td></tr></tbody></table><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image042.png" alt="img"></p><p>利用sqoop从mysql导入数据到hbase：</p><table><thead><tr><th><code>**# sqoop import --connect jdbc:mysql://192.168.174.101:3306/crime --username root --password 123456 --table log --columns &quot;dates,category,descript,dayofweek,pddistrict,resolution,address,x,y,id&quot; --column-family &quot;info&quot; --hbase-create-table --hbase-table &quot;log&quot; --hbase-row-key &quot;id&quot; --num-mappers 1 --split-by id**</code></th></tr></thead><tbody><tr><td><code>**hbase(main):001:0&gt; scan ‘log’**</code></td></tr></tbody></table><p>(scan 查询数目太多，没有办法截下来)</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image044.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image046.png" alt="img"></p><h2 id="5-5-Hive与Hbase的数据映射"><a href="#5-5-Hive与Hbase的数据映射" class="headerlink" title="5.5  Hive与Hbase的数据映射"></a>5.5  Hive与Hbase的数据映射</h2><p>将HBase数据仓库中log数据映射到Hive数据库的外部表hive_log中</p><table><thead><tr><th><code>**hive&gt; create external table hive_log(Dates string, Category string, Descript String, DayOfWeek string ,PdDistrict string, Resolution string, Address String, X string, Y string,id int )STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;info:dates,info:category,info:descript,info:dayofweek,info:pddistrict,info:resolution,info:address,info:x,info:y,:key&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;log&quot;);**</code></th></tr></thead><tbody><tr><td><code>**hive&gt; show tables;**</code></td></tr><tr><td><code>**hive&gt; desc hive_log;**</code></td></tr></tbody></table><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image048.png" alt="img"></p><p>至此，数据仓库、两个数据库之间的数据已经全部加载完成。</p><h1 id="六．Hive数据分析"><a href="#六．Hive数据分析" class="headerlink" title="六．Hive数据分析"></a>六．Hive数据分析</h1><p>6.1 分析犯罪类型发生的次数最多的10个</p><table><thead><tr><th>hive&gt; select category,count(*) as category_count from  log group by category order by category_count desc limit 10;</th></tr></thead><tbody><tr><td><strong><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image050.png" alt="img"></strong></td></tr></tbody></table><p>6.2 分析发生犯罪最多的20个街区</p><table><thead><tr><th>hive&gt; select pddistrict,count(*) as pddistrict_count  from log group by pddistrict order by pddistrict_count desc limit 20;</th></tr></thead><tbody><tr><td><strong><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image052.png" alt="img"></strong></td></tr></tbody></table><p>6.3 分析每个星期内发证犯罪的情况</p><table><thead><tr><th>hive&gt; select dayofweek,count(*) from log group by dayofweek  ;</th></tr></thead><tbody><tr><td><strong><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image054.png" alt="img"></strong></td></tr></tbody></table><h1 id="七．Spark数据分析与可视化"><a href="#七．Spark数据分析与可视化" class="headerlink" title="七．Spark数据分析与可视化"></a>七．Spark数据分析与可视化</h1><p>这里的Spark的编程语言选择的Python，运行的平台是Jupyter</p><h2 id="7-1-Spark数据可视化"><a href="#7-1-Spark数据可视化" class="headerlink" title="7.1 Spark数据可视化"></a>7.1 Spark数据可视化</h2><h3 id="7-1-1-各犯罪类型数量统计"><a href="#7-1-1-各犯罪类型数量统计" class="headerlink" title="7.1.1 各犯罪类型数量统计"></a>7.1.1 各犯罪类型数量统计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.rcParams[<span class="hljs-string">&#x27;figure.figsize&#x27;</span>]=(<span class="hljs-number">20</span>,<span class="hljs-number">9</span>) plt.style.use(<span class="hljs-string">&#x27;dark_background&#x27;</span>) sns.countplot(dataset[<span class="hljs-string">&#x27;Category&#x27;</span>],palette=<span class="hljs-string">&#x27;gnuplot&#x27;</span>) plt.title(<span class="hljs-string">&#x27;Major Crimes in  Sanfrancisco&#x27;</span>,fontweight=<span class="hljs-number">30</span>,fontsize=<span class="hljs-number">20</span>) plt.xticks(rotation=<span class="hljs-number">90</span>)  plt.show() <br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image056.png" alt="img"></p><p>由图可以明显的看出来，在SanFrancisco LARCENY&#x2F;THEFT和OTHER OFFENSES这两类犯罪是最多的</p><h3 id="7-1-2-各个街区的犯罪对比示意图"><a href="#7-1-2-各个街区的犯罪对比示意图" class="headerlink" title="7.1.2 各个街区的犯罪对比示意图"></a>7.1.2 各个街区的犯罪对比示意图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">df =  pd.crosstab(dataset[<span class="hljs-string">&#x27;Category&#x27;</span>],dataset[<span class="hljs-string">&#x27;PdDistrict&#x27;</span>])<br>color =  plt.cm.Greys(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>))<br>df.div(df.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>).astype(<span class="hljs-built_in">float</span>),  axis = <span class="hljs-number">0</span>).plot.bar(stacked = <span class="hljs-literal">True</span>, color = color, figsize = (<span class="hljs-number">18</span>, <span class="hljs-number">12</span>))<br>plt.title(<span class="hljs-string">&#x27;District vs Category of  Crime&#x27;</span>, fontweight = <span class="hljs-number">30</span>, fontsize = <span class="hljs-number">20</span>)<br>plt.xticks(rotation = <span class="hljs-number">90</span>)<br>plt.show()  <br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image058.png" alt="img"></p><p>有图可以观察到，地区之间差异还是挺大的，southern地区犯罪率较高，治安最好的是Richmond</p><h3 id="7-1-3-按时间对犯罪进行划分并分析"><a href="#7-1-3-按时间对犯罪进行划分并分析" class="headerlink" title="7.1.3 按时间对犯罪进行划分并分析"></a>7.1.3 按时间对犯罪进行划分并分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset[<span class="hljs-string">&#x27;year&#x27;</span>] = dataset[<span class="hljs-string">&#x27;Dates&#x27;</span>].dt.year  <br>dataset[<span class="hljs-string">&#x27;month&#x27;</span>] = dataset[<span class="hljs-string">&#x27;Dates&#x27;</span>].dt.month  <br>dataset[<span class="hljs-string">&#x27;day&#x27;</span>] = dataset[<span class="hljs-string">&#x27;Dates&#x27;</span>].dt.day  dataset[<span class="hljs-string">&#x27;hour&#x27;</span>] = dataset[<span class="hljs-string">&#x27;Dates&#x27;</span>].dt.hour  <br>plt.figure(figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">19</span>))  <br>plt.style.use(<span class="hljs-string">&#x27;fivethirtyeight&#x27;</span>)     <br>year_group = dataset.groupby(<span class="hljs-string">&#x27;year&#x27;</span>).size() <br>plt.subplot(<span class="hljs-number">311</span>) <br>plt.plot(year_group.index[:-<span class="hljs-number">1</span>],year_group[:-<span class="hljs-number">1</span>],<span class="hljs-string">&#x27;ks-&#x27;</span>) plt.xlabel(<span class="hljs-string">&#x27;year&#x27;</span>)  <br>month_group = dataset.groupby(<span class="hljs-string">&#x27;month&#x27;</span>).size()<br>plt.subplot(<span class="hljs-number">312</span>)  <br>plt.plot(month_group,<span class="hljs-string">&#x27;ks-&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;month&#x27;</span>)  <br>day_group = dataset.groupby(<span class="hljs-string">&#x27;day&#x27;</span>).size()  <br>plt.subplot(<span class="hljs-number">313</span>)  <br>plt.plot(day_group,<span class="hljs-string">&#x27;ks-&#x27;</span>)  <br>plt.xlabel(<span class="hljs-string">&#x27;day&#x27;</span>)  <br>plt.show()  <br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image060.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image064.png" alt="img"></p><p>从上图可知，在2011年前SF的犯罪数基本上呈递减趋势，2011后数量激增，案件高发期是在一年中的5月和10月，在每个月的月初会有涨幅。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">week_group =  dataset.groupby([<span class="hljs-string">&#x27;DayOfWeek&#x27;</span>,<span class="hljs-string">&#x27;hour&#x27;</span>]).size()<br><span class="hljs-comment">#多重分组  </span><br>week_group = week_group.unstack()<br><span class="hljs-comment">#对分组后的多重索引转为xy索引    </span><br>week_group.T.plot(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))<span class="hljs-comment">#行列互换后画</span><br>plt.xlabel(<span class="hljs-string">&#x27;hour of day&#x27;</span>,size=<span class="hljs-number">15</span>)  <br>plt.ylabel(<span class="hljs-string">&#x27;Number of crimes&#x27;</span>,size=<span class="hljs-number">15</span>)  <br>plt.show()  <br></code></pre></td></tr></table></figure><p>·<img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image066.png" alt="img"></p><p>可以看出，案件高发时间是在12点和18点左右，凌晨后数量会显著减少，在周五周六的晚上8点后案件发生率会比平时要高。</p><h3 id="7-1-4-高发案件的时间结合地点的分析"><a href="#7-1-4-高发案件的时间结合地点的分析" class="headerlink" title="7.1.4 高发案件的时间结合地点的分析"></a>7.1.4 高发案件的时间结合地点的分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">cate_group = dataset.groupby(by=<span class="hljs-string">&#x27;Category&#x27;</span>).size()  <br>top6 = <span class="hljs-built_in">list</span>(cate_group.index[:<span class="hljs-number">6</span>])  <br>tmp = dataset[dataset[<span class="hljs-string">&#x27;Category&#x27;</span>].isin(top6)]  <br>tmp_group = tmp.groupby([<span class="hljs-string">&#x27;Category&#x27;</span>,<span class="hljs-string">&#x27;hour&#x27;</span>]).size()<br>tmp_group = tmp_group.unstack()  <br>tmp_group.T.plot(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">6</span>),style=<span class="hljs-string">&#x27;o-&#x27;</span>)  <br>plt.show()  <br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image068.png" alt="img"></p><p>时间上与上述分析是一致的，对于偷盗类案件在12、18点发生率更高；assault类案件在晚上6点后没有下降趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tmp2 = tmp.groupby([<span class="hljs-string">&#x27;Category&#x27;</span>,<span class="hljs-string">&#x27;PdDistrict&#x27;</span>]).size() tmp2.unstack().T.plot(kind=<span class="hljs-string">&#x27;bar&#x27;</span>,figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">6</span>),rot=<span class="hljs-number">45</span>) plt.show()  <br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image070.png" alt="img"></p><p>从上图可知，犯罪率最高的Southern地区，偷窃类、暴力冲突类案件数量最多，车辆失窃类案件较少，猜测可能属于贫困地区，治安很好的地区Park,Richmond中，毒品、人身攻击类案件比例明显较少.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">tmp3 = tmp.groupby([<span class="hljs-string">&#x27;Category&#x27;</span>,<span class="hljs-string">&#x27;DayOfWeek&#x27;</span>]).size()  <br>tmp3 = tmp3.unstack()  <br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>):      <br>    tmp3.iloc[i] = tmp3.iloc[i]/tmp3.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)[i] <span class="hljs-built_in">print</span>(tmp3)  <br>tmp3.T.plot(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">6</span>),style=<span class="hljs-string">&#x27;o-&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;weekday&quot;</span>,size=<span class="hljs-number">20</span>)  <br><span class="hljs-comment">#plt.axes.set_xticks([])  </span><br>plt.xticks([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-string">&#x27;Mon&#x27;</span>,<span class="hljs-string">&#x27;Tue&#x27;</span>,<span class="hljs-string">&#x27;Wed&#x27;</span>,<span class="hljs-string">&#x27;Thur&#x27;</span>,<span class="hljs-string">&#x27;Fri&#x27;</span>,<span class="hljs-string">&#x27;Sat&#x27;</span>,<span class="hljs-string">&#x27;Sun&#x27;</span>])  <br>plt.show()  <br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image072.png" alt="img"></p><p>趋势不太一样的是ＢＡＤ　ＣＨＥＣＫ类案件，在周三发生最多，周末有急剧下降的趋势；其余多数案件，除了other offenses外，都在周五周六有所增多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">mon_g = tmp.groupby([<span class="hljs-string">&#x27;Category&#x27;</span>,<span class="hljs-string">&#x27;month&#x27;</span>]).size()  <br>mon_g = mon_g.unstack()  <br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>):      <br>    mon_g.iloc[i] = mon_g.iloc[i]/mon_g.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)[i] mon_g.T.plot(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">6</span>),style=<span class="hljs-string">&#x27;o-&#x27;</span>)  <br>plt.show()  <br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image074.png" alt="img"></p><p>分类变化趋势与总体基本一致，2-6月和8-12月是案件高发期，1-2月badcheck案发率较低。</p><h3 id="7-1-5-高发案件的时间趋势分析"><a href="#7-1-5-高发案件的时间趋势分析" class="headerlink" title="7.1.5 高发案件的时间趋势分析"></a>7.1.5 高发案件的时间趋势分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">ddf =  tmp.groupby([<span class="hljs-string">&#x27;Category&#x27;</span>,pd.Grouper(<span class="hljs-string">&#x27;Dates&#x27;</span>)]).size() ddf = ddf.unstack().fillna(<span class="hljs-number">0</span>)  <br>ddf = ddf.T  <br>df2 = ddf.resample(<span class="hljs-string">&#x27;m&#x27;</span>,how=<span class="hljs-string">&#x27;sum&#x27;</span>)<span class="hljs-comment">#按月求和</span><br>plt.style.use(<span class="hljs-string">&#x27;dark_background&#x27;</span>)  <br>moav = df2.rolling(<span class="hljs-number">12</span>).mean()<span class="hljs-comment">#每12个月统计平均，相当于加了个窗  </span><br>i = <span class="hljs-number">1</span>  <br><span class="hljs-keyword">for</span> cat <span class="hljs-keyword">in</span> df2.columns:      <br>    plt.figure(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">15</span>))    <br>    ax =  plt.subplot(<span class="hljs-number">6</span>,<span class="hljs-number">1</span>,i)<br>    plt.plot(df2.index,df2[cat])<br>    plt.plot(df2.index,moav[cat])<br>    plt.title(cat)<br>    i+=<span class="hljs-number">1</span><br>df2.plot()  <br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image076.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image078.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image080.png" alt="img"> <img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image082-1640092446805.png" alt="img"> <img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image084-1640092449858.png" alt="img"> <img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image086-1640092452099.png" alt="img"> <img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image088-1640092454204.png" alt="img"></p><p>可见，不同种类的案件随时间是有不同变化的，如assault在15年后急剧下降，可能有专项整治等活动。</p><h2 id="7-2-Spark数据分析"><a href="#7-2-Spark数据分析" class="headerlink" title="7.2 Spark数据分析"></a>7.2 Spark数据分析</h2><h3 id="7-2-1-Spark-Mlib介绍"><a href="#7-2-1-Spark-Mlib介绍" class="headerlink" title="7.2.1 Spark.Mlib介绍"></a>7.2.1 Spark.Mlib介绍</h3><p>Spark数据分析建模主要利用的是Spark.Mlib下的机器学习进行分类、预测、回归问题。Mlib其实就是将数据以RDD的形式进行表示，在分布式数据集上调用各种算法。</p><p><strong>Mlib</strong>的使用方法：</p><p>MLlib中包含能够在集群上运行良好的并行算法，如kmeans、分布式RF、交替最小二乘等，这能够让MLib中的每个算法都能够适用于大规模数据集</p><p>也可以将同一算法的不同参数列表通过parallelize()，在不同节点上运行，最终找到性能最好的一组参数，这可以节省小规模数据集上参数选择的时间。</p><p>  <strong>Mlib</strong>中的数据类型：</p><p>  Vector：在mllib.linalg.vectors中，既支持稠密向量，也支持稀疏向量</p><p>  LabeledPoint：在mllib.regression中，用于监督学习算法中，表示带有标签的数据点</p><p>  Rating：在mllib.recommendation中，用于产品推荐，表示用户对一个产品的打分</p><p>各种Label类：每个Model都是训练算法的结果，可以用train进行训练，用predict进行预测</p><h3 id="7-2-2-基于Describe属性分词处理的数据分析"><a href="#7-2-2-基于Describe属性分词处理的数据分析" class="headerlink" title="7.2.2 基于Describe属性分词处理的数据分析"></a>7.2.2 基于Describe属性分词处理的数据分析</h3><p>流程和scikit-learn版本的很相似，包含3个步骤：</p><p>1.regexTokenizer: 利用正则切分单词</p><p>2.stopwordsRemover: 移除停用词</p><p>3.countVectors: 构建词频向量</p><p>RegexTokenizer：基于正则的方式进行文档切分成单词组 inputCol: 输入字段；outputCol: 输出字段；pattern： 匹配模式，根据匹配到的内容切分单词。</p><p>CountVectorizer：构建词频向量；covabSize: 限制的词频数；minDF：如果是float，则表示出现的百分比小于minDF,不会被当做关键词</p><p>StringIndexer将一列字符串label编码为一列索引号，根据label出现的频率排序，最频繁出现的label的index为0</p><p>该例子中，label会被编码成从0-32的整数，最频繁的label被编码成0</p><p>Pipeline是基于DataFrame的高层API，可以方便用户构建和调试机器学习流水线，可以使得多个机器学习算法顺序执行，达到高效的数据处理的目的。</p><table><thead><tr><th><strong>以词频作为特征，利用逻辑回归进行分类</strong>    LR用于监督式分类问题，可以使用SGD等方法对LR进行训练，    clearThreshold之后，LR会输出原始概率，也可以设置概率阈值，直接输出分类结果</th></tr></thead><tbody><tr><td>drop_list  &#x3D; [‘Dates’, ‘DayOfWeek’, ‘PdDistrict’, ‘Resolution’, ‘Address’, ‘X’, ‘Y’]  data &#x3D;  data.select([column for column in data.columns if column not in drop_list])  from pyspark.ml.feature import RegexTokenizer,  StopWordsRemover, CountVectorizer  from pyspark.ml.classification import  LogisticRegression  # regular expression tokenizer  regexTokenizer &#x3D;  RegexTokenizer(inputCol&#x3D;”Descript”, outputCol&#x3D;”words”,  pattern&#x3D;”\W”)  # stop words  add_stopwords &#x3D;  [“http”,”https”,”amp”,”rt”,”t”,”c”,”the”]    stopwordsRemover &#x3D;  StopWordsRemover(inputCol&#x3D;”words”,  outputCol&#x3D;”filtered”).setStopWords(add_stopwords)  # bag of words count  countVectors &#x3D;  CountVectorizer(inputCol&#x3D;”filtered”,  outputCol&#x3D;”features”,vocabSize&#x3D;10000, minDF&#x3D;5)</td></tr><tr><td>label_stringIdx &#x3D; StringIndexer(inputCol &#x3D; “Category”,  outputCol &#x3D; “label”)  pipeline &#x3D; Pipeline(stages&#x3D;[regexTokenizer, stopwordsRemover,  countVectors, label_stringIdx])  # Fit the pipeline to training documents.  pipelineFit &#x3D; pipeline.fit(data)  dataset &#x3D; pipelineFit.transform(data)  (trainingData, testData) &#x3D; dataset.randomSplit([0.7, 0.3], seed &#x3D; 100)     lr &#x3D; LogisticRegression(maxIter&#x3D;20, regParam&#x3D;0.3, elasticNetParam&#x3D;0)  lrModel &#x3D; lr.fit(trainingData)  predictions &#x3D; lrModel.transform(testData)  predictions.filter(predictions[‘prediction’] &#x3D;&#x3D; 0) \      .select(“Descript”,”Category”,”probability”,”label”,”prediction”)  \      .orderBy(“probability”, ascending&#x3D;False) \    .show(n &#x3D; 10, truncate &#x3D; 30)  evaluator &#x3D;  MulticlassClassificationEvaluator(predictionCol&#x3D;”prediction”)  evaluator.evaluate(predictions)</td></tr><tr><td><strong><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image090.png" alt="img"></strong></td></tr><tr><td><strong>以TF-ID作为特征，利用逻辑回归进行分类</strong>  TFIDF是一种从文本文档生成特征向量的简单方法，文档中的词有2个统计值：TF与IDF，TF指的是每个词咋文档中出现的次数，IDF用于衡量一个词在整个文档语料库中出现的(逆)频繁程度  HashingTF用于计算TF，IDF用于IDF，hashingTF用的是哈希的方法，生成稀疏向量</td></tr><tr><td>from pyspark.ml.feature import HashingTF, IDF  hashingTF &#x3D; HashingTF(inputCol&#x3D;”filtered”,  outputCol&#x3D;”rawFeatures”, numFeatures&#x3D;10000)  idf &#x3D; IDF(inputCol&#x3D;”rawFeatures”,  outputCol&#x3D;”features”, minDocFreq&#x3D;5)   #minDocFreq: remove sparse terms  pipeline &#x3D; Pipeline(stages&#x3D;[regexTokenizer,  stopwordsRemover, hashingTF, idf, label_stringIdx])  pipelineFit &#x3D; pipeline.fit(data)  dataset &#x3D; pipelineFit.transform(data)     lr &#x3D; LogisticRegression(maxIter&#x3D;20, regParam&#x3D;0.3,  elasticNetParam&#x3D;0)  lrModel &#x3D; lr.fit(trainingData)  predictions &#x3D; lrModel.transform(testData)  predictions.filter(predictions[‘prediction’] &#x3D;&#x3D; 0) \      .select(“Descript”,”Category”,”probability”,”label”,”prediction”)  \      .orderBy(“probability”, ascending&#x3D;False) \    .show(n &#x3D; 10,  truncate &#x3D; 30)      evaluator &#x3D;  MulticlassClassificationEvaluator(predictionCol&#x3D;”prediction”)  evaluator.evaluate(predictions)</td></tr><tr><td><strong><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image092.png" alt="img"></strong></td></tr><tr><td><strong>交叉验证</strong>  用交叉验证来优化参数，这里针对基于词频特征的逻辑回归模型进行优化</td></tr><tr><td>#交叉验证  from pyspark.ml import Pipeline  pipelinec &#x3D; Pipeline(stages&#x3D;[regexTokenizer,  stopwordsRemover, countVectors, label_stringIdx])  pipelineFit &#x3D; pipelinec.fit(data)  dataset &#x3D; pipelineFit.transform(data)  (trainingData, testData) &#x3D; dataset.randomSplit([0.7,  0.3], seed &#x3D; 100)  lr &#x3D; LogisticRegression(maxIter&#x3D;20, regParam&#x3D;0.3,  elasticNetParam&#x3D;0)  from pyspark.ml.tuning import ParamGridBuilder,  CrossValidator  # Create ParamGrid for Cross Validation  paramGrid &#x3D; (ParamGridBuilder()          .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter          .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2])              # Elastic Net Parameter (Ridge &#x3D; 0)  #        .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations  #        .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features          .build())  from pyspark.ml.evaluation import  MulticlassClassificationEvaluator  evaluator &#x3D;  MulticlassClassificationEvaluator(predictionCol&#x3D;”prediction”)  # Create 5-fold CrossValidator  cv &#x3D; CrossValidator(estimator&#x3D;lr, \              estimatorParamMaps&#x3D;paramGrid, \              evaluator&#x3D;evaluator, \              numFolds&#x3D;5)  cvModel &#x3D; cv.fit(trainingData)  predictions &#x3D; cvModel.transform(testData)  # Evaluate best model  evaluator.evaluate(predictions)</td></tr><tr><td><strong><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image093.png" alt="img"></strong></td></tr><tr><td><strong>朴素贝叶斯</strong></td></tr><tr><td>from pyspark.ml.classification import NaiveBayes  nb &#x3D; NaiveBayes(smoothing&#x3D;1)  model &#x3D; nb.fit(trainingData)  predictions &#x3D; model.transform(testData)  predictions.filter(predictions[‘prediction’] &#x3D;&#x3D; 0) \      .select(“Descript”,”Category”,”probability”,”label”,”prediction”)  \      .orderBy(“probability”, ascending&#x3D;False) \    .show(n &#x3D; 10,  truncate &#x3D; 30)     from pyspark.ml.evaluation import  MulticlassClassificationEvaluator  evaluator &#x3D;  MulticlassClassificationEvaluator(predictionCol&#x3D;”prediction”)  evaluator.evaluate(predictions)</td></tr><tr><td><strong><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image095.png" alt="img"></strong></td></tr><tr><td><strong>随机森林</strong>  参数解释：  ata：由LabeledPoint组成的rdd  numClasses：分类任务时，有该参数，表示类别数量  impurity：节点的不纯度测量，对于分类可以使用gini系数或者信息熵，对回归只能是varainace  maxDepth：数的最大深度，默认为5。  maxBins：在构建各节点时，将数据分到多少个箱子中  cateoricalFeaturesInfo：指定哪些特征是用于分类的，以及有多少个分类。  numTrees，即决策树的个数。  * featureSubsetStrategy：在每个节点上做决定时所考虑的特征的数量，可以是auto、all、sqrt、log2、onethird等，数目越大，计算的代价越大。  * seed：采用的随机数种子</td></tr><tr><td>from pyspark.ml.classification import  RandomForestClassifier  rf &#x3D; RandomForestClassifier(labelCol&#x3D;”label”,  \                  featuresCol&#x3D;”features”, \                numTrees &#x3D; 100, \                maxDepth &#x3D; 4, \                maxBins &#x3D; 32)  # Train model with Training Data  rfModel &#x3D; rf.fit(trainingData)  predictions &#x3D; rfModel.transform(testData)  predictions.filter(predictions[‘prediction’] &#x3D;&#x3D; 0) \      .select(“Descript”,”Category”,”probability”,”label”,”prediction”)  \      .orderBy(“probability”, ascending&#x3D;False) \    .show(n &#x3D; 10,  truncate &#x3D; 30)      evaluator &#x3D; MulticlassClassificationEvaluator(predictionCol&#x3D;”prediction”)  evaluator.evaluate(predictions)</td></tr><tr><td><strong><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image097.png" alt="img"></strong></td></tr></tbody></table><p>上面的结果可以看出：朴素贝叶斯是优秀的、鲁棒的通用模型。同样选择使用交叉验证的逻辑回归也是个明智的选择。但是选择交叉验证的逻辑回归时需要注意一点：由于使用了交叉验证，训练时间会过长，在实际的应用场景中要根据业务选择最合适的模型。</p><h1 id="八．问题"><a href="#八．问题" class="headerlink" title="八．问题"></a>八．问题</h1><ol><li><p>Question:Hbase出现ERROR: Can’t get master address from ZooKeeper; znode data &#x3D;&#x3D; null解决办法</p><p>Reason: 出现此问题可能是zookeeper不稳定造成的，采用的是虚拟机，经常挂起的状态，使用hbase的list命令出现下面错误，这个可能是hbase的稳定性造成的。或者是运行hbase(zookeeper)的用户无法写入zookeeper文件，导致znode data为空。</p><p>Solution：方法一：重启hbase。Stop-hbase.sh  start-hbase.sh</p><p>​        方法二：修改hbase-site.xml中的hbase.zookeeper.property.dataDir文件夹的权限，并注意rootdir中的IP设定很重要，需要设定对应的IP与hadoop下的core-site.xml中fs.defaultFS中的路径不相同，后重启hbase、zookeeper</p></li><li><p>Question：在使用sqoop进行一系列数据传输的小问题：</p><p>2.1：ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException:</p><p>2.2：ERROR mapreduce.ExportJobBase: Export job failed!</p><p>​ERROR tool.ExportTool: Error during export: </p><p>​Export job failed!</p><p>Solution：2.1 这个是由于mysql-connector-java的bug造成的，出错时我用的是mysql-connector-java-5.1.10-bin.jar，更新成mysql-connector-java-5.1.32-bin.jar就可以了。同时也要注意语句中ip的拼写</p><p>2.2 第一步：完整的检查mysql和hive表的结构(字段名称和数据类型)是否一致。</p><p>第二步：查看数据有没有导入？如果数据没有导入，请在检查第一步操作，可以将mysql和hive中的时间类型都改成string或varchar类型试一下。如果有导入，但是导入的数据不全或者不对。说明肯定是你的数据类型和实际的数据不一致。下面分两种情况。</p><p>第三步：在mysql到hive中，请务必检查你的数据中是否包含hive建表默认的换行符 \n (LINES TERMINATED BY ‘\n’)。 </p><p>第四步：在mysql到hive中，请务必检查你的数据中是否包含hive建表常用的字段分隔符 \t(FIELDS TERMINATED BY ‘\t’)。如果有，则sqoop语句中 fields-terminated-by 参数不能用 \t</p></li><li><p>Question：Input path does not exist:hdfs:&#x2F;&#x2F;Master:9000&#x2F;user&#x2F;hive&#x2F;wavehouse&#x2F;crime.db&#x2F;log</p><p>Reason：其实很简单，log是外部表，在对应hive数据库的存放地址内是访问不到的</p><p>Solution：需要建一个内部表复制一下外部表就可以访问到了</p></li><li><p>Question：在挂起回复以后，hbsae的子节点上的进程HRegionServer挂掉。</p><p>Reason：注意查看log日志报告的具体错误信息。ZooKeeper delete failed after 4 attempts，zookeeper对于Hmaster的选举产生了阻碍效果。Solution：</p><p>首先关闭Hbase、Zookeeper。查看Hbase自带zookeeper是否占用2181端口。</p><p>启动zookeeper</p><ul><li>直接启动客户端脚本 zkCli.sh</li><li>查看Zookeeper节点信息 ls &#x2F;</li><li>递归删除Hbase节点 rmr &#x2F;hbase</li><li>退出客户端 quit</li><li>重启Zookeeper服务  zkServer.sh restart</li></ul><p>当然也有可能是时间不一致导致的，注意同步各节点的时间</p><p>再启动Hbase即可。</p></li></ol><h1 id="九．小结"><a href="#九．小结" class="headerlink" title="九．小结"></a>九．小结</h1><p>本次大作业的设计尽可能地运用到了本课程中所有的学习部分：Hadoop分布式处理框架、Hdfs分布式文件系统、Hbase分布式列存储数据库、Hive数据仓库、Zookeeper分布式协作服务、Spark基于内存的分布式计算框架、Sqoop数据同步工具。当作是我对本次课程的一次检测。当然还存在很多基础的框架应用等待我们学习如Kafka分布式消息队列、Flume日志收集工具Strom、Flink分布式流计算框架、Oozie工作流调度器等。大数据的学习永无止境！</p><p>本课设采用的数据集是旧金山的犯罪数据。虽然不是国内数据，时效也不太好，但是构成了一个完整的处理数据的流程。数据清洗，数据分析，数据建模，数据预测。当然在由于hive语句没有python熟练的缘故，Hql调用的比较少，只能简单查看一些。高级一些的数据属性之间的关联仍然是较少的。但好在在python可视化的时候，利用python做了一些数据的处理，也呈现出一些良好的效果，观察到一些有用的走势和波动。同样在数据建模的时候，运用的比较简单，对于数据降维，多属性数据的特征提取这类方法运用的不深。单纯的通过单一的模型来处理效果并不是很理想，需要我进一步利用网络对贝叶斯处理好的数据进行深层次的辨析。也许会得到更好的效果。当然在kaggle上也有很多大神在这个竞赛中提交了相当有趣的代码，例如xgboost和lightgbm，这值得我去学习，尝试着接下里来这些代码部署到spark中实现。</p><p>本次作业的完成，预示一段学习的结束，是对我这段时间学习的评价。同样也为我揭开了下一步学习的序幕。在实现各个过程中的细节和不足都成我继续前进的动力，对于更好更便捷的框架更吸引着我。��</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>PySpark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——1.HADOOP相关知识</title>
    <link href="/2022/01/01/BigData&amp;Linux/1.Hadoop%E7%9B%B8%E5%85%B3/"/>
    <url>/2022/01/01/BigData&amp;Linux/1.Hadoop%E7%9B%B8%E5%85%B3/</url>
    
    <content type="html"><![CDATA[<h1 id="HADOOP-相关知识"><a href="#HADOOP-相关知识" class="headerlink" title="HADOOP 相关知识"></a>HADOOP 相关知识</h1><h2 id="Hadoop的优势"><a href="#Hadoop的优势" class="headerlink" title="Hadoop的优势"></a>Hadoop的优势</h2><p>⒈高可靠性。Hadoop按位存储和处理数据的能力值得人们信赖。<br>⒉高扩展性。Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。<br>⒊高效性。Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。<br>⒋高容错性。Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。</p><h2 id="Hadoop启动后能看到哪些进程"><a href="#Hadoop启动后能看到哪些进程" class="headerlink" title="Hadoop启动后能看到哪些进程"></a>Hadoop启动后能看到哪些进程</h2><p>可以看到主节点上有三个进程 : Namenode、Secondarnamenode、Resourcemanager<br>从节点上有两个进程 : Datanode和Nodenodemanager</p><ul><li>Namenode：它是Hadoop 中的主服务器，管理文件系统名称空间和对集群中存储的文件的访问。</li><li>Secondarnamenode：它不是 namenode 的冗余守护进程，而是提供周期检查点和清理任务。用来监控HDFS状态的辅助后台程序</li><li>Resourcemanager: YARN中，ResourceManager负责集群中所有资源的统一管理和分配，它接收来自各个节点的资源汇报信息，并把这些信息按照一定的策略分配给各个应用程序与每个节点的管理者和每个应用开发者一起工作。</li><li>Datanode：负责管理它所在结点上存储的数据的读写，及存储数据；每个Datanode结点会周期性地向Namenode发送心跳信号和文件块状态报告；执行数据的流水线复制。</li><li>Nodenodemanager：运行在单个节点上的代理，它管理Hadoop集群中单个计算节点，功能包括与ResourceManager保持通信，管理Container的生命周期、监控每个Container的资源使用(内存、CPU等）情况、追踪节点健康状况、管理日志和不同应用程序用到的附属服务</li></ul>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>10.Nosql分片复制</title>
    <link href="/2022/01/01/BigData&amp;Linux/10.Nosql%E5%88%86%E7%89%87%E5%A4%8D%E5%88%B6/"/>
    <url>/2022/01/01/BigData&amp;Linux/10.Nosql%E5%88%86%E7%89%87%E5%A4%8D%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="一、系统环境"><a href="#一、系统环境" class="headerlink" title="一、系统环境"></a>一、系统环境</h1><p>Centos MongoDB 关闭防火墙</p><table><thead><tr><th><strong>192.168.174.131</strong></th><th><strong>192.168.174.132</strong></th><th><strong>192.168.174.133</strong></th></tr></thead><tbody><tr><td>mongos</td><td>mongos</td><td>mongos</td></tr><tr><td>config server</td><td>config server</td><td>config server</td></tr><tr><td>shard server1 主节点</td><td>shard server1 副节点</td><td>shard server1 仲裁</td></tr><tr><td>shard server2 仲裁</td><td>shard server2 主节点</td><td>shard server2 副节点</td></tr><tr><td>shard server3 副节点</td><td>shard server3 仲裁</td><td>shard server3 主节点</td></tr></tbody></table><p>端口分配：</p><ul><li><p>mongos：20000</p></li><li><p>config：21000</p></li><li><p>shard1：27001</p></li><li><p>shard2：27002</p></li><li><p>shard3：27003</p></li></ul><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204437568.png" alt="image-20211221204437568"></p><p>三台机器的配置服务(21000)形成复制集，分片1、2、3也在各机器都部署一个实例，它们之间形成复制集，客户端直接连接3个路由服务与之交互，配置服务和分片服务对客户端是透明的。</p><h1 id="二、安装MangoDB"><a href="#二、安装MangoDB" class="headerlink" title="二、安装MangoDB"></a>二、安装MangoDB</h1><h2 id="2-1下载解压MongoDB"><a href="#2-1下载解压MongoDB" class="headerlink" title="2.1下载解压MongoDB"></a>2.1下载解压MongoDB</h2><p>tar -xzvf<br>mongodb-linux-x86_64-rhel70-3.2.22.tgz -C &#x2F;nosql&#x2F;</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204457194.png"></p><p>改名：</p><figure class="highlight plaintext"><figcaption><span>basic</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs visual">cd /nosql/<br><br>mv mongodb-linux-x86_64-rhel70-3.2.22 mongodb<br></code></pre></td></tr></table></figure><h2 id="2-2创建路由、配置、分片等的相关目录与文件"><a href="#2-2创建路由、配置、分片等的相关目录与文件" class="headerlink" title="2.2创建路由、配置、分片等的相关目录与文件"></a>2.2创建路由、配置、分片等的相关目录与文件</h2><p>分别在每台机器建立conf、mongos、config、shard1、shard2、shard3六个目录，因为mongos不存储数据，只需要建立日志文件目录即可。</p><p>启动配置文件存放的文件夹：mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;conf </p><p>路由服务日志存放文件：  mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;mongos&#x2F;log</p><p>配置服务数据存放目录：  mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;config&#x2F;data</p><p>配置服务日志存放文件：  mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;config&#x2F;log</p><p>分片1服务数据存放目录：mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;shard1&#x2F;data</p><p>分片1服务日志存放文件：mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;shard1&#x2F;log</p><p>分片2服务数据存放目录： mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;shard2&#x2F;data</p><p>分片2服务日志存放文件： mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;shard2&#x2F;log</p><p>分片3服务数据存放目录： mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;shard3&#x2F;data</p><p>分片3服务日志存放文件：mkdir -p &#x2F;nosql&#x2F;mongodb&#x2F;shard3&#x2F;log</p><h2 id="2-3配置环境变量"><a href="#2-3配置环境变量" class="headerlink" title="2.3配置环境变量"></a>2.3配置环境变量</h2><p>vi &#x2F;etc&#x2F;profile</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> MONGODB_HOME=/nosql/mongodb<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$MONGODB_HOME</span>/bin:<span class="hljs-variable">$PATH</span><br></code></pre></td></tr></table></figure><p>立即生效：</p><p>source &#x2F;etc&#x2F;profile</p><h1 id="三、config-server配置服务器"><a href="#三、config-server配置服务器" class="headerlink" title="三、config server配置服务器"></a>三、config server配置服务器</h1><h2 id="3-1（三台机器）添加配置文件"><a href="#3-1（三台机器）添加配置文件" class="headerlink" title="3.1（三台机器）添加配置文件"></a>3.1（三台机器）添加配置文件</h2><p>vi &#x2F;nosql&#x2F;mongodb&#x2F;conf&#x2F;config.conf</p><h2 id="配置文件内容"><a href="#配置文件内容" class="headerlink" title="配置文件内容"></a>配置文件内容</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">pidfilepath = /nosql/mongodb/config/log/configsrv.pid<br>dbpath = /nosql/mongodb/config/data<br>logpath = /nosql/mongodb/config/log/congigsrv.log<br>logappend =<span class="hljs-literal">true</span><br><br>bind_ip =0.0.0.0<br>port = 21000<br>fork = <span class="hljs-literal">true</span><br>configsvr =<span class="hljs-literal">true</span><br><br><span class="hljs-comment">#副本集名称</span><br>replSet =configs<br><span class="hljs-comment">#设置最大连接数</span><br>maxConns =20000<br></code></pre></td></tr></table></figure><h2 id="3-2配置副本集"><a href="#3-2配置副本集" class="headerlink" title="3.2配置副本集"></a>3.2配置副本集</h2><p>启动三台服务器的config server</p><p>mongod -f &#x2F;nosql&#x2F;mongodb&#x2F;conf&#x2F;config.conf</p><p>登录任意一台配置服务器，初始化配置副本集</p><p>连接 MongoDB：  mongo –port 21000</p><dl><dt>其中，”_id”</dt><dd>“configs”应与配置文件中配置的 replicaction.replSetName 一致，”members” 中的 “host” 为三个节点的 ip 和 port。响应内容如下：</dd></dl><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204545001.png" alt="image-20211221204545001"></p><h1 id="四、配置分片副本集"><a href="#四、配置分片副本集" class="headerlink" title="四、配置分片副本集"></a>四、配置分片副本集</h1><h2 id="4-1设置第一个分片副本集"><a href="#4-1设置第一个分片副本集" class="headerlink" title="4.1设置第一个分片副本集"></a>4.1设置第一个分片副本集</h2><p>①配置文件</p><p>vi &#x2F;nosql&#x2F;mongodb&#x2F;conf&#x2F;shard1.conf</p><p>#配置文件内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">pidfilepath = /nosql/mongodb/shard1/log/shard1.pid<br>dbpath = /nosql/mongodb/shard1/data<br>logpath = /nosql/mongodb/shard1/log/shard1.<span class="hljs-built_in">log</span><br>logappend =<span class="hljs-literal">true</span><br>bind_ip =0.0.0.0<br>port = 27001<br>fork = <span class="hljs-literal">true</span><br><span class="hljs-comment">#副本集名称</span><br><br>replSet =shard1<br><span class="hljs-comment">#declare this is a shard db of a cluster;</span><br>shardsvr = <span class="hljs-literal">true</span><br><span class="hljs-comment">#设置最大连接数</span><br>maxConns=20000<br></code></pre></td></tr></table></figure><p>②启动三台服务器的shard1 server</p><p>mongod -f &#x2F;nosql&#x2F;mongodb&#x2F;conf&#x2F;shard1.conf</p><p>③登陆任意一台服务器，初始化副本集</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204651678.png" alt="image-20211221204651678"></p><h2 id="4-2设置第二个分片副本集"><a href="#4-2设置第二个分片副本集" class="headerlink" title="4.2设置第二个分片副本集"></a>4.2设置第二个分片副本集</h2><p>过程与设置第一个分片副本集类似，这里直接给出设置结果：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204657752.png" alt="image-20211221204657752"></p><h2 id="4-3设置第三个分片副本集"><a href="#4-3设置第三个分片副本集" class="headerlink" title="4.3设置第三个分片副本集"></a>4.3设置第三个分片副本集</h2><p>过程与设置第一个分片副本集类似，这里直接给出设置结果：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204703056.png" alt="image-20211221204703056"></p><h2 id="4-4配置路由器mongos"><a href="#4-4配置路由器mongos" class="headerlink" title="4.4配置路由器mongos"></a>4.4配置路由器mongos</h2><p>三台机器）先启动配置服务器和分片服务器,后启动路由实例启动路由实例:</p><p>vi &#x2F;nosql&#x2F;mongodb&#x2F;conf&#x2F;mongos.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#内容</span><br>pidfilepath = /nosql/mongodb/mongos/log/mongos.pid<br>logpath = /nosql/mongodb/mongos/log/mongos.log<br>logappend =<span class="hljs-literal">true</span><br><br>bind_ip =0.0.0.0<br>port = 20000<br>fork = <span class="hljs-literal">true</span><br><span class="hljs-comment">#监听的配置服务器,只能有1个或者3个 configs为配置服务器的副本集名字</span><br>configdb =<br>configs/192.168.252.121:21000,192.168.252.122:21000,192.168.252.123:21000<br><span class="hljs-comment">#设置最大连接数</span><br>maxConns =20000<br></code></pre></td></tr></table></figure><p>启动三台服务器的mongos server</p><p>mongos -f &#x2F;nosql&#x2F;mongodb&#x2F;conf&#x2F;mongos.conf</p><h1 id="五、串联路由服务器"><a href="#五、串联路由服务器" class="headerlink" title="五、串联路由服务器"></a>五、串联路由服务器</h1><p>目前搭建了mongodb配置服务器、路由服务器，各个分片服务器，不过应用程序连接到mongos路由服务器并不能使用分片机制，还需要在程序里设置分片配置，让分片生效。</p><h2 id="5-1串联路由服务器与分配副本集"><a href="#5-1串联路由服务器与分配副本集" class="headerlink" title="5.1串联路由服务器与分配副本集"></a>5.1串联路由服务器与分配副本集</h2><p>登陆任意一台mongos ：mongo –port 20000</p><p>使用admin数据库： use admin</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204745082.png" alt="image-20211221204745082"></p><h2 id="5-2查看集群状态"><a href="#5-2查看集群状态" class="headerlink" title="5.2查看集群状态"></a>5.2查看集群状态</h2><p>响应内容如下：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204750630.png" alt="image-20211221204750630"></p><h1 id="六、启用集合分片生效"><a href="#六、启用集合分片生效" class="headerlink" title="六、启用集合分片生效"></a>六、启用集合分片生效</h1><p>目前配置服务、路由服务、分片服务、副本集服务都已经串联起来了，但我们的目的是希望插入数据，数据能够自动分片。连接在mongos上，准备让指定的数据库、指定的集合分片生效。</p><h2 id="6-1指定testdb分片生效"><a href="#6-1指定testdb分片生效" class="headerlink" title="6.1指定testdb分片生效"></a>6.1指定testdb分片生效</h2><p>db.runCommand(<br>{ enablesharding :”testdb”});</p><h2 id="6-2指定数据库里需要分片的集合和片键，哈希id-分片"><a href="#6-2指定数据库里需要分片的集合和片键，哈希id-分片" class="headerlink" title="6.2指定数据库里需要分片的集合和片键，哈希id 分片"></a>6.2指定数据库里需要分片的集合和片键，哈希id 分片</h2><p>db.runCommand(<br>{ shardcollection : “testdb.table1”,key : {“id”:<br>“hashed”} } );</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204816544.png" alt="image-20211221204816544"></p><h2 id="6-3测试分片配置结果"><a href="#6-3测试分片配置结果" class="headerlink" title="6.3测试分片配置结果"></a>6.3测试分片配置结果</h2><p>我们设置testdb的 table1 表需要分片，根据 id 自动分片到 shard1 ，shard2，shard3 上面去。要这样设置是因为不是所有mongodb 的数据库和表都需要分片。</p><p>切换到 testdb 数据库 插入测试数据</p><p>use testdb;</p><p>for(i&#x3D;1;i&lt;&#x3D;100000;i++){db.table1.insert({“id”:i,”name”:”penglei”})};</p><p>查看此时的集群状态，并分组查看总数量：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/image-20211221204825840.png" alt="image-20211221204825840"></p><p>分组查看总数量是：100000</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>MongoDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——11.Spark的搭建</title>
    <link href="/2022/01/01/BigData&amp;Linux/11.Spark/"/>
    <url>/2022/01/01/BigData&amp;Linux/11.Spark/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark的搭建"><a href="#Spark的搭建" class="headerlink" title="Spark的搭建"></a>Spark的搭建</h1><h1 id="安装Scala"><a href="#安装Scala" class="headerlink" title="安装Scala"></a>安装Scala</h1><p>由于Scala只是一个应用软件，只需要安装在master节点即可。</p><h2 id="上传scala安装包"><a href="#上传scala安装包" class="headerlink" title="上传scala安装包"></a>上传scala安装包</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/bigdata/</span>packet/scala-<span class="hljs-number">2.11</span>.<span class="hljs-number">8</span><br></code></pre></td></tr></table></figure><h2 id="解压scala安装包"><a href="#解压scala安装包" class="headerlink" title="解压scala安装包"></a>解压scala安装包</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tar</span> -zxvf scala-<span class="hljs-number">2</span>.<span class="hljs-number">11</span>.<span class="hljs-number">8</span>.tgz -C /bigdata<br></code></pre></td></tr></table></figure><h2 id="环境变量配置（三台机器都做一遍）："><a href="#环境变量配置（三台机器都做一遍）：" class="headerlink" title="环境变量配置（三台机器都做一遍）："></a>环境变量配置（三台机器都做一遍）：</h2><p>vim &#x2F;etc&#x2F;profile</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#scala</span><br><span class="hljs-built_in">export</span> SCALA_HOME= /bigdata/scala-2.11.8<br><span class="hljs-built_in">export</span> PATH =<span class="hljs-variable">$SCALA_HOME</span>/bin:<span class="hljs-variable">$PATH</span><br></code></pre></td></tr></table></figure><p>使环境变量生效：</p><p>source &#x2F;etc&#x2F;profile</p><h2 id="验证Scala是否安装成功"><a href="#验证Scala是否安装成功" class="headerlink" title="验证Scala是否安装成功"></a>验证Scala是否安装成功</h2><p>输入scala命令，如下进入scala环境，则证明scala安装成功：</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image002.png" alt="img"></p><h2 id="分发到从节点"><a href="#分发到从节点" class="headerlink" title="分发到从节点"></a>分发到从节点</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scp</span>  -rp /bigdata/scala-<span class="hljs-number">2</span>.<span class="hljs-number">11</span>.<span class="hljs-number">8</span>  node1: /bigdata<br><span class="hljs-attribute">scp</span>  -rp /bigdata/scala-<span class="hljs-number">2</span>.<span class="hljs-number">11</span>.<span class="hljs-number">8</span>  node2:/ bigdata<br></code></pre></td></tr></table></figure><h1 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h1><h2 id="上传spark安装包"><a href="#上传spark安装包" class="headerlink" title="上传spark安装包"></a>上传spark安装包</h2><p>&#x2F;bigdata&#x2F;packet&#x2F;spark-2.4.8-bin-hadoop2.6.tgz</p><h2 id="解压spark安装包"><a href="#解压spark安装包" class="headerlink" title="解压spark安装包"></a>解压spark安装包</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tar</span> -zxvf  spark-<span class="hljs-number">2</span>.<span class="hljs-number">4</span>.<span class="hljs-number">8</span>-bin-hadoop2.<span class="hljs-number">6</span>.tgz -C /bigdata<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image004.png" alt="img"></p><h2 id="配置spark参数"><a href="#配置spark参数" class="headerlink" title="配置spark参数"></a>配置spark参数</h2><h3 id="配置spark-env-sh文件："><a href="#配置spark-env-sh文件：" class="headerlink" title="配置spark-env.sh文件："></a>配置spark-env.sh文件：</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">cp</span> spark-env.<span class="hljs-keyword">sh</span>.template spark-env.<span class="hljs-keyword">sh</span><br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image006.png" alt="img"></p><p># vim spark-env.sh</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/bigdata/jdk1.8.0<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_HOOME</span>=/bigdata/hadoop-2.6.0<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_CONF_DIR</span>=/bigdata/hadoop-2.6.0/etc/hadoop<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SCALA_HOME</span>=/bigdata/scala-2.11.8<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_HOST</span>=master<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_PORT</span>=7077<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_CORES</span>=1<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_MEMORY</span>=1G<br></code></pre></td></tr></table></figure><h3 id="配置slaves文件："><a href="#配置slaves文件：" class="headerlink" title="配置slaves文件："></a>配置slaves文件：</h3><p># cp slaves.template slaves</p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image010.png" alt="img"></p><p># vim slaves</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gcode"><span class="hljs-symbol">node1</span><br><span class="hljs-symbol">node2</span><br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image012.png" alt="img"></p><h2 id="配置spark环境变量"><a href="#配置spark环境变量" class="headerlink" title="配置spark环境变量"></a>配置spark环境变量</h2><p>vim &#x2F;etc&#x2F;profile</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">SCALA_HOME</span>=/bigdata/scala-2.11.8<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$SCALA_HOME</span>/bin:$PATH<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image014.png" alt="img"></p><p>使环境变量生效：source &#x2F;etc&#x2F;profile</p><h2 id="分发配置文件到子机"><a href="#分发配置文件到子机" class="headerlink" title="分发配置文件到子机"></a>分发配置文件到子机</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">scp -r <span class="hljs-regexp">/bigdata/</span>spark-<span class="hljs-number">2.4</span>.<span class="hljs-number">8</span> node1:/bigdata<br>scp -r <span class="hljs-regexp">/bigdata/</span>spark-<span class="hljs-number">2.4</span>.<span class="hljs-number">8</span> node2:/bigdata<br><br>scp <span class="hljs-regexp">/etc/</span>profile  node1:<span class="hljs-regexp">/etc/</span>profile<br>scp <span class="hljs-regexp">/etc/</span>profile  node2:<span class="hljs-regexp">/etc/</span>profile<br></code></pre></td></tr></table></figure><h2 id="修改执行命令"><a href="#修改执行命令" class="headerlink" title="修改执行命令"></a>修改执行命令</h2><p>因为hadoop和spark的快捷启动sh是一样的，所以这里对两者进行修改，修改为hadoop-start-all.sh和spark-start-all.sh</p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>spark 在启动spark之前一定要检查hadoop是否开启。</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">spark-<span class="hljs-keyword">start</span>-<span class="hljs-keyword">all</span>.sh<br></code></pre></td></tr></table></figure><p>各个节点的进程:<img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image016.png" alt="img"></p><p><img src="https://gitee.com/xiang976young/note/raw/master/img/clip_image018.png" alt="img"></p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——13.Q&amp;A Notice2</title>
    <link href="/2022/01/01/BigData&amp;Linux/13.Question/"/>
    <url>/2022/01/01/BigData&amp;Linux/13.Question/</url>
    
    <content type="html"><![CDATA[<h1 id="Q-amp-A-Notice"><a href="#Q-amp-A-Notice" class="headerlink" title="Q&amp;A Notice"></a>Q&amp;A Notice</h1><h2 id="Mysql安装遇见问题："><a href="#Mysql安装遇见问题：" class="headerlink" title="Mysql安装遇见问题："></a>Mysql安装遇见问题：</h2><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs subunit"><span class="hljs-keyword">error: </span>Failed dependencies:  MySQL conflicts with mysql<span class="hljs-string">-5</span>.1.71<span class="hljs-string">-1</span>.el6.x86_64   <br></code></pre></td></tr></table></figure><p>解决办法</p><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs perl">ps aux | <span class="hljs-keyword">grep</span> mysql   <br> <span class="hljs-keyword">kill</span> -<span class="hljs-number">9</span> 进程号<br></code></pre></td></tr></table></figure><p>mysql的相关命令是</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/etc/i</span>nit.d<span class="hljs-regexp">/mysql status/</span>start<span class="hljs-regexp">/stop/</span>..<br></code></pre></td></tr></table></figure><h2 id="Hbase挂掉子节点："><a href="#Hbase挂掉子节点：" class="headerlink" title="Hbase挂掉子节点："></a>Hbase挂掉子节点：</h2><p>子节点的HRegionServer进程挂掉了</p><p>查看LOG日志</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">zookeeper.RecoverableZooKeep er: ZooKeeper <span class="hljs-built_in">delete</span> failed <span class="hljs-keyword">after</span> <span class="hljs-number">4</span> attempts <br></code></pre></td></tr></table></figure><p>首先关闭hbase、zookeeper。</p><p>查看hbase自带zookeeper是否占用2181端口。 启动zookeeper</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs vim">zkCli.<span class="hljs-keyword">sh</span><br><span class="hljs-keyword">ls</span> /<br>rmr /hbase<br><span class="hljs-keyword">quit</span><br>zkServer.<span class="hljs-keyword">sh</span> restart<br></code></pre></td></tr></table></figure><p>当然也有可能是时间不一致导致的，注意同步各节点的时间 再启动hbase即可。</p><h2 id="解决关闭Hadoop时no-namenode-to-stop异常"><a href="#解决关闭Hadoop时no-namenode-to-stop异常" class="headerlink" title="解决关闭Hadoop时no namenode to stop异常"></a>解决关闭Hadoop时no namenode to stop异常</h2><p>在实行了hadoop-stop-all.sh命令以后，相关hadoop进程没有关掉。</p><p>在$hadoop-2.6.0&#x2F;sbin&#x2F;hadoop-daemon.sh中可以发现：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">if [ -f $pid ]; then<br>     ....#省略n多行<br>    else<br>      echo no $command to stop<br> fi<br></code></pre></td></tr></table></figure><p> 这样就很明显了，如果pid文件不存在就会打印:no xxx to stop</p><p>那么pid是什么文件，为什么会不存在，找到pid变量的声明语句，在脚本文件的第107行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pid=$HADOOP_PID_DIR/hadoop-$HADOOP_IDENT_STRING-$command.pid #第107行<br></code></pre></td></tr></table></figure><p> 接着再找HADOOP_PID_DIR变量的声明部分：</p><p> 首先在脚本注释部分找了很关键的一句话：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#   HADOOP_PID_DIR   The pid files are stored. /tmp by default.</span><br></code></pre></td></tr></table></figure><p> 我们知道了，HADOOP_PID_DIR 变量保存的是pid文件的存储路径。默认存储在&#x2F;tmp目录中，代码如下：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs abnf">if [ <span class="hljs-string">&quot;$HADOOP_PID_DIR&quot;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;&quot;</span> ]<span class="hljs-comment">; then   //97~99行</span><br>  HADOOP_PID_DIR<span class="hljs-operator">=/</span>tmp<br>fi<br></code></pre></td></tr></table></figure><p> 那么这个pid文件是啥呢。Hadoop启动后，会把进程的PID号存储在一个文件中，这样执行stop-dfs脚本时就可以按照进程PID去关闭进程了。</p><p> 现在问题原因很明确了，就是&#x2F;tmp目录下的hadoop-*.pid的文件找不到了。可以翻看对应的文件夹，除了我们需要的，其他啥都有！</p><p> 我们知道&#x2F;tmp是临时目录，系统会定时清理该目录中的文件。显然把pid文件放在这里是不靠谱的，pid文件长时间不被访问，早被清理了!</p><p> 既然Hadoop不知道需要关闭哪些进程了，那我们只能手动关闭了！</p><p> 先用ps -ef查看namenode\datanode等进程的PID，然后用kill -9干掉即可！</p><p>为了避免以上情况重复发生，我们需要修改pid文件的位置，以免pid文件被清理，从而影响到我们脚本的运行。</p><p> 修改pid文件存放目录，只需要在hadoop-daemon.sh脚本中添加一行声明即可：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">HADOOP_PID_DIR=<span class="hljs-regexp">/root/</span>hadoop/pid  <span class="hljs-comment">#第25行</span><br></code></pre></td></tr></table></figure><p> 记住要先关闭Hadoop再修改，不然你修改完又无法关闭了！同样的道理，你还需要修改yarn-daemon.sh</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">YARN_PID_DIR=<span class="hljs-regexp">/root/</span>hadoop/pid<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——3.HDFS shell操作</title>
    <link href="/2022/01/01/BigData&amp;Linux/3.HDFS%20shell/"/>
    <url>/2022/01/01/BigData&amp;Linux/3.HDFS%20shell/</url>
    
    <content type="html"><![CDATA[<h1 id="HDFS-shell操作"><a href="#HDFS-shell操作" class="headerlink" title="HDFS shell操作"></a>HDFS shell操作</h1><p>配置hadoop环境变量 vi &#x2F;etc&#x2F;profile：<br>export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.6.0<br>export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$PATH<br>然后执行 source &#x2F;etc&#x2F;profile</p><p>hdfs ；hdfs中输入的路径需要是绝对路径<br>通过hadoop fs -xx 或者 hdfs dfs -xx<br>查看hdfs目录：如查看根目录：hadoop fs -ls &#x2F;<br>创建目录命令：hadoop fs -mkdir &#x2F;data<br>删除目录或者文件：hadoop fs -rm -r &#x2F;data<br>hdfs 将linux本地文件上传到hdfs 的路径下：如 hadoop fs -put dianxin_data.csv &#x2F;data<br>hdfs 将文件下载到本地。hadoop fs -get &#x2F;data&#x2F;dianxin_data.csv &#x2F;usr&#x2F;local&#x2F;soft&#x2F;<br>hdfs 移动一个文件：hadoop fs -mv<br>hdfs 查看文件内容 ：hadooop fs -cat &#x2F;data&#x2F;dianxin_data.csv</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>HDFS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——2.HADOOP搭建</title>
    <link href="/2022/01/01/BigData&amp;Linux/2.Hadoop%E6%90%AD%E5%BB%BA/"/>
    <url>/2022/01/01/BigData&amp;Linux/2.Hadoop%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="HADOOP搭建"><a href="#HADOOP搭建" class="headerlink" title="HADOOP搭建"></a>HADOOP搭建</h1><h2 id="准备："><a href="#准备：" class="headerlink" title="准备："></a>准备：</h2><p>需要三台虚拟机（修改好IP地址并安装JAVA）</p><p>修改当前网络时间：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">date</span> -s ”<span class="hljs-number">2021</span>-<span class="hljs-number">10</span>-<span class="hljs-number">23</span> <span class="hljs-number">08</span>:<span class="hljs-number">47</span>:<span class="hljs-number">00</span>“<br></code></pre></td></tr></table></figure><p>关闭防火墙并且关闭防火墙自启动</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">service iptables <span class="hljs-built_in">stop</span>/status/<span class="hljs-built_in">start</span>/resrtart<span class="hljs-comment"> //防火墙的相关命令</span><br>chkconfig iptables off<span class="hljs-comment"> //关闭防火墙自启动</span><br></code></pre></td></tr></table></figure><p>设置三台主机ssh免密登陆</p><p>现修改&#x2F;etc&#x2F;hosts的映射 添加三台机子的IP与name</p><p>将映射文件传给其他节点</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">scp <span class="hljs-regexp">/etc/</span>hosts node1:<span class="hljs-regexp">/etc/</span>hosts<br>scp <span class="hljs-regexp">/etc/</span>hosts node2:<span class="hljs-regexp">/etc/</span>hosts<br></code></pre></td></tr></table></figure><p>主节点执行ssh-keygen -t rsa 并一直回车 生成密钥</p><p>将密钥拷贝到其他两个子节点，实现免密码登录到子节点。命令如下：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">ssh-<span class="hljs-keyword">copy</span>-<span class="hljs-built_in">id</span> -i node1 <br>ssh-<span class="hljs-keyword">copy</span>-<span class="hljs-built_in">id</span> -i node2<br></code></pre></td></tr></table></figure><p>实现主节点master本地免密码登录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/.ssh<br><span class="hljs-built_in">cat</span> ./id_rsa.pub &gt;&gt; ./authorized_keys<br></code></pre></td></tr></table></figure><h2 id="安装hadoop："><a href="#安装hadoop：" class="headerlink" title="安装hadoop："></a>安装hadoop：</h2><p>新建文件目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> /bigdata    <br></code></pre></td></tr></table></figure><p>解压hadoop-2.6.0</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tar</span> -zxvf hadoop-<span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span>.tar.gz -C /bigdata<br></code></pre></td></tr></table></figure><p>修改hadoop的配置环境，目录是在&#x2F;bigdata&#x2F;hadoop-2.6.0&#x2F;etc&#x2F;hadoop内</p><ul><li><p>修改hadoop-env.sh  根据个人环境JAVA的安装目录来</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/bigdata/jdk1.8.0<br></code></pre></td></tr></table></figure></li><li><p>修改slaves </p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gcode"><span class="hljs-symbol">node1</span><br><span class="hljs-symbol">node2</span><br></code></pre></td></tr></table></figure></li><li><p>修改core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://master:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/bigdata/hadoop-2.6.0/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.trash.interval<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1440<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><p>修改hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.permissions<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><p>修改yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>604800<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>20480<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>2048<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <br>         <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <br>         <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>2.1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><p>修改mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs xml">cp mapred-site.xml.template mapred-site.xml<br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>   <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master:10020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master:19888<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure></li></ul><p>将修改好的hadoop配置发送到其他子机上：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scp</span> -r /bigdata/hadoop-<span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span>  node1:/bigdata<br><span class="hljs-attribute">scp</span> -r /bigdata/hadoop-<span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span>  node2:/bigdata<br></code></pre></td></tr></table></figure><p>环境配置就直接在&#x2F;etc&#x2F;profile里面写就行了</p><p>然后在主节点终端 先对namenode格式化 然后在启动hadoop即可</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">hdfs namenode -<span class="hljs-keyword">format</span><br><span class="hljs-keyword">start</span>-<span class="hljs-keyword">all</span>.sh<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——5.搭建zookeeper：</title>
    <link href="/2022/01/01/BigData&amp;Linux/5.Zookeeper%E5%AE%89%E8%A3%85/"/>
    <url>/2022/01/01/BigData&amp;Linux/5.Zookeeper%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h1 id="搭建zookeeper："><a href="#搭建zookeeper：" class="headerlink" title="搭建zookeeper："></a>搭建zookeeper：</h1><p> 1、下载zookeeper并解压 </p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tar</span> -zxvf zookeeper-<span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">6</span>.tar.gz -C /bigdata**<br></code></pre></td></tr></table></figure><p><img src="/../src/clip_image002-1640091876388.gif" alt="img"></p><p>2、修改zookeeper的配置文件，并建立数据目录data和日志目录logs</p><p><strong>cd zookeeper-3.4.6</strong></p><p><strong>mkdir data</strong></p><p><strong>mkdir logs</strong></p><p><img src="/../src/clip_image004-1640091876388.gif" alt="img"></p><p><strong>cd conf&#x2F;</strong></p><p><strong>cp zoo_sample.cfg zoo.cfg</strong></p><p><img src="/../src/clip_image006.gif" alt="img"></p><p><strong>vi zoo.cfg</strong></p><p>添加以下内容</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">dataDir</span>=/bigdata/zookeeper-<span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">6</span>/data<br><span class="hljs-attribute">dataLogDir</span>=/bigdata/zookeeper-<span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">6</span>/logs<br><span class="hljs-attribute">server</span>.<span class="hljs-number">1</span>=master:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">2</span>=node1:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">3</span>=node2:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br></code></pre></td></tr></table></figure><p><img src="/../src/clip_image008.gif" alt="img"></p><p><strong>cd data</strong></p><p>新建一个<strong>myid</strong>文件</p><p><strong>vi myid</strong></p><p><strong>1</strong></p><p><img src="/../src/clip_image010.gif" alt="img"></p><p>3、复制master的zookeeper-3.4.6到<strong>node1</strong>和<strong>node2</strong>上</p><p><img src="/../src/clip_image012.gif" alt="img"></p><p><img src="/../src/clip_image014.gif" alt="img"></p><p>4、分别修改<strong>node1</strong>和<strong>node2</strong>上myid的值为2和3</p><p><strong>cd &#x2F;bigdata&#x2F;zookeeper-3.4.6&#x2F;data</strong></p><p><strong>vi myid</strong></p><p><strong>2</strong></p><p><strong>vi myid</strong></p><p><strong>3</strong></p><img src="../src/clip_image016.gif" alt="img" style="zoom:33%;" /><p> 5、分别启动master,<strong>node1</strong>,<strong>node2</strong>上的zookeeper（zookeeper三台都要启动）</p><p><strong>zkServer.sh start</strong> </p><p><img src="/../src/clip_image020.gif" alt="img"></p><p>(可以在环境变量里设置，以便于下次不用再进入bin中操作)</p><p>6、查看zookeeper的状态</p><p><strong>zkServer.sh status</strong></p><p><img src="/../src/clip_image022.gif" alt="img"></p><p>7、验证zookeeper集群，启动客户端</p><p><strong>bin&#x2F;zkCli.sh</strong></p><p><img src="/../src/clip_image028.gif" alt="img"></p><p><img src="/../src/clip_image030.gif" alt="img"></p><p>至此：zookeeper集群安装完毕！</p><p>PS:</p><p>由于zk运行一段时间后，会产生大量的日志文件，把磁盘空间占满，导致整个机器进程都不能活动了，所以需要定期清理这些日志文件，方法如下：</p><ul><li>写一个脚本文件cleanup.sh内容如下：</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">java</span> -cp zookeeper.jar:lib/slf4j-api-<span class="hljs-number">1</span>.<span class="hljs-number">6</span>.<span class="hljs-number">1</span>.jar:lib/slf4j-log4j12-<span class="hljs-number">1</span>.<span class="hljs-number">6</span>.<span class="hljs-number">1</span>.jar:lib/log4j-<span class="hljs-number">1</span>.<span class="hljs-number">2</span>.<span class="hljs-number">15</span>.jar:conf org.apache.zookeeper.server.PurgeTxnLog &lt;dataDir&gt; &lt;snapDir&gt; -n &lt;count&gt;<br></code></pre></td></tr></table></figure><p> 其中：dataDir：即上面配置的dataDir的目录； snapDir：即上面配置的dataLogDir的目录；count：保留前几个日志文件，默认为3</p><p><img src="/../src/clip_image032.gif" alt="img"></p><ul><li>通过crontab写定时任务，来完成定时清理日志的需求</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">crontab</span> -e <span class="hljs-number">0</span> <span class="hljs-number">0</span> * * /opt/zookeeper-<span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">10</span>/bin/cleanup.sh<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Zookeeper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——6.安装hbase</title>
    <link href="/2022/01/01/BigData&amp;Linux/6.Hbase%E6%90%AD%E5%BB%BA/"/>
    <url>/2022/01/01/BigData&amp;Linux/6.Hbase%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="安装hbase"><a href="#安装hbase" class="headerlink" title="安装hbase"></a>安装hbase</h1><p>1．解压缩hbase的软件包，使用命令：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tar</span> -zxvf hbase-<span class="hljs-number">1</span>.<span class="hljs-number">3</span>.<span class="hljs-number">0</span>-bin.tar.gz <br></code></pre></td></tr></table></figure><p>2．进入hbase的配置目录，在hbase-env.sh文件里面加入java环境变量.即：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> JAVA_HOME=/bigdata/jdk1.8.0_121<br><span class="hljs-comment">#关闭HBase自带的Zookeeper,使用Zookeeper集群：</span><br><span class="hljs-built_in">export</span> HBASE_MANAGES_ZK=<span class="hljs-literal">false</span> <br></code></pre></td></tr></table></figure><p>\3. 编辑hbase-site.xml ，添加配置文件：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>　　<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <br>　　　　<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.rootdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <br>　　　　<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://master:9000/hbase<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span> <br>　　<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span> <br>　　<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <br>　　　　<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.cluster.distributed<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <br>　　　　<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span> <br>　　<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span> <br>　　<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <br>　　　　<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <br>　　　　<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master,node1,node2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span> <br>　　<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>\4. 编辑配置目录下面的文件regionservers命令：</p><p>vi  regionservers  </p><p>加入如下内容：</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gcode"><span class="hljs-symbol">node1</span><br><span class="hljs-symbol">node2</span><br></code></pre></td></tr></table></figure><p>\5. 把Hbase复制到其他机器，命令如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">scp -r <span class="hljs-regexp">/bigdata/</span>hbase-<span class="hljs-number">1.0</span>.<span class="hljs-number">1.1</span> node1: /bigdata<br>scp -r <span class="hljs-regexp">/bigdata/</span>hbase-<span class="hljs-number">1.0</span>.<span class="hljs-number">1.1</span> ndoe2: /bigdata<br></code></pre></td></tr></table></figure><p>\6. 在node1机器开启hbase服务。命令如下：</p><p>start-hbase.sh</p><p>（先在环境变量里配置好）</p><p>在master、node1、node2中的任意一台机器使用命令</p><p>hbase shell</p><p>进入hbase自带的shell环境，然后使用命令version等，进行查看hbase信息及建立表等操作。 </p><p><img src="/../src/clip_image002-1640092020441.gif" alt="img"></p><p>也可以打开master:60010在端口打开</p><p><img src="/../src/clip_image004-1640092020442.gif" alt="img"></p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Hbase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——8.Hive安装</title>
    <link href="/2022/01/01/BigData&amp;Linux/8.Hive%E5%AE%89%E8%A3%85/"/>
    <url>/2022/01/01/BigData&amp;Linux/8.Hive%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><h2 id="首先安装MySQL"><a href="#首先安装MySQL" class="headerlink" title="首先安装MySQL"></a>首先安装MySQL</h2><p>1、查看mysql的依赖 </p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic">rpm -qa | grep mysql<br></code></pre></td></tr></table></figure><p>2、删除mysql的依赖</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs basic">rpm -e --nodeps `rpm -qa | grep mysql`<br>rpm -e --nodeps `rpm -qa | grep MySQL`<br></code></pre></td></tr></table></figure><p>3、离线安装mysql </p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs basic">rpm -ivh MySQL-server-<span class="hljs-number">5.1</span>.<span class="hljs-number">73</span>-<span class="hljs-number">1</span>.glibc23.x86_64.rpm   <br>rpm -ivh MySQL-client-<span class="hljs-number">5.1</span>.<span class="hljs-number">73</span>-<span class="hljs-number">1</span>.glibc23.x86_64.rpm<br></code></pre></td></tr></table></figure><p>4、启动mysql服务 （centos6系统）</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic">service mysql start<br></code></pre></td></tr></table></figure><p>5、加入到开机启动项  </p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic">chkconfig mysql <span class="hljs-keyword">on</span><br></code></pre></td></tr></table></figure><p>6、访问mysql服务并修改权限</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic">mysql -uroot -p123456<br></code></pre></td></tr></table></figure><p>切换数据库：use mysql;</p><p>查看用户权限表： select user,host from user</p><p>update user set host &#x3D; ‘%’ where user &#x3D; ‘root’ （提示报错不用管，忽略）</p><p>刷新权限：flush privileges;</p><h2 id="再安装HIVE"><a href="#再安装HIVE" class="headerlink" title="再安装HIVE"></a>再安装HIVE</h2><p>1、解压hive的安装包</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tar</span> -zxvf apache-hive-<span class="hljs-number">1</span>.<span class="hljs-number">2</span>.<span class="hljs-number">1</span>-bin.tar.gz <br></code></pre></td></tr></table></figure><p>修改下目录名称</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">mv</span> apache-hive-<span class="hljs-number">1</span>.<span class="hljs-number">2</span>.<span class="hljs-number">1</span>-bin hive-<span class="hljs-number">1</span>.<span class="hljs-number">2</span>.<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>2、备份配置文件</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">cp hive-env.sh.<span class="hljs-keyword">template</span> hive-env.sh<br>cp hive-<span class="hljs-keyword">default</span>.xml.<span class="hljs-keyword">template</span> hive-site.xml<br></code></pre></td></tr></table></figure><p>3、修改配置文件</p><p>修改hive-env.sh</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">HADOOP_HOME</span>=/bigdata/hadoop-<span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">JAVA_HOME</span>=/bigdata/jdk1.<span class="hljs-number">8</span>.<span class="hljs-number">0</span>_171<br><span class="hljs-attribute">HIVE_HOME</span>=/bigdata/hive-<span class="hljs-number">1</span>.<span class="hljs-number">2</span>.<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p><img src="/../src/clip_image002.gif" alt="img"></p><p>修改hive-site.xml</p><p>&lt;建议使用模式匹配进行查找替换，文件很长</p><p>也可以把xml文件放到windows环境下，利用sublimetext3等工具搜索替换&gt;</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>jdbc:mysql://master:3306/hive?useSSL=false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>root<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>123456<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.querylog.location<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/bigdata/hive-1.2.1/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.exec.local.scratchdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/bigdata/hive-1.2.1/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/bigdata/hive-1.2.1/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure><p>4、替换相关的jar包</p><p>拷贝mysql驱动到&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hive-1.2.1&#x2F;lib目录下</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">cp <span class="hljs-regexp">/bigdata/</span>packet<span class="hljs-regexp">/mysql-connector-java-5.1.17.jar ../</span>lib/<br></code></pre></td></tr></table></figure><p>将hadoop的jline-0.9.94.jar的jar替换成hive的版本。</p><p>hive的 jline-2.12.jar 位置在 &#x2F;bigdata&#x2F;hive-1.2.1&#x2F;lib&#x2F;jline-2.12.jar</p><p>将Hadoop的删除</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">rm</span> -rf  /bigdata/hadoop-<span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span>/share/hadoop/yarn/lib/jline-<span class="hljs-number">0</span>.<span class="hljs-number">9</span>.<span class="hljs-number">94</span>.jar<br></code></pre></td></tr></table></figure><p>然后将hive的jar拷过去hadoop下：</p><p>命令：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">cp  <span class="hljs-regexp">/bigdata/</span>hive-<span class="hljs-number">1.2</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/lib/</span>jline-<span class="hljs-number">2.12</span>.jar <span class="hljs-regexp">/bigdata/</span>hadoop-<span class="hljs-number">2.6</span>.<span class="hljs-number">0</span><span class="hljs-regexp">/share/</span>hadoop<span class="hljs-regexp">/yarn/</span>lib/<br></code></pre></td></tr></table></figure><p>5、启动在主节点&#x2F;bigdata&#x2F;hive-1.2.1&#x2F;bin目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./schematool -dbType mysql -initSchema<br></code></pre></td></tr></table></figure><p> &#x2F;&#x2F;初始化元数据</p><p>6、启动Hive   .&#x2F;hive <img src="/../src/clip_image004.gif" alt="img"></p><p><strong>遇见的问题：</strong></p><p>在安装mysql的时候，可能会有centos自带的mysql会有端口冲突。</p><p>如果遇见error: Failed dependencies: MySQL conflicts with mysql-5.1.71-1.el6.x86_64  使用rpm -e mysql-5.1.73-5.el6_7.1.x86_64 –nodeps删除他</p><p>如果发现MySQL的服务不在状态 就可以ps找一下进程号 kill杀死 重新启动一下就OK了 ps aux | grep mysql  kill -9 进程号</p><p>mysql 的相关命令是&#x2F;etc&#x2F;init.d&#x2F;mysql status&#x2F;stop&#x2F;start&#x2F;…</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——7.Hbase Shell的用法</title>
    <link href="/2022/01/01/BigData&amp;Linux/7.Hbase%20Shell/"/>
    <url>/2022/01/01/BigData&amp;Linux/7.Hbase%20Shell/</url>
    
    <content type="html"><![CDATA[<h1 id="Hbase-Shell的用法"><a href="#Hbase-Shell的用法" class="headerlink" title="Hbase Shell的用法"></a>Hbase Shell的用法</h1><ul><li>whoami    我是谁    whoami</li><li>version    返回hbase版本信息    version</li><li>status    返回hbase集群的状态信息    status</li><li>table_help    查看如何操作表    table_help</li><li>create    创建表    create ‘表名’, ‘列族名1’, ‘列族名2’, ‘列族名N’</li><li>alter    修改列族    添加一个列族：alter ‘表名’, ‘列族名’</li><li>删除列族：alter ‘表名’, {NAME&#x3D;&gt; ‘列族名’, METHOD&#x3D;&gt; ‘delete’}</li><li>describe    显示表相关的详细信息    describe ‘表名’</li><li>list    列出hbase中存在的所有表    list</li><li>exists    测试表是否存在    exists ‘表名’</li><li>put    添加或修改的表的值    put ‘表名’, ‘行键’, ‘列族名’, ‘列值’</li><li>put ‘表名’, ‘行键’, ‘列族名:列名’, ‘列值’</li><li>scan    通过对表的扫描来获取对用的值    scan ‘表名’</li><li>扫描某个列族： scan ‘表名’, {COLUMN&#x3D;&gt;‘列族名’}</li><li>扫描某个列族的某个列： scan ‘表名’, {COLUMN&#x3D;&gt;‘列族名:列名’}</li><li>查询同一个列族的多个列： scan ‘表名’, {COLUMNS &#x3D;&gt; [ ‘列族名1:列名1’, ‘列族名1:列名2’, …]}</li><li>get    获取行或单元（cell）的值    get ‘表名’, ‘行键’</li><li>get ‘表名’, ‘行键’, ‘列族名’</li><li>count    统计表中行的数量    count ‘表名’</li><li>incr    增加指定表行或列的值    incr ‘表名’, ‘行键’, ‘列族:列名’, 步长值</li><li>get_counter    获取计数器    get_counter ‘表名’, ‘行键’, ‘列族:列名’</li><li>delete    删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）    删除列族的某个列： delete ‘表名’, ‘行键’, ‘列族名:列名’</li><li>deleteall    删除指定行的所有元素值    deleteall ‘表名’, ‘行键’</li><li>truncate    重新创建指定表    truncate ‘表名’</li><li>enable    使表有效    enable ‘表名’</li><li>is_enabled    是否启用    is_enabled ‘表名’</li><li>disable    使表无效    disable ‘表名’</li><li>is_disabled    是否无效    is_disabled ‘表名’</li><li>drop    删除表    drop的表必须是disable的</li><li>disable ‘表名’</li><li>drop ‘表名’</li><li>shutdown    关闭hbase集群（与exit不同）    </li><li>tools    列出hbase所支持的工具    </li><li>exit    退出hbase shell</li></ul>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Hbase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BigData——9.Hive笔记</title>
    <link href="/2022/01/01/BigData&amp;Linux/9.HIVE%20%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/01/01/BigData&amp;Linux/9.HIVE%20%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="Hive笔记"><a href="#Hive笔记" class="headerlink" title="Hive笔记"></a>Hive笔记</h1><h2 id="hive的存储格式："><a href="#hive的存储格式：" class="headerlink" title="hive的存储格式："></a>hive的存储格式：</h2><p>1 textfile 没有压缩，但是比较占用存储空间。读取效率最高<br>2 rcfile 有压缩，空间占用小。 比较节省空间。<br>3 SequenceFile 适合小文件存储。<br>hive和hdfs的关系：<br>hive的存储是基于hdfs的，在hive中创建一个库，实际上就是在hdfs上的当前用户<br>路径下创建了一个目录；<br>比如 create database sz；<br>hdfs:&#x2F;&#x2F;master:9000&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;sz.db<br>hive中的库和hdfs的映射关系，是存放在mysql中的；</p><h2 id="hive的操作"><a href="#hive的操作" class="headerlink" title="hive的操作"></a>hive的操作</h2><p>hive创建库：create database 库名<br>hive 查看数据名：show databases;<br>hive切换数据库：use sz；<br>hive建表：<br>create [EXTERNAL] table vv_stat_fact &#x2F;&#x2F;EXTERNAL关键字，标识是外部表的<br>(<br>userid  string, &#x2F;&#x2F;表中的字段和类型<br>stat_date string,<br>tryvv int,<br>sucvv int,<br>ptime float<br>)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ &#x2F;&#x2F;指定列的分隔符<br>STORED AS textfile &#x2F;&#x2F;指定压缩格式，如果需要直接将数据文本加载到表中，压缩格式需要时textfile<br>location ‘&#x2F;testdata&#x2F;‘; &#x2F;&#x2F;可以自定义数据存储的hdfs的位置目录<br>例子：<br>create table wc<br>(<br>userid  string<br>)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\n’<br>STORED AS textfile<br>hive中表就是hdfs上的一个目录。<br>hive建表，默认的存储路径在&#x2F;user&#x2F;hive&#x2F;warehouse<br>创建hive的外部表：<br>create EXTERNAL table wc_ext<br>(<br>userid  string<br>)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\n’<br>STORED AS textfile<br>hive 删除表：drop table wc_ext;<br>如果是普通表，会删除hdfs上的对应的目录，同时会删除数据文件。<br>如果是外部表的话，只会删除mysql中元信息，并不会删除hdfs上对应的目录和数据</p><p>练习：创建员工表总表<br>EMPNO\ENAME\JOB\MGR\HIREDATE \SAL\COMM\DEPTNO<br>字段是：工号，姓名，工作岗位，部门经理，受雇日期，薪金，奖金，部门编号<br>create table emp<br>(<br>EMPNO  string,<br>ENAME  string,<br>JOB string,<br>MGR string,<br>HIREDATE string,<br>SAL int,<br>COMM int,<br>DEPTNO string<br>)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’<br>STORED AS textfile<br>将dept.txt数据插入表字段（DEPTNO、DNAME、LOC）<br>create table deptl<br>(<br>DEPTNO  string,<br>DNAME  string,<br>LOC string<br>)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’<br>STORED AS textfile<br>location ‘&#x2F;testdata&#x2F;‘;</p><p>hive的分区表：<br>优点：避免全表扫描，提高查询效率。<br>创建分区表;<br>create table dept_p<br>(<br>DEPTNO  string,<br>DNAME  string,<br>LOC string<br>) PARTITIONED BY (dt string)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’<br>STORED AS textfile;<br>手动增加分区：<br>alter table dept_p add partition (dt&#x3D;’20190715’);<br>查看表的属性：desc dept_p</p><p>hive 加载数据：<br>load命令</p><p>load data [local] inpath ‘数据的路径’ into table 表名称；</p><p>给一个表增加&#x2F;删除分区命令：目标表必须是分区表，建表的时候就要指定<br>   alter table test_table add partition (pt&#x3D;xxxx)<br>    alter table test_table drop if exists partition(…);<br>    例子：<br>    增加分区<br>    alter table dept_p add partition(dt&#x3D;’20180707’);<br>    删除一个分区：<br>    alter table dept_p drop partition(dt&#x3D;’20180707’);<br>如何规划建分区表还是普通表：<br>如果数据量每天或者有规律的增加，可以预知数据量会非常大。可以采用分区表<br>如果数据量比较小或者是静态数据。可以创建普通表。</p><h2 id="hive语法："><a href="#hive语法：" class="headerlink" title="hive语法："></a><strong>hive语法：</strong></h2><p>where用法：用于过滤数据，尤其是分区表的数据，添加过滤条件<br>select * from emp where sal &gt; 1000 and comm &gt; 1000;<br>分区表同样把分区字段当做where条件过滤字段。<br>不支持子查询条件:如<br>select * from emp where sal &gt; (select max(sal) as s from emp ) and comm &gt; 1000;</p><p>join的用法：<br>left join 用法：会按照左边的表进行输出，右边无论有没有关联上，左边都会全部输出<br>右边没有关联上的，会以null补充<br>zs 10  10 shanghai<br>lisi  20  null<br>假设想要查出员工表emp中部门所在地是纽约的员工的姓名和薪水。<br>select e.ename,e.sal from emp e left join dept d on e.deptno&#x3D;d.deptno where d.loc&#x3D;’NEW YORK’;<br>第二种写法：<br>select e.ename,e.sal,d.loc from emp e left join (select deptno,loc from dept  where loc&#x3D;’NEW YORK’) d on e.deptno&#x3D;d.deptno where  d.loc is not null </p><p>group by 分组聚合：<br>例子：求每个部门中的平均薪水是多少：<br>select deptno,round(avg(sal),1) as a_s from emp group by deptno ;<br>在hive中 参与计算的维度，必须要出现在 group by 后面<br>distinct 去重：</p><h2 id="hive-常用的函数："><a href="#hive-常用的函数：" class="headerlink" title="hive 常用的函数："></a>hive 常用的函数：</h2><p>if(条件表达式,条件为真返回值,否则返回)；<br>例子，<br>如果emp中小于1000员工，name每个员工工资+500<br>select s.ename,s.sal+500 from (select * from emp where sal &lt; 1000) s;<br>用if函数写<br>select ename,if(sal&lt;1000,sal+500,sal) as sal from emp;<br>也可以用case when<br>select ename,case when sal&lt;1000 then sal+500 else sal end as sal from emp;<br>null 判断：<br>select ename,comm from emp where comm is not null;<br>窗口函数之row_number() over()：<br>例子：想要查询每个部门中前2名工资高的员工。<br>select deptno,ename,sal  from<br>(select deptno,ename,sal,row_number() over(partition by deptno order by sal desc ) as rn from emp) e<br>where e.rn&lt;&#x3D;2 ;</p><p>高级函数：split() 可以对目标字段进行切分，返回数组类型<br>explode 可以将行转列；<br>wordcount的例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> w.word,<span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) cn <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> explode(split(userid,<span class="hljs-string">&#x27;,&#x27;</span>)) <span class="hljs-keyword">as</span> w<br>ord <span class="hljs-keyword">from</span> wc) w <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> w.word;<br></code></pre></td></tr></table></figure><p>hive笔记3<br>日期函数：<br>datediff<br>例子： select datediff(‘2019-07-16’,’2019-07-13’)<br>date_add(string startdate, int days)<br>例子：date_add(‘2019-07-16’,50)<br>from_unixtime 将时间戳转换成 日期函数：<br>select from_unixtime(1563261771)</p><p>字符串函数：concat<br>select concat(‘zhangsan’,’lisi’)</p><p>函数练习的例子：数据<br>2018&#x2F;6&#x2F;1,10<br>2018&#x2F;6&#x2F;2,11<br>2018&#x2F;6&#x2F;3,11<br>2018&#x2F;6&#x2F;4,12<br>2018&#x2F;6&#x2F;5,14<br>2018&#x2F;6&#x2F;6,15<br>2018&#x2F;6&#x2F;7,13<br>2018&#x2F;6&#x2F;8,37<br>2018&#x2F;6&#x2F;9,18<br>2018&#x2F;6&#x2F;10,19<br>2018&#x2F;6&#x2F;11,10<br>2018&#x2F;6&#x2F;12,11<br>2018&#x2F;6&#x2F;13,11<br>2018&#x2F;6&#x2F;14,12<br>2018&#x2F;6&#x2F;15,14<br>2018&#x2F;6&#x2F;16,15</p><p>需求：想要每隔7天的平均消费金额。<br>2018-6-1<del>2018-6-7 20<br>2018-6-8</del>2018-6-14 15</p><p>第一步：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>)<br></code></pre></td></tr></table></figure><p>第二步：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> concat_ws(<span class="hljs-string">&#x27;-&#x27;</span>,split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>))<br></code></pre></td></tr></table></figure><p>第三步：对日期求商 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-built_in">floor</span>(datediff(concat_ws(<span class="hljs-string">&#x27;-&#x27;</span>,split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>)),<span class="hljs-string">&#x27;2018-6-1&#x27;</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>)<span class="hljs-operator">*</span><span class="hljs-number">7</span><br></code></pre></td></tr></table></figure><p>第四步：对日期进行加</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> date_add(<span class="hljs-string">&#x27;2018-6-1&#x27;</span>,<span class="hljs-built_in">cast</span>(<span class="hljs-built_in">floor</span>(datediff(concat_ws(<span class="hljs-string">&#x27;-&#x27;</span>,split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>)),<span class="hljs-string">&#x27;2018-6-1&#x27;</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>)<span class="hljs-operator">*</span><span class="hljs-number">7</span> <span class="hljs-keyword">as</span> <span class="hljs-type">int</span>))<br><span class="hljs-keyword">select</span> date_add(<span class="hljs-string">&#x27;2018-6-1&#x27;</span>,<span class="hljs-built_in">cast</span>(<span class="hljs-built_in">floor</span>(datediff(concat_ws(<span class="hljs-string">&#x27;-&#x27;</span>,split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>)),<span class="hljs-string">&#x27;2018-6-1&#x27;</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>)<span class="hljs-operator">*</span><span class="hljs-number">7</span><span class="hljs-operator">+</span><span class="hljs-number">6</span> <span class="hljs-keyword">as</span> <span class="hljs-type">int</span>))<br></code></pre></td></tr></table></figure><p>第五步拼接日期：(hive中group by 后面是不允许跟别名，需要跟实际参与分组的字段)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> concat(date_add(<span class="hljs-string">&#x27;2018-6-1&#x27;</span>,<span class="hljs-built_in">cast</span>(<span class="hljs-built_in">floor</span>(datediff(concat_ws(<span class="hljs-string">&#x27;-&#x27;</span>,split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>)),<span class="hljs-string">&#x27;2018-6-1&#x27;</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>)<span class="hljs-operator">*</span><span class="hljs-number">7</span> <span class="hljs-keyword">as</span> <span class="hljs-type">int</span>)),<span class="hljs-string">&#x27;~&#x27;</span>,date_add(<span class="hljs-string">&#x27;2018-6-1&#x27;</span>,<span class="hljs-built_in">cast</span>(<span class="hljs-built_in">floor</span>(datediff(concat_ws(<span class="hljs-string">&#x27;-&#x27;</span>,split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>)),<span class="hljs-string">&#x27;2018-6-1&#x27;</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>)<span class="hljs-operator">*</span><span class="hljs-number">7</span><span class="hljs-operator">+</span><span class="hljs-number">6</span> <span class="hljs-keyword">as</span> <span class="hljs-type">int</span>))) <span class="hljs-keyword">as</span> dt,round(<span class="hljs-built_in">avg</span>(cost),<span class="hljs-number">1</span>) <br><span class="hljs-keyword">from</span> dp <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> concat(date_add(<span class="hljs-string">&#x27;2018-6-1&#x27;</span>,<span class="hljs-built_in">cast</span>(<span class="hljs-built_in">floor</span>(datediff(concat_ws(<span class="hljs-string">&#x27;-&#x27;</span>,split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>)),<span class="hljs-string">&#x27;2018-6-1&#x27;</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>)<span class="hljs-operator">*</span><span class="hljs-number">7</span> <span class="hljs-keyword">as</span> <span class="hljs-type">int</span>)),<span class="hljs-string">&#x27;~&#x27;</span>,date_add(<span class="hljs-string">&#x27;2018-6-1&#x27;</span>,<span class="hljs-built_in">cast</span>(<span class="hljs-built_in">floor</span>(datediff(concat_ws(<span class="hljs-string">&#x27;-&#x27;</span>,split(date_time,<span class="hljs-string">&#x27;/&#x27;</span>)),<span class="hljs-string">&#x27;2018-6-1&#x27;</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>)<span class="hljs-operator">*</span><span class="hljs-number">7</span><span class="hljs-operator">+</span><span class="hljs-number">6</span> <span class="hljs-keyword">as</span> <span class="hljs-type">int</span>))) <br></code></pre></td></tr></table></figure><p>结果</p><p>2018-06-01<del>2018-06-07    12.3<br>2018-06-08</del>2018-06-14    16.9<br>2018-06-15<del>2018-06-21    16.6<br>2018-06-22</del>2018-06-28    23.3<br>2018-06-29<del>2018-07-05    37.9<br>2018-07-06</del>2018-07-12    47.6<br>2018-07-13<del>2018-07-19    54.3<br>2018-07-20</del>2018-07-26    36.0<br>2018-07-27<del>2018-08-02    51.3<br>2018-08-03</del>2018-08-09    75.4<br>2018-08-10~2018-08-16    80.7</p><p>create table if not exists  movie(<br>  stat_date string,<br>  userid string,<br>  uid string,<br>  version string,<br>  country string,<br>  province string,<br>  movie_tryvv int,<br>  movie_sucvv int,<br>  movie_ptime int<br>)<br> PARTITIONED BY (<br>  dt string)<br>clustered by (userid) into 3000 buckets<br>ROW FORMAT DELIMITED<br>  FIELDS TERMINATED BY ‘,’<br>STORED AS textfile;</p><p>清空表数据：truncate table xxx<br>加载分区表数据<br>load data local inpath ‘&#x2F;usr&#x2F;local&#x2F;soft&#x2F;datadir&#x2F;movedata.csv’ into tab<br>le movie_vv partition(dt&#x3D;’20190717’);<br>简单指标：<br>pv:页面访问次数<br>uv：用户独立访问次数<br>select count(*) as pv, count(distinct userid) as uv from movie_vv;</p><p>练习：一句sql算出，可以用到if函数<br>0-5分钟，观影人数。5-20分钟观影人数，50-100</p><p>hive的udf：<br>分三类udf<br>：udf 输入一行数据返回一行数据，一对一的关系<br>：UDAF输入多行数据，返回一行数据，通常是聚合函数<br>：UDTF 输入一行数据，返回多行数据，通常是行转列的操作，或者切割的操作。</p><p>第一步：maven添加依赖：<br>      <dependency><br>            <groupId>org.apache.hive</groupId><br>            <artifactId>hive-exec</artifactId><br>            <version>1.2.1</version><br>        </dependency><br>第二步：编写udf的程序：<br>import org.apache.hadoop.hive.ql.exec.UDF;</p><p>public class hive_udf_demo extends UDF{<br>&#x2F;&#x2F;方法名称必须是：evaluate<br>&#x2F;&#x2F;这里的参数 s 就是sql中函数需要传入的那个字段，<br>    public String evaluate(String s){<br>        &#x2F;&#x2F;在函数内实现自己的业务逻辑。<br>        if (s&#x3D;&#x3D;”JONES”) {<br>            s&#x3D;”shangdan”;<br>        }<br>        return s.toLowerCase();<br>    }</p><p>}<br>第三步：将jar包上传到hive中<br>第四步：hive加载jar包<br>add jar &#x2F;usr&#x2F;local&#x2F;soft&#x2F;datadir&#x2F;hive_udf_low.jar;<br>第五步：创建hive函数<br>CREATE TEMPORARY FUNCTION my_lower as ‘hadoop04.shujia04.hive_udf_demo<br>‘;</p><p>hive的shell 操作：<br>可以创建一个shell脚本，脚本中通过hive -e的方式写sql<br>#!&#x2F;bin&#x2F;bash<br>source &#x2F;etc&#x2F;profile<br>hive -e ‘select * from sz.dp’<br>启动命令：比如shell 文件名称是 hivesql.sh<br>sh hivesql.sh<br>也可以通过hql文件写sql，然后执行文件<br>hive -f test.hql</p><p>将命令加入到定时服务器<br><a href="https://baijiahao.baidu.com/s?id=1609952845993989858&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1609952845993989858&amp;wfr=spider&amp;for=pc</a><br>打开定时任务清单：crontab -e</p><h2 id="exercise"><a href="#exercise" class="headerlink" title="exercise"></a>exercise</h2><p>一 将empdata.txt数据插入hive表。<br>EMPNO\ENAME\JOB\MGR\HIREDATE \SAL\COMM\DEPTNO<br>字段中文名字依次是：工号，姓名，工作岗位，部门经理，受雇日期，薪金，奖金，部门编号<br>将dept.txt数据插入表字段（DEPTNO、DNAME、LOC）<br>10,ACCOUNTING,NEW YORK<br>10,ACCOUNTING,shanghai<br>20,RESEARCH,DALLAS<br>30,SALES,CHICAGO<br>40,OPERATIONS,BOSTON</p><p>然后查询（sql和运行结果截图）：<br>1． 列出至少有4个员工的所有部门编号和名称。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> ss.DEPTNO,d.DNAME <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> DEPTNO <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> DEPTNO,<span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">as</span> cn <span class="hljs-keyword">from</span> emp <span class="hljs-keyword">group</span> <br><span class="hljs-keyword">by</span> DEPTNO)s  <span class="hljs-keyword">where</span> s.cn<span class="hljs-operator">&gt;=</span><span class="hljs-number">4</span>)ss <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> dept d <span class="hljs-keyword">on</span> ss.DEPTNO<span class="hljs-operator">=</span>d.DEPTNO; <br><br><span class="hljs-keyword">select</span> ss.DEPTNO,d.DNAME <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> DEPTNO,<span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">as</span> cn <span class="hljs-keyword">from</span> emp <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> DEPTNO <span class="hljs-keyword">having</span> <span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <span class="hljs-operator">&gt;</span> <span class="hljs-number">4</span>)ss <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> dept d <span class="hljs-keyword">on</span> ss.DEPTNO<span class="hljs-operator">=</span>d.DEPTNO; <br></code></pre></td></tr></table></figure><p>2． 列出薪金比“SCOTT”多的所有员工。<br>     第一种：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> ENAME,SAL <span class="hljs-keyword">from</span> emp <span class="hljs-keyword">where</span> sal<span class="hljs-operator">&gt;</span>(<span class="hljs-keyword">select</span> sal <span class="hljs-keyword">from</span> emp <span class="hljs-keyword">where</span> ENAME<span class="hljs-operator">=</span><span class="hljs-string">&#x27;SCOTT&#x27;</span>);<br></code></pre></td></tr></table></figure><p>​     第二种: </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> e.ENAME, e.SAL <span class="hljs-keyword">from</span> (<br> <span class="hljs-keyword">select</span> ENAME,SAL,<span class="hljs-number">1</span> <span class="hljs-keyword">as</span> cid  <span class="hljs-keyword">from</span> emp)e <br> <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> (<span class="hljs-keyword">select</span> SAL,<span class="hljs-number">1</span> <span class="hljs-keyword">as</span> cid  <span class="hljs-keyword">from</span> emp <span class="hljs-keyword">where</span> ENAME<span class="hljs-operator">=</span><span class="hljs-string">&#x27;SCOTT&#x27;</span> )s <br> <span class="hljs-keyword">on</span> e.cid<span class="hljs-operator">=</span>s.cid <span class="hljs-keyword">where</span> e.SAL<span class="hljs-operator">&gt;</span>s.SAL;<br></code></pre></td></tr></table></figure><p>3． 列出所有员工的姓名及其直接上级的姓名。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> e.ENAME,ee.ENAME <span class="hljs-keyword">from</span> hemp e <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> (<span class="hljs-keyword">select</span> empno,ENAME,MGR <span class="hljs-keyword">from</span> hemp ) ee <span class="hljs-keyword">on</span> e.MGR<span class="hljs-operator">=</span>ee.empno;<br></code></pre></td></tr></table></figure><p>4． 列出受雇日期早于其直接上级的所有员工。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> e.ENAME <span class="hljs-keyword">from</span> hemp e <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> (<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> hemp ) ee <span class="hljs-keyword">on</span> e.MGR<span class="hljs-operator">=</span>ee.empno <span class="hljs-keyword">where</span> e.hiredate<span class="hljs-operator">&lt;</span>ee.hiredate;<br></code></pre></td></tr></table></figure><p>5． 列出部门名称和这些部门的员工信息，同时列出那些没有员工的部门。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span>  <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> dept d <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> emp  e <span class="hljs-keyword">on</span> d.DEPTNO<span class="hljs-operator">=</span>e.DEPTNO;<br></code></pre></td></tr></table></figure><p>6． 列出所有“CLERK”（办事员）的姓名及其部门名称。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> j.ENAME,d.DNAME <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">where</span> JOB<span class="hljs-operator">=</span> &quot;CLERK&quot; ) j <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> hdept d <span class="hljs-keyword">on</span> j.deptno<span class="hljs-operator">=</span>d.deptno;<br></code></pre></td></tr></table></figure><p>7． 列出最低薪金大于1500的各种工作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span>  <span class="hljs-keyword">distinct</span> JOB <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">where</span> SAL <span class="hljs-operator">&gt;</span> <span class="hljs-number">1500</span>;<br></code></pre></td></tr></table></figure><p>8． 列出在部门“SALES”（销售部）工作的员工的姓名，假定不知道销售部的部门编号</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> e.ENAME <span class="hljs-keyword">from</span> hemp e <span class="hljs-keyword">join</span>  hdept d <span class="hljs-keyword">on</span> d.deptno<span class="hljs-operator">=</span>e.deptno <span class="hljs-keyword">where</span> d.DNAME<span class="hljs-operator">=</span>&quot;SALES&quot;;<br></code></pre></td></tr></table></figure><p>9． 列出薪金高于公司平均薪金的所有员工。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> e.ENAME <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span>  <span class="hljs-operator">*</span>,<span class="hljs-number">1</span> <span class="hljs-keyword">as</span> cid <span class="hljs-keyword">from</span> emp) e<br> <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> (<span class="hljs-keyword">select</span> <span class="hljs-built_in">avg</span>(SAL) cn,<span class="hljs-number">1</span> <span class="hljs-keyword">as</span> cid <span class="hljs-keyword">from</span> emp) s <br> <span class="hljs-keyword">on</span> e.cid<span class="hljs-operator">=</span>s.cid <br> <span class="hljs-keyword">where</span> e.sal<span class="hljs-operator">&gt;</span>s.cn;<br></code></pre></td></tr></table></figure><p>10．列出与“SCOTT”从事相同工作的所有员工。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> p.ENAME <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">where</span>  ENAME<span class="hljs-operator">=</span><span class="hljs-string">&#x27;SCOTT&#x27;</span> ) e <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> (<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">where</span>  ENAME<span class="hljs-operator">!=</span><span class="hljs-string">&#x27;SCOTT&#x27;</span> ) p <span class="hljs-keyword">on</span> e.JOB<span class="hljs-operator">=</span>p.JOB;<br></code></pre></td></tr></table></figure><p>11．列出薪金等于部门30中员工的薪金的所有员工的姓名和薪金。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> ep.ENAME,ep.SAL <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> sal <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">where</span> DEPTNO<span class="hljs-operator">=</span><span class="hljs-string">&#x27;30&#x27;</span>) e <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> hemp ep <span class="hljs-keyword">on</span> e.SAL<span class="hljs-operator">=</span>ep.SAL;<br></code></pre></td></tr></table></figure><p>12．列出薪金高于在部门30工作的所有员工的薪金的员工姓名和薪金。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> e.ENAME,e.SAL <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span>  <span class="hljs-operator">*</span>,<span class="hljs-number">1</span> <span class="hljs-keyword">as</span> cid <span class="hljs-keyword">from</span> hemp) e <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> (<span class="hljs-keyword">select</span> <span class="hljs-built_in">max</span>(SAL) cn,<span class="hljs-number">1</span> <span class="hljs-keyword">as</span> cid <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">where</span>  DEPTNO<span class="hljs-operator">=</span><span class="hljs-string">&#x27;30&#x27;</span>) s <span class="hljs-keyword">on</span> e.cid<span class="hljs-operator">=</span>s.cid <span class="hljs-keyword">where</span> e.sal<span class="hljs-operator">&gt;</span>s.cn;<br></code></pre></td></tr></table></figure><p>13．列出在每个部门工作的员工数量、平均工资和平均服务期限。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span>  DEPTNO,<span class="hljs-built_in">count</span>(ENAME) <span class="hljs-keyword">as</span> name ,<span class="hljs-built_in">avg</span>(sal) <span class="hljs-keyword">as</span> avg_sal,round(<span class="hljs-built_in">avg</span>(datediff(<span class="hljs-built_in">current_timestamp</span>,hiredate)),<span class="hljs-number">1</span>) <span class="hljs-keyword">as</span> avg_day <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> DEPTNO;<br></code></pre></td></tr></table></figure><p>14．列出所有员工的姓名、部门名称和工资。 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> e.ENAME,d.DNAME,e.sal <span class="hljs-keyword">from</span> hemp e <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> hdept d <span class="hljs-keyword">on</span> e.DEPTNO<span class="hljs-operator">=</span>d.DEPTNO;<br></code></pre></td></tr></table></figure><p>15．列出所有部门的详细信息和部门人数。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> hdept d <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> (<span class="hljs-keyword">select</span> DEPTNO,<span class="hljs-built_in">count</span>(ENAME) <span class="hljs-keyword">as</span> cn <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> DEPTNO )  e <span class="hljs-keyword">on</span> e.DEPTNO<span class="hljs-operator">=</span>d.DEPTNO;<br></code></pre></td></tr></table></figure><p>16．列出各种工作的最低工资。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> job,<span class="hljs-built_in">min</span>(sal)  <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> job;<br></code></pre></td></tr></table></figure><p>17．列出各个部门的MANAGER（经理）的最低薪金。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> DEPTNO,<span class="hljs-built_in">min</span>(SAL)  <span class="hljs-keyword">FROM</span> hemp <span class="hljs-keyword">where</span> job <span class="hljs-operator">=</span><span class="hljs-string">&#x27;MANAGER&#x27;</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> DEPTNO;<br></code></pre></td></tr></table></figure><p>18．列出所有员工的年工资,列出年薪最高的前3个员工。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> ENAME,sal<span class="hljs-operator">*</span><span class="hljs-number">12</span><span class="hljs-operator">+</span>COMM <span class="hljs-keyword">as</span> all_m <span class="hljs-keyword">from</span> hemp <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> all_m <span class="hljs-keyword">desc</span> limit <span class="hljs-number">3</span>;<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> ENAME,sal<span class="hljs-operator">*</span><span class="hljs-number">12</span><span class="hljs-operator">+</span>COMM <span class="hljs-keyword">as</span> all_m <span class="hljs-keyword">from</span> hemp) e <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> e.all_m <span class="hljs-keyword">desc</span> limit <span class="hljs-number">3</span>;<br></code></pre></td></tr></table></figure><p>思考： 列出每个部门薪水前两名最高的人员名称以及薪水</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> ename,sal,deptno <span class="hljs-keyword">from</span> <br>(<span class="hljs-keyword">select</span> ename,sal,deptno,<span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> deptno <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> sal <span class="hljs-keyword">desc</span>) <span class="hljs-keyword">as</span> num <span class="hljs-keyword">from</span> hemp ) <br><span class="hljs-keyword">where</span> num <span class="hljs-operator">&lt;=</span><span class="hljs-number">2</span>;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
